<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Systematically Exploring Redundancy Reduction in Summarizing Long Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Xiao</surname></persName>
							<email>xiaowen3@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
							<email>carenini@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Systematically Exploring Redundancy Reduction in Summarizing Long Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our analysis of large summarization datasets indicates that redundancy is a very serious problem when summarizing long documents. Yet, redundancy reduction has not been thoroughly investigated in neural summarization. In this work, we systematically explore and compare different ways to deal with redundancy when summarizing long documents. Specifically, we organize the existing methods into categories based on when and how the redundancy is considered. Then, in the context of these categories, we propose three additional methods balancing non-redundancy and importance in a general and flexible way. In a series of experiments, we show that our proposed methods achieve the state-of-the-art with respect to ROUGE scores on two scientific paper datasets, Pubmed and arXiv, while reducing redundancy significantly. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Summarization is the task of shortening a given document(s) while maintaining the most important information. In general, a good summarizer should generate a summary that is syntactically accurate, semantically correct, coherent, and non-redundant <ref type="bibr" target="#b21">(Saggion and Poibeau, 2013)</ref>. While extractive methods tend to have better performance on the first two aspects, they are typically less coherent and more redundant than abstractive ones, where new sentences are often generated by sentence fusion and compression, which helps detecting and removing redundancy <ref type="bibr" target="#b7">(Lebanoff et al., 2019)</ref>. Although eliminating redundancy has been initially and more intensely studied in the field of multidocument summarization <ref type="bibr" target="#b12">(Lloret and Sanz, 2013)</ref>, because important sentences selected from multiple documents (about the same topic) are more likely to be redundant than sentences from the same document, generating a non-redundant summary should still be one of the goals for single document summarization <ref type="bibr" target="#b10">(Lin et al., 2009)</ref>.</p><p>Generally speaking, there is a trade-off between importance and diversity (non-redundancy) <ref type="bibr" target="#b5">(Jung et al., 2019)</ref>, which is reflected in the two phases, sentence scoring and sentence selection <ref type="bibr" target="#b25">(Zhou et al., 2018)</ref> in which extractive summarization task can be naturally decomposed. The former typically scores sentences based on importance, while the latter selects sentences based on their scores, but also possibly taking other factors (including redundancy) into account.</p><p>Traditionally, in non-neural approaches the tradeoff between importance and redundancy has been carefully considered, with sentence selection picking sentences by optimizing an objective function that balances the two aspects <ref type="bibr" target="#b1">(Carbonell and Goldstein, 1998;</ref><ref type="bibr" target="#b20">Ren et al., 2016)</ref>. In contrast, more recent works on neural extractive summarization models has so far over-emphasized sentence importance and the corresponding scoring phase, while paying little attention to how to reduce redundancy in the selection phase, where they simply apply a greedy algorithm to select sentences (e.g., <ref type="bibr" target="#b2">Cheng and Lapata (2016)</ref>; <ref type="bibr" target="#b24">Xiao and Carenini (2019)</ref>). Notice that this is especially problematic for long documents, where redundancy tends to be a more serious problem, as we have observed in key datasets. Improving redundancy reduction in neural extractive summarization for long documents is a major goal of this paper.</p><p>Indeed, some recently proposed neural methods aim to reduce redundancy, but they either do that implicitly or inflexibly and only focusing on short documents (e.g., news). For instance, some models learn to reduce redundancy when predicting the scores <ref type="bibr" target="#b13">(Nallapati et al., 2016a)</ref>, or jointly learn to score and select sentences <ref type="bibr" target="#b25">(Zhou et al., 2018)</ref> in an implicit way. However, whether these strategies actually help reducing redundancy is still an open empirical question. The only neural attempt of explicitly reduce redundancy in the sentence selection phase is the Trigram Blocking technique, used in recent extractive summarization models on news datasets (e.g., <ref type="bibr" target="#b11">(Liu and Lapata, 2019)</ref>). However, the effectiveness of such strategy on the summarization of long documents has not been tested. Finally, a very recent work by <ref type="bibr" target="#b0">Bi et al. (2020)</ref> attempts to reduce redundancy in more sophisticated ways, but still focusing on news. Furthermore, since it relies on BERT, such model is unsuitable to deal with long documents (with over 3,000 words).</p><p>To address this rather confusing situation, characterized by unclear connections between all the proposed neural models, by their limited focus on short documents, and by spotty evaluations, in this paper we systematically organize existing redundancy reduction methods into three categories, and compare them with respect to the informativeness and redundancy of the generated summary for long documents. In particular, to perform a fair comparison we re-implement all methods by modifying a common basic model <ref type="bibr" target="#b24">(Xiao and Carenini, 2019)</ref>, which is a top performer on long documents without considering redundancy. Additionally, we propose three new methods that we argue will reduce redundancy more explicitly and flexibly in the sentence scoring and sentence selection phase by deploying more suitable decoders, loss functions and/or sentence selection algorithms, again building for a fair comparison on the common basic model <ref type="bibr" target="#b24">(Xiao and Carenini, 2019)</ref>.</p><p>To summarize, our main contributions in this paper are: we first examine popular datasets, and show that redundancy is a more serious problem when summarizing long documents (e.g., scientific papers) than short ones (e.g. news). Secondly, we not only reorganize and re-implement existing neural methods for redundancy reduction, but we also propose three new general and flexible methods. Finally, in a series of experiments, we compare existing and proposed methods on long documents (i.e., the Pubmed and arXiv datasets), with respect to ROUGE scores <ref type="bibr" target="#b9">(Lin, 2004)</ref> and redundancy scores <ref type="bibr" target="#b18">(Peyrard et al., 2017;</ref><ref type="bibr" target="#b4">Feigenblat et al., 2017)</ref>.</p><p>As a preview, empirical results reveal that the proposed methods achieve state-of-the-art performance on ROUGE scores, on the two scientific paper datasets, while also reducing the redundancy significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In traditional extractive summarization, the process is treated as a discrete optimization problem balancing between importance scores and redundancy scores, with techniques like Maximal Marginal Relevance(MMR) <ref type="bibr" target="#b1">(Carbonell and Goldstein, 1998)</ref>, redundancy-aware feature-based sentence classifiers <ref type="bibr" target="#b20">(Ren et al., 2016)</ref> and graph-based submodular selection <ref type="bibr" target="#b10">(Lin et al., 2009)</ref>.</p><p>In recent years, researchers have explored neural extractive summarization solutions, which score sentences by training the neural models on a large corpus, and simply apply a greedy algorithm for sentence selection <ref type="bibr" target="#b2">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b13">Nallapati et al., 2016a)</ref>. Although a model with a sequence decoder might plausibly encode redundancy information implicitly, <ref type="bibr" target="#b6">Kedzie et al. (2018)</ref> empirically show that this is not the case, since non auto-regressive models (the ones scoring each sentence independently), perform on par with models with a sequence decoder. In one of our new methods, to effectively capture redundancy information, we specify a new loss that explicitly consider redundancy when training the neural model.</p><p>Beyond a greedy algorithm, the Trigram Blocking is frequently used to explicitly reduce redundancy in the sentence selection phase <ref type="bibr" target="#b11">(Liu and Lapata, 2019)</ref>. In essence, a new sentence is not added to the summary if it shares a 3-gram with the previously added one. <ref type="bibr" target="#b16">Paulus et al. (2017)</ref> first adopt the strategy for abstractive summarization, which forces the model not to produce the same trigram twice in the generated summaries, as a simplified version of MMR <ref type="bibr" target="#b1">(Carbonell and Goldstein, 1998)</ref>. Arguably, this method is too crude for documents with relatively long sentences or specific concentrations (e.g. scientific papers), where some technical terms, possibly longer than 2-grams, are repeated frequently in the 'important sentences' (even in the reference summaries). To address this limitation, we propose a neural version of MMR to deal with redundancy within the sentence selection phase in a more flexible way, that can be tuned to balance importance and non-redundancy as needed.</p><p>The idea of MMR has also inspired <ref type="bibr" target="#b25">Zhou et al. (2018)</ref>, who propose a model jointly learning to score and select the sentences. Yet, this work not only focuses on summarizing short documents (i.e., news), but also uses MMR implicitly, and arguably sub-optimally, by learning a score that only indirectly captures the trade-off between relevance and redundancy. To improve on this approach, in this paper we propose a third new method, in which importance and redundancy are explicitly weighted, while still making the sentence scoring and selection benefit from each other by fine tuning the trained neural model through a Reinforcement Learning (RL) mechanism.</p><p>Finally, <ref type="bibr" target="#b0">Bi et al. (2020)</ref> is the most recent (still unpublished) work on reducing redundancy in neural single document summarization. However, their goal is very different form ours, since they focus on relatively short documents in the news domain.</p><p>3 Measuring Redundancy: metrics and comparing long vs. short documents</p><p>We use the following two relatively new metrics to measure redundancy in the source documents and in the generated summaries. Unique n-gram ratio 2 : proposed in <ref type="bibr" target="#b18">Peyrard et al. (2017)</ref>, it measures n-grams uniqueness; the lower it is, the more redundant the document is.</p><formula xml:id="formula_0">U niq_ngram_ratio = count(uniq_n_gram) count(n_gram)</formula><p>Normalized Inverse of Diversity (NID): captures redundancy, as the inverse of a diversity metric with length normalization. Diversity is defined as the entropy of unigrams in the document <ref type="bibr" target="#b4">(Feigenblat et al., 2017)</ref>. Since longer documents are more likely to have a higher entropy, we normalize the diversity with the maximum possible entropy for the document log(|D|). Thus, we have:</p><formula xml:id="formula_1">N ID = 1 − entropy(D) log(|D|)</formula><p>Note that higher NID indicates more redundancy.</p><p>When we compare the redundancy of long vs. short documents with respect to these two metrics on four popular datasets for summarization (CN-NDM <ref type="bibr" target="#b14">(Nallapati et al., 2016b)</ref>, Xsum <ref type="bibr" target="#b15">(Narayan et al., 2018)</ref>, Pubmed and arXiv <ref type="bibr" target="#b3">(Cohan et al., 2018)</ref>), we observe that long documents are substantially more redundant than short ones (as it was already pointed out in the past <ref type="bibr" target="#b22">(Stewart and Carbonell, 1998)</ref>). <ref type="table">Table 1</ref> shows the basic statistics of each dataset, along with the average NID <ref type="figure">Figure 1</ref>: The average unique n-gram ratio in the documents across different datasets. To reduce the effect of length difference, stopwords were removed.</p><p>scores, while <ref type="figure">Figure 1</ref> shows the average Unique n-gram Ratio for the same datasets. These observations provide further evidence that redundancy is a more serious problem in long documents. In addition, notice that the sentences in the scientific paper datasets are much longer than in the news datasets, which plausibly makes it even harder to balance between importance and non-redundancy. <ref type="bibr">Datasets</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Redundancy Reduction Methods</head><p>We systematically organize neural redundancy reduction methods into three categories, and compare prototypical methods from each category.</p><p>A The decoder is designed to implicitly take redundancy into account.</p><p>B In the sentence scoring phase, explicitly learn to reduce the redundancy.</p><p>C In the sentence selection phase, select sentences with less redundancy.</p><p>In this section, we describe different methods from each category. To compare them in a fair way, we build all of them on a basic ExtSum-LG model (see §4.1), by modifying the decoder and the loss function in the sentence selection phase or the sentence selection algorithm. In <ref type="table" target="#tab_2">Table 2</ref>, we summarize the architecture (Encoder, Decoder, Loss Function and sentence selection algorithm) of all the methods we compare.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Models</head><p>We consider two baseline models. One is an influential unsupervised method explicitly balancing importance and redundancy (Naive MMR). The other is our basic neural supervised model not dealing with redundancy at all (ExtSum-LG), to which we add different redundancy reduction mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naive MMR</head><p>MMR <ref type="bibr" target="#b1">(Carbonell and Goldstein, 1998</ref>) is a traditional extractive summarization method, which re-ranks the candidate sentences with a balance between query-relevance(importance) and information novelty(non-redundancy). Given a document D, at each step, MMR selects one sentence from the candidate set D \Ŝ that is relevant with the query Q, while containing little redundancy with the current summaryŜ. Note that if there is no specific query, then the query is the representation of the whole document. The method can be formally specified as:</p><formula xml:id="formula_2">M M R = arg max s i ∈D\Ŝ [λSim 1 (s i , Q) − (1 − λ) max s j ∈Ŝ Sim 2 (s i , s j )]</formula><p>where Sim 1 (s i , Q) measures the similarity between the candidate sentence s i and the query, indicating the importance of s i , while max s j ∈Ŝ Sim 2 (s i , s j ) measures the similarity between the candidate sentence s i and the current summaryŜ, representing the redundancy, and λ is the balancing factor. In this work, all the Sim are computed as the cosine similarity between the embeddings of the sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExtSum-LG</head><p>For the basic model, we use the current state-ofthe-art model <ref type="bibr" target="#b24">(Xiao and Carenini, 2019)</ref> on the summarization of long documents. It is a novel extractive summarization model incorporating local context and global context in the encoder, with an MLP layer as decoder and cross-entropy as the loss function. For the sentence selection phase, it greedily picks the sentences according to the score predicted by the neural model. In this method, redundancy is not considered, so it is a good testbed for adding and comparing redundancy reduction methods. Specifically, for a document D = {s 1 , s 2 , ..., s n }, the output of the encoder is h i for each sentence s i , and the decoder gives output P (y i ) as the confidence score on the importance of sentence s i . Finally, the model is trained on the Cross Entropy Loss :</p><formula xml:id="formula_3">L ce = − n i=1 (y i log P (y i )+(1−y i ) log (1 − P (y i ))</formula><p>4.2 Implicitly Reduce Redundancy in the neural model (Category A, <ref type="table" target="#tab_2">Table 2</ref>)</p><p>In this section, we describe two decoders from previous work, in which the redundancy of the summary is considered implicitly. SummaRuNNer Decoder: Nallapati et al. (2016a) introduce a decoder that computes a sentence score based on its salience, novelty(nonredundancy) and position to decide whether it should be included in the summary. Formally:</p><formula xml:id="formula_4">P (y i ) = σ(W c h i #Content +h i W s d #Salience −h T i W r tanh(summ i ) #N ovelty +W ap p a i + W rp p r i #P osition +b) #Bias</formula><p>where h i is the hidden state of sentence i from the encoder, d is the document representation , summ i is the summary representation, updated after each decoding step , and p a i , p r i are absolute and relative position embeddings, respectively. Once P (y i ) is obtained for each sentence i, a greedy algorithm selects the sentences to form the final summary.</p><p>Notice that although SummaRuNNer does contain a component assessing novelty, it would be inappropriate to view this model as explicitly dealing with redunadany because the novelty component is not directly supervised.</p><p>NeuSum Decoder: One of the main drawback of SummaRuNNer decoder is that it always score the sentences in order, i.e., the former sentences are not influenced by the latter ones. In addition, it only considers redundancy in the sentence scoring phase, while simply using a greedy algorithm to select sentences according to the resulting scores.</p><p>To address these problems, <ref type="bibr" target="#b25">Zhou et al. (2018)</ref> propose a new decoder to identify the relative gain of sentences, jointly learning to score and select sentences. In such decoder, instead of feeding the sentences and getting the scores in order, they use a mechanism similar to the pointer network <ref type="bibr" target="#b23">(Vinyals et al., 2015)</ref> to predict the scores of all the sentences at each step, select the sentence with the highest score, and feed it to the next step of sentence selection. As for the loss function, they use the KL divergence between the predicted score distribution and the relative ROUGE F1 gain at each step. To be specific, the loss computed at step t is:</p><formula xml:id="formula_5">L t = D KL (P t ||Q t ) P t (y i ) = exp(σ(h i )) n j=1 exp(σ(h j )) Q t (y i ) = exp(τg t (y i )) n j=1 exp(τg t (y j )) g t (y i ) = r1(S t−1 ∪ s i ) − r1(S t−1 )</formula><p>where P t , Q t are the predicted and ground truth relative gain respectively, g t (y i ) is the ROUGE F1 gain with respect to the current partial summary S t−1 for sentence s i , andg t (y i ) is the Min-Max normalized g t (y i ). τ is a smoothing factor, which is set to 200 empirically on the Pubmed dataset. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Explicitly Reduce Redundancy in</head><p>Sentence Scoring (Category B, <ref type="table" target="#tab_2">Table 2)</ref> We propose a new method to explicitly learn to reduce redundancy when scoring the sentences. RdLoss: Although <ref type="bibr" target="#b25">Zhou et al. (2018)</ref> jointly train the decoder to score and select sentences, it still learns to reduce redundancy implicitly, and the method does not allow controlling the degree of redundancy. To address this limitation, we propose a rather simple method to explicitly force the model 3 Due to the complexity of generating the target distribution Q, we only experiment with this method on Pubmed.</p><p>to reduce redundancy in the sentence scoring phase by adding a redundancy loss term to the original loss function, motivated by the success of a similar strategy of adding a bias loss term in the gender debiasing task <ref type="bibr" target="#b19">(Qian et al., 2019)</ref>. Our new loss term L rd is naturally defined as the expected redundancy contained in the resulting summary, as shown below:</p><formula xml:id="formula_6">L = βL ce + (1 − β)L rd L rd = n i=1 n j=1 P (y i )P (y j )Sim(s i , s j )</formula><p>where P (y i ), P (y j ) are the confidence scores of sentence i and j on whether to select the sentences in the generated summary, and Sim(s i , s j ) is the similarity, i.e. redundancy between sentence i and j. 4 By adding the redundancy loss term, we penalize it more if two sentences are similar to each other and both of them have high confidence scores. β is a balance factor, controlling the degree of redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Explicitly Reduce Redundancy in</head><p>Sentence Selection (Category C, <ref type="table" target="#tab_2">Table 2)</ref> We first introduce an existing method and then propose two novel methods that explicitly reduce redundancy in the sentence selection phase.</p><p>Trigram Blocking is widely used in recent extractive summarization models on the news dataset (e.g. <ref type="bibr" target="#b11">Liu and Lapata (2019)</ref>). Intuitively, it borrows the idea of MMR to balance the importance and non-redundancy when selecting sentences. In particular, given the predicted sentence scores, instead of just selecting sentences greedily according to the scores, the current candidate is added to the summary only if it does not have trigram overlap with the previous selected sentences. Otherwise, the current candidate sentence is ignored and the next one is checked, until the length limit is reached.</p><p>MMR-Select: Inspired by the existence of a relevance/redundancy trade-off, we propose MMR-Select, a simple method to eliminate redundancy when a neural summarizer selects sentences to form a summary, in a way that is arguably more flexible than Trigram Blocking with a balance factor λ.</p><p>With the confidence score computed by the basic model, P = {P (y 1 ), P (y 2 ), ..., P (y n )}, instead of picking sentences greedily, we pick the sentences according to the MMR-score, which is defined based on MMR and updated after each single sentence being selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMR-Select = arg max</head><formula xml:id="formula_7">s i ∈D\Ŝ [MMR-score i ] MMR-score i = λP (y i ) − (1 − λ) max s j ∈Ŝ Sim(s i , s j )]</formula><p>The main difference between the Naive MMR and MMR-Select falls into the computation of the importance score. In the Naive MMR, the importance score is the similarity between each sentence and the query, or the whole document, while in MMR-Select, the importance score is computed by a trained neural model.</p><p>MMR-Select+ : The main limitation of MMR-Select is that the sentence scoring phase and the sentence selection phase cannot benefit from each other, because they are totally separate.</p><p>To promote synergy between these two phases, we design a new method, MMR-Select+, shown in <ref type="figure" target="#fig_0">Figure 2</ref>, which synergistically combines three components: the basic model, the original crossentropy loss L ce (in blue), and an RL mechanism (in green) whose loss is L rd . The neural model is then trained on a mixed objective loss L with γ as the scaling factor. Zooming on the details of the RL component, it first generates a summaryŜ by applying the MMR selection described for MMR-Select, which is to greedily pick sentences according to MMR-score, as well as the corresponding label assignmentŶ = {ŷ 1 ,ŷ 2 , ...,ŷ n } (ŷ i = 1 if s i is selected,ŷ i = 0 otherwise). Then, the expected reward is computed based on the ROUGE score betweenŜ and the gold-standard human abstractive summary S weighted by the probability of theŶ labels. Notice that we also adopt the self-critical strategy <ref type="bibr" target="#b16">(Paulus et al., 2017)</ref> to help accelerating the convergence by adding a baseline summaryS, which is generated by greedily picking the sentences according to P . r(S) is the reward of this baseline summary and it is subtracted from r(Ŝ) to only positively reward summaries which are better than the baseline. Formally, the whole MMR-Select+ model can be specified as follows:</p><formula xml:id="formula_8">L = γL rd + (1 − γ)L ce L rd = −(r(Ŝ) − r(S)) n i=1 log P (ŷ i ) r(S ) = 1 3 k∈{1,2,L} ROUGE-k(S , S)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we describe the settings, results and analysis of the experiments of different methods on the Pubmed and arXiv datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Settings</head><p>Following previous work, we use GloVe (Pennington et al., 2014) as word embedding, and the average word embedding as the distributed representation of sentences. To be comparable with Xiao and Carenini (2019), we set word length limit of the generated summaries as 200 on both datasets. <ref type="bibr">6</ref> We tune the hyperparameter λ and β in the respective methods on the validation set, and set λ = 0.6, β = 0.3 for both datasets. Following previous work (e.g., <ref type="bibr" target="#b8">Li et al. (2019)</ref>), γ was set to 0.99. For training MMR-Select+, the learning rate is lr = 1e − 6; we start with the pretrained ExtSumm-LG model. As for the evaluation metric, we use ROUGE scores as the measurement of importance while using the Unique N-gram Ratio and NID defined in Section 3 as the measurements of redundancy. <ref type="figure">Figure 3</ref>: The average ROUGE scores, average unique n-gram ratios, and average NID scores with different λ used in the MMR-Select on the validation set. Remember that the higher the Unique n-gram Ratio, the lower NID, the less redundancy contained in the summary.   <ref type="table">Table 4</ref>: Unique n-gram ratio and NID score on the two datasets. † indicates significant differences from <ref type="bibr" target="#b24">(Xiao and Carenini, 2019)</ref> with confidence level 99%, while ‡ indicates significant differences from all the other models with confidence level 99% on the Bootstrap Significance test. Noting the higher the Unique n-gram Ratio, the lower NID, the less redundancy contained in the summary.Green numbers means it's better than ExtSum-LG on the certain metric, and the red numbers means worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Finetuning λ</head><p>Consistently with previous work <ref type="bibr" target="#b5">(Jung et al., 2019)</ref>, when we finetune λ of MMR Select on the validation set, we pinpoint the trade off between importance and non-redundancy in the generated summary (see <ref type="figure">Figure 3</ref>). For λ ≤ 0.6, as we increase the weight of importance score, the average ROUGE scores continuously increase while the redundancy/diversity increases/drops rapidly. But since extractive methods can only reuse sentences from the input document, there is an upper bound on how much the generated summary can match the ground-truth summary, so when λ &gt; 0.6, the ROUGE score even drops by a small margin, while the redundancy/diversity still increases/drops. Then the problem to solve for future work is how to increase the peak, which could be done by either applying finer units (e.g., clauses instead of sentences) or further improve the model that predicts the importance score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Overall Results and Analysis</head><p>The experimental results for the ROUGE scores are shown in <ref type="table" target="#tab_4">Table 3</ref>, whereas results for redundancy scores (Unique N-gram Ratio and NID score) are shown in <ref type="table">Table 4</ref>. With respect to the balance between importance and non-redundancy, despite the trade-off between the two aspects, all of the three methods we propose can reduce redundancy significantly while also improving the ROUGE score significantly compared with the ExtSum-LG basic neural model. In contrast, the NeuSum Decoder and Trigram Blocking effectively reduce redundancy, but in doing that they hurt the importance aspect considerably. Even worse, the SR Decoder is dominated by the basic model on both aspects.</p><p>Focusing on the redundancy aspect <ref type="table">(Table 4)</ref>, Trigram Blocking makes the largest improvement on redundancy reduction, but with a large drop in ROUGE scores. This is in striking contrast with results on news datasets <ref type="bibr" target="#b11">(Liu and Lapata, 2019)</ref>, where Trigram Blocking reduced redundancy while also improving the ROUGE score significantly. Plausibly, the difference between the performances across datasets might be the result of the inflexibility of the method. In both Pubmed and arXiv datasets, the sentences are much longer than those in the news dataset (See <ref type="table">Table 1</ref>), and therefore, simply dropping candidate sentences with 3-gram overlap may lead to incorrectly missing sentences with substantial important information.</p><p>Furthermore, another insight revealed in <ref type="table">Table  4</ref> is that dealing with redundancy in the sentence selection phase is consistently more effective than doing it in the sentence scoring phase, regardless of whether this happens implicitly (NeuSum &gt; SR Decoder) or explicitly (Trigram Blocking, MMR-Select/+ &gt; RdLoss).</p><p>Moving to more specific findings about particular systems, we already noted that while the NeuSum Decoder reduces redundancy effectively, it performs poorly on the ROUGE score, something that did not happen with news datasets. A possible explanation is that the number of sentences selected for the scientific paper datasets (on average 6-7 sentences) is almost twice the number of sentences selected for news; and as it was mentioned in the original paper <ref type="bibr" target="#b25">(Zhou et al., 2018)</ref>, the precision of NeuSum drops rapidly after selecting a few sentences.</p><p>Other results confirm established beliefs. The considerable difference between Naive MMR and MMR-Select was expected given the recognized power of neural network over unsupervised methods. Secondly, the unimpressive performance of the SR decoder confirms that the in-order sequence scoring is too limited for effectively predicting importance score and reducing redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">More Insights of the Experiments</head><p>In addition to the main experiment results discussed above, we further explore the performance on informativeness (ROUGE score) and redundancy (Unique N-gram Ratio) of different redundancy reduction methods under two different conditions, namely the degree of redundancy and the length of the source documents. <ref type="figure">Figure 4</ref> shows the results on the Pubmed dataset, while further results of a similar analysis on the arXiv dataset can be found in the Appendices. With respect to the degree of redundancy, (upper part of <ref type="figure">Figure 4)</ref>, the less redundant the document is, the less impact the redundancy reduction methods have. Among all the methods, although Trigram Blocking works the best with respect to reducing redundancy, it hurts the informativeness the most. However, it is still a good choice for a rather less redundant document (e.g. the documents in the last two bins with avg Unique N-gram Ratio over 0.7), which is also consistent with the previous works showing the Trigram Blocking works well on the news datasets, which tends to be less redundant (see §3). As for all the other methods, although they have the same trends, MMR-Select+ performs the best on both informativeness and redundancy reduction, especially for the more redundant documents.</p><p>Regarding to the length of the source document (bottom part of <ref type="figure">Figure 4</ref>) , as the document become longer, both informativeness and redundancy in the summary generated by all methods increases and then decrease once hitting the peak. MMR-Select+ and MMR-Select are the best choices to balance between the informativeness and redundancy -they are the only two methods having the higher ROUGE scores and higher Unique N-gram ratios across different lengths, even for the short documents with less than 50 sentences.</p><p>Besides, we also conduct experiments on generating summaries with different length limit, where we found that our new methods are stable across different summary lengths <ref type="figure">(Figure. 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future work</head><p>Balancing sentence importance and redundancy is a key problem in extractive summarization. By examining large summarization datasets, we find that longer documents tend to be more redundant. Therefore in this paper, we systematically explore and compare existing and newly proposed methods for redundancy reduction in summarizing long doc- uments. Experiments indicate that our novel methods achieve SOTA on the ROUGE scores, while significantly reducing redundancy on two scientific paper datasets (Pubmed and arXiv). Interestingly, we show that redundancy reduction in sentence selection is more effective than in the sentence scoring phase, a finding to be further investigated .</p><p>Additional venues for future work include experimenting with generating summaries at finer granularity than sentences, as suggested by our analysis of the λ parameter. We also intend to explore other ways to assess redundancy, moving from computing the cosine similarity between sentence embeddings, to a pre-trained neural model for sentence similarity. Finally, we plan to run human evaluations to assess the quality of the generated summaries. This is quite challenging for scientific papers, as it requires participants to possess sophisticated domain-specific background knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>In these Appendices, we show more analysis of the experimental results. <ref type="figure">Figure 6</ref> shows the performance on informativeness (ROUGE score) and redundancy (Unique N-gram Ratio) of different redundancy reduction methods under different conditions on the arXiv dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Analysis on arXiv Dataset under conditions</head><p>Comparing with the Pubmed dataset, the documents in the arXiv dataset tend to be longer and more redundant, as the majority of the documents in the Pubmed dataset have less than 100 sentences with average Unique N-gram Ratio in the 0.5 − 0.6 interval, while the majority of the documents in the arXiv dataset have number of sentences in the range 100 to 300 with average Unique N-gram Ratio in the 0.6 − 0.7 interval. Consistent with the result on the Pubmed dataset, the Trigram Blocking method is the best choice for rather less redundant documents (with average Unique N-gram Ratio larger than 0.7), and the MMR-Select+ is the one better or equivalent to the original model across different degree of redundancy, ignoring the outliers. With respect to the length of the documents, the MMR-Select+ and MMR-Select are consistently the most effective methods for balancing redundancy and informativeness on documents with different length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Analysis on Selection Overlap</head><p>To explore the difference made by applying different redundancy reduction methods on the original method(ExtSumLG), we compare the selected sentences by all the methods, and show the overlap ratios between every two methods, as well as the total number and the average length of selected sentences in the test set, in <ref type="table">Table 5</ref> and <ref type="table" target="#tab_7">Table  6</ref> for Pubmed dataset and arXiv dataset respectively. As we can see from the tables, except for the SR Decoder, all the other methods tend to select more and shorter sentences than the original summarizer. Regarding the overlap between the original method and the others, we observe that among all the three categories, the methods in category A tend to produce large differences, since these methods change the structure of the original model.  <ref type="table">Table 5</ref>: Micro overlap ratio (%) between the selections of different methods and the total number and the average length of selected sentences in the test set of Pubmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Analysis on Recall and Precision of ROUGE Scores</head><p>We also provide the Precision and Recall of the ROUGE scores in the main experiment, the results of Pubmed and arXiv datasets are shown in <ref type="table" target="#tab_8">Table 7</ref> and <ref type="table">Table 8</ref>, respectively. It is interesting to see that the NeuSum Decoder tends to have a high precision but low recall, indicating that the generated summaries tend to be shorter and contain less useful information than the original method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Analysis on the Relative Position of Selections</head><p>We also show the relative position distribution of the selected sentences on both datasets in <ref type="figure" target="#fig_2">Figure 7</ref> to verify if any redundancy reduction method has a particular tendency to select sentences in particular position of the documents. However, as shown in the figure, the trends are all rather similar for all methods.    <ref type="table">Table 8</ref>: Rouge Recall and Precision of different summarization models on the Pubmed dataset. Green numbers means it's better than ExtSum-LG on the certain metric, and the red numbers means worse. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The pipeline of the MMR-Select+ method, whereŜ,Ŷ andS,Ȳ are the summary and labels generated by the MMR-Select algorithm and the normal greedy algorithm, respectively. S and Y are the ground truth summary and the oracle labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Comparing the average ROUGE scores and average unique n-gram ratios of different models on the Pubmed dataset, conditioned on different degrees of redundancy and lengths of the document (extremely long documents -i.e., 1% of the dataset are not shown because of space constraints). 7 Comparing the average ROUGE scores and average unique n-gram ratios of different models with different word length limits on the Pubmed dataset. See Appendices for similar results on arXiv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>The relative position distribution of different redundancy reduction methods on Pubmed(left) and arXiv(right) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc># Doc. # words/doc. # words/sent. NID</figDesc><table><row><cell>Xsum</cell><cell>203k</cell><cell>429</cell><cell>22.8</cell><cell>0.188</cell></row><row><cell cols="2">CNNDM 270k</cell><cell>823</cell><cell>19.9</cell><cell>0.205</cell></row><row><cell>Pubmed</cell><cell>115k</cell><cell>3142</cell><cell>35.1</cell><cell>0.255</cell></row><row><cell>arXiv</cell><cell>201k</cell><cell>6081</cell><cell>29.2</cell><cell>0.267</cell></row></table><note>Table 1: Longer documents are more redundant</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The architecture of redundancy reduction methods. Bold methods are proposed in this paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Rouge score of different summarization models on the Pubmed and arXiv datasets. † indicates significantly better than the ExtSum-LG with confidence level 99% on the Bootstrap Significance test. Green numbers means it's better than ExtSum-LG on the certain metric, and the red numbers means worse.</figDesc><table><row><cell cols="2">Categ. Model</cell><cell cols="3">Pubmed Unigram% Bigram% Trigram%</cell><cell>NID</cell><cell cols="3">arXiv Unigram% Bigram% Trigram%</cell><cell>NID</cell></row><row><cell>C</cell><cell>Naive MMR</cell><cell>56.55</cell><cell>90.93</cell><cell>96.95</cell><cell>0.1881</cell><cell>53.01</cell><cell>88.82</cell><cell>96.28</cell><cell>0.1992</cell></row><row><cell>-</cell><cell>ExtSum-LG</cell><cell>53.02</cell><cell>87.29</cell><cell>94.37</cell><cell>0.2066</cell><cell>52.17</cell><cell>87.19</cell><cell>95.38</cell><cell>0.2088</cell></row><row><cell>A</cell><cell>+SR Decoder</cell><cell>52.88</cell><cell>87.17</cell><cell>94.32</cell><cell>0.2070</cell><cell>51.98</cell><cell>87.08</cell><cell>95.31</cell><cell>0.2097</cell></row><row><cell>A</cell><cell>+NeuSum Decoder</cell><cell>54.88  †</cell><cell>88.71  †</cell><cell>95.13  †</cell><cell>0.1993  †</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>B</cell><cell>+RdLoss</cell><cell>53.23  †</cell><cell>87.41</cell><cell>94.43</cell><cell>0.2052  †</cell><cell>52.17</cell><cell>87.20</cell><cell>95.36</cell><cell>0.2085</cell></row><row><cell>C</cell><cell>+Trigram Blocking</cell><cell>57.58  †  ‡</cell><cell cols="3">93.05  †  ‡ 98.56  †  ‡ 0.1818  †  ‡</cell><cell>56.12  †  ‡</cell><cell cols="3">92.38  †  ‡ 98.94  †  ‡ 0.1876  †  ‡</cell></row><row><cell>C</cell><cell>+MMR-Select</cell><cell>53.76  †</cell><cell>88.04  †</cell><cell>94.96  †</cell><cell>0.2022</cell><cell>52.80  †</cell><cell>87.64  †</cell><cell>95.40</cell><cell>0.2055  †</cell></row><row><cell>C</cell><cell>+MMR-Select+</cell><cell>53.93  †</cell><cell>88.32</cell><cell>95.14</cell><cell>0.2014</cell><cell>52.76  †</cell><cell>87.78  †</cell><cell>95.70  †</cell><cell>0.2055  †</cell></row><row><cell>-</cell><cell>Oracle</cell><cell>56.66</cell><cell>89.25</cell><cell>95.55</cell><cell>0.2036</cell><cell>56.74</cell><cell>90.81</cell><cell>96.82</cell><cell>0.2029</cell></row><row><cell>-</cell><cell>Reference</cell><cell>56.69</cell><cell>89.45</cell><cell>95.95</cell><cell>0.2005</cell><cell>58.92</cell><cell>90.13</cell><cell>97.02</cell><cell>0.1970</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Micro overlap ratio (%) between the selections of different methods and the total number and the average length of selected sentences in the test set of arXiv. 51.10 19.75 22.68 39.66 45.96 A +NeuSum Decoder 44.36 49.24 19.74 21.58 40.29 44.62 B +RdLoss 44.30 51.09 20.11 22.88 40.09 46.11 C +Trigram Blocking 42.67 48.54 17.51 19.73 38.45 43.64 C +MMR-Select 44.25 51.09 19.98 22.75 40.08 46.07 C +MMR-Select+ 44.28 51.27 20.01 22.86 40.03 46.24</figDesc><table><row><cell cols="2">Categ. Model</cell><cell>ROUGE-1</cell><cell>Pubmed ROUGE-2</cell><cell>ROUGE-L</cell></row><row><cell></cell><cell></cell><cell cols="3">Prec. Recall Prec. Recall Prec. Recall</cell></row><row><cell>C</cell><cell>Naive MMR</cell><cell cols="3">36.45 42.56 11.05 12.64 31.39 36.53</cell></row><row><cell>-</cell><cell>ExtSum-LG 9</cell><cell cols="3">44.05 51.08 19.82 22.71 39.74 45.97</cell></row><row><cell>A</cell><cell>+SR Decoder</cell><cell>44.00</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Rouge Recall and Precision of different summarization models on the Pubmed dataset. Green numbers means it's better than ExtSum-LG on the certain metric, and the red numbers means worse. 54.77 15.68 22.29 34.60 48.59 C +Trigram Blocking 38.04 52.71 13.98 19.47 33.71 46.61 C +MMR-Select 38.85 54.33 15.39 21.74 34.56 48.24 C +MMR-Select+ 38.75 54.67 15.41 21.96 34.44 48.51</figDesc><table><row><cell cols="2">Categ. Model</cell><cell cols="2">ROUGE-1</cell><cell cols="2">Arxiv ROUGE-2</cell><cell cols="2">ROUGE-L</cell></row><row><cell></cell><cell></cell><cell cols="6">Prec. Recall Prec. Recall Prec. Recall</cell></row><row><cell>C</cell><cell>Naive MMR</cell><cell cols="2">29.61 42.69</cell><cell>7.45</cell><cell cols="3">10.78 24.92 35.82</cell></row><row><cell>-</cell><cell>ExtSum-LG 10</cell><cell cols="6">38.60 54.64 15.38 22.00 34.17 48.26</cell></row><row><cell>A</cell><cell>+SR Decoder</cell><cell cols="6">38.65 54.99 15.47 22.28 34.24 48.64</cell></row><row><cell>A</cell><cell>+NeuSum Decoder</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>B</cell><cell>+RdLoss</cell><cell>38.92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code can be found herehttp://www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In this paper, all the unique n-gram ratios are shown in percentage.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Noting that we define Sim(si, si) as 0</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The results of ExtSum-LG were obtained by re-running their model.6  A document representation in Unsupervised MMR is similarly computed by averaging the embeddings of all the words.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank reviewers and the UBC-NLP group for their insightful comments. This research was supported by the Language &amp; Speech Innovation Lab of Cloud BU, Huawei Technologies Co., Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Celikyilmaz</surname></persName>
		</author>
		<title level="m">AREDSUM: Adaptive Redundancy-Aware Iterative Sentence Ranking for Extractive Document Summarization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="DOI">10.1145/290941.291025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1046</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goharian</surname></persName>
		</author>
		<idno>abs/1804.05685</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised queryfocused multi-document summarization using the cross entropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Feigenblat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Roitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odellia</forename><surname>Boni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Konopnicki</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080690</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="961" to="964" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Earlier Isn&apos;t Always Better: Subaspect Analysis on Corpus and System Biases in Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehee</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Mentch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1327</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3322" to="3333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Content selection in deep learning models of summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Kedzie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1208</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1818" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analyzing sentence fusion in abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Muchovej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5413</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on New Frontiers in Summarization</title>
		<meeting>the 2nd Workshop on New Frontiers in Summarization<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="104" to="110" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with distributional semantic rewards for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deren</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1623</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6040" to="6046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graphbased submodular selection for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASRU.2009.5373486</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting>the 2009 IEEE Workshop on Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="381" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1387</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tackling redundancy in text summarization through different levels of language analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Sanz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csi.2012.08.001</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Standards &amp; Interfaces</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="507" to="518" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1611.04230</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gulçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to score system summaries for better content selection evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Botschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4510</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization</title>
		<meeting>the Workshop on New Frontiers in Summarization<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reducing gender bias in word-level language models with a gender-equalizing loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusu</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urwa</forename><surname>Muaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Won</forename><surname>Hyun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-2031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="223" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A redundancy-aware sentence regression framework for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Automatic Text Summarization: Past, Present and Future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-28569-1_1</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="21" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Summarization: (1) using mmr for diversity -based reranking and (2) evaluating summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<idno type="DOI">10.3115/1119089.1119120</idno>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extractive summarization of long documents by combining global and local context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1298</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3002" to="3012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural document summarization by jointly learning to score and select sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="663" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
