<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-05-30">30 May 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<email>yliu@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
							<email>cjsun@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
							<email>linl@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<email>wangxl@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-05-30">30 May 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we proposed a sentence encoding-based model for recognizing text entailment. In our approach, the encoding of sentence is a two-stage process. Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate a firststage sentence representation. Secondly, attention mechanism was employed to replace average pooling on the same sentence for better representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence's first-stage representation to attend words appeared in itself, which is called "Inner-Attention" in our paper . Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus has proved the effectiveness of "Inner-Attention" mechanism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given a pair of sentences, the goal of recognizing text entailment (RTE) is to determine whether the hypothesis can reasonably be inferred from the premises. There were three types of relation in RTE, Entailment (inferred to be true), Contradiction (inferred to be false) and Neutral (truth unknown).A few examples were given in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Traditional methods to RTE has been the dominion of classifiers employing hand engineered features, which heavily relied on natural language processing pipelines and external resources. Formal reasoning methods <ref type="bibr" target="#b0">(Bos and Markert, 2005)</ref> were P The boy is running through a grassy area.</p><p>The boy is in his room. C H A boy is running outside. E The boy is in a park. N also explored by many researchers, but not been widely used because of its complexity and domain limitations.</p><p>Recently published Stanford Natural Language Inference (SNLI 1 ) corpus makes it possible to use deep learning methods to solve RTE problems. So far proposed deep learning approaches can be roughly categorized into two groups: sentence encoding-based models and matching encodingbased models. As the name implies, the encoding of sentence is the core of former methods, while the latter methods directly model the relation between two sentences and didn't generate sentence representations at all.</p><p>In view of universality, we focused our efforts on sentence encoding-based model. Existing methods of this kind including: LSTMs-based model, GRUsbased model, TBCNN-based model and SPINNbased model. Single directional LSTMs and GRUs suffer a weakness of not utilizing the contextual information from the future tokens and Convolutional Neural Networks didn't make full use of information contained in word order. Bidirectional LSTM utilizes both the previous and future context by processing the sequence on two directions which helps to address the drawbacks mentioned above.  <ref type="bibr" target="#b3">(Tan et al., 2015)</ref> A recent work by <ref type="bibr">(Rocktäschel et al., 2015)</ref> improved the performance by applying a neural attention model that didn't yield sentence embeddings.</p><p>In this paper, we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering, or external resources. The basic model is based on building biL-STM models on both premises and hypothesis. The basic mean pooling encoder can roughly form a intuition about what this sentence is talking about. Obtained this representation, we extended this model by utilize an Inner-Attention mechanism on both sides. This mechanism helps generate more accurate and focused sentence representations for classification. In addition, we introduced a simple effective input strategy that get ride of same words in hypothesis and premise, which further boosts our performance. Without parameter tuning, we improved the art-of-the-state performance of sentence encodingbased model by nearly 2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our approach</head><p>In our work, we treated RTE task as a supervised three-way classification problem. The overall architecture of our model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The design of this model we follow the idea of Siamese Network, that the two identical sentence encoders share the same set of weights during training, and the two sentence representations then combined together to generated a "relation vector" for classification. As we can see from the figure, the model mainly consists of three parts. From top to bottom were: (A). The sentence input module; (B). The sentence encoding module; (C). The sentence matching module. We will explain the last two parts in detail in the following subsection. And the sentence input module will be introduced in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence Encoding Module</head><p>Sentence encoding module is the fundamental part of this model. To generate better sentence representations, we employed a two-step strategy to encode sentences. Firstly, average pooling layer was built on top of word-level biLSTMs to produce sentence vector. This simple encoder combined with the sentence matching module formed the basic architecture of our model. With much less parameters, this basic model alone can outperformed art-ofstate method by a small margin. (refer to <ref type="table" target="#tab_4">Table 3</ref>). Secondly, attention mechanism was employed on the same sentence, instead of using target sentence representation to attend words in source sentence, we used the representation generated in previous stage to attend words appeared in the sentence itself, which results in a similar distribution with other attention mechanism weights. More attention was given to important words. <ref type="bibr">2</ref> The idea of "Inner-attention" was inspired by the observation that when human read one sentence, people usually can roughly form an intuition about which part of the sentence is more important according past experience. And we implemented this idea using attention mechanism in our model. The attention mechanism is formalized as follows:</p><formula xml:id="formula_0">M = tanh(W y Y + W h R ave ⊗ e L ) α = sof tmax(w T M ) R att = Y α T</formula><p>where Y is a matrix consisting of output vectors of biLSTM, R ave is the output of mean pooling layer, α denoted the attention vector and R att is the attention-weighted sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentence Matching Module</head><p>Once the sentence vectors are generated. Three matching methods were applied to extract relations between premise and hypothesis.</p><p>• Concatenation of the two representations • Element-wise product • Element-wise difference This matching architecture was first used by <ref type="bibr" target="#b1">(Mou et al., 2015)</ref>. Finally, we used a SoftMax layer over the output of a non-linear projection of the generated matching vector for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DataSet</head><p>To evaluate the performance of our model, we conducted our experiments on Stanford Natural Language Inference (SNLI) corpus <ref type="bibr" target="#b0">(Bos and Markert, 2005)</ref>. At 570K pairs, SNLI is two orders of magnitude larger than all other resources of its type. The dataset is constructed by crowdsourced efforts, each sentence written by humans. The target labels comprise three classes: Entailment, Contradiction, and Neutral 2 Recently, <ref type="bibr">(Yang et al., 2016)</ref> proposed a Hierarchical Attention model on the task of document classification also used for but the target representation in attention their mechanism is randomly initialized.</p><p>(two irrelevant sentences). We applied the standard train/validation/test split, containing 550k, 10k, and 10k samples, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter Setting</head><p>The training objective of our model is cross-entropy loss, and we use minibatch SGD with the Rmsprop (Tieleman and Hinton, 2012) for optimization. The batch size is 128. A dropout layer was applied in the output of the network with the dropout rate set to 0.25. In our model, we used pretrained 300D Glove 840B vectors <ref type="bibr" target="#b1">(Pennington et al., 2014)</ref> to initialize the word embedding. Out-of-vocabulary words in the training set are randomly initialized by sampling values uniformly from (0.05, 0.05). All of these embedding are not updated during training . We didn't tune representations of words for two reasons: 1. To reduced the number of parameters needed to train. 2. Keep their representation stays close to unseen similar words in inference time, which improved the model's generation ability. The model is implemented using open-source framework Keras. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Input Strategy</head><p>In this part, we investigated four strategies to modify the input on our basic model which helps us increase performance, the four strategies are:</p><p>• Inverting Premises <ref type="bibr">(Sutskever et al., 2014)</ref> • Doubling Premises (Zaremba and Sutskever, 2014) • Doubling Hypothesis • Differentiating Inputs (Removing same words appeared in premises and hypothesis)</p><p>Experimental results were illustrated in <ref type="table" target="#tab_2">Table 2</ref>. As we can see from it, doubling hypothesis and differentiating inputs both improved our model's performance.While the hypothesises usually much shorter than premises, doubling hypothesis may absorb this difference and emphasize the meaning twice via this strategy. Differentiating input strategy forces the model to focus on different part of the two sentences which may help the classification for Neutral and Contradiction examples as we observed that our model tended to assign unconfident instances to Entailment. And the original input sentences appeared in <ref type="figure" target="#fig_0">Figure 1</ref> are:  While most of the words in this pair of sentences are same or close in semantic, It is hard for model to distinguish the difference between them, which resulted in labeling it with Neutral or Entailment. Through differentiating inputs strategy, this kind of problems can be solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison Methods</head><p>In this part, we compared our model against the following art-of-the-state baseline approaches: The cat refers to concatenation, -and • denote element-wise difference and product, respectively. Much simpler and easy to understand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results and Qualitative Analysis</head><p>Although the classification of RTE example is not solely relying on representations obtained from attention, it is still instructive to analysis Inner-Attention mechanism as we witnessed a large performance increase after employing it. We handpicked several examples from the dataset to visualize. In order to make the weights more discriminated, we didn't use a uniform colour atla cross sentences. That is, each sentence have its own color atla, the lightest color and the darkest color denoted the smallest attention weight the biggest value  within the sentence, respectively. Visualizations of Inner-Attention on these examples are depicted in <ref type="figure" target="#fig_2">Figure 2</ref>. We observed that more attention was given to Nones, Verbs and Adjectives. This conform to our experience that these words are more semantic richer than function words. While mean pooling regarded each word of equal importance, the attention mechanism helps re-weight words according to their importance. And more focused and accurate sentence representations were generated based on produced attention vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future work</head><p>In this paper, we proposed a bidirectional LSTMbased model with Inner-Attention to solve the RTE problem. We come up with an idea to utilize attention mechanism within sentence which can teach itself to attend words without the information from another one. The Inner-Attention mechanism helps produce more accurate sentence representa-tions through attention vectors. In addition, the simple effective diversing input strategy introduced by us further boosts our results. And this model can be easily adapted to other sentence-matching models. Our future work including:</p><p>1. Employ this architecture on other sentencematching tasks such as Question Answer, Paraphrase and Sentence Text Similarity etc. 2. Try more heuristics matching methods to make full use of the sentence vectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of Bidirectional LSTM model with Inner-Attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>LSTM enc: 100D LSTM encoders + MLP. (Bowman et al., 2015) • GRU enc: 1024D GRU encoders + skip-thoughts + cat, -. (Vendrov et al., 2015) • TBCNN enc: 300D Tree-based CNN encoders + cat, • , -. (Mou et al., 2015) • SPINN enc: 300D SPINN-NP encoders + cat, • , -. (Bowman et al., 2016) • Static-Attention: 100D LSTM + static attention. (Rocktäschel et al., 2015) • WbW-Attention: 100D LSTM + word-by-word attention. (Rocktäschel et al., 2015)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Inner-Attention Visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of three types of label in RTE, where P stands for Premises and H stands for Hypothesis</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different input strategies</figDesc><table><row><cell>Hypothesis: Two man in polo shirts and tan pants in-</cell></row><row><cell>volved in a heated discussion about Canon.</cell></row><row><cell>Label: Contradiction</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of different models on SNLI.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://nlp.stanford.edu/projects/snli/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Premise: Two man in polo shirts and tan pants immersed in a pleasant conversation about photograph.3 http://keras.io/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all anonymous reviewers for their hard work!</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognising textual entailment with logical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markert2005] Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<idno>arXiv:1603.06021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A fast unified model for parsing and sentence understanding</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognizing entailment and contradiction by tree-based convolution</title>
		<idno type="arXiv">arXiv:1512.08422</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Glove: Global vectors for word representation. Rocktäschel et al.2015] Tim Rocktäschel</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Reasoning about entailment with neural attention</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop. COURS-ERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<idno>arXiv:1511.06361</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy</editor>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Sanja Fidler, and Raquel Urtasun. 2015. Orderembeddings of images and language</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<idno type="arXiv">arXiv:1410.4615</idno>
		<title level="m">Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Zaremba and Sutskever2014</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
