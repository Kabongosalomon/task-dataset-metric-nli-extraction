<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Cross-Modal Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Computer Science</orgName>
								<orgName type="department" key="dep2">Element AI</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin, Montreal</settlement>
									<country>China, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Element AI</orgName>
								<orgName type="department" key="dep2">Pedro O. Pinheiro Element AI</orgName>
								<address>
									<settlement>Montreal, Montreal</settlement>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Cross-Modal Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Metric-based meta-learning techniques have successfully been applied to fewshot classification problems. In this paper, we propose to leverage cross-modal information to enhance metric-based few-shot learning methods. Visual and semantic feature spaces have different structures by definition. For certain concepts, visual features might be richer and more discriminative than text ones. While for others, the inverse might be true. Moreover, when the support from visual information is limited in image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning. Based on these two intuitions, we propose a mechanism that can adaptively combine information from both modalities according to new image categories to be learned. Through a series of experiments, we show that by this adaptive combination of the two modalities, our model outperforms current uni-modality few-shot learning methods and modality-alignment methods by a large margin on all benchmarks and few-shot scenarios tested. Experiments also show that our model can effectively adjust its focus on the two modalities. The improvement in performance is particularly large when the number of shots is very small.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning methods have achieved major advances in areas such as speech, language and vision <ref type="bibr" target="#b24">[25]</ref>. These systems, however, usually require a large amount of labeled data, which can be impractical or expensive to acquire. Limited labeled data lead to overfitting and generalization issues in classical deep learning approaches. On the other hand, existing evidence suggests that human visual system is capable of effectively operating in small data regime: humans can learn new concepts from a very few samples, by leveraging prior knowledge and context <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>. The problem of learning new concepts with small number of labeled data points is usually referred to as few-shot learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref> (FSL).</p><p>Most approaches addressing few-shot learning are based on meta-learning paradigm <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b12">13]</ref>, a class of algorithms and models focusing on learning how to (quickly) learn new concepts. Metalearning approaches work by learning a parameterized function that embeds a variety of learning tasks and can generalize to new ones. Recent progress in few-shot image classification has primarily been made in the context of unimodal learning. In contrast to this, employing data from another modality can help when the data in the original modality is limited. For example, strong evidence supports the hypothesis that language helps recognizing new visual objects in toddlers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b44">45]</ref>. This suggests that semantic features from text can be a powerful source of information in the context of few-shot image classification.</p><p>Exploiting auxiliary modality (e.g., attributes, unlabeled text corpora) to help image classification when data from visual modality is limited, have been mostly driven by zero-shot learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref> (ZSL). ZSL aims at recognizing categories whose instances have not been seen during training. In contrast to few-shot learning, there is no small number of labeled samples from the original modality to help recognize new categories. Therefore, most approaches consist of aligning the two modalities during training. Through this modality-alignment, the modalities are mapped together and forced to have the same semantic structure. This way, knowledge from auxiliary modality is transferred to the visual side for new categories at test time <ref type="bibr" target="#b8">[9]</ref>.</p><p>However, visual and semantic feature spaces have heterogeneous structures by definition. For certain concepts, visual features might be richer and more discriminative than text ones. While for others, the inverse might be true. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates this remark. Moreover, when the number of support images from visual side is very small, information provided from this modality tend to be noisy and local. On the contrary, semantic representations (learned from large unsupervised text corpora) can act as more general prior knowledge and context to help learning. Therefore, instead of aligning the two modalities (to transfer knowledge to the visual modality), for few-shot learning in which information are provided from both modalities during test, it is better to treat them as two independent knowledge sources and adaptively exploit both modalities according to different scenarios. Towards this end, we propose Adaptive Modality Mixture Mechanism (AM3), an approach that adaptively and selectively combines information from two modalities, visual and semantic, for few-shot learning.</p><p>AM3 is built on top of metric-based meta-learning approaches. These approaches perform classification by comparing distances in a learned metric space (from visual data). On the top of that, our method also leverages text information to improve classification accuracy. AM3 performs classification in an adaptive convex combination of the two distinctive representation spaces with respect to image categories. With this mechanism, AM3 can leverage the benefits from both spaces and adjust its focus accordingly. For cases like <ref type="figure" target="#fig_0">Figure 1</ref>(Left), AM3 focuses more on the semantic modality to obtain general context information. While for cases like <ref type="figure" target="#fig_0">Figure 1</ref>(Right), AM3 focuses more on the visual modality to capture rich local visual details to learn new concepts.</p><p>Our main contributions can be summarized as follows: (i) we propose adaptive modality mixture mechanism (AM3) for cross-modal few-shot classification. AM3 adapts to few-shot learning better than modality-alignment methods by adaptively mixing the semantic structures of the two modalities. (ii) We show that our method achieves considerable boost in performance over different metric-based meta-learning approaches. (iii) AM3 outperforms by a considerable margin current (single-modality and cross-modality) state of the art in few-shot classification on different datasets and different number of shots. (iv) We perform quantitative investigations to verify that our model can effectively adjust its focus on the two modalities according to different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Few-shot learning. Meta-learning has a prominent history in machine learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">52]</ref>. Due to advances in representation learning methods <ref type="bibr" target="#b10">[11]</ref> and the creation of new few-shot learning datasets <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref>, many deep meta-learning approaches have been applied to address the few-shot learning problem . These methods can be roughly divided into two main types: metric-based and gradient-based approaches.</p><p>Metric-based approaches aim at learning representations that minimize intra-class distances while maximizing the distance between different classes. These approaches rely on an episodic training framework: the model is trained with sub-tasks (episodes) in which there are only a few training samples for each category. For example, matching networks <ref type="bibr" target="#b52">[53]</ref> follows a simple nearest neighbour framework. In each episode, it uses an attention mechanism (over the encoded support) as a similarity measure for one-shot classification.</p><p>In prototypical networks <ref type="bibr" target="#b46">[47]</ref>, a metric space is learned where embeddings of queries of one category are close to the centroid (or prototype) of supports of the same category, and far away from centroids of other classes in the episode. Due to the simplicity and good performance of this approach, many methods extended this work. For instance, Ren et al. <ref type="bibr" target="#b38">[39]</ref> propose a semi-supervised few-shot learning approach and show that leveraging unlabeled samples outperform purely supervised prototypical networks. Wang et al. <ref type="bibr" target="#b53">[54]</ref> propose to augment the support set by generating hallucinated examples. Task-dependent adaptive metric (TADAM) <ref type="bibr" target="#b34">[35]</ref> relies on conditional batch normalization <ref type="bibr" target="#b4">[5]</ref> to provide task adaptation (based on task representations encoded by visual features) to learn a taskdependent metric space.</p><p>Gradient-based meta-learning methods aim at training models that can generalize well to new tasks with only a few fine-tuning updates. Most these methods are built on top of model-agnostic metalearning (MAML) framework <ref type="bibr" target="#b6">[7]</ref>. Given the universality of MAML, many follow-up works were recently proposed to improve its performance on few-shot learning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21]</ref>. Kim et al. <ref type="bibr" target="#b17">[18]</ref> and Finn et al. <ref type="bibr" target="#b7">[8]</ref> propose a probabilistic extension to MAML trained with variational approximation. Conditional class-aware meta-learning (CAML) <ref type="bibr" target="#b15">[16]</ref> conditionally transforms embeddings based on a metric space that is trained with prototypical networks to capture inter-class dependencies. Latent embedding optimization (LEO) <ref type="bibr" target="#b40">[41]</ref> aims to tackle MAML's problem of only using a few updates on a low data regime to train models in a high dimensional parameter space. The model employs a low-dimensional latent model embedding space for update and then decodes the actual model parameters from the low-dimensional latent representations. This simple yet powerful approach achieves current state of the art result in different few-shot classification benchmarks. Other metalearning approaches for few-shot learning include using memory architecture to either store exemplar training samples <ref type="bibr" target="#b41">[42]</ref> or to directly encode fast adaptation algorithm <ref type="bibr" target="#b37">[38]</ref>. Mishra et al. <ref type="bibr" target="#b31">[32]</ref> use temporal convolution to achieve the same goal. Current approaches mentioned above rely solely on visual features for few-shot classification. Our contribution is orthogonal to current metric-based approaches and can be integrated into them to boost performance in few-shot image classification.</p><p>Zero-shot learning. Current ZSL methods rely mostly on visual-auxiliary modality alignment <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b57">58]</ref>. In these methods, samples for the same class from the two modalities are mapped together so that the two modalities obtain the same semantic structure. There are three main families of modality alignment methods: representation space alignment, representation distribution alignment and data synthetic alignment.</p><p>Representation space alignment methods either map the visual representation space to the semantic representation space <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b8">9]</ref>, or map the semantic space to the visual space <ref type="bibr" target="#b58">[59]</ref>. Distribution alignment methods focus on making the alignment of the two modalities more robust and balanced to unseen data <ref type="bibr" target="#b43">[44]</ref>. ReViSE <ref type="bibr" target="#b13">[14]</ref> minimizes maximum mean discrepancy (MMD) of the distributions of the two representation spaces to align them. CADA-VAE <ref type="bibr" target="#b43">[44]</ref> uses two VAEs <ref type="bibr" target="#b18">[19]</ref> to embed information for both modalities and align the distribution of the two latent spaces. Data synthetic methods rely on generative models to generate image or image feature as data augmentation <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b53">54]</ref> for unseen data to train the mapping function for more robust alignment. ZSL does not have access to any visual information when learning new concepts. Therefore, ZSL models have no choice but to align the two modalities. This way, during test the image query can be directly compared to auxiliary information for classification <ref type="bibr" target="#b58">[59]</ref>. Few-shot learning, on the other hand, has access to a small amount of support images in the original modality during test. This makes alignment methods from ZSL seem unnecessary and too rigid for FSL. For few-shot learning, it would be better if we could preserve the distinct structures of both modalities and adaptively combine them for classification according to different scenarios. In Section 4 we show that by doing so, AM3 outperforms directly applying modality alignment methods for few-shot learning by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we explain how AM3 adaptively leverages text data to improve few-shot image classification. We start with a brief explanation of episodic training for few-shot learning and a summary of prototypical networks followed by the description of the proposed adaptive modality mixture mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Episodic Training</head><p>Few-shot learning models are trained on a labeled dataset D train and tested on D test . The class sets are disjoint between D train and D test . The test set has only a few labeled samples per category. Most successful approaches rely on an episodic training paradigm: the few shot regime faced at test time is simulated by sampling small samples from the large labeled set D train during training.</p><p>In general, models are trained on K-shot, N -way episodes. Each episode e is created by first sampling N categories from the training set and then sampling two sets of images from these categories:</p><formula xml:id="formula_0">(i) the support set S e = {(s i , y i )} N ×K i=1</formula><p>containing K examples for each of the N categories and (ii) the query set Q e = {(q j , y j )} Q j=1 containing different examples from the same N categories. The episodic training for few-shot classification is achieved by minimizing, for each episode, the loss of the prediction on samples in query set, given the support set. The model is a parameterized function and the loss is the negative loglikelihood of the true class of each query sample:</p><formula xml:id="formula_1">L(θ) = E (Se,Qe) − Qe t=1 log p θ (y t |q t , S e ) ,<label>(1)</label></formula><p>where (q t , y t ) ∈ Q e and S e are, respectively, the sampled query and support set at episode e and θ are the parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Prototypical Networks</head><p>We build our model on top of metric-based meta-learning methods. We choose prototypical network <ref type="bibr" target="#b46">[47]</ref> for explaining our model due to its simplicity. We note, however, that the proposed method can potentially be applied to any metric-based approach.</p><p>Prototypical networks use the support set to compute a centroid (prototype) for each category (in the sampled episode) and query samples are classified based on the distance to each prototype. The model is a convolutional neural network <ref type="bibr" target="#b25">[26]</ref> f : R nv → R np , parameterized by θ f , that learns a n p -dimensional space where samples of the same category are close and those of different categories are far apart.</p><p>For every episode e, each embedding prototype p c (of category c) is computed by averaging the embeddings of all support samples of class c:</p><formula xml:id="formula_2">p c = 1 |S c e | (si,yi)∈S c e f (s i ) ,<label>(2)</label></formula><p>where S c e ⊂ S e is the subset of support belonging to class c. The model produces a distribution over the N categories of the episode based on a softmax <ref type="bibr" target="#b3">[4]</ref> over (negative) distances d of the embedding of the query q t (from category c) to the embedded prototypes:</p><formula xml:id="formula_3">p(y = c|q t , S e , θ) = exp(−d(f (q t ), p c )) k exp(−d(f (q t ), p k )) .<label>(3)</label></formula><p>We consider d to be the Euclidean distance. The model is trained by minimizing Equation 1 and the parameters are updated with stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Modality Mixture Mechanism</head><p>The information contained in semantic concepts can significantly differ from visual contents. For instance, 'Siberian husky' and 'wolf', or 'komondor' and 'mop', might be difficult to discriminate with visual features, but might be easier to discriminate with language semantic features.</p><p>w c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; p c 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; S c e &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;  = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; g &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; f &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; prototypes convex combination h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; e c &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; In zero-shot learning, where no visual information is given at test time (that is, the support set is void), algorithms need to solely rely on an auxiliary (e.g., text) modality. On the other extreme, when the number of labeled image samples is large, neural network models tend to ignore the auxiliary modality as it is able to generalize well with large number of samples <ref type="bibr" target="#b19">[20]</ref>.</p><formula xml:id="formula_4">{'dog'} W &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V h N 1 w W N h w k 0 Z p v U X U C L B p c h 7 j H s = " &gt; A A A B 8 n i c b V D L S s N A F L 3 x W e u r 6 t J N s A i u S i K C L o t u X F a w D 0 h D m U w n 7 d D J T J i 5 E U r o Z 7 h x o Y h b v 8 a d f + O k z U J b D w w c z r m X O f d E q e A G P e / b W V v f 2 N z a r u x U d / f 2 D w 5 r R 8 c d o z J N W Z s q o X Q v I o Y J L l k b O Q r W S z U j S S R Y N 5 r c F X 7 3 i W n D l X z E a c r C h I w k j z k l a K W g n x A c U y L y 7 m x Q q 3 s N b w 5 3 l f g l q U O J 1 q D 2 1 R 8 q m i V M I h X E m M D 3 U g x z o p F T w W b V f m Z Y S u i E j F h g q S Q J M 2 E + j z x z z 6 0 y d G O l 7 Z P o z t X f G z l J j J k</formula><formula xml:id="formula_5">V h N 1 w W N h w k 0 Z p v U X U C L B p c h 7 j H s = " &gt; A A A B 8 n i c b V D L S s N A F L 3 x W e u r 6 t J N s A i u S i K C L o t u X F a w D 0 h D m U w n 7 d D J T J i 5 E U r o Z 7 h x o Y h b v 8 a d f + O k z U J b D w w c z r m X O f d E q e A G P e / b W V v f 2 N z a r u x U d / f 2 D w 5 r R 8 c d o z J N W Z s q o X Q v I o Y J L l k b O Q r W S z U j S S R Y N 5 r c F X 7 3 i W n D l X z E a c r C h I w k j z k l a K W g n x A c U y L y 7 m x Q q 3 s N b w 5 3 l f g l q U O J 1 q D 2 1 R 8 q m i V M I h X E m M D 3 U g x z o p F T w W b V f m Z Y S u i E j F h g q S Q J M 2 E + j z x z z 6 0 y d G O l 7 Z P o z t X f G z l J j J k</formula><p>Few-shot learning scenario fits in between these two extremes. Thus, we hypothesize that both visual and semantic information can be useful for few-shot learning. Moreover, given that visual and semantic spaces have different structures, it is desirable that the proposed model exploits both modalities adaptively, given different scenarios. For example, when it meets objects like 'ping-pong balls' which has many visually similar counterparts, or when the number of shots is very small from the visual side, it relies more on text modality to distinguish them.</p><p>In AM3, we augment metric-based FSL methods to incorporate language structure learned by a wordembedding model W (pre-trained on unsupervised large text corpora), containing label embeddings of all categories in D train ∪ D test . In our model, we modify the prototype representation of each category by taking into account their label embeddings.</p><p>More specifically, we model the new prototype representation as a convex combination of the two modalities. That is, for each category c, the new prototype is computed as:</p><formula xml:id="formula_6">p c = λ c · p c + (1 − λ c ) · w c ,<label>(4)</label></formula><p>where λ c is the adaptive mixture coefficient (conditioned on the category) and w c = g(e c ) is a transformed version of the label embedding for class c. The representation e c is the pre-trained word embedding of label c from W. This transformation g : R nw → R np , parameterized by θ g , is important to guarantee that both modalities lie on the space R np of the same dimension and can be combined. The coefficient λ c is conditioned on category and calculated as follows:</p><formula xml:id="formula_7">λ c = 1 1 + exp(−h(w c )) ,<label>(5)</label></formula><p>where h is the adaptive mixing network, with parameters θ h . <ref type="figure" target="#fig_3">Figure 2</ref>(left) illustrates the proposed model. The mixing coefficient λ c can be conditioned on different variables. In Appendix F we show how performance changes when the mixing coefficient is conditioned on different variables.</p><p>The training procedure is similar to that of the original prototypical networks. However, the distances d (used to calculate the distribution over classes for every image query) are between the query and the cross-modal prototype p c :</p><formula xml:id="formula_8">p θ (y = c|q t , S e , W) = exp(−d(f (q t ), p c )) k exp(−d(f (q t ), p k )) ,<label>(6)</label></formula><p>where θ = {θ f , θ g , θ h } is the set of parameters. Once again, the model is trained by minimizing Equation 1. Note that in this case the probability is also conditioned on the word embeddings W. <ref type="figure" target="#fig_3">Figure 2</ref>(right) illustrates an example on how the proposed method works. Algorithm 1, on supplementary material, shows the pseudocode for calculating the episode loss. We chose prototypical network <ref type="bibr" target="#b46">[47]</ref> for explaining our model due to its simplicity. We note, however, that AM3 can potentially be applied to any metric-based approach that calculates prototypical embeddings p c for categories. As shown in next section, we apply AM3 on both ProtoNets and TADAM <ref type="bibr" target="#b34">[35]</ref>. TADAM is a task-dependent metric-based few-shot learning method, which currently performs the best among all metric-based FSL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we compare our model, AM3, with three different types of baselines: uni-modality few-shot learning methods, modality-alignment methods and metric-based extensions of modalityalignment methods. We show that AM3 outperforms the state of the art of each family of baselines. We also verify the adaptiveness of AM3 through quantitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We conduct main experiments with two widely used few-shot learning datasets: miniImageNet <ref type="bibr" target="#b52">[53]</ref> and tieredImageNet <ref type="bibr" target="#b38">[39]</ref>. We also experiment on CUB-200 <ref type="bibr" target="#b54">[55]</ref>, a widely used zero-shot learning dataset. We evaluate on this dataset to provide a more direct comparison with modality-alignment methods. This is because most modality-alignment methods have no published results on few-shot datasets. We use GloVe <ref type="bibr" target="#b36">[37]</ref> to extract the word embeddings for the category labels of the two image few-shot learning data sets. The embeddings are trained with large unsupervised text corpora.</p><p>More details about the three datasets can be found in Appendix B.</p><p>Baselines. We compare AM3 with three family of methods. The first is uni-modality few-shot learning methods such as MAML <ref type="bibr" target="#b6">[7]</ref>, LEO <ref type="bibr" target="#b40">[41]</ref>, Prototypical Nets <ref type="bibr" target="#b46">[47]</ref> and TADAM <ref type="bibr" target="#b34">[35]</ref>. LEO achieves current state of the art among uni-modality methods. The second fold is modality alignment methods. CADA-VAE <ref type="bibr" target="#b43">[44]</ref>, among them, has the best published results on both zero and few-shot learning. To better extend modality alignment methods to few-shot setting, we also apply the metricbased loss and the episode training of ProtoNets on their visual side to build a visual representation space that better fits few-shot scenario. This leads to the third fold baseline, modality alignment methods extended to metric-based FSL.</p><p>Details of baseline implementations can be found in Appendix C.</p><p>AM3 Implementation. We test AM3 with two backbone metric-based few-shot learning methods: ProtoNets and TADAM. In our experiments, we use the stronger ProtoNets implementation of <ref type="bibr" target="#b34">[35]</ref>, which we call ProtoNets++. Prior to AM3, TADAM achieves the current state of the art among all metric-based few-shot learning methods. For details on network architectures, training and evaluation procedures, see Apprendix D. Source code is released at https://github.com/ElementAI/am3. <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> show classification accuracy on miniImageNet and on tieredImageNet, respectively. We conclude multiple results from these experiments. First, AM3 outperforms its backbone methods by a large margin in all cases tested. This indicates that when properly employed, text modality can be used to boost performance in metric-based few-shot learning framework very effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Second, AM3 (with TADAM backbone) achieves results superior to current state of the art (in both single modality FSL and modality alignment methods). The margin in performance is particularly remarkable in the 1-shot scenario. The margin of AM3 w.r.t. uni-modality methods is larger with smaller number of shots. This indicates that the lower the visual content is, the more important semantic information is for classification. Moreover, the margin of AM3 w.r.t. modality alignment methods is larger with smaller number of shots. This indicates that the adaptiveness of AM3 would be more effective when the visual modality provides less information. A more detailed analysis about the adaptiveness of AM3 is provided in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Test Accuracy 5-way 1-shot 5-way 5-shot 5-way 10-shot</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uni-modality few-shot learning baselines</head><p>Matching Network <ref type="bibr" target="#b52">[53]</ref> 43.56 ± 0.84% 55.31 ± 0.73% -Prototypical Network <ref type="bibr" target="#b46">[47]</ref> 49.42 ± 0.78% 68.20 ± 0.66% 74.30 ± 0.52% Discriminative k-shot <ref type="bibr" target="#b1">[2]</ref> 56.30 ± 0.40% 73.90 ± 0.30% 78.50 ± 0.00% Meta-Learner LSTM <ref type="bibr" target="#b37">[38]</ref> 43.44 ± 0.77% 60.60 ± 0.71% -MAML <ref type="bibr" target="#b6">[7]</ref> 48.70 ± 1.84% 63.11 ± 0.92% -ProtoNets w Soft k-Means <ref type="bibr" target="#b38">[39]</ref> 50.41 ± 0.31% 69.88 ± 0.20% -SNAIL <ref type="bibr" target="#b31">[32]</ref> 55.71 ± 0.99% 68.80 ± 0.92% -CAML <ref type="bibr" target="#b15">[16]</ref> 59.23 ± 0.99% 72.35 ± 0.71% -LEO <ref type="bibr" target="#b40">[41]</ref> 61.76 ± 0.08% 77. <ref type="bibr" target="#b58">59</ref>    <ref type="bibr" target="#b27">[28]</ref>.</p><p>Finally, it is also worth noting that all modality alignment baselines get a significant performance improvement when extended to metric-based, episodic, few-shot learning framework. However, most of modality alignment methods (original and extended), perform worse than current state-of-the-art uni-modality few-shot learning method. This indicates that although modality alignment methods are effective for cross-modality in ZSL, it does not fit few-shot scenario very much. One possible reason is that when aligning the two modalities, some information from both sides could be lost because two distinct structures are forced to align.</p><p>We also conducted few-shot learning experiments on CUB-200, a popular dataest for ZSL dataset, to better compare with published results of modality alignment methods. All the conclusion discussed above hold true on CUB-200. Moreover, we also conduct ZSL and generalized FSL experiments to verify the importance of the proposed adaptive mechanism. Results on on this dataset are shown in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adaptiveness Analysis</head><p>We argue that the adaptive mechanism is the main reason for the performance boosts observed in the previous section. We design an experiment to quantitatively verify that the adaptive mechanism of AM3 can adjust its focus on the two modalities reasonably and effectively. <ref type="figure" target="#fig_4">Figure 3</ref>(a) shows the accuracy of our model compared to the two backbones tested (ProtoNets++ and TADAM) on miniImageNet for 1-10 shot scenarios. It is clear from the plots that the gap between AM3 and the corresponding backbone gets reduced as the number of shots increases. <ref type="figure" target="#fig_4">Figure 3(b)</ref> shows the mean and std (over whole validation set) of the mixing coefficient λ for different shots and backbones.</p><p>First, we observe that the mean of λ correlates with number of shots. This means that AM3 weighs more on text modality (and less on visual one) as the number of shots (hence, the number of visual data points) decreases. This trend suggests that AM3 can automatically adjust its focus more to text modality to help classification when information from the visual side is very low. Second, we can also observe that the variance of λ (shown in <ref type="figure" target="#fig_4">Figure 3</ref>(b)) correlates with the performance gap of AM3 and its backbone methods (shown in <ref type="figure" target="#fig_4">Figure 3(a)</ref>). When the variance of λ decreases with the increase of number of shots, the performance gap also shrinks. This indicates that the adaptiveness of AM3 on category level plays a very important role for the performance boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a method that can adaptively and effectively leverage cross-modal information for few-shot classification. The proposed method, AM3, boosts the performance of metric-based approaches by a large margin on different datasets and settings. Moreover, by leveraging unsupervised textual data, AM3 outperforms state of the art on few-shot classification by a large margin. The textual semantic features are particularly helpful on the very low (visual) data regime (e.g. one-shot). We also conduct quantitative experiments to show that AM3 can reasonably and effectively adjust its focus on the two modalities.</p><p>than one word, the embedding is generated by averaging them. We also experimented with fastText embeddings <ref type="bibr" target="#b16">[17]</ref> and observed similar performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baselines</head><p>For modality alignment baselines, we follow CADA-VAE <ref type="bibr" target="#b43">[44]</ref>'s few-shot experimental setting. During training, we randomly sample N -shot images for the test classes, and add them in the training data to train the alignment model. During test, we compare the image query and the class embedding candidates in the aligned space to make decisions as in ZSL and GZSL.</p><p>For the meta-learning extensions of modality alignment methods, instead of including the N -shot images into training data, we follow the standard episode training (explained in Section 3) of metricbased meta-learning approach and train models only with samples from training classes. Moreover, during training, we add an additional loss illustrated in Equation 1 and 3, to ensure the metric space learned on the visual side matching the few-shot test scenario. At test, we employ the standard few-shot testing approach (described in Appendix D) and calculate the prototype representations of test classes as follows:</p><formula xml:id="formula_9">p c = Σ i r c i + w c N + 1 ,<label>(7)</label></formula><p>where r i is the representation of the i-th support image. For both training and test, we need a visual representation space to calculate prototype representations. For DeViSE, they are calculated in its visual space before the transformer <ref type="bibr" target="#b8">[9]</ref>. For both ReViSE and CADA-VAE, prototype representations are calculated in the latent space. For f-CLSWGAN, they are calculated in the discriminator's input space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation Details of AM3 Experiments</head><p>We model the visual feature extractor f with a ResNet-12 <ref type="bibr" target="#b11">[12]</ref>, which has shown to be very effective for few-shot classification <ref type="bibr" target="#b34">[35]</ref>. This network produces embeddings of dimension 512. We use this backbone in all the modality-alignment baselines mentioned above and in AM3 implementations (with both backbones). We call ProtoNets++ the prototypical network <ref type="bibr" target="#b46">[47]</ref> implementation with this more powerful backbone.</p><p>The semantic transformation g is a neural network with one hidden layer with 300 units which also outputs a 512-dimensional representation. The transformation h of the mixture mechanism also contains one hidden layer with 300 units and outputs a single scalar for λ c . On both g and h networks, we use ReLU non-linearity <ref type="bibr" target="#b9">[10]</ref> and dropout <ref type="bibr" target="#b48">[49]</ref> (we set the dropout coefficient to be 0.7 on miniImageNet and 0.9 on tieredImageNet).</p><p>The model is trained with stochastic gradient descent with momentum <ref type="bibr" target="#b50">[51]</ref>. We use an initial learning rate of 0.1 and a fixed momentum coefficient of 0.9. On miniImageNet, we train every model for 30,000 iterations and anneal the learning rate by a factor of ten at iterations 15,000, 17,500 and 19,000. On tieredImageNet, models are trained for 80,000 iterations and the learning rate is reduced by a factor of ten at iteration 40,000, 50,000, 60,000.</p><p>The training procedure composes a few-shot training batch from several tasks, where a task is a fixed selection of 5 classes. We found empirically that the best number of tasks per batch are 5,2 and 1 for 1-shot, 5-shot and 10-shot, respectively. The number of query per batch is 24 for 1-shot, 32 for 5-shot and 64 for 10-shot. All our experiments are evaluated following the standard approach of few-shot classification: we randomly sample 1,000 tasks from the test set each having 100 random query samples, and average the performance of the model on them.</p><p>All hyperparameters were chosen based on accuracy on validation set. All our results are reported with an average over five independent run (with a fixed architecture and different random seeds) and with 95% confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Results on CUB-200</head><p>We also conduct experiments on CUB-200 to better compare with modality-alignment baselines from ZSL. <ref type="table" target="#tab_3">Table 3</ref> shows the results. For 0-shot scenario, AM3 degrades to the simplest modality alignment method that maps the text semantic space to the visual space. Therefore, without the adaptive mechanism, AM3 performs roughly the same with DeViSE, which indicates that the adaptive mechanism play the main role on the performance boost we observed in FSL. The results on other few-shot cases on CUB-200 are consistent with the other two few-shot learning data sets.</p><p>We also conduct generalized few-shot learning experiments as reported for CADA-VAE in <ref type="bibr" target="#b43">[44]</ref> to compare AM3 with the published FSL results for CADA-VAE. <ref type="figure" target="#fig_5">Figure 4</ref> shows that AM3-ProtoNets outperforms CADA-VAE in every case tested. We consider as a metric the harmonic mean (H-acc) between the accuracy of seen and unseen classes, as defined in <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Model Test Accuracy   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Ablation study on the input of the adaptive mechanism</head><p>We also perform an ablation study to see how the adaptive mechanism performs with respect to different features. <ref type="table" target="#tab_5">Table 4</ref> shows results, on both datasets, of our method with three different inputs for the adaptive mixing network h: (i) the raw GloVe embedding (h(e)), (ii) the visual representation (h(p)) and (iii) a concatenation of both the query and the language embedding (h(q, w)).</p><p>We observe that conditioning on transformed GloVe features performs better than on the raw features. Also, conditioning on semantic features performs better than when conditioning on visual ones, suggesting that the former space has a more appropriate structure to the adaptive mechanism than the latter. Finally, we note that conditioning on the query and semantic embeddings helps with the ProtoNets++ backbone but not with TADAM.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Concepts have different visual and semantic feature space. (Left) Some categories may have similar visual features and dissimilar semantic features. (Right) Other can possess same semantic label but very distinct visual features. Our method adaptively exploits both modalities to improve classification performance in low-shot regime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>m k Z 0 s I p p l r x D / 8 4 I M 4 5 s w 5 z L N k E m 6 + C j O h I v K L e 5 3 h 1 w z i m J q C a G a 2 6 w u H R N N K N q W q r Y E f / n k V d K 5 b P i W P 1 z V m 7 d l H R U 4 h T O 4 A B + u o Q n 3 0 I I 2 U F D w D K / w 5 q D z 4 r w 7 H 4 v R N a f c O Y E / c D 5 / A J G + k W 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>m k Z 0 s I p p l r x D / 8 4 I M 4 5 s w 5 z L N k E m 6 + C j O h I v K L e 5 3 h 1 w z i m J q C a G a 2 6 w u H R N N K N q W q r Y E f / n k V d K 5 b P i W P 1 z V m 7 d l H R U 4 h T O 4 A B + u o Q n 3 0 I I 2 U F D w D K / w 5 q D z 4 r w 7 H 4 v R N a f c O Y E / c D 5 / A J G + k W 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V h N 1 w W N h w k 0 Z p v U X U C L B p c h 7 j H s = " &gt; A A A B 8 n i c b V D L S s N A F L 3 x W e u r 6 t J N s A i u S i K C L o t u X F a w D 0 h D m U w n 7 d D J T J i 5E U r o Z 7 h x o Y h b v 8 a d f + O k z U J b D w w c z r m X O f d E q e A G P e / b W V v f 2 N z a r u x U d / f 2 D w 5 r R 8 c d o z J N W Z s q o X Q v I o Y J L l k b O Q r W S z U j S S R Y N 5 r c F X 7 3 i W n D l X z E a c r C h I w k j z k l a K W g n x A c U y L y 7 m x Q q 3 s N b w 5 3 l f g l q U O J 1 q D 2 1 R 8 q m i V M I h X E m M D 3 U g x z o p F T w W b V f m Z Y S u i E j F h g q S Q J M 2 E + j z x z z 6 0 y d G O l 7 Z P o z t X f G zl J j J k m k Z 0 s I p p l r x D / 8 4 I M 4 5 s w 5 z L N k E m 6 + C j O h I v K L e 5 3 h 1 w z i m J q C a G a 2 6 w u H R N N K N q W q r Y E f / n k V d K 5 b P i W P 1 z V m 7 d l H R U 4 h T O 4 A B + u o Q n 3 0 I I 2 U F D w D K / w 5 q D z 4 r w 7 H 4 v R N a f c O Y E / c D 5 / A J G + k W 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V h N 1 w W N h w k 0 Z p v U X U C L B p c h 7 j H s = " &gt; A A A B 8 n i c b V D L S s N A F L 3 x W e u r 6 t J N s A i u S i K C L o t u X F a w D 0 h D m U w n 7 d D J T J i 5 E U r o Z 7 h x o Y h b v 8 a d f + O k z U J b D w w c z r m X O f d E q e A G P e / b W V v f 2 N z a r u x U d / f 2 D w 5 r R 8 c d o z J N W Z s q o X Q v I o Y J L l k b O Q r W S z U j S S R Y N 5 r c F X 7 3 i W n D l X z E a c r C h I w k j z k l a K W g n x A c U y L y 7 m x Q q 3 s N b w 5 3 l f g l q U O J 1 q D 2 1 R 8 q m i V M I h X E m M D 3 U g x z o p F T w W b V f m Z Y S u i E j F h g q S Q J M 2 E + j z x z z 6 0 y d G O l 7 Z P o z t X f G z l J j J k m k Z 0 s I p p l r x D / 8 4 I M 4 5 s w 5 z L N k E m 6 + C j O h I v K L e 5 3 h 1 w z i m J q C a G a 2 6 w u H R N N K N q W q r Y E f / n k V d K 5 b P i W P 1 z V m 7 d l H R U 4 h T O 4 A B + u o Q n 3 0 I I 2 U F D w D K / w 5 q D z 4 r w 7 H 4 v R N a f c O Y E / c D 5 / A J G + k W 0 = &lt; / l a t e x i t &gt; p c &lt; l a t e x i t s h a 1 _ b a s e 6 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>(Left) Adaptive modality mixture model. The final category prototype is a convex combination of the visual and the semantic feature representations. The mixing coefficient is conditioned on the semantic label embedding. (Right) Qualitative example of how AM3 works. Assume query sample q has category i. (a) The closest visual prototype to the query sample q is p j . (b) The semantic prototypes. (c) The mixture mechanism modify the positions of the prototypes, given the semantic embeddings. (d) After the update, the closest prototype to the query is now the one of the category i, correcting the classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>(a) Comparison of AM3 and its corresponding backbone for different number of shots (b) Average value of λ (over whole validation set) for different number of shot, considering both backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>H-acc of generalized few-shot learning on CUB-200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Few-shot classification accuracy on test split of miniImageNet. Results in the top use only visual features. Modality alignment baselines are shown on the middle and our results (and their backbones) on the bottom part.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">± 0.12%</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Modality alignment baselines</cell><cell></cell></row><row><cell>DeViSE [9]</cell><cell>37.43±0.42%</cell><cell cols="2">59.82±0.39%</cell><cell>66.50±0.28%</cell></row><row><cell>ReViSE [14]</cell><cell>43.20±0.87%</cell><cell cols="2">66.53±0.68%</cell><cell>72.60±0.66%</cell></row><row><cell>CBPL [29]</cell><cell>58.50±0.82%</cell><cell cols="2">75.62±0.61%</cell><cell>-</cell></row><row><cell>f-CLSWGAN [57]</cell><cell>53.29±0.82%</cell><cell cols="2">72.58±0.27%</cell><cell>73.49±0.29%</cell></row><row><cell>CADA-VAE [44]</cell><cell>58.92±1.36%</cell><cell cols="2">73.46±1.08%</cell><cell>76.83±0.98%</cell></row><row><cell cols="5">Modality alignment baselines extended to metric-based FSL framework</cell></row><row><cell>DeViSE-FSL ReViSE-FSL f-CLSWGAN-FSL CADA-VAE-FSL</cell><cell cols="4">56.99 ± 1.33% 72.63 ± 0.72% 76.70 ± 0.53% 57.23 ± 0.76% 73.85 ± 0.63% 77.21 ± 0.31% 58.47 ± 0.71% 72.23 ± 0.45% 76.90 ± 0.38% 61.59 ± 0.84% 75.63 ± 0.52% 79.57 ± 0.28%</cell></row><row><cell></cell><cell cols="2">AM3 and its backbones</cell><cell></cell></row><row><cell>ProtoNets++ AM3-ProtoNets++ TADAM [35] AM3-TADAM</cell><cell cols="4">56.52 ± 0.45% 74.28 ± 0.20% 78.31 ± 0.44% 65.21 ± 0.30% 75.20 ± 0.27% 78.52 ± 0.28% 58.56 ± 0.39% 76.65 ± 0.38% 80.83 ± 0.37% 65.30 ± 0.49% 78.10 ± 0.36% 81.57 ± 0.47 %</cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Test Accuracy</cell></row><row><cell></cell><cell cols="2">5-way 1-shot</cell><cell cols="2">5-way 5-shot</cell></row><row><cell cols="3">Uni-modality few-shot learning baselines</cell><cell></cell></row><row><cell cols="5">MAML  † [7] Proto. Nets with Soft k-Means [39] 53.31 ± 0.89% 72.69 ± 0.74% 51.67 ± 1.81% 70.30 ± 0.08% Relation Net  † [50] 54.48 ± 0.93% 71.32 ± 0.78% Transductive Prop. Nets [28] 54.48 ± 0.93% 71.32 ± 0.78% LEO [41] 66.33 ± 0.05% 81.44 ± 0.09%</cell></row><row><cell></cell><cell cols="2">Modality alignment baselines</cell><cell></cell></row><row><cell>DeViSE [9]</cell><cell cols="2">49.05±0.92%</cell><cell cols="2">68.27±0.73%</cell></row><row><cell>ReViSE [14]</cell><cell cols="2">52.40±0.46%</cell><cell cols="2">69.92±0.59%</cell></row><row><cell>CADA-VAE [44]</cell><cell cols="2">58.92±1.36%</cell><cell cols="2">73.46±1.08%</cell></row><row><cell cols="5">Modality alignment baselines extended to metric-based FSL framework</cell></row><row><cell>DeViSE-FSL ReViSE-FSL CADA-VAE-FSL</cell><cell cols="4">61.78 ± 0.43% 77.17 ± 0.81% 62.77 ± 0.31% 77.27 ± 0.42% 63.16 ± 0.93% 78.86 ± 0.31%</cell></row><row><cell></cell><cell cols="2">AM3 and its backbones</cell><cell></cell></row><row><cell>ProtoNets++ AM3-ProtoNets++ TADAM [35] AM3-TADAM</cell><cell cols="4">58.47 ± 0.64% 78.41 ± 0.41% 67.23 ± 0.34% 78.95 ± 0.22% 62.13 ± 0.31% 81.92 ± 0.30% 69.08 ± 0.47% 82.58 ± 0.31%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Few-shot classification accuracy on test split of tieredImageNet. Results in the top use only visual features. Modality alignment baselines are shown in the middle and our results (and their backbones) in the bottom part.</figDesc><table /><note>† deeper net, evaluated in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Few-shot classification accuracy on unseen-test split of CUB-200.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance of our method when the adaptive mixing network is conditioned on different features. Last row is the original model.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Algorithm for Episode Loss Algorithm 1: Training episode loss computation for adaptive cross-modality few-shot learning. M is the total number of classes in the training set, N is the number of classes in every episode, K is the number of supports for each class, K Q is the number of queries for each class, W is the pretrained label embedding dictionary.</p><p>Input:</p><p>] end for end for B Descriptions of data sets miniImageNet. This dataset is a subset of ImageNet ILSVRC12 dataset <ref type="bibr" target="#b39">[40]</ref>. It contains 100 randomly sampled categories, each with 600 images of size 84 × 84. For fair comparison with other methods, we use the same split proposed by Ravi et al. <ref type="bibr" target="#b37">[38]</ref>, which contains 64 categories for training, 16 for validation and 20 for test.</p><p>tieredImageNet. This dataset is a larger subset of ImageNet than miniImageNet. It contains 34 high-level category nodes (779,165 images in total) that are split in 20 for training, 6 for validation and 8 for test. This leads to 351 actual categories for training, 97 for validation and 160 for the test. There are more than 1,000 images for each class. The train/val/test split is done according to their higher-level label hierarchy. According to Ren et al. <ref type="bibr" target="#b38">[39]</ref>, splitting near the root of ImageNet hierarchy results in a more realistic (and challenging) scenario with training and test categories that are less similar. <ref type="bibr" target="#b54">[55]</ref> is a fine-grained and medium scale dataset with respect to both number of images and number of classes, i.e. 11,788 images from 200 different types of birds annotated with 312 attributes <ref type="bibr" target="#b57">[58]</ref>. We chose the split proposed by Xian et al. <ref type="bibr" target="#b57">[58]</ref>. We used the 312-dimensional hand-crafted attribution as the semantic modality for fair comparison with other published modality alignment methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-200. Caltech-UCSD-Birds 200-2011 (CUB-200)</head><p>Word embeddings. We use GloVe <ref type="bibr" target="#b36">[37]</ref> to extract the semantic embeddings for the category labels. GloVe is an unsupervised approach based on word-word co-occurrence statistics from large text corpora. We use the Common Crawl version trained on 840B tokens. The embeddings are of dimension 300. When a category has multiple (synonym) annotations, we consider the first one. If the first one is not present in GloVe's vocabulary we use the second. If there is no annotation in GloVe's vocabulary for a category (4 cases in tieredImageNet), we randomly sample each dimension of the embedding from a uniform distribution with the range (-1, 1). If an annotation contains more</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-generalization: learning novel classes from a single example by feature replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative k-shot learning using probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Bartlomiej Swikatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the optimization of a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Cloutier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Gecsei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Optimality in Biological and Artificial Networks</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bridle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing: Algorithms, Architectures and Applications</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Harm de Vries, Aaron Courville, and Yoshua Bengio. Feature-wise transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strub</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Distill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object classification from a single example utilizing class relevance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter R Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning robust visualsemantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Kang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On beyond zebra: the relation of linguistic and visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jackendoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to learn with conditional class dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farshid</forename><surname>Varno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hérve</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fasttext</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousmane</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Boquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonchang</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<title level="m">Deep prior. NIPS workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Cognitive Science Society</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The importance of shape in early lexical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive development</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-data learning of new tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Zero and few shot learning with semantic feature synthesis and competitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiechao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08332</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Categorization and naming in children: Problems of induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellen M Markman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A generative model for zero shot learning using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Shiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boris N Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sygnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning. on learning now to learn: The meta-meta-meta</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technische Universitat Munchen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Diploma thesis</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Schönfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The development of embodied cognition: Six lessons from babies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial life</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A developmental approach to machine learning? Frontiers in psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><forename type="middle">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A generative adversarial approach for zero-shot learning from noisy texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
