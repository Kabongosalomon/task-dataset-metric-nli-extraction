<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
							<email>youshan@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Huang</surname></persName>
							<email>huangtao@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of CST</orgName>
								<orgName type="laboratory">Dian Group</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Yang</surname></persName>
							<email>yangmingmin@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
							<email>wangfei@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<email>qianchen@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training a supernet matters for one-shot neural architecture search (NAS) methods since it serves as a basic performance estimator for different architectures (paths). Current methods mainly hold the assumption that a supernet should give a reasonable ranking over all paths. They thus treat all paths equally, and spare much effort to train paths. However, it is harsh for a single supernet to evaluate accurately on such a huge-scale search space (e.g., 7 21 ). In this paper, instead of covering all paths, we ease the burden of supernet by encouraging it to focus more on evaluation of those potentially-good ones, which are identified using a surrogate portion of validation data. Concretely, during training, we propose a multi-path sampling strategy with rejection, and greedily filter the weak paths. The training efficiency is thus boosted since the training space has been greedily shrunk from all paths to those potentiallygood ones. Moreover, we further adopt an exploration and exploitation policy by introducing an empirical candidate path pool. Our proposed method GreedyNAS is easy-tofollow, and experimental results on ImageNet dataset indicate that it can achieve better Top-1 accuracy under same search space and FLOPs or latency level, but with only ∼60% of supernet training cost. By searching on a larger space, our GreedyNAS can also obtain new state-of-the-art architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>By dint of automatic feature engineering, deep neural networks (DNNs) have achieved remarkable success in various computer vision tasks, such as image classification <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>, visual generation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, image retrieval <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref> and semantic comprehension <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>. In contrast, neural architecture search (NAS) aims at automatically learning the network architecture to further boost the performance for target tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19]</ref>. Nevertheless, previous NAS methods in general suffer from * Equal contributions. huge computation budget, such as 2000 GPU days of reinforcement learning <ref type="bibr" target="#b42">[43]</ref> and 3150 GPU days of evolution <ref type="bibr" target="#b25">[26]</ref> with hundreds of GPUs.</p><p>Current One-shot NAS methods boost the search efficiency by modeling NAS as a one-shot training process of an over-parameterized supernet. As a result, various architectures can be derived from the supernet, and share the same weights. For example, DARTS <ref type="bibr" target="#b20">[21]</ref> and its variants <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1]</ref> parameterize the supernet with an additional categorical distribution for indicating what operations we want to keep. In contrast, recent single path methods adopt a non-parametric architecture modeling, and split the searching into two consecutive stages, i.e., supernet training and architecture sampling. For training supernet, only a single path consisting of a single operation choice is activated and gets optimized by regular gradient-based optimizers. After the supernet is trained well, it is regarded as a performance estimator for all architectures (i.e., paths). Then the optimal architecture can be searched using a hold-out validation dataset via random search <ref type="bibr" target="#b15">[16]</ref> or (reinforced) evolutionary <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4]</ref> algorithms under specified hardware constraint (e.g., FLOPs and latency). As only one path is activated for training, the memory cost coheres with that of traditional network training, and scales well on large-scale datasets (e.g., ImageNet <ref type="bibr" target="#b26">[27]</ref>).</p><p>Supernet matters for it serves as a fundamental performance estimator of different architectures (paths). Current methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> hold the assumption that the supernet should estimate the (relative) performance accurately for all paths, and thus all paths are treated equally and trained simultaneously. However, the paths contained in the supernet are of fairly huge scale (e.g., 7 <ref type="bibr" target="#b20">21</ref> ). Hence it can be harsh for a single supernet to evaluate and give reasonable ranking on such a quantity of paths at the same time. In fact, the ultimate aim of supernet is only to identify a bunch of optimal paths. But the huge search space implies significant variance and variety of paths; there exist many architectures of inferior quality in terms of accuracy performance. <ref type="bibr" target="#b0">1</ref> Since the weights of all paths are highly shared, if a weak path is sampled and gets trained, it would disturb the weights of those potentially-good paths. This disturbance will undermine their eventual performance estimation and affect the searched optimal architecture accordingly. The supernet is thus not supposed to care much on these weak paths and get updated for them. Besides, training on those weak paths actually involves unnecessary update of weights, and slows down the training efficiency more or less.</p><p>In this paper, we ease the training burden by encouraging a greedy supernet. A greedy supernet is capable of shifting its focus on performance estimation of those potentiallygood paths instead of all paths. Concretely, during the supernet training, we propose a multi-path sampling strategy with rejection to filter the weak paths, so the supernet will greedily train those potentially-good paths. This path filtering can be efficiently implemented via evaluation using a surrogate portion of validation dataset, without harming the computation cost too much. Moreover, we also adopt an exploration and exploitation policy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref> by introducing a candidate pool, which dynamically tracks those potentiallygood paths discovered during training. In this way, the supernet improves its training efficiency by switching its training space from all paths into those potentially-good ones, and further into candidate pool by sampling from it, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We implement our proposed method GreedyNAS on the large-scale benchmark ImageNet dataset <ref type="bibr" target="#b26">[27]</ref>, and extensive experimental results indicate our superiority in terms of accuracy performance and supernet training efficiency. For example, with the same search space, our method can achieve higher Top-1 accuracy than that of other comparison methods under the same FLOPs or latency level, but reduces approximate 40% of supernet training cost. By searching on a larger space, we can also obtain new stateof-the-art architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>One-shot NAS methods mainly aim to train an overparameterized network (a.k.a supernet) that comprises all architectures (paths), which share the same weights mutually. Then the optimal architecture can be derived or searched from the supernet. There are mainly two categories of one-shot NAS methods <ref type="bibr" target="#b8">[9]</ref>, which differ in how the architectures are modeled and elaborated as follows.</p><p>Parameterized architectures. To use the gradientbased optimizers for direct searching, an real-valued categorical distribution (architecture parameter) is usually introduced in the supernet, and can be thus jointly learned with the supernet weights, such as DARTS <ref type="bibr" target="#b20">[21]</ref>, FBNet <ref type="bibr" target="#b37">[38]</ref> and MdeNAS <ref type="bibr" target="#b41">[42]</ref>. When the supernet training is finished, the optimal architecture can be induced by sampling from the categorical distribution. However, it may suffer from the huge GPU memory consumption. ProxylessNAS <ref type="bibr" target="#b0">[1]</ref> alleviates this issue by factorizing the searching into multiple binary selection tasks while Single-Path-NAS <ref type="bibr" target="#b28">[29]</ref> uses superkernels to encode all operation choices. Basically, they are difficult to integrate a hard hardware constraint (e.g., FLOPs and latency) during search but resort to relaxed regularization terms <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Sampled single-path architectures. By directly searching the discrete search space, the supernet is trained by sampling and optimizing a single path. The sampling can be uniform sampling <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> or multi-path sampling with fairness <ref type="bibr" target="#b3">[4]</ref>. After the supernet is trained, it is supposed to act as a performance estimator for different paths. And the optimal path can be searched by various searchers, such as random search and evolutionary algorithms <ref type="bibr" target="#b5">[6]</ref>. For example, ScarletNAS <ref type="bibr" target="#b2">[3]</ref> employs a multi-objective searcher <ref type="bibr" target="#b21">[22]</ref> to consider classification error, FLOPs and model size for better paths. Different to the previous parameterized methods, the hard hardware constraint can be easily integrated in the searchers. Our proposed method GreedyNAS is cast into this category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Rethinking path training of supernet</head><p>In Single-path One-shot NAS, we utilize an overparameterized supernet N with parameter Ω to substantialize a search space, which is formulated as a directed acyclic graph (DAG). In the DAG, feature maps act as the nodes, while the operations (or transformations) between feature maps are regarded as edges for connecting sequential nodes. Assume the supernet N has L layers, and each layer N l is allocated with O operation choices O = {o i }, which can be basic convolution, pooling, identity or different types of building blocks, such as MobileNetV2 block <ref type="bibr" target="#b27">[28]</ref> and Shuf-fleNetV2 block <ref type="bibr" target="#b22">[23]</ref>. Then each architecture (i.e., path) denoted as a can be represented by a tuple of size L, i.e., a = (o 1 , o 2 , ..., o L ) where o j ∈ O, ∀j = 1, 2, ..., L. As a result, the search space A is discrete, and there will be O L (e.g., 7 21 ) architectures in total, namely, |A| = O L .</p><p>Training supernet matters since it is expected to serve as a fundamental performance estimator. Due to the consideration of memory consumption, single-path NAS methods implement training by sampling a single path a from A, then the sampled paths are all optimized on the training data D tr . It can be formulated as minimizing an expected loss over the space A, i.e.,</p><formula xml:id="formula_0">Ω * = arg min Ω E a∼p(A) [ [L(ω a ; D tr )] ] ,<label>(1)</label></formula><p>where ω a refers to the parameter of path a, and p(A) is a discrete sampling distribution over A.</p><p>After the supernet N (Ω * ) is trained well, we can evaluate the quality of each path by calculating its (Top-1) accuracy (ACC) on the validation dataset D val , and the optimal path a * corresponds to the maximum ACC, i.e.,</p><formula xml:id="formula_1">a * = arg max a∈A ACC(ω * a , D val ),<label>(2)</label></formula><p>where ω * a ⊂ Ω * w.r.t. path a in the trained supernet N (Ω * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reshaping sampling distribution p(A)</head><p>Current methods assume that the supernet should provide a reasonable ranking over all architectures in A. Thus all paths a are treated equally, and optimized simultaneously <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>. Then the sampling distribution p(A) amounts to a uniform distribution p(A) = U (A) over A, i.e., p(a) = 1 |A| I(a ∈ A),</p><p>where I(·) is an indicator function. However, as previously discussed, it is a demanding requirement for the supernet to rank accurately for all paths at the same time. In the huge search space A, there might be some paths of inferior quality. Since the weights are highly shared in the same supernet, training on these weak paths does have negative influence on the evaluation of those potentially-good paths. To alleviate this disturbance, an intuitive idea is to block the training of these weak paths.</p><p>For simplifying the analysis, we assume the search space A can be partitioned into two subsets A good and A weak by an Oracle good but unknown supernet N o , where</p><formula xml:id="formula_3">A = A good A weak , A good A weak = ∅,<label>(4)</label></formula><p>and A good indicates the potentially-good paths while A weak is for weak paths, i.e.,</p><formula xml:id="formula_4">ACC(a, N o , D val ) ≥ ACC(b, N o , D val )<label>(5)</label></formula><p>holds for all a ∈ A good , b ∈ A weak on validation dataset D val . Then to screen the weak paths and ease the burden of the supernet training, we can just sample from the potentially-good paths A good instead of all paths A. </p><formula xml:id="formula_5">p(a; N o , D val ) = 1 |A good | I(a ∈ A good ).<label>(6)</label></formula><p>In this way, the supernet is expected to thoroughly get trained on the potentially-good paths and thus give decent performance ranking. Besides, since the valid search space has been shrunken from A into A good , the training efficiency of supernet is improved accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Greedy path filtering</head><p>Nevertheless, in the supernet training the Oracle supernet N o is unknown, thus we can not sample paths according to Eq.(6) since it relies on N o . In this paper, we propose to use greedy strategy and during training, current supernet N † is progressively regarded as a proxy of the Oracle N o . Thus during the supernet training, we greedily sample paths according to the reshaped sampling distribution given by current N † , namely, p(A) = U (A good ; N † , D val ). The sampled paths will get optimized, then the supernet is get updated and evolves to a decent performance estimator over A good .</p><p>However, a natural question arises: even given a supernet N † , how can we sample from the shaped distribution p(A) = U (A good ; N † , D val )? In other words, how can we accurately identify whether a path is from A good or A weak ? Note that the partition of A is determined by traversing all paths in A as Eq. <ref type="formula" target="#formula_4">(5)</ref>, which is not affordable in computation. Since we can not accurately know whether a single path is good or weak, to solve this issue, we propose a multi-path sampling strategy with rejection.</p><p>Suppose we uniformly sample a path from A, then it amounts to be sampled from A good with probability q = |A good |/|A|, and sampled from A weak with probability 1 − q. In this way, if we sample multiple paths independently at a time, we have the following results based on binomial distribution. Input: supernet N with parameter Ω,validation data D val , number of sampled multiple paths m, number of kept paths k, candidate pool P with sampling probability . <ref type="bibr">1:</ref> if without candidate pool P then 2:</p><formula xml:id="formula_6">sample m paths {a i } m i=1 i.i.d. w.r.t. a i ∼ U (A) 3: else 4: sample m paths {a i } m i=1 i.i.d. w.r.t. a i ∼ (1 − ) · U (A) + · U (P) 5: end if 6:</formula><p>randomly sample a batchD val in D val 7: evaluate the loss i of each path a i onD val 8: rank the paths by i , and get Top-k indexes {t i } k i=1 9: return k paths {a ti } k i=1 and filter the rest</p><formula xml:id="formula_7">paths are from A good with probability m j=k C j m q j (1 − q) m−j ,<label>(7)</label></formula><p>where q = |A good |/|A|.</p><p>From Theorem 1, we can see by sampling m paths, the probability that at least k paths are from A good is very high when the proportion of potentially-good paths q is medially large or k is medially small (see <ref type="figure" target="#fig_1">Figure 2</ref>). For example, if we conservatively assume 60% paths have the potential to be good (i.e., q = 0.6), we will have 83.38% confidence to say at least 5 out of 10 paths are sampled from A good . In this way, based on the definition of Eq.(4) and Eq.(5), we just rank the sampled m paths using validation data D val , keep the Top-k paths and reject the remaining paths.</p><p>However, ranking m paths involves calculation of ACC over all validation dataset D val as Eq. <ref type="formula" target="#formula_4">(5)</ref>, which is also computationally intensive during the supernet training. <ref type="bibr" target="#b1">2</ref> In fact, in our multi-path sampling, what we care about is the obtained ranking; we empirically find that it suffices to rank based on the loss (e.g., cross entropy loss for classification) over a surrogate subset of D val (e.g., 1k images on ImageNet dataset), denoted asD val . The consistency between this rank and that given by ACC on all D val is fairly significant. More details and analysis refer to the ablation studies in Section 5.3.1. Then the sampling works as Algorithm 1.</p><p>As a result, path filtering can be efficiently implemented for it can run in a simple feed-forward mode (e.g., eval() mode in Pytorch) on a small portion of validation data. In this sense, we block the weak paths greedily during the supernet training. And the validation dataD val acts as a rough filter to prevent the training of those low-quality or even harmful paths, so that the supernet can get sufficient training on those potentially-good ones. <ref type="bibr" target="#b1">2</ref> For example, the size of D val on ImageNet dataset is 50k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Greedy training of supernet.</head><p>Input: supernet N with parameter Ω, training data D tr , validation data D val , number of sampled multiple paths m, number of kept paths k, max iteration T , training data loader D 1: initialize candidate pool P = ∅, 2: set a Scheduler of pool sampling probability <ref type="bibr">3:</ref> for τ = 1, .., T /k do <ref type="bibr">4:</ref> get the pool sampling probability by Scheduler <ref type="bibr">5:</ref> sample k paths {a ti } k i=1 out of m paths using Algorithm 1 with pool sampling probability <ref type="bibr">6:</ref> update candidate pool P using {a ti } k i=1 7:</p><p>for i = 1, .., k do update the weights ω at i of path a ti using gradientbased optimizer <ref type="bibr">10:</ref> end for 11: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Approach: GreedyNAS</head><p>In this section, we formally illustrated our proposed NAS method (a.k.a. GreedyNAS) based on a greedy supernet. Our GreedyNAS is composed with three procedures, i.e., supernet training, searching paths and retraining the searched optimal path. The last retraining corresponds to conventional training a given network. We mainly elaborate the first two as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Greedy training of supernet</head><p>As previously discussed, we propose to maintain a greedy supernet during its training.</p><p>By doing this, we gradually approximate the sampling p(A) = U (A good ; N † , D val ) by keeping the Top-k paths and filtering the bottom m − k paths by evaluating usingD val . Then those weak paths are prevented from getting trained, which allows the supernet to focus more on those potentially-good paths and switch its training space from A into A good .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training with exploration and exploitation</head><p>After the greedy path filtering, we have actually identified some potentially-good paths, which amount to some empirically-good ones given by current supernet. Then to further improve the training efficiency, inspired by the Monte Carlo tree search <ref type="bibr" target="#b13">[14]</ref> and deep Q-learning (DQN) <ref type="bibr" target="#b23">[24]</ref>, we propose to train the supernet with an exploration and exploitation (E-E) strategy by reusing these paths.</p><p>Concretely, we introduce a candidate pool P to store the potentially-good paths discovered during training. Each path a is represented as a tuple of operation choices. Besides, each a i is also allocated with an evaluation loss i .</p><p>The candidate pool is thus formulated as a fixed-size ordered queue with priority . With more potentially-good paths involved, the candidate pool can be maintained by a min-heap structure in real time.</p><p>As a result, we can conservatively implement local search by sampling from the candidate pool since it consists of a smaller number (but promising) of paths. However, this greedy exploitation brings in the risks of losing path diversity for the training. In this way, we also favor a global search with the hope of probing other promising paths that are yet to be sampled and get trained, which can be easily fulfilled by uniform sampling from A. For achieve a balanced trade-off of exploration and exploitation, we adopt a typical -sampling policy, i.e., implementing uniform sampling both from A and pool P (line 4 of Algorithm 1),</p><formula xml:id="formula_8">a ∼ (1 − ) · U (A) + · U (P),<label>(8)</label></formula><p>where ∈ [0, 1] indicates the probability of sampling from the pool P. Note that candidate pool runs through the training process of supernet; however, it might be not reliable at first since the priority is calculated based on a much lesstrained supernet. In this case, we propose to actively anneal the pool sampling probability from 0 to a pre-defined level. In our experiment, we find = 0.8 will be a good option.</p><p>Training with exploration and exploitation encourages the supernet to refine the already-found good paths as well as probing new territory for more better paths. Besides, it actually also contributes to our greedy path filtering by improving our filtering confidence. Basically, the collected potentially-good paths can be regarded as a subset of A good , then sampling from P amounts to increasing the probability q of Theorem 1 into</p><formula xml:id="formula_9">q = + (1 − )|A good |/|A|,<label>(9)</label></formula><p>which refers to the proportion of potentially-good paths. For example, assume we evenly sample from P or A ( = 0.5), then the probability of sampling at least 5 good paths out of 10 paths will rise from 83.38% to 99.36% according to Theorem 1. Comparing reducing r = k/m to increase the sampling confidence, sampling with P is almost costneglectable since we only need to maintain a min-heap. The supernet thus gradually shifts its training from A good more to P, and the training efficiency will be further improved accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Stopping principle via candidate pool</head><p>Different to conventional networks, a supernet serves as a performance estimator and it is difficult to judge when it is trained well. Current single-path NAS methods control its training by manually specifying an maximum epoch number. In our GreedyNAS, however, we propose an adaptive stopping principle based on the candidate pool. Candidate pool P indicates a bunch of best empirical paths, and it is updated dynamically during the supernet training. In consequence, if a supernet is trained well, the pool P should tend to be steady. This steadiness can be measured by the update frequency π of candidate pool P, i.e.,</p><formula xml:id="formula_10">π := |P t P| |P| ≤ α,<label>(10)</label></formula><p>where P t refers to the old P in previous t iterations. Thus smaller π implies that fewer new paths are involved in the pool P within t iterations, and P is more steady. Given a certain tolerance level α 3 , when the update frequency π is less than α, we believe the supernet has been trained enough, and its training can be stopped accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Searching with candidate pool</head><p>After the supernet is trained, we can use supernet to evaluate the quality (ACC) of each path on validation dataset D val , and search the optimal path a * as Eq.(2). However, enumerating all paths in A is prohibitively computationintensive. One remedy is by dint of evolutionary algorithms <ref type="bibr" target="#b10">[11]</ref> or reinforced version (e.g., MoreMNAS <ref type="bibr" target="#b4">[5]</ref>), which takes the supernet as an off-the-shelf evaluator. In our paper, we adopt the multi-objective NSGA-II <ref type="bibr" target="#b5">[6]</ref> algorithm for searching, where the hardware constraint can be easily integrated in the evolution process. If a path violates the pre-defined hardware constraint (e.g., under 330 FLOPs), we just ditch it for good.</p><p>Besides, evolutionary algorithms need to initialize population with size N pop before implementing iterative mutation and crossover. Current methods usually random sample N pop paths under the constraint as initial population. In contrast, our method makes the initialization with the help of candidate pool P, and select its Top-N pop paths instead. As <ref type="figure" target="#fig_4">Figure 3</ref> shows, searching with candidate pool can boost the evolutionary performance for supplying a good initial population. The ACC of searched paths using candidate pool is on average higher than that using random initialization. More details of our searching algorithm refer to the supplementary materials. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Configuration and settings</head><p>Dataset. We conduct the architecture search on the challenging ImageNet dataset <ref type="bibr" target="#b26">[27]</ref>. As <ref type="bibr" target="#b0">[1]</ref>, we randomly sample 50,000 images (50 images per class) from training dateset as the validation dataset (|D val | = 50K), and the rest of training images are used for training. Moreover, we use the original validation dataset as the test dataset to report the accuracy performance.</p><p>Search space. Following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>, we adopt the same macro-structure of supernet for fair comparison as shown in <ref type="table">Table 5</ref> (see supplementary materials). Moreover, we use MobileNetV2 inverted bottleneck <ref type="bibr" target="#b27">[28]</ref> as the basic building block. For each building block, the convolutional kernel size is within {3, 5, 7} and expansion ratio is selected in {3, 6}. An identity block is also attached for flexible depth search. As a result, with 21 building blocks, the search space is of size (3 × 2 + 1) 21 = 7 <ref type="bibr" target="#b20">21</ref> . In addition, we also implement searching on a larger space by augmenting each building block with an squeeze-and-excitation (SE) option. The size of the larger search space is thus 13 <ref type="bibr" target="#b20">21</ref> .</p><p>Supernet training. For training the supernet, Algorithm 1 is adopted to sample 10 paths and filter 5 paths. We randomly sample 1000 images (1 image per class) from the validation dataset for evaluating paths in Algorithm 1. For training each path, we use a stochastic gradient descent (SGD) optimizer with momentum 0.9 and Nesterov acceleration. The learning rate is decayed with cosine annealing strategy from initial value 0.12. The batch size is 1024. As for candidate pool, we empirically find 1000 is a good option for pool size |P|, which approximates the amount of paths involved in one epoch. The candidate sampling probability is linearly increased from 0 to 0.8. Instead of specifying an epoch number <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4]</ref>, we use the proposed principle to stop the supernet training with tolerance α = 0.08.</p><p>Evolutionary searching. For searching with NSGA-II [6] algorithm, we set the population size as 50 and the number of generations as 20. The population is initialized by the candidate pool P while other comparison methods use random initialization. During searching, we use constraint of FLOPs or latency. All our experiments use Qualcomm® Snapdragon 855 mobile hardware development kit (HDK) to measure the latency.</p><p>Retraining. To train the obtained architecture, we use the same strategy as <ref type="bibr" target="#b0">[1]</ref> for search space without SE. As for the augmented search space, we adopt a RMSProp optimizer with 0.9 momentum as Mnasnet <ref type="bibr" target="#b29">[30]</ref>. Learning rate is increased from 0 to 0.064 in the first 5 epochs with batch size 512, and then decays 0.03 every 3 epochs. Besides, exponential moving average is also adopted with decay 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance comparison with state-of-the-art methods</head><p>Searching on same search space. For fair comparison, we first benchmark our GreedyNAS to the same search space as <ref type="bibr" target="#b0">[1]</ref> to evaluate our superiority to other Single-path One-shot NAS methods. We also cover a baseline method Random Search, which shares the same supernet training strategy with Uniform Sampling <ref type="bibr" target="#b10">[11]</ref>; but during search, instead of using evolutionary algorithms it randomly samples 1000 paths, and retrains the rank-1 path according to Top-1 ACC on the supernet. As <ref type="table" target="#tab_0">Table 1</ref> shows, when searching with similar 320 FLOPs, our GreedyNAS achieves the highest Top-1 ACC. We further align our searched constraint to latency of 80 ms. <ref type="table" target="#tab_0">Table 1</ref> indicates that with similar latency, GreedyNAS is still consistently superior to other comparison methods. For example, GreedyNAS can search an architecture with 74.93% Top-1 ACC, enjoying a 0.43% improvement over uniform sampling, which in a way illustrates the superiority of our greedy supernet to a uniform supernet.</p><p>Besides advantages on the classification performance of searched models, we also evaluate our superiority in terms of supernet training efficiency. Since the main differences of our GreedyNAS and other Single-path One-shot NAS methods lie in the supernet training, we report in <ref type="table" target="#tab_0">Table 1</ref> the supernet training cost. To eliminate the efficiency gap due to different implementation tools (e.g., GPU types, dataloader wrappers), we calculated the accumulated number of images involved in a whole gradient-based optimization step, i.e., #optimization in <ref type="table" target="#tab_0">Table 1</ref>. Our GreedyNAS has an additional evaluation process during training, thus we also report the accumulated number of images for forward evaluation, i.e., #evaluation. For overall efficiency comparison, we empirically find the cost of a whole optimization step is approximately 3.33 times larger than that of a forward evaluation. The corresponding corrected #optimization is covered accordingly. From <ref type="table" target="#tab_0">Table 1</ref>, we can see that the training cost of our GreedyNAS is much smaller than that of other comparison methods, which indicates GreedyNAS enjoys significant efficiency in supernet training since it greedily shrinks its training space into those potentially-good paths. Besides, we also implement Random Search and Uniform Sampling using same training cost of GreedyNAS, denoted as Random Search-E and Uniform Sampling-E, respectively. The results show that with decreased iterations of supernet training, the searched architectures are inferior to those of larger iterations. In contrast, our method can achieve higher accuracy by a large margin (almost 1%). This implies that GreedyNAS is capable of learning a decent supernet with much less iterations.</p><p>Searching on augmented search space. To comprehensively illustrate our superiority to various state-of-theart NAS methods, we implement searching by augmenting the current space with an SE option. Moreover, we search the architectures under different FLOPs constraint. But we also report the corresponding latency and parameter capacity to comprehensively analyze the statistics of searched models. As <ref type="table" target="#tab_1">Table 2</ref> shows, our GreedyNAS achieves new state-of-the-art performance with respect to different FLOPs and latency levels. For example, with similar FLOPs and latency, GreedyNAS-C has higher Top-1 ACC than the competing SCARLET method by a margin of 0.6%. Our searched models are visualized in <ref type="figure" target="#fig_7">Figure 6</ref> (see supplementary materials). It shows smaller network (GreedyNAS-C) tends to select more identity blocks to reduce the FLOPs while larger networks will exploit more SE modules (GreedyNAS-A&amp;B) to further improve the accuracy performance. Moreover, GreedyNAS-A adopts more 3×3 kernels to have smaller latency since 3×3 kernels are optimized more maturely in mobile inference framework. We also report our real training cost in <ref type="table" target="#tab_1">Table 2</ref> based on Tesla V100 GPU. It shows that GreedyNAS can significantly reduce the training time compared to other NAS methods, which empirically validates the efficiency of GreedyNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Effect of evaluation in path filtering</head><p>To filter the weak paths, GreedyNAS evaluates each path by a small portion (1000) of validation images as a surrogate for the whole validation dataset (50K images). We first investigate whether this approximation suffices in our experiment. By random sampling 1000 paths from supernet, we examine the correlation of two path orderings, which are generated by ranking the evaluation results using 1000 and 50K validation images, respectively. In <ref type="table" target="#tab_2">Table 3</ref>, we report the widely-used Spearman rho <ref type="bibr" target="#b24">[25]</ref> and Kendall tau <ref type="bibr" target="#b12">[13]</ref> rank correlation coefficient, which are in the range [0, 1] and larger values mean stronger correlation. We also cover three types of supernets, i.e., randomly initialized, trained by uniform sampling and our greedy sampling.</p><p>From <ref type="table" target="#tab_2">Table 3</ref>, we can see that our greedy supernet achieves fairly high rank correlation coefficient (0.997 and 0.961), which indicates that the ranking of greedy supernet using 1000 validation images is significantly consistent with that of all validation dataset. Moreover, supernet trained with uniform sampling has smaller correlation coefficient, even with different evaluation images (see left   <ref type="figure" target="#fig_5">Figure 4</ref>). This implies in a sense that our greedy supernet is more discriminative since it can use less images to identify whether a path is good or weak. Nevertheless, as left <ref type="figure" target="#fig_5">Figure 4</ref> shows, too few evaluation images might have weak correlation while too many evaluation images mean greater evaluation cost. But 1000 evaluation images enjoy a balanced trade-off between the rank correlation and evaluation cost. Note that we also report the results w.r.t. ranking using the ACC of 1000 images, which is smaller than that using loss. This results from that during training the value of loss might be more informative than that of ACC. As for the random supernet, the correlation coefficient is fairly small (0.155 and 0.113). This makes sense since the ranking is based on the classification performance; however, a random supernet fails to learn sufficient domainrelated information but gives disordered ranking of paths. This smaller correlation coefficient implies that it might be not sensible to implement greedy sampling from a random supernet since the ranking evaluated by 1000 validation images will be rather noisy. In this way, we record the trend of rank correlation coefficients with uniform sampling in right <ref type="figure" target="#fig_5">Figure 4</ref>. It shows that with more iterations, the correlation coefficients increase and at 10K iteration, they tend to be steady at a high level (e.g., 0.81 for Kendall Tau). As a result, in our GreedyNAS we propose to have a warm-up stage by uniform sampling for 10K iterations, so that we can safely use 1000 validation images to evaluate paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Effect of path filtering and candidate pool</head><p>To study the effect of our proposed path filtering and the candidate pool, we implement experiments on the search space without SE. In our GreedyNAS, path filtering is to block the training of weak paths. In contrast, the use of candidate pool is mainly three-fold as shown in <ref type="table" target="#tab_3">Table 4</ref>. First, we can sample from it as the exploitation process; second, we can initialize the evolutionary searching with the pool for better paths; third, we can use it to adaptively stop the supernet training. Then we control each factor and obtain 6 variants of GreedyNAS as well as 6 corresponding searched architectures Net1∼Net6. For fair comparison, we search all nets under 330M FLOPs. Besides, if the candidate pool is not used for stopping training, we specify a maximum epoch 60 as <ref type="bibr" target="#b2">[3]</ref>.</p><p>As <ref type="table" target="#tab_3">Table 4</ref> shows, comparing with the baseline Net1 (Net3), Net2 (Net6) achieves 0.28% (0.41%) better Top-1 ACC, which indicates that path filtering does contribute to the supernet training, and thus improves the searching results. By involving the candidate pool, Net6 can increase its accuracy from 74.59% (Net2) to 74.89%. In specific, initialization with candidate pool in evolutionary algorithms enables to have a 0.18% gain on Top-1 ACC since it helps to search paths with higher ACC on supernet (also see <ref type="figure" target="#fig_4">Figure  3</ref>). Note that stopping by candidate pool usually saves training cost; however, full training with candidate pool (Net5) seems to drop the accuracy a bit (0.05% w.r.t. Net6). It might result from that extreme greedy exploitation on the candidate pool harms the supernet training instead. Then the stopping in a sense brings benefits for a more balanced trade-off between exploration and exploitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Training a supernet is a key issue for Single-path Oneshot NAS methods. In stead of treating all paths equally, we propose to greedily focus on training those potentially-good ones. This greedy path filtering can be efficiently implemented by our proposed multi-path sampling strategy with rejection. Besides, we also adopt an exploration and exploitation policy and introduce a candidate pool to further boost the supernet training efficiency. Our proposed method GreedyNAS shows significant superiority in terms of both accuracy performance and training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of evolutionary searching in Section 4.2</head><p>We present the details of our adopted NSGA-II <ref type="bibr" target="#b5">[6]</ref> evolutionary algorithm in the following Algorithm 3. In our experiment, population size N pop = 50 and number of generations T = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Evolutionary Architecture Search</head><p>Input: supernet N , candidate pool P, population size N pop , number of generations T , validation data D val , constraints C. Output: architecture with highest validation accuracy under constraints.</p><p>1: Initialize populations P 0 with P so that |P 0 | = N pop and P 0 satisfies constraints C.</p><p>2: E = ∅; # evaluation set E which stores all evaluated architectures with accuracy 3: for i = 0, 1, ..., T − 1 do 4:</p><formula xml:id="formula_11">Q i = make-new-pop(P i );</formula><p># generate offspring population Q i using binary tournament selection, recombination, and mutation operators 5:</p><formula xml:id="formula_12">R i = P i ∪ Q i ; 6:</formula><p>F i = f ast-non-dominated-sort(R i ); # generate all nondominated fronts of R i 7:</p><p>P i+1 = ∅ and j = 0;</p><p>8:</p><formula xml:id="formula_13">while |P i+1 | + |F i | ≤ N pop do 9:</formula><p>crowding-distance-assignment(F i ); # calculate crowding-distance in F j 10: E i = evaluation-architecture(F j , D val , C); # evaluate architecture with constraints and validation data <ref type="bibr" target="#b13">14</ref>:</p><formula xml:id="formula_14">P i+1 = P i+1 ∪ F</formula><formula xml:id="formula_15">E = E ∪ E i # extend E i to E 15:</formula><p>Sort(F j , E i ); # sort in descending order using E i 16:</p><formula xml:id="formula_16">P i+1 = P i+1 ∪ F j [1 : (N pop − |P i+1 |)]; # choose the first (N pop − |P i+1 |) elements of F j 17: Q i+1 = make-new-pop(P i+1 );</formula><p># make new population with constraints 18: end for <ref type="bibr">19:</ref> return architecture with highest accuracy in E B. More Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Details of (augmented) search space</head><p>The macro-structure of supernet is presented in <ref type="table">Table 5</ref>, where each operation choice for Choice Block is attached in <ref type="table" target="#tab_5">Table  6</ref>. <ref type="table">Table 5</ref>: Macro-structure of supernet. "input" indicates the size of feature maps for each layer, and "channels" means for the number of output channels. "repeat" is for the number of stacking same blocks, and "stride" is for the stride of first block when stacked for multiple times. "MB1 K3" refers to   <ref type="table" target="#tab_0">Table 5, where ID means for an identity mapping.   block type  expansion ratio kernel SE  MB1 K3  1  3  no  ID  ---MB3 K3  3  3  no  MB3 K5  3  5  no  MB3 K7  3  7  no  MB6 K3  6  3  no  MB6 K5  6  5  no  MB6 K7  6  7  no  MB3 K3 SE  3  3  yes  MB3 K5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Details of rank correlation coefficient</head><p>In ablation study 5.3.1, we use two Spearman rho <ref type="bibr" target="#b24">[25]</ref> and Kendall tau <ref type="bibr" target="#b12">[13]</ref> rank correlation coefficient to evaluate the correlation of two path orderings, which are generated by ranking the evaluation results using 1000 and 50K validation images, denoted as r and s, respectively. Basically, we aim to calculate the correlation of r and s.</p><p>For Spearman rho rank correlation coefficient, it is simply the Pearson correlation coefficient between random variable r and s, if we regard r and s as two observation vectors of random variable r and s, i.e., ρ S = cov(r, s) σ r σ s , where cov(·, ·) is the covariance of two variables, and σ r (σ s ) is the standard deviations of r (s). Based on observation vectors, it can be more efficiently calculated as</p><formula xml:id="formula_17">ρ S = 1 − 6 n i=1 (r i − s i ) 2 n(n 2 − 1) ,</formula><p>where n = 1000 in our experiment.</p><p>For Kendall tau rank correlation coefficient, it focuses on the pairwise ranking performance. For any pair (r i , r j ) and (s i , s j ), it is said to be concordant if r i &gt; r j and s i &gt; s j hold simultaneously, or also for r i &lt; r j and s i &lt; s j . Otherwise, it will be called disconcordant. Then the coefficient is calculated as ρ K = #concordant pairs -#disconcordant pairs #all pairs ,</p><p>where #all pairs = C 2 n refers to the total number of pairs. In this way, if two rankings r and s are exactly the same, ρ K will be 1 while if the two are completely different (i.e., one ranking is the reverse of the other), ρ K will be -1. According to the definition, it can also be calculated in a closed-form as</p><formula xml:id="formula_18">ρ K = 2 n(n − 1) i&lt;j sign(r i − r j )sign(s i − s j ),</formula><p>where sign(·) is the sign function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. More ablation studies B.4.1 Performance of trained supernet</head><p>To further investigate the performance of the trained supernet, we implement two different searching methods (random search and evolutionary search) on various trained supernet, i.e., greedy supernet, uniform supernet (full training) and uniform supernet-E (same training cost with GreedyNAS). The results can be regarded as supplementary for <ref type="table" target="#tab_0">Table 1</ref>.  <ref type="table" target="#tab_8">Table 7</ref>, we can see that a greedy supernet improves consistently the classification accuracy in terms of different searchers. This validates the superiority of our greedy supernet since it helps searchers to probe better architectures. Moreover, to comprehensively investigate the effect of supernets, we implement systematic sampling 4 to sample 30 paths from 50×20 = 1000 paths, which are discovered by the evolutionary algorithms and ranked according to the accuracy on supernet. Then we retrain these 30 paths from scratch, and report their distribution histogram in <ref type="figure" target="#fig_6">Figure 5</ref>.</p><p>As shown in <ref type="figure" target="#fig_6">Figure 5</ref>, we can see that on average, paths searched with our greedy supernet have higher retraining Top-1 accuracy than that with uniform supernet. This implies that our greedy supernet serves as a better performance estimator, so that those good paths can be eventually identified and searched. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Visualization of searched architectures</head><p>We visualize the searched architectures by our GreedyNAS method in <ref type="figure" target="#fig_7">Figure 6</ref>.  <ref type="table" target="#tab_1">Table 2</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Diagram of supernet training for our proposed GreedyNAS. The supernet greedily shrinks its training space from all paths (red and blue dots) into potentiallygood paths (red dots), and further into candidate pool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Probability of sampling at least k potentially-good paths out of m paths. X-axis: r = k/m. q = |A good |/|A|. The sampling distribution p(A) is equivalently reshaped by truncation on A good , i.e., p(A) = U (A good ; N o , D val ) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 1 .Algorithm 1</head><label>11</label><figDesc>If m paths are sampled uniformly i.i.d. from A, and A good and A weak are defined as Eq.(4) and Eq.(5) based on supernet N † , then it holds that at least k (k ≤ m) Greedy path filtering w.t/w.o candaidate pool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Histogram of accuracy of searched paths on supernet by evolutionary searching method (with or without candidate pool).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Rank correlation coefficient of 1000 paths measured by the loss of N validation images and ACC of the whole 50K validation images. Left: Comparison (Kendall tau) of supernet by uniform and greedy sampling w.r.t. different number N of evaluation images. Right: N = 1K w.r.t. different training iterations of supernet by uniform sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Top-1 accuracy histogram of 30 systematically sampled paths from 1000 paths searched by evolutionary algorithm after trained from scratch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of searched architectures by GreedyNAS in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of classification performance and supernet training efficiency w.r.t. different searching methods on Ima-geNet dataset under same search space. #optimization means the accumulated #examples calculated for a whole optimization step, while #evaluation is for that of forward evaluation. corrected #optimization is based on our statistics that cost of a whole optimization step is 3.33 times larger than that of forward evaluation. Details of calculation refer to supplementary materials.</figDesc><table><row><cell>Methods</cell><cell cols="4">performance Top-1 (%) FLOPs latency Params</cell><cell cols="3">supernet training efficiency #optimization #evaluation corrected #optimization</cell></row><row><cell>Proxyless-R (mobile) [1]</cell><cell>74.60</cell><cell>320M</cell><cell>79 ms</cell><cell>4.0M</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Random Search</cell><cell>74.07</cell><cell>321M</cell><cell>69 ms</cell><cell>3.6M</cell><cell>1.23M×120</cell><cell>-</cell><cell>147.6M</cell></row><row><cell>Uniform Sampling [11]</cell><cell>74.50</cell><cell>326M</cell><cell>72 ms</cell><cell>3.8M</cell><cell>1.23M×120</cell><cell>-</cell><cell>147.6M</cell></row><row><cell>FairNAS-C [4]</cell><cell>74.69</cell><cell>321M</cell><cell>75 ms</cell><cell>4.4M</cell><cell>1.23M×150</cell><cell>-</cell><cell>184.5M</cell></row><row><cell>Random Search-E</cell><cell>73.88</cell><cell>320M</cell><cell>91 ms</cell><cell>3.7M</cell><cell>1.23M×73</cell><cell>-</cell><cell>89.8M</cell></row><row><cell>Uniform Sampling [11]-E</cell><cell>74.17</cell><cell>320M</cell><cell>94 ms</cell><cell>3.6M</cell><cell>1.23M×73</cell><cell>-</cell><cell>89.8M</cell></row><row><cell>GreedyNAS (FLOPs≤ 322M)</cell><cell>74.85</cell><cell>320M</cell><cell>89 ms</cell><cell>3.8M</cell><cell>1.23M×46</cell><cell>2.40M×46</cell><cell>89.7M</cell></row><row><cell>GreedyNAS (latency≤ 80ms)</cell><cell>74.93</cell><cell>324M</cell><cell>78 ms</cell><cell>4.1M</cell><cell>1.23M×46</cell><cell>2.40M×46</cell><cell>89.7M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of searched architectures w.r.t. different state-of-the-art NAS methods. †: searched on CIFAR-10, ‡: TPU, : reported by<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table><row><cell>Methods</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell><cell>FLOPs (M)</cell><cell>latency (ms)</cell><cell>Params (M)</cell><cell>Memory cost</cell><cell>training cost (GPU days)</cell><cell>search cost (GPU days)</cell></row><row><cell>SCARLET-C [4]</cell><cell>75.6</cell><cell>92.6</cell><cell>280</cell><cell>67</cell><cell>6.0</cell><cell>single path</cell><cell>10</cell><cell>12</cell></row><row><cell>MobileNetV2 1.0 [28]</cell><cell>72.0</cell><cell>91.0</cell><cell>300</cell><cell>38</cell><cell>3.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MnasNet-A1 [30]</cell><cell>75.2</cell><cell>92.5</cell><cell>312</cell><cell>55</cell><cell>3.9</cell><cell>single path + RL</cell><cell>288  ‡</cell><cell>-</cell></row><row><cell>GreedyNAS-C</cell><cell>76.2</cell><cell>92.5</cell><cell>284</cell><cell>70</cell><cell>4.7</cell><cell>single path</cell><cell>7</cell><cell>&lt; 1</cell></row><row><cell>Proxyless-R (mobile) [1]</cell><cell>74.6</cell><cell>92.2</cell><cell>320</cell><cell>79</cell><cell>4.0</cell><cell>two paths</cell><cell>15</cell><cell>-</cell></row><row><cell>FairNAS-C [4]</cell><cell>74.7</cell><cell>92.1</cell><cell>321</cell><cell>75</cell><cell>4.4</cell><cell>single path</cell><cell>10</cell><cell>2</cell></row><row><cell>Uniform Sampling [11]</cell><cell>74.7</cell><cell>-</cell><cell>328</cell><cell>-</cell><cell>-</cell><cell>single path</cell><cell>12</cell><cell>&lt; 1</cell></row><row><cell>SCARLET-B [4]</cell><cell>76.3</cell><cell>93.0</cell><cell>329</cell><cell>104</cell><cell>6.5</cell><cell>single path</cell><cell>10</cell><cell>12</cell></row><row><cell>GreedyNAS-B</cell><cell>76.8</cell><cell>93.0</cell><cell>324</cell><cell>110</cell><cell>5.2</cell><cell>single path</cell><cell>7</cell><cell>&lt; 1</cell></row><row><cell>SCARLET-A [4]</cell><cell>76.9</cell><cell>93.4</cell><cell>365</cell><cell>118</cell><cell>6.7</cell><cell>single path</cell><cell>10</cell><cell>12</cell></row><row><cell>EfficientNet-B0 [31]</cell><cell>76.3</cell><cell>93.2</cell><cell>390</cell><cell>82</cell><cell>5.3</cell><cell>single path</cell><cell>-</cell><cell>-</cell></row><row><cell>DARTS [21]</cell><cell>73.3</cell><cell>91.3</cell><cell>574</cell><cell>-</cell><cell>4.7</cell><cell>a whole supernet</cell><cell>4  †</cell><cell>-</cell></row><row><cell>GreedyNAS-A</cell><cell>77.1</cell><cell>93.3</cell><cell>366</cell><cell>77</cell><cell>6.5</cell><cell>single path</cell><cell>7</cell><cell>&lt; 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Rank correlation coefficient of 1000 paths measured by the loss (ACC) of 1K validation images and ACC of 50K validation images w.r.t. different types of supernets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Spearman rho</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Kendall tau</cell></row><row><cell></cell><cell cols="12">random uniform(ACC) greedy random uniform(ACC) greedy</cell></row><row><cell></cell><cell cols="7">0.155 0.968(0.869) 0.997</cell><cell cols="5">0.113 0.851(0.699) 0.961</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>correlation coefficient</cell><cell>0.7 0.8 0.9</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Uniform Sampling Greedy Sampling</cell><cell>correlation coefficient</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell>Spearman rho Kendall tau</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>125</cell><cell>250</cell><cell>500</cell><cell>1K</cell><cell>2K</cell><cell>5K</cell><cell>10K</cell><cell>0</cell><cell>2K</cell><cell>4K</cell><cell>8K</cell><cell>10K 14K 30K 15W</cell></row><row><cell></cell><cell></cell><cell cols="5">number of validation images</cell><cell></cell><cell></cell><cell></cell><cell cols="3">training iterations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of accuracy performance of searched paths by GreedyNAS w.r.t. different usage of path filtering and candidate pool.</figDesc><table><row><cell></cell><cell>path filtering</cell><cell cols="3">candidate pool evolutionary (exploitation) initialization stopping sampling training</cell><cell>Top-1 (%)</cell></row><row><cell>Net1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.31</cell></row><row><cell>Net2</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.59</cell></row><row><cell>Net3</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>74.48</cell></row><row><cell>Net4</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>74.71</cell></row><row><cell>Net5</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>74.84</cell></row><row><cell>Net6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell>input</cell><cell>block</cell><cell cols="3">channels repeat stride</cell></row><row><cell>224 2 × 3</cell><cell>3 × 3 conv</cell><cell>32</cell><cell>1</cell><cell>2</cell></row><row><cell>112 2 × 32</cell><cell>MB1 K3</cell><cell>16</cell><cell>1</cell><cell>1</cell></row><row><cell>112 2 × 16</cell><cell>Choice Block</cell><cell>32</cell><cell>4</cell><cell>2</cell></row><row><cell>56 2 × 32</cell><cell>Choice Block</cell><cell>40</cell><cell>4</cell><cell>2</cell></row><row><cell>28 2 × 40</cell><cell>Choice Block</cell><cell>80</cell><cell>4</cell><cell>2</cell></row><row><cell>14 2 × 80</cell><cell>Choice Block</cell><cell>96</cell><cell>4</cell><cell>1</cell></row><row><cell>14 2 × 96</cell><cell>Choice Block</cell><cell>192</cell><cell>4</cell><cell>2</cell></row><row><cell>7 2 × 192</cell><cell>Choice Block</cell><cell>320</cell><cell>1</cell><cell>1</cell></row><row><cell>7 2 × 320</cell><cell>1 × 1 conv</cell><cell>1280</cell><cell>1</cell><cell>1</cell></row><row><cell cols="2">7 2 × 1280 global avgpool</cell><cell>-</cell><cell>1</cell><cell>-</cell></row><row><cell>1280</cell><cell>FC</cell><cell>1000</cell><cell>1</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Operation choices for each MobileNetV2-based Choice Block in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>In our GreedyNAS, when equipped with the stopping principle of candidate pool, the supernet training is stopped at approximately 46-th epoch. Thus the accumulated number of examples calculated for a whole optimization step is equal to #optimization=1.23M × 46, where 1.23M refers to the quantity of training dataset. As for the path filtering, we evaluate 10 paths based on 1000 validation images, and select 5 paths for training, whose batch size is 1024. In this way, the number of images for evaluation amounts to Given our empirical findings that the cost of a whole optimization step is approximately 3.33 times larger than that of a forward evaluation, the corrected #optimization is thus corrected #optimization = #optimization + #evaluation/3.33, = 1.23M × 46 + 2.40M × 46/3.33, = 89.7M.</figDesc><table><row><cell>SE</cell><cell>3</cell><cell></cell><cell></cell><cell>5</cell><cell>yes</cell></row><row><cell>MB3 K7 SE</cell><cell>3</cell><cell></cell><cell></cell><cell>7</cell><cell>yes</cell></row><row><cell>MB6 K3 SE</cell><cell>6</cell><cell></cell><cell></cell><cell>3</cell><cell>yes</cell></row><row><cell>MB6 K5 SE</cell><cell>6</cell><cell></cell><cell></cell><cell>5</cell><cell>yes</cell></row><row><cell>MB6 K7 SE</cell><cell>6</cell><cell></cell><cell></cell><cell>7</cell><cell>yes</cell></row><row><cell>B.2. Calculating corrected #optimization in Table 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">#evaluation = 1.23M ×</cell><cell>1000 1024</cell><cell>×</cell><cell>10 5</cell><cell>× 46,</cell></row><row><cell cols="3">= 2.40M × 46.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison of performance on ImageNet dataset of searched architectures w.r.t. different supernets under same search space.</figDesc><table><row><cell>supernet</cell><cell>searcher</cell><cell cols="2">Top-1 (%) FLOPs</cell></row><row><cell>uniform</cell><cell>random</cell><cell>74.07</cell><cell>321M</cell></row><row><cell>uniform-E</cell><cell>random</cell><cell>73.88</cell><cell>320M</cell></row><row><cell>greedy</cell><cell>random</cell><cell>74.22</cell><cell>321M</cell></row><row><cell>uniform</cell><cell>evolutionary</cell><cell>74.50</cell><cell>326M</cell></row><row><cell cols="2">uniform-E evolutionary</cell><cell>74.17</cell><cell>320M</cell></row><row><cell>greedy</cell><cell>evolutionary</cell><cell>74.85</cell><cell>320M</cell></row><row><cell>From</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For example, in a same supernet, MobileNetV2<ref type="bibr" target="#b27">[28]</ref> can achieve 72.0% Top-1 accuracy on ImageNet dataset while an extreme case of almost all identity operations only has 24.1%<ref type="bibr" target="#b2">[3]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">α = 0.08 suffices in our experiment.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://en.wikipedia.org/wiki/Systematic_sampling</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detnas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10979</idno>
		<title level="m">Neural architecture search on object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06022</idno>
		<title level="m">Scarletnas: Bridging the gap between scalability and fairness in neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-objective reinforced evolution in mobile neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01074</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: Nsga-ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrit</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamt</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised semantic-preserving adversarial hashing for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4032" to="4044" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two-stream deep hashing with class-specific centers for supervised image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Irlas: Inverse reinforcement learning for architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9021" to="9029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00420</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attribute-aware attention model for fine-grained representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2040" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maurice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bandit based montecarlo planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levente</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning student networks with few data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A real-time cross-modality correlation filtering method for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07072</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graph-guided architecture search for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiwen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06793</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Nsga-net: a multi-objective genetic algorithm for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03522</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">S pearman rank correlation coefficient. Encyclopedia of statistical sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pirie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Single-path nas: Designing hardware-efficient convnets in less than 4 hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02877</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reborn filters: Pruning convolutional neural networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bringing giant neural networks down to earth with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perceptual adversarial networks for image-to-image transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4066" to="4079" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evolutionary generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="921" to="934" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The devil of face recognition is in the noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liren</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="765" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep comprehensive correlation mining for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8150" to="8159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shared predictive cross-modal deep quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5292" to="5303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning from multiple teacher networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1285" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multinomial distribution learning for effective neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
