<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 YOU ONLY NEED ADVERSARIAL SUPERVISION FOR SEMANTIC IMAGE SYNTHESIS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-19">19 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Sushko</surname></persName>
							<email>vadim.sushko@bosch.com.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Schönfeld</surname></persName>
							<email>edgar.schoenfeld@bosch.com.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gall</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Bonnartificialintelligence</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Max Planck Institute for Informaticsartificialintelligen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 YOU ONLY NEED ADVERSARIAL SUPERVISION FOR SEMANTIC IMAGE SYNTHESIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-19">19 Mar 2021</date>
						</imprint>
					</monogr>
					<note>* Equal contribution. Correspondence to 1 Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite their recent successes, GAN models for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Historically, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limiting the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially-and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity with better alignment to their input label maps, making the use of the perceptual loss superfluous. Moreover, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image change. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve an average improvement of 6 FID and 5 mIoU points over the state of the art across different datasets using only adversarial supervision. Semantic SPADE (Park et al., 2019) Our model (OASIS), sampled with different noise label map with VGG w/o VGG w/o VGG</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure" target="#fig_5">Figure 1</ref><p>: Existing semantic image synthesis models heavily rely on the VGG-based perceptual loss to improve the quality of generated images. In contrast, our model can synthesize diverse and high-quality images while only using an adversarial loss, without any external supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Conditional generative adversarial networks (GANs) <ref type="bibr">(Mirza &amp; Osindero, 2014)</ref> synthesize images conditioned on class labels , text <ref type="bibr">(Reed et al., 2016;</ref><ref type="bibr" target="#b32">Zhang et al., 2018a)</ref>, other images <ref type="bibr">(Isola et al., 2017;</ref><ref type="bibr" target="#b15">Huang et al., 2018)</ref>, or semantic label maps <ref type="bibr" target="#b25">(Wang et al., 2018;</ref><ref type="bibr" target="#b35">Park et al., 2019)</ref>. In this work, we focus on the latter, addressing semantic image synthesis. Semantic image synthesis enables rendering of realistic images from user-specified layouts, without the use of an intricate graphic engine. Therefore, its applications range widely from content creation and image editing to generating training data that needs to adhere to specific semantic requirements <ref type="bibr" target="#b25">(Wang et al., 2018;</ref><ref type="bibr" target="#b7">Chen &amp; Koltun, 2017)</ref>. Despite the recent progress on stabilizing GANs <ref type="bibr" target="#b13">(Gulrajani et al., 2017;</ref><ref type="bibr">Miyato et al., 2018;</ref><ref type="bibr" target="#b31">Zhang &amp; Khoreva, 2019)</ref> and developing their architectures <ref type="bibr">Karras et al., 2019)</ref>, state-of-the-art GAN-based semantic image synthesis models <ref type="bibr" target="#b35">(Park et al., 2019;</ref><ref type="bibr">Liu et al., 2019)</ref> still greatly suffer from training instabilities and poor image quality when trained only with adversarial supervision (see <ref type="figure" target="#fig_5">Fig. 1</ref>). An established practice to overcome this issue is to employ a perceptual loss <ref type="bibr" target="#b25">(Wang et al., 2018)</ref> to train the generator, in addition to the discriminator loss. The perceptual loss aims to match intermediate features of synthetic and real images, that are estimated via an external perception network. A popular choice for such a network is VGG <ref type="bibr" target="#b19">(Simonyan &amp; Zisserman, 2015)</ref>, pre-trained on ImageNet <ref type="bibr" target="#b9">(Deng et al., 2009)</ref>. Although the perceptual loss substantially improves the accuracy of previous methods, it comes with the computational overhead introduced by utilizing an extra network for training. Moreover, it usually dominates over the adversarial loss during training, which can have a negative impact on the diversity and quality of generated images, as we show in our experiments. Therefore, in this work we propose a novel, simplified model that achieves state-of-the-art results without requiring a perceptual loss.</p><p>A fundamental question for GAN-based semantic image synthesis models is how to design the discriminator to efficiently utilize information from the given semantic label maps. Conventional methods <ref type="bibr" target="#b35">(Park et al., 2019;</ref><ref type="bibr" target="#b25">Wang et al., 2018;</ref><ref type="bibr">Liu et al., 2019;</ref><ref type="bibr">Isola et al., 2017)</ref> adopt a multi-scale classification network, taking the label map as input along with the image, and making a global image-level real/fake decision. Such a discriminator has limited representation power, as it is not incentivized to learn high-fidelity pixel-level details of the images and their precise alignment with the input semantic label maps. To mitigate this issue, we propose an alternative architecture for the discriminator, re-designing it as an encoder-decoder semantic segmentation network <ref type="bibr">(Ronneberger et al., 2015)</ref>, and directly exploiting the given semantic label maps as ground truth via a (N +1)-class cross-entropy loss (see <ref type="figure" target="#fig_0">Fig. 3</ref>). This new discriminator provides semantically-aware pixel-level feedback to the generator, partitioning the image into segments belonging to one of the N real semantic classes or the fake class. Enabled by the discriminator per-pixel response, we further introduce a La-belMix regularization, which fosters the discriminator to focus more on the semantic and structural differences of real and synthetic images. The proposed changes lead to a much stronger discriminator, that maintains a powerful semantic representation of objects, giving more meaningful feedback to the generator, and thus making the perceptual loss supervision superfluous (see <ref type="figure" target="#fig_5">Fig. 1</ref>).</p><p>Next, we propose to enable multi-modal synthesis of the generator via 3D noise sampling. Previously, directly using 1D noise as input was not successful for semantic image synthesis, as the generator tended to mostly ignore it or synthesized images of poor quality <ref type="bibr">(Isola et al., 2017;</ref><ref type="bibr" target="#b25">Wang et al., 2018)</ref>. Thus, prior work <ref type="bibr" target="#b25">(Wang et al., 2018;</ref><ref type="bibr" target="#b35">Park et al., 2019)</ref> resorted to using an image encoder to produce multi-modal outputs. In this work, we propose a lighter solution. Empowered by our stronger discriminator, the generator can effectively synthesize different images by simply re-sampling a 3D noise tensor, which is used not only as the input but also combined with intermediate features via conditional normalization at every layer. Such noise is spatially sensitive, so we can re-sample it both globally (channel-wise) and locally (pixel-wise), allowing to change not only the appearance of the whole scene, but also of specific semantic classes or any chosen areas (see <ref type="bibr">Fig. 2)</ref>. We call our model OASIS, as it needs only adversarial supervision for semantic image synthesis.</p><p>In summary, our main contributions are: (1) We propose a novel segmentation-based discriminator architecture, that gives more powerful feedback to the generator and eliminates the necessity of the perceptual loss supervision. (2) We present a simple 3D noise sampling scheme, notably increasing the diversity of multi-modal synthesis and enabling complete or partial change of the generated image. (3) With the OASIS model, we achieve high quality results on the ADE20K, Cityscapes and COCO-stuff datasets, on average improving the state of the art by 6 FID and 5 mIoU points, while  <ref type="figure" target="#fig_5">Figure 2</ref>: OASIS multi-modal synthesis results. The 3D noise can be sampled globally (first 2 rows), changing the whole scene, or locally (last 2 rows), partially changing the image. For the latter, we sample different noise per region, like the bed segment (in red) or arbitrary areas defined by shapes.</p><p>relying only on adversarial supervision. We show that images synthesized by OASIS exhibit much higher diversity and more closely follow the color and texture distributions of real images. Our code and pretrained models are available at https://github.com/boschresearch/OASIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Semantic image synthesis. Pix2pix (Isola et al., 2017) first proposed to use conditional GANs <ref type="bibr">(Mirza &amp; Osindero, 2014)</ref> for semantic image synthesis, adopting an encoder-decoder generator which takes semantic label maps as input, and employing a PatchGAN discriminator. Since then, various generator and discriminator modifications have been introduced <ref type="bibr" target="#b25">(Wang et al., 2018;</ref><ref type="bibr" target="#b35">Park et al., 2019;</ref><ref type="bibr">Liu et al., 2019;</ref><ref type="bibr" target="#b24">Tang et al., 2020c;</ref><ref type="bibr">b;</ref><ref type="bibr">Ntavelis et al., 2020)</ref>. Besides GANs, <ref type="bibr" target="#b7">Chen &amp; Koltun (2017)</ref>   <ref type="bibr" target="#b25">(Wang et al., 2018)</ref> and <ref type="bibr" target="#b35">SPADE (Park et al., 2019)</ref> all employed a multi-scale PatchGAN discriminator, that takes an image and its semantic label map as input. <ref type="bibr">CC-FPSE (Liu et al., 2019)</ref> proposed a feature-pyramid discriminator, embedding both images and label maps into a joint feature map, and then consecutively upsampling it in order to classify it as real/fake at multiple scales.</p><p>LGGAN <ref type="bibr" target="#b24">(Tang et al., 2020c)</ref> introduced a classification-based feature learning module to learn more discriminative and class-specific features. In this work, we propose to use a pixel-wise semantic segmentation network as a discriminator instead of multi-scale image classifiers as in the above approaches, and to directly exploit the semantic label maps for its supervision. Segmentation-based discriminators have been shown to improve semantic segmentation <ref type="bibr" target="#b20">(Souly et al., 2017)</ref> and unconditional image synthesis <ref type="bibr">(Schönfeld et al., 2020)</ref>, but to the best of our knowledge have not been explored for semantic image synthesis and our work is the first to apply adversarial semantic segmentation loss for this task.</p><p>Generator architectures. Conventionally, the semantic label map is provided to the image generation pipeline via an encoder <ref type="bibr">(Isola et al., 2017;</ref><ref type="bibr" target="#b25">Wang et al., 2018;</ref><ref type="bibr" target="#b24">Tang et al., 2020c;</ref><ref type="bibr">b;</ref><ref type="bibr">Ntavelis et al., 2020)</ref>. However, it is shown to be suboptimal at preserving the semantic information until the later stages of image generation. Therefore, SPADE introduced a spatially-adaptive normalization layer . OASIS outperforms SPADE, while being simpler and lighter: it uses only adversarial loss supervision and a single segmentation-based discriminator, without relying on heavy external networks. Furthermore, OASIS learns to synthesize multi-modal outputs by directly re-sampling the 3D noise tensor, instead of using an image encoder as in SPADE.</p><p>that directly modulates the label map onto the generator's hidden layer outputs at various scales. Alternatively, CC-FPSE proposed to use spatially-varying convolution kernels conditioned on the label map. Struggling with generating diverse images from noise, both Pix2pixHD and SPADE resorted to having an image encoder in the generator design to enable multi-modal synthesis. The generator then combines the extracted image style with the label map to reconstruct the original image. By alternating the style vector, one can generate multiple outputs conditioned on the same label map. However, using an image encoder is a resource demanding solution. In this work, we enable multi-modal synthesis directly through sampling of a 3D noise tensor injected at every layer of the network. Differently from structured noise injection of <ref type="bibr">Alharbi &amp; Wonka (2020)</ref> and class-specific latent codes of Zhu et al. <ref type="formula" target="#formula_1">(2020)</ref>, we inject the 3D noise along with label maps and adjust it to image resolution, also enabling re-sampling of selected semantic segments (see <ref type="figure" target="#fig_5">Fig. 2</ref>).</p><p>Perceptual losses. <ref type="bibr" target="#b11">Gatys et al. (2015)</ref>; <ref type="bibr" target="#b10">Gatys et al. (2016);</ref><ref type="bibr">Johnson et al. (2016)</ref> and <ref type="bibr" target="#b3">Bruna et al. (2016)</ref> were pioneers at exploiting perceptual losses to produce high-quality images for superresolution and style transfer using convolutional networks. For semantic image synthesis, the VGGbased perceptual loss was first introduced by CRN, and later adopted by Pix2pixHD. Since then, it has become a default for training the generator <ref type="bibr" target="#b35">(Park et al., 2019;</ref><ref type="bibr">Liu et al., 2019;</ref><ref type="bibr" target="#b21">Tan et al., 2020;</ref><ref type="bibr" target="#b22">Tang et al., 2020a)</ref>. As the perceptual loss is based on a VGG network pre-trained on Ima-geNet <ref type="bibr" target="#b9">(Deng et al., 2009)</ref>, methods relying on it are constrained by the ImageNet domain and the representational power of VGG. With the recent progress on GAN training, e.g. by architecture designs and regularization techniques, the actual necessity of the perceptual loss requires a reassessment. We experimentally show that such loss imposes unnecessary constraints on the generator, significantly limiting sample diversity. While our model, trained without the VGG loss, achieves improved image diversity while not compromising image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OASIS MODEL</head><p>In this section, we present our OASIS model, which, in contrast to other semantic image synthesis methods, needs only adversarial supervision for generator training. Using SPADE as a starting point (Sec. 3.1), we first propose to re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as ground truth (Sec. 3.2). Empowered by spatiallyand semantically-aware feedback of the new discriminator, we next re-design the SPADE generator, enabling its effective multi-modal synthesis via 3D noise sampling (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">THE SPADE BASELINE</head><p>We choose SPADE as our baseline as it is a state-of-the-art model and a relatively simple representative of conventional semantic image synthesis models. As depicted in <ref type="figure" target="#fig_0">Fig. 3</ref>, the discriminator of SPADE largely follows the PatchGAN multi-scale discriminator <ref type="bibr">(Isola et al., 2017)</ref>, adopting two image classification networks operating at different resolutions. Both of them take the channel-wise concatenation of the semantic label map and the real/synthesized image as input, and produce true/fake classification scores. On the generator side, SPADE adopts spatially-adaptive normalization layers to effectively integrate the semantic label map into the synthesis process from low to high scales. Additionally, the image encoder is used to extract the style vector from the reference image and then combine it with a 1D noise vector for multi-modal synthesis. The training loss of SPADE consists of three terms, namely, an adversarial loss, a feature matching loss and the VGG-based perceptual loss: L = max G min D L adv + λ fm L fm + λ vgg L vgg . Overall, SPADE is a resource demanding model at both training and test time, i.e., with two PatchGAN discriminators, an image encoder in addition to the generator, and the VGG loss. In the following, we revisit its architecture and introduce a simpler and more efficient model that offers better performance with less complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OASIS DISCRIMINATOR</head><p>For the generator to learn to synthesize images that are well aligned with the input semantic label maps, we need a powerful discriminator that coherently captures discriminative semantic features at different image scales. While classification-based discriminators, such as PatchGAN, take label maps as input concatenated to images, they can afford to ignore them and make the decision solely on image patch realism. Thus, we propose to cast the discriminator task as a multi-class semantic segmentation problem to directly utilize label maps for supervision, and accordingly alter its architecture to an encoder-decoder segmentation network (see <ref type="figure" target="#fig_0">Fig. 3</ref>). Encoder-decoder networks have proven to be effective for semantic segmentation <ref type="bibr" target="#b1">(Badrinarayanan et al., 2016;</ref><ref type="bibr" target="#b6">Chen et al., 2018)</ref>. Thus, we build our discriminator architecture upon U-Net <ref type="bibr">(Ronneberger et al., 2015)</ref>, which consists of the encoder and decoder connected by skip connections. This discriminator architecture is multi-scale through its design, integrating information over up-and down-sampling pathways and through the encoder-decoder skip connections. For details on the architecture see App. C.1.</p><p>The segmentation task of the discriminator is formulated to predict the per-pixel class label of the real images, using the given semantic label maps as ground truth. In addition to the N semantic classes from the label maps, all pixels of the fake images are categorized as one extra class. Overall, we have N + 1 classes in the semantic segmentation problem, and thus propose to use a (N +1)-class cross-entropy loss for training. Considering that the N semantic classes are usually imbalanced and that the per-pixel size of objects varies for different semantic classes, we weight each class by its inverse per-pixel frequency, giving rare semantic classes more weight. In doing so, the contributions of each semantic class are equally balanced, and, thus, the generator is also encouraged to adequately synthesize less-represented classes. Mathematically, the new discriminator loss is expressed as:</p><formula xml:id="formula_0">L D = −E (x,t)   N c=1 α c H×W i,j t i,j,c log D(x) i,j,c   − E (z,t)   H×W i,j log D(G(z, t)) i,j,c=N +1   ,<label>(1)</label></formula><p>where x denotes the real image; (z, t) is the noise-label map pair used by the generator G to synthesize a fake image; and the discriminator D maps the real or fake image into a per-pixel (N +1)-class prediction probability. The ground truth label map t has three dimensions, where the first two correspond to the spatial position (i, j) ∈ H × W , and the third one is a one-hot vector encoding the class c ∈ {1, .., N +1}. The class balancing weight α c is the inverse of the per-pixel class frequency</p><formula xml:id="formula_1">α c = H × W H×W i,j E t [1[t i,j,c = 1]] .<label>(2)</label></formula><p>LabelMix regularization. In order to encourage our discriminator to focus on differences in content and structure between the fake and the real classes, we propose a LabelMix regularization. Based on the semantic layout, we generate a binary mask M to mix a pair (x,x) of real and fake images conditioned on the same label map: <ref type="figure" target="#fig_5">Fig. 4</ref>. Given the mixed image, we further train the discriminator to be equivariant under the LabelMix operation. This is achieved by adding a consistency loss term L cons to Eq. 1:</p><formula xml:id="formula_2">LabelMix(x,x, M ) = M x + (1 − M ) x, as visu- alized in</formula><formula xml:id="formula_3">L cons = D logits LabelMix(x,x, M ) − LabelMix D logits (x), D logits (x), M 2 ,<label>(3)</label></formula><p>where D logits are the logits attained before the last softmax activation layer, and · is the L 2 norm. This consistency loss compares the output of the discriminator on the LabelMix image with the LabelMix of its outputs, penalizing the discriminator for inconsistent predictions. LabelMix is different to CutMix <ref type="bibr" target="#b30">(Yun et al., 2019)</ref>, which randomly samples the binary mask M . A random mask will introduce inconsistency between the pixel-level classes and the scene layout provided by the label map. For an object with the semantic class c, it will contain pixels from both real and fake images, resulting in two labels, i.e. c and N + 1. To avoid such inconsistency, the mask of LabelMix is generated according to the label map, providing natural borders between semantic regions, see <ref type="figure" target="#fig_5">Fig. 4</ref> (Mask M ). Under LabelMix regularization, the generator is encouraged to respect the natural semantic boundaries, improving pixel-level realism while also considering the class segment shapes.</p><formula xml:id="formula_4">Label map Real image x Fake imagex Mask M LabelMix (x,x) D LabelMix (x,x) LabelMix (Dx,Dx)</formula><p>Figure 4: LabelMix regularization. Real x and fakex images are mixed using a binary mask M , sampled based on the label map, resulting in LabelMix <ref type="bibr">(x,x)</ref> . The consistency regularization then minimizes the L2 distance between the logits of D LabelMix <ref type="bibr">(x,x)</ref> and LabelMix <ref type="bibr">(Dx,Dx)</ref> . In this visualization, black corresponds to the fake class in the N +1 segmentation output.</p><p>Other variants. Besides the proposed (N +1)-class cross entropy loss, there are other ways to train the segmentation-based discriminator with the label map. One can concatenate the label map to the input image, analogous to SPADE. Another option is to use projection, by taking the inner product between the last linear layer output and the embedded label map, analogous to class-label conditional GANs <ref type="bibr">(Miyato &amp; Koyama, 2018)</ref>. For both alternatives, the training loss is pixel-level real/fake binary cross-entropy <ref type="bibr">(Schönfeld et al., 2020)</ref>. From the label map encoding perspective, these two variants use labels map as input (concatenated to image or at last linear layer), propagating it forward through the network. The (N +1)-setting uses the label map for loss computation, so it is propagated backward via gradient updates. Backward propagation ensures that the discriminator learns semantic-aware features, in contrast to forward propagation, where the label map alignment is not as strongly enforced. Performance comparison of the label map encodings is shown in <ref type="table" target="#tab_6">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">OASIS GENERATOR</head><p>To stay in line with the OASIS discriminator design, the training loss for the generator is changed to</p><formula xml:id="formula_5">L G = −E (z,t)   N c=1 α c H×W i,j t i,j,c log D(G(z, t)) i,j,c   ,<label>(4)</label></formula><p>which is a direct outcome of the non-saturation trick <ref type="bibr" target="#b12">(Goodfellow et al., 2014)</ref> to Eq. 1. We next re-design the generator to enable multi-modal synthesis through noise sampling. SPADE is deterministic in its default setup, but can be trained with an extra image encoder to generate multi-modal outputs. We introduce a simpler version, that enables synthesis of diverse outputs directly from input noise. For this, we construct a noise tensor of size 64×H×W , matching the spatial dimensions of the label map H×W . Channel-wise concatenation of the noise and label map forms a 3D tensor used as input to the generator and also as a conditioning at every spatially-adaptive normalization layer. In doing so, intermediate feature maps are conditioned on both the semantic labels and the noise (see <ref type="figure" target="#fig_0">Fig. 3</ref>). With such a design, the generator produces diverse, noise-dependent images. As the 3D noise is channel-and pixel-wise sensitive, at test time, one can sample the noise globally, per-channel, and locally, per-segment or per-pixel, for controlled synthesis of the whole scene or of specific semantic objects. For example, when generating a scene of a bedroom, one can re-sample the noise locally and change the appearance of the bed alone (see <ref type="figure" target="#fig_5">Fig. 2</ref>). Note that for simplicity during training we sample the 3D noise tensor globally, i.e. per-channel, replicating each channel value spatially along the height and width of the tensor. We analyse alternative ways of sampling 3D noise during training in App. A.7. Using image styles via an encoder, as in SPADE, is also possible in our setting, by replacing noise with encoder features. Lastly, to further reduce the complexity, we remove the first residual block in the generator, reducing the number of parameters from 96M to 72M (see App. C.2) without a noticeable performance loss (see <ref type="table" target="#tab_4">Table 3</ref>).</p><p>Published as a conference paper at ICLR 2021</p><p>Label map Ground truth Pix2pixHD SPADE CC-FPSE OASIS <ref type="figure" target="#fig_5">Figure 5</ref>: Qualitative comparison of OASIS with other methods on ADE20K. Trained with only adversarial supervision, our model generates images with better perceptual quality and structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conduct experiments on three challenging datasets: ADE20K (Zhou et al., 2017), COCO-stuff <ref type="bibr" target="#b4">(Caesar et al., 2018)</ref> and Cityscapes <ref type="bibr" target="#b8">(Cordts et al., 2016)</ref>. Following Qi et al. <ref type="formula" target="#formula_0">(2018)</ref>, we also evaluate OASIS on ADE20K-outdoors, a subset of ADE20K containing outdoor scenes. We follow the experimental setting of Park et al. <ref type="formula" target="#formula_0">(2019)</ref>. We did not use the GAN feature matching loss for OASIS, as we did not observe any improvement with it (see App. A.5), and used the VGG loss only for ablations with λ VGG = 10. We did not experience any training instabilities and, thus, did not employ any extra stabilization techniques. All our models use an exponential moving average (EMA) of the generator weights with 0.9999 decay. For further training details refer to App. C.3.</p><p>Following prior work <ref type="bibr">(Isola et al., 2017;</ref><ref type="bibr" target="#b25">Wang et al., 2018;</ref><ref type="bibr" target="#b35">Park et al., 2019;</ref><ref type="bibr">Liu et al., 2019)</ref>, we evaluate models quantitatively on the validation set using the Fréchet Inception Distance (FID) <ref type="bibr" target="#b14">(Heusel et al., 2017)</ref> and mean Intersection-over-Union (mIoU). FID is known to be sensitive to both quality and diversity and has been shown to be well aligned with human judgement <ref type="bibr" target="#b14">(Heusel et al., 2017)</ref>. We show additional evaluation of quality and diversity with "improved precision and recall" in App. A.9. Mean IoU is used to assess the alignment of the generated image with the ground truth label map, computed via a pre-trained semantic segmentation network. We use Uper-Net101 <ref type="bibr" target="#b27">(Xiao et al., 2018)</ref> for ADE20K, multi-scale DRN-D-105 <ref type="bibr" target="#b29">(Yu et al., 2017)</ref> for Cityscapes, and DeepLabV2 <ref type="bibr" target="#b5">(Chen et al., 2015)</ref> for COCO-Stuff. We additionally propose to compare color and texture statistics between generated and real images on ADE20K to better understand how the perceptual loss influences performance. For this, we compute color histograms in LAB space and measure the earth mover's distance between the real and generated sets <ref type="bibr">(Rubner et al., 2000)</ref>. We measure the texture similarity to the real data as the χ 2 -distance between Local Binary Patterns histograms <ref type="bibr">(Ojala et al., 1996)</ref>. As different classes have different color and texture distributions, we aggregate histogram distances separately per class and then take the mean. Lower values for the texture and color distances indicate a closer similarity to real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MAIN RESULTS</head><p>We use SPADE as our baseline, using the authors' implementation 1 . For a fair comparison, we train this model without the feature matching loss and using EMA <ref type="bibr" target="#b28">(Yaz et al., 2018)</ref> at test phase, which   we further refer to as SPADE+. We found that the feature matching loss has a negligible impact (see App. A.5), while EMA significantly increases the performance for all metrics (see <ref type="table" target="#tab_2">Table 1</ref>).</p><p>OASIS outperforms the current state of the art on all datasets with an average improvement of 6 FID and 5 mIoU points <ref type="table" target="#tab_2">(Table 1)</ref>. Importantly, OASIS achieved the improvement via adversarial supervision alone. On the contrary, SPADE+ does not produce images of high visual quality without the perceptual loss, and struggles to learn the color and texture distribution of real images <ref type="figure" target="#fig_1">(Fig. 6)</ref>. A strong discriminator is the key factor for good performance: without a rich training signal from the discriminator, the SPADE+ generator has to learn through minimizing the VGG loss. With the stronger OASIS discriminator, the perceptual loss does not overtake the generator supervision (see App. A.2), allowing to produce images with the color and texture distribution closer to the real data. <ref type="figure" target="#fig_5">Fig. 5</ref> shows a qualitative comparison of our results to previous models. Our approach noticeably improves image quality, synthesizing finer textures and more natural colors. With the powerful feedback from the discriminator, OASIS is able to learn the appearance of small or rarely occurring semantic classes (which is reflected in the per-class IoU scores presented in App. A.3), producing plausible results even for complex scenes with rare classes and reducing unnatural artifacts.</p><p>Multi-modal image synthesis. In contrast to previous work, OASIS can produce diverse images by directly re-sampling input 3D noise. As 3D noise modulates features directly at every layer of the generator at different scales, matching their resolution, it affects both global and local characteristics of the image. Thus, the noise can be sampled globally, varying the whole image, or locally, resulting in the selected object change while preserving the rest of the scene (see <ref type="figure" target="#fig_5">Fig. 2</ref>).</p><p>To measure the variation in the multi-modal generation, we evaluate MS-SSIM <ref type="bibr" target="#b26">(Wang et al., 2003)</ref> and LPIPS <ref type="bibr">(Zhang et al., 2018b)</ref> between images generated from the same label map. We generate 20 images and compute the mean pairwise scores, and then average over all label maps. The lower the MS-SSIM and the higher the LPIPS scores, the more diverse the generated images are. To assess the effect of the perceptual loss and the noise sampling on diversity, we train SPADE+ with 3D noise or the image encoder, and with or without the perceptual loss. <ref type="table" target="#tab_3">Table 2</ref> shows that OASIS, without perceptual loss, improves over SPADE+ with the image encoder, both in terms of image diversity (MS-SSIM, LPIPS) and quality (mean FID, mIoU across 20 realizations). Using 3D noise further increases diversity for SPADE+. However, a strong quality-diversity tradeoff exists for SPADE+: 3D noise improves diversity at the cost of quality, and the perceptual loss improves quality at the cost of diversity. For OASIS, the VGG loss also reduces diversity but does not noticeably affect quality. Note that in our experiments LabelMix does not notably affect diversity (see App. A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ABLATIONS</head><p>We conduct ablations on ADE20K to evaluate our proposed changes. The main ablation shows the impact of our new discriminator, lighter generator, LabelMix and 3D noise. Further ablations are concerned with architecture changes and the label map encodings in the discriminator, where for fair comparison we use no 3D noise and LabelMix. Main ablation. <ref type="table" target="#tab_4">Table 3</ref> shows that SPADE+ scores low on the image quality metrics without the perceptual loss. Replacing the SPADE+ discriminator with the OASIS discriminator, while keeping the generator fixed, improves FID and mIoU by more than 30 points. Changing the SPADE+ generator to the lighter OASIS generator leads to a negligible degradation of 0.3 in FID and 0.5 in mIoU. With La-belMix FID improves further by ∼ 1 point (more ablations on LabelMix in App. A.4). Adding 3D noise improves FID but degrades mIoU, as diversity complicates the task of the pre-trained semantic segmentation network used to compute the score. For OASIS the perceptual loss deteriorates FID by more than 2 points, but improves mIoU. Overall, without the perceptual loss the new discriminator is the key to the performance boost over SPADE+. Ablation on the discriminator architecture. We train the OASIS generator with three alternative discriminators: the original multi-scale PatchGAN consisting of two networks, a single-scale PatchGAN, and a ResNet-based discriminator, corresponding to the encoder of the U-Net shaped OASIS discriminator. <ref type="table" target="#tab_5">Table 4</ref> shows that the alternative discriminators only perform well with perceptual supervision, while the OASIS discriminator achieves superior performance independent of it. The single-scale discriminators even collapse without the perceptual loss (highlighted in red in <ref type="table" target="#tab_5">Table 4</ref>). Ablation on the label map encoding. We study four different label map encodings: input concatenation, as in SPADE, projection conditioned on the label map <ref type="bibr">(Miyato &amp; Koyama, 2018)</ref>, employing label maps as ground truth for the N +1 segmentation loss, or for the class-balanced N +1 loss (see Sec. 3.2). As shown in <ref type="table" target="#tab_6">Table 5</ref>, input concatenation is not sufficient without additional perceptual loss supervision, leading to training collapse. Without perceptual loss, the N +1 loss outperforms the input concatenation and the projection in both the FID and mIoU metrics. The class balancing noticeably improves mIoU due to better supervision for rarely occurring semantic classes. More ablations can be found in App. A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work we propose OASIS, a semantic image synthesis model that only relies on adversarial supervision to achieve high fidelity image synthesis. In contrast to previous work, our model eliminates the need for a perceptual loss, which often imposes extra constraints on image quality and diversity. This is achieved via detailed spatial and semantic-aware supervision from our novel segmentation-based discriminator, which uses semantic label maps as ground truth for training. With this powerful discriminator, OASIS can easily generate diverse multi-modal outputs by re-sampling the 3D noise, both globally and locally, allowing to change the appearance of the whole scene and of individual objects. OASIS significantly improves over the state of the art in terms of image quality and diversity, while being simpler and more lightweight than previous methods.   <ref type="table" target="#tab_8">Table A</ref> we present a summarized version of our ablations for the ADE20K and Cityscapes dataset. The following observations can be made:</p><p>(1) Looking at the 2nd row of <ref type="table" target="#tab_8">Table A</ref>, we see that the main performance gain comes from the discriminator design (major) (OASIS D,G). The OASIS generator is a lighter version of the SPADE generator, which does not result in a performance improvement <ref type="table" target="#tab_4">(Table 3)</ref>, but has significantly less parameters. A second source of improvement is LabelMix.</p><p>(2) The mIoU can drop when 3D noise is added, as diversity complicates the task of the pre-trained semantic segmentation network that is used to compute the mIoU score. Note that the purpose of noise is not to improve the image quality (FID) but to improve diversity (MS-SSIM).</p><p>(3) The perceptual loss can hurt performance and diversity by biasing the generator towards Ima-geNet, as in this case the target distribution is more difficult to recreate fully. By punishing diversity, the perceptual loss encourages generating images with more standard semantic features This facilitates the task of external pretrained segmenters, and consequently helps to raise the mIoU metric. <ref type="table" target="#tab_2">Table 1</ref> and <ref type="figure" target="#fig_5">Figure 1</ref> illustrate that performance of SPADE+ strongly depends on the perceptual loss. In contrast, OASIS achieves high quality without this loss <ref type="table" target="#tab_2">(Table 1)</ref>. We find the explanation in the fact, that the SPADE+ Patch-GAN discriminator does not provide a strong training signal for the generator. At the absence of strong supervision from the discriminator, the generator resorts to learning mostly from the VGG loss. The loss curves in <ref type="figure" target="#fig_4">Fig. A support this finding:</ref> throughout the training the SPADE+ model focuses on minimizing the VGG loss, keeping the adversarial generator loss more or less constant. In contrast, OASIS significantly improves adversarial generator loss during training, learning to fool the segmentation-based OASIS discriminator. That indicates a better adversarial balance, when the generator learns semantically meaningful features that the segmenter judges as real. The difference in scales of G loss for models comes from different objectives, since SPADE+ optimizes binary cross entropy, and OASIS minimizes multi-class cross entropy with N +1 classes. As seen in <ref type="table" target="#tab_2">Table 1</ref> in the main paper, OASIS significantly outperforms previous approaches in mIoU. We found that the improvement comes mainly from the better IoU scores achieved for lessrepresented semantic classes. To illustrate the gain, we report per-class IoU scores on ADE20k, COCO-Stuff and Cityscapes in Tables B, C and D. For visualization purposes, we sorted the semantic classes of all datasets, ordering by their pixel-wise frequency in the training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 THE INFLUENCE OF VGG ON TRAINING DYNAMICS</head><p>Taking ADE20k as an example, <ref type="table" target="#tab_8">Table B</ref> highlights that the relative gain in mIoU is especially high for the group of less-represented semantic classes, that cover less than 3% of all the images. For these rare classes the relative gain over the baseline exceeds 40%. We found that the gain majorly comes from the per-class balancing applied in the OASIS loss function. In order to illustrate this effect, we train OASIS without the proposed balancing. <ref type="table" target="#tab_8">Table B</ref> reveals, this baseline reaches a bit higher score for frequent classes, but shows worse performance for the rarely occurring ones. This is expected, as the balancing down-weights the objects met frequently while up-weights infrequent classes. We thus conclude that the balancing draws the attention of the discriminator to rarely occurring semantic classes, which results in a much higher quality of the generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 ABLATION ON LABELMIX</head><p>Consistency regularization for the segmentation output of the discriminator requires a method of generating binary masks. Therefore, we compare the effectiveness of CutMix <ref type="bibr" target="#b30">(Yun et al., 2019)</ref> and our proposed LabelMix. Both methods produce binary masks, but only LabelMix respects the boundaries between semantic classes in the label map.    but not as much as LabelMix (69.3). We suspect that since the images are already partitioned through the label map, an additional partition through CutMix results in a dense patchwork of areas that differ by semantic class and real-fake class identity. This may introduce additional label noise during training for the discriminator. To avoid such inconsistency between semantic classes and real-fake identity, the mask of LabelMix is generated according to the label map, providing natural borders between semantic regions, so that the real and fake objects are placed side-by-side without interfering each other. Under LabelMix regularization, the generator is encouraged to respect the natural semantic class boundaries, improving pixel-level realism while also considering the class segment shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 ABLATION ON FEATURE MATCHING LOSS</head><p>We measure the effect of the feature matching loss (FM) in the absence and presence of the perceptual loss (VGG). <ref type="table" target="#tab_8">Table F</ref> and G present the results for OASIS on Cityscapes and SPADE+ on ADE20K. For both SPADE+ and OASIS we observe that the feature matching loss does only affect the FID notably when no perceptual loss is used.</p><p>In the case where no perceptual loss is used, we observe that the feature matching prolongs the time until SPADE+ collapses, resulting in a better FID score (49.7 vs 60.7). Consequently, the mIoU also improves. Hence, the role of the FM loss in the training of SPADE+ is to stabilize the training through additional self-supervision. This observation is in line with the general observation that SPADE and other semantic image synthesis models require the help of additional losses because the adversarial supervision through the discriminator is not strong enough. In comparison, we did not observe any training collapses in OASIS, despite not using any extra losses. For OASIS, the feature matching loss results in a worse FID (by 0.8 points) in the absence of the perceptual loss. We also observe a degradation of 1.1 mIoU points through the FM loss, in the case where the perceptual supervision is present. This indicates that the FM loss negatively affects the strong supervision from the semantic segmentation adversarial loss of OASIS.  <ref type="table" target="#tab_8">Table H</ref> presents the FID and mIoU performance of OASIS with two discriminators operating at scales 1 and 0.5 on Cityscapes. One can see that an additional discriminator at scale 0.5 does not improve performance, but slightly worsens it. The reason that no performance gain is visible is that the OASIS discriminator already encodes multi-scale information through its U-Net structure: skip connections between encoder, decoder and individual blocks integrate information at all scales. In contrast, SPADE requires two discriminators to capture information at different scales. Our 3D noise can contain the same sampled vector for each pixel, or different vectors for different regions. This allows for different noise sampling schemes during training. <ref type="table" target="#tab_8">Table I</ref> shows the effect of using different methods of sampling 3D noise for different locations during training: Image-level sampling creates one global 1D noise vector and replicates it along the height and width of the label map to create a 3D noise tensor. Region-level sampling relies on generating one 1D noise vector per label, and stacking them in 3D to match the height and width of the label map. Pixel-level sampling creates different noise for every spatial position, with no replication taking place. Mix switches between image-level and region-level sampling via a coin flip decision at every training step. With no obvious winner in performance, we choose the simplest scheme (image-level) for our experiments.</p><p>By choosing image-level sampling for training, we thus generate a single 1D latent noise vector of size 64, broadcast it to 64xHxW and concatenate with the label map (NxHxW). This new composite tensor is used as input to the 1st generator layer and at all SPADE-norm layers. The noise is not ignored for the following reasons:</p><p>(1) The noise modulates the activations directly at every layer, so it is very hard to ignore. Here, it is important to emphasize how the noise is used: For SPADE it was observed that label maps are paid more attention to if they are used for location-sensitive conditional batch normalization (CBN). Analogously, we observe that the noise is also paid more attention to when it is injected via CBN. Like label maps, which are 3D tensors of stacked one-hot vectors, we stack noise vectors into a 3D tensor of the same dimensions. Thus, in the same way that SPADE is spatially sensitive to labels, OASIS is spatially sensitive to both labels and noise.</p><p>(2) The 3D broadcasting strategy provides a spatially uniform signal making it easy to embed semantic meaning into the latent code (see interpolations, <ref type="figure" target="#fig_5">Fig. I , J)</ref>. As noise modulates features at different scales in the generator, matching their resolution, it affects both global and local characteristics. This is why a generator trained with image-level noise can perform region-level manipulation at inference <ref type="figure" target="#fig_4">(Fig. F, H)</ref>. However, more evolved spatial noise sampling schemes can be explored in the future.  We performed all our extensive ablations on ADE20K and Cityscapes, due to their shorter training time. Training on ADE20K and Cityscapes takes circa 10 days on 4 Tesla V100 GPUs while training on COCO-stuff can stretch to 4 weeks. Therefore, we only executed essential experiments on COCOstuff. We compare the results of these experiments in <ref type="table" target="#tab_8">Table J</ref>. For SPADE+, it can be seen that without the external perceptual supervision of VGG, training collapses (with FID 99.1 at the best checkpoint before collapse). In contrast, for OASIS image quality is better without VGG (16.7 vs 18.0 FID). When 3D noise is added to OASIS, sampling of multi-modal images is enabled (0.61 vs 1.0 MS-SSIM), with very similar performance in synthesis quality (17.0 vs 16.7 FID) and slightly worse mIoU (44.1 vs 45.5 mIoU) due to the increased variation of generated samples, as the semantic segmentation task of the pre-trained segmentation network becomes harder.</p><p>A.9 ADDITIONAL EVALUATION METRICS Currently, the FID score is the most widely adopted metric for quantifying image quality of GAN models. However, it is often argued that the FID score does not adequately disentangle synthesis quality and diversity <ref type="bibr">(Kynkäänniemi et al., 2019)</ref>. Recently, a series of metrics have been proposed to address this issue by measuring scores related to the concepts of precision and recall <ref type="bibr">(Ravuri &amp; Vinyals, 2019;</ref><ref type="bibr" target="#b18">Shmelkov et al., 2018;</ref><ref type="bibr" target="#b16">Sajjadi et al., 2018;</ref><ref type="bibr">Kynkäänniemi et al., 2019)</ref>. Here, we have a closer look at the "improved precision and recall" score proposed by <ref type="bibr">(Kynkäänniemi et al., 2019)</ref>, where precision is the probability that a generated image falls into the estimated support of the real image distribution, and recall is the probability that a real image falls into the estimated support of the generator distribution. Precision and recall can be interpreted as sample quality and diversity. <ref type="table" target="#tab_8">Table K</ref> presents a comparison of precision (P) and recall R) between SPADE+ and OASIS. It can be seen that OASIS outperforms SPADE+ both in terms of image quality and variety. In this section we present a visual comparison between OASIS and other semantic image synthesis methods. Firstly, we show images generated by <ref type="bibr" target="#b35">SPADE (Park et al., 2019)</ref>, <ref type="bibr">CC-FPSE (Liu et al., 2019)</ref> and OASIS on ADE20k, <ref type="bibr">COCO-Stuff, and Cityscapes (in Figures B, C, and D, respectively)</ref>. A further comparison for SPADE, SPADE+ and OASIS is presented in <ref type="figure" target="#fig_5">Figure E</ref>. We observed that OASIS often produces more visually plausible images than the previous methods. Our method commonly produces finer textures, especially for complex and large semantic objects, e.g building facades, mountains, water.</p><p>We also note that OASIS usually generates brighter and more diverse colors, compared to other methods. As we showed in Section 4 in the main paper, the diversity in colors partially comes from the fact that the feature space of the OASIS generator is not constrained by the VGG loss. We observed that images, generated by SPADE and CC-FPSE, typically have closer colors, while OASIS frequently generates objects with completely different color tones. This also forms one of the failure modes of our approach, when the colors of objects fall out of distribution and seem unnatural (see <ref type="figure" target="#fig_5">Figure G)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 MULTI-MODAL IMAGE SYNTHESIS</head><p>Multi-modal image synthesis for a given label map is easy for OASIS: we simply re-sample noise like in a conventional unconditional GAN model. Since OASIS employs a 3D noise tensor (64-channels×height×width), the noise can be re-sampled entirely ("globally") or only for specific regions in the 2D image plane ("locally"). For our visualizations, we replicate a single 64-dimensional noise vector along the spatial dimensions for global sampling. For local sampling, we re-sample a new noise vector and use it to replace the global noise vector at every spatial position within a restricted area of interest. The results are shown in <ref type="figure" target="#fig_5">Figure F</ref>. The generated images are diverse and of high quality. We observe different degrees of variety for different object classes. For example, buildings change drastically in appearance and often change their spatial orientation with respect to the road. On the other side, many common objects (like tables) vary in color, texture, and illumination, but do not change shapes as they are restricted by the fine details of the region that is outlined for them in the label map.</p><p>Local noise re-sampling does not have to be restricted to only semantic class areas: in <ref type="figure" target="#fig_4">Figure H</ref> we sample a different noise vector for the left and right half of the image, as well as for arbitrarily shaped regions. In effect, the two areas can differ substantially. However, often a bridging element is found between two areas, such as clouds extending partly from one region to the other region of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 LATENT SPACE INTERPOLATIONS</head><p>In <ref type="figure" target="#fig_5">Figure I</ref> we present images that are the results of linear interpolations in the latent space (see <ref type="figure" target="#fig_5">Fig. I</ref>), using an OASIS model trained on the ADE20K dataset. To generate the images, we sample two noise vectors z ∈ R 64 and interpolate them with three intermediate points. The images are synthesized for these five different noise inputs while the label map is held fixed. Note that in <ref type="figure" target="#fig_5">Figure  I</ref> we only vary the noise globally, not locally (See Section 3.3 in the main paper). In contrast, <ref type="figure" target="#fig_5">Figure  J</ref> shows local interpolations. For this, we only re-sample the 3D noise in the area corresponding to a single semantic class. The effect is that only the appearance of the selected semantic class varies while the rest of the image remains fixed. It can be observed that strong changes in a local area can slightly affect the surroundings if the local area is also very big. As such, the clouds are slightly different in the first and last panel of the mountain row and tree row in <ref type="figure" target="#fig_5">Figure J</ref>.</p><p>We see from <ref type="figure" target="#fig_5">Figure I</ref> and J that the trajectories in the latent space are smooth and semantically meaningful. For example, we observe transitions from winter to summer, day to night, green trees to leafless trees, shiny parquet to matt carpet, as well as smooth transitions between buildings with different architectural styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 APPLICATION TO UNLABELLED DATA</head><p>OASIS has a unique property that its discriminator is trained to be an image segmenter. We observed that it shows good performance on this task, reaching the mIoU of 40.0 on ADE20K validation set. For comparison, current state of the art on ADE20K is a mIoU of 46.91, achieved by ResNeST <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>. Such a good segmentation performance allows OASIS to be applied to unlabelled images: given an unseen image without a ground truth annotation, OASIS can predict a label map via the discriminator. Subsequently feeding this prediction to the generator allows to synthesize a scene with the same layout but different style. This property is shown in <ref type="figure" target="#fig_5">Fig. K</ref>. Due to the good segmentation performance, the recreated scenes closely follow the ground truth label map of the original image. The high sensitivity of OASIS to the 3D noise enforces good variability, so the recreations are different from each other. We believe that creating multiple versions of one image while retaining the layout can be useful for data-augmentation.</p><p>B.5 LABELMIX <ref type="figure" target="#fig_5">Figure L</ref> shows additional visual examples of LabelMix regularization, as described in Section 3.2 in the main paper. It can be seen that the discriminator prediction on the mixed images often differs from the mix of individual predictions on real and fake images. In particular, regions that are classified as real in the latter are classified as fake when the images are mixed. This means that the discriminator takes the global context into account for local predictions and thereby often bases the prediction on arbitrary details that should not affect the real-fake class identity. In return, the consistency regularization helps to minimize the difference between these two predictions.    <ref type="bibr">(x,x)</ref> . The consistency regularization then minimizes the distance between the logits of D LabelMix <ref type="bibr">(x,x)</ref> and LabelMix <ref type="bibr">(Dx,Dx)</ref> . In this visualization, black corresponds to the fake class in the N +1 segmentation output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label map</head><p>The generator architecture is built from SPADE ResNet blocks and includes a concatenation of 3D noise with the label map along the channel dimension as an option. The generator can be either trained directly on the label maps or with 3D noise concatenated to the label maps. The latter option is shown in <ref type="table" target="#tab_8">Table M.</ref> OASIS generator drops the first residual block used in Park et al. <ref type="formula" target="#formula_0">(2019)</ref>, which decreases the number of learnable parameters from 96M to 72M. The optional 3D noise injection brings additionally 2M parameters. This sampling scheme is five times lighter than the image encoder used by SPADE (10M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 LEARNING OBJECTIVE AND TRAINING DETAILS</head><p>Learning objective. We train our model with (N +1)-class cross entropy as an adversarial loss. Additionally, the discriminator is regularized with the proposed LabelMix consistency regularization. The full OASIS learning objective thus takes the following form: , where x denotes the real image and (z, t) is the noise-label map. Our objective function is different from SPADE. Their model uses hinge adversarial loss and adds the VGG perceptual loss and a feature matching loss to train the generator. For an easier comparison, we provide the objective function of SPADE: <ref type="bibr">t,x)</ref> [min(0, −1 + D(t, x))] − E (z,t) [min(0, −1 − log D(t, G(z, t))] , where F is the pre-trained VGG network.</p><formula xml:id="formula_6">L SPADE G = −E (z,t) [D(t, G(z, t))] + λ FM E (z,t,x) T i=1 D (i) k (t, x) − D (i) k (t, G(z, t)) 1 + + λ VGG E (z,t,x) N i=1 F (i) (x) − F (i) (G(z, t)) 1 , L SPADE D = −E (</formula><p>Training details. We follow the experimental setting of <ref type="bibr" target="#b35">(Park et al., 2019)</ref>. The image resolution is set to 256x256 for ADE20K and COCO-Stuff and 256x512 for Cityscapes. The Adam <ref type="bibr">(Kingma &amp; Ba, 2015)</ref> optimizer was used with momentums β = (0, 0.999) and constant learning rates (0.0001, 0.0004) for G and D. We did not apply the GAN feature matching loss, and used the VGG perceptual loss only for ablations with λ VGG = 10. The coefficient for LabelMix λ LM was set to 5 for ADE20k and Cityscapes, and to 10 for COCO-Stuff. All our models use an exponential moving average (EMA) of the generator weights with 0.9999 decay . All the experiments were run on 4 Tesla V100 GPUs, with a batch size of 20 for Cityscapes, and 32 for ADE20k and COCO-Stuff. The training epochs are 200 on ADE20K and Cityscapes, and 100 for the larger COCO-Stuff dataset. On average, a complete forward-backward pass with batch size 32 on Ade20k takes around 0.95ms per training image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>SPADE (left) vs. OASIS (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Histogram distances to real data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>VGG and adversarial G losses for SPADE and OASIS, trained with the perceptual loss A.3 PER-CLASS IOU SCORES</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Qualitative comparison of OASIS with other methods on Cityscapes. Qualitative comparison of OASIS with SPADE and SPADE+ using ADE20K (row 1-3), COCO-stuff (row 4-6) and Cityscapes (row 7-9). generated by OASIS on ADE20K with 256 × 256 resolution using different 3D noise inputs. For each label map the noise is re-sampled globally (first row) or locally in the areas marked in red (second row). Note that the images are not stitched together but generated in single forward passes. SPADE CC-FPSE OASISFigureG: Failure mode of OASIS. Our model generates diverse images, sometimes producing object with outlier colors and textures. We compare to Park et al. (2019) and Liu et al. (2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure H :</head><label>H</label><figDesc>Images generated by OASIS in one forward pass (no collage), with different noise vectors for different image regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure I :</head><label>I</label><figDesc>Global latent space interpolations between images generated by OASIS for various outdoor and indoor scenes in the ADE20K dataset at resolution 256 × 256. Latent space interpolations in local regions of the 3D noise, corresponding to a single semantic class. The noise is only changed within the restricted area. Trained on the ADE20K dataset at resolution 256 × 256.GT label map Input image Segmentation Recreation 1 Recreation 2 Recreation 3 Figure K: After training, the OASIS discriminator can be used to segment images. Columns 1-3 show the ground truth label map, real image, and segmentation of the discriminator. Using the predicted label map the generator can produce multiple versions of the original image by resampling noise (Recreations 1-3). Note that this alleviates the need of ground truth maps during inference. Label map Real image x Fake imagex Mask M LabelMix (x,x) D LabelMix (x,x) LabelMix (Dx,Dx) Figure L: Visual examples of LabelMix regularization. Real x and fakex images are mixed using a binary mask M , sampled based on the label map, resulting in LabelMix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>t i,j,c log D(x) i,j,c   − E (z,t) log D(G(z, t)) i,j,c=N +1   + + λ LM D logits LabelMix(x,x, M ) − LabelMix D logits (x), D logits (x), M 2 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison with other methods across datasets.Bold denotes the best performance.</figDesc><table><row><cell>Method</cell><cell># param VGG</cell><cell cols="7">ADE20K FID↓ mIoU↑ FID↓ mIoU↑ FID↓ mIoU↑ FID↓ mIoU↑ ADE-outd. Cityscapes COCO-stuff</cell></row><row><cell>CRN</cell><cell>84M</cell><cell cols="7">73.3 22.4 99.0 16.5 104.7 52.4 70.4 23.7</cell></row><row><cell>SIMS</cell><cell>56M</cell><cell>n/a</cell><cell>n/a</cell><cell cols="3">67.7 13.1 49.7 47.2</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell cols="2">Pix2pixHD 183M</cell><cell cols="7">81.8 20.3 97.8 17.4 95.0 58.3 111.5 14.6</cell></row><row><cell>LGGAN</cell><cell>n/a</cell><cell cols="2">31.6 41.6</cell><cell>n/a</cell><cell>n/a</cell><cell>57.7 68.4</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>CC-FPSE</cell><cell>131M</cell><cell cols="2">31.7 43.7</cell><cell>n/a</cell><cell>n/a</cell><cell cols="3">54.3 65.5 19.2 41.6</cell></row><row><cell>SPADE</cell><cell>102M</cell><cell cols="7">33.9 38.5 63.3 30.8 71.8 62.3 22.6 37.4</cell></row><row><cell>SPADE+</cell><cell>102M</cell><cell cols="7">32.9 42.5 51.1 32.1 47.8 64.0 21.7 38.8 60.7 21.0 65.4 22.7 61.4 47.6 99.1 16.1</cell></row><row><cell>OASIS</cell><cell>94M</cell><cell cols="7">28.3 48.8 48.6 40.4 47.7 69.3 17.0 44.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Multi-modal synthesis evaluation on ADE20K. Bold and red denote the best and the worst performance, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">with VGG Texture</cell><cell>3.2</cell><cell>Color</cell><cell>3.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>w/o VGG</cell><cell></cell><cell>2.4</cell></row><row><cell cols="3">Method Multi-mod. VGG MS-SSIM↓ LPIPS↑ FID↓ mIoU↑</cell><cell>1.7</cell><cell>1.8</cell><cell>2.1 2.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1.4</cell><cell></cell></row><row><cell>SPADE+ Encoder</cell><cell>0.85</cell><cell>0.16 33.4 40.2</cell><cell></cell><cell></cell></row><row><cell>SPADE+ 3D noise OASIS 3D noise</cell><cell>0.35 0.53 0.88 0.65</cell><cell>0.50 58.4 18.7 0.36 34.4 36.2 0.15 31.6 50.8 0.35 28.3 48.8</cell><cell cols="3">OASIS SPADE+</cell><cell>OASIS SPADE+</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>OASIS ablation on ADE20K. Bold denotes the best performance.</figDesc><table><row><cell>G</cell><cell>D</cell><cell>VGG LabelMix FID↓ mIoU↑</cell></row><row><cell cols="2">SPADE+ SPADE+</cell><cell>60.7 21.0</cell></row><row><cell cols="2">SPADE+ OASIS</cell><cell>29.0 52.1</cell></row><row><cell>OASIS</cell><cell>OASIS</cell><cell>29.3 51.6 28.4 50.6</cell></row><row><cell>OASIS +3D noise</cell><cell>OASIS</cell><cell>28.3 48.8 31.6 50.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation on the D architecture. Bold denotes the best performance, red highlights collapsed runs.</figDesc><table><row><cell>D architecture</cell><cell>w/o VGG FID↓ mIoU↑ FID↓ mIoU↑ with VGG</cell></row><row><cell cols="2">MS-PatchGAN (2x) 60.7 21.0 32.9 42.5</cell></row><row><cell>PatchGAN</cell><cell>197 0.62 34.2 42.2</cell></row><row><cell cols="2">ResNet-PatchGAN 147 0.42 32.4 45.1</cell></row><row><cell>OASIS</cell><cell>29.3 51.6 29.2 51.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation on the label map encoding. Bold denotes the best performance, red highlights collapsed runs.</figDesc><table><row><cell>Label encoding</cell><cell>w/o VGG FID↓ mIoU↑ FID↓ mIoU↑ with VGG</cell></row><row><cell cols="2">Input concatenation 280 0.02 30.0 43.9</cell></row><row><cell>Projection</cell><cell>32.4 44.9 28.0 46.9</cell></row><row><cell>N+1 loss</cell><cell>28.3 47.2 28.6 49.8</cell></row><row><cell cols="2">Balanced N+1 loss 29.3 51.6 29.2 51.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision (ECCV), 2016. Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018b. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</figDesc><table><row><cell cols="3">APPENDIX This supplementary material to the main paper is structured as follows: A Additional quantitative results. A.1 Main ablation study on two datasets. A.2 The influence of the perceptual loss on training dynamics. Richard Zhen Zhu, Zhiliang Xu, Ansheng You, and Xiang Bai. Semantically multi-modal image synthesis. A.3 Per-class IoU scores across different datasets. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. A.4 Comparing LabelMix and CutMix for consistency regularization. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International A.5 Ablation on the Feature Matching loss. Conference on Learning Representations (ICLR), 2015. A.6 Ablation on using multiple OASIS discriminators.</cell></row><row><cell cols="3">Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved A.7 Ablation on noise sampling strategies during training.</cell></row><row><cell cols="3">precision and recall metric for assessing generative models. In Advances in Neural Information A.8 Additional experiments on COCO-stuff. Processing Systems, pp. 3927-3936, 2019. A.9Additional image quality metrics.</cell></row><row><cell>Ke Li and Jitendra Malik. B: Additional qualitative results.</cell><cell>Implicit maximum likelihood estimation.</cell><cell>arXiv preprint</cell></row><row><cell cols="2">arXiv:1809.09087, 2018. B.1 Visual comparison of OASIS to other works.</cell><cell></cell></row><row><cell cols="2">B.2 Multi-modal synthesis results for different label maps.</cell><cell></cell></row><row><cell cols="2">B.3 Interpolations between multi-modal images in latent space.</cell><cell></cell></row><row><cell cols="2">B.4 Application to unlabelled data.</cell><cell></cell></row><row><cell cols="2">B.5 Additional visual LabelMix examples.</cell><cell></cell></row><row><cell cols="2">C: A detailed description of the OASIS architecture and its training details.</cell><cell></cell></row><row><cell cols="2">C.1 Discriminator architecture.</cell><cell></cell></row><row><cell cols="2">C.2 Generator architecture.</cell><cell></cell></row><row><cell cols="2">C.3 Learning objective and training details.</cell><cell></cell></row><row><cell cols="2">A QUANTITATIVE RESULTS</cell><cell></cell></row><row><cell cols="2">A.1 SUMMARIZED MAIN ABLATION OVER TWO DATASETS</cell><cell></cell></row><row><cell cols="3">Suman Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models.</cell></row><row><cell cols="2">In Advances in Neural Information Processing Systems, pp. 12268-12279, 2019.</cell><cell></cell></row><row><cell cols="3">Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak</cell></row><row><cell cols="3">Lee. Generative adversarial text to image synthesis. In International Conference on Machine</cell></row><row><cell>learning (ICML), 2016.</cell><cell></cell><cell></cell></row></table><note>Ke Li, Tianhao Zhang, and Jitendra Malik. Diverse image synthesis from semantic layouts via conditional imle. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4220-4229, 2019. Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, et al. Learning to predict layout-to-image condi- tional convolutions for semantic image synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784, 2014. Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In International Conference on Learning Representations (ICLR), 2018. Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations (ICLR), 2018. Evangelos Ntavelis, Andrés Romero, Iason Kastanis, Luc Van Gool, and Radu Timofte. Sesame: Semantic editing of scenes by adding, manipulating or erasing objects. arXiv:2004.04977, 2020. Timo Ojala, Matti Pietikäinen, and David Harwood. A comparative study of texture measures with classification based on featured distributions. Pattern recognition, 29(1):51-59, 1996.Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Xiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun. Semi-parametric image synthesis. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018.Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi- cal image segmentation. In MICCAI, 2015. Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover's distance as a metric for image retrieval. International Journal of Computer Vision (IJCV), 2000.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A :</head><label>A</label><figDesc>Summarized ablation on two datasets. Bold denotes the best performance. Red denotes the worst performance among experiments with 3D noise. Green denotes the major performance gains that are caused by the proposed OASIS discriminator and LabelMix.</figDesc><table><row><cell>Method</cell><cell cols="6">Cityscapes FID↓ mIoU↑ MS-SSIM↓ FID↓ mIoU↑ MS-SSIM↓ ADE20K</cell></row><row><cell>SPADE+</cell><cell>61.4</cell><cell>47.6</cell><cell>1.0</cell><cell>60.7</cell><cell>21.0</cell><cell>1.0</cell></row><row><cell>+ OASIS D, G</cell><cell>54.1</cell><cell>67.6</cell><cell>1.0</cell><cell>29.3</cell><cell>51.6</cell><cell>1.0</cell></row><row><cell>+ 3D noise</cell><cell>51.5</cell><cell>66.3</cell><cell>0.62</cell><cell>28.9</cell><cell>47.3</cell><cell>0.63</cell></row><row><cell>+ LabelMix</cell><cell>47.7</cell><cell>69.3</cell><cell>0.64</cell><cell>28.3</cell><cell>48.8</cell><cell>0.65</cell></row><row><cell>+ VGG</cell><cell>46.1</cell><cell>72.0</cell><cell>0.84</cell><cell>31.6</cell><cell>50.8</cell><cell>0.88</cell></row><row><cell>In</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table B :</head><label>B</label><figDesc>Per-class IoU scores on ADE20k. Bold denotes the best performance.</figDesc><table><row><cell cols="2">Classes IDs Occupied area</cell><cell>SPADE+</cell><cell>mIoU OASIS without per-class balancing</cell><cell>OASIS</cell></row><row><cell></cell><cell></cell><cell>(with VGG)</cell><cell>(without VGG)</cell><cell>(without VGG)</cell></row><row><cell>0 -29</cell><cell>86.4%</cell><cell>63.7</cell><cell>69.1</cell><cell>68.8</cell></row><row><cell>30 -59</cell><cell>7.2%</cell><cell>47.4</cell><cell>52.4</cell><cell>56.6</cell></row><row><cell>60 -89</cell><cell>3.5%</cell><cell>45.3</cell><cell>47.0</cell><cell>51.5</cell></row><row><cell>90 -119</cell><cell>1.8%</cell><cell>29.3</cell><cell>36.2</cell><cell>41.5</cell></row><row><cell>120 -149</cell><cell>1.0%</cell><cell>26.2</cell><cell>31.2</cell><cell>39.7</cell></row><row><cell>0-149 (all classes)</cell><cell>100%</cell><cell>42.4</cell><cell>47.2</cell><cell>51.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table C :</head><label>C</label><figDesc>Per-class IoU scores on COCO-Stuff. Bold denotes the best performance.</figDesc><table><row><cell>Classes IDs</cell><cell>Area</cell><cell cols="2">mIoU SPADE+ OASIS</cell></row><row><cell>0 -35</cell><cell>69.3%</cell><cell>51.1</cell><cell>59.0</cell></row><row><cell>36 -69</cell><cell>15.9%</cell><cell>43.9</cell><cell>50.3</cell></row><row><cell>70 -103</cell><cell>8.7%</cell><cell>40.5</cell><cell>40.9</cell></row><row><cell>104 -137</cell><cell>4.5%</cell><cell>35.9</cell><cell>36.6</cell></row><row><cell>138 -171</cell><cell>1.4%</cell><cell>22.1</cell><cell>40.6</cell></row><row><cell>0-171 (all classes)</cell><cell>100%</cell><cell>38.8</cell><cell>45.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table D :</head><label>D</label><figDesc>Per-class IoU scores on Cityscapes. Bold denotes the best performance.</figDesc><table><row><cell>Classes IDs</cell><cell>Area</cell><cell cols="2">mIoU SPADE+ OASIS</cell></row><row><cell>0 -2</cell><cell>75.6%</cell><cell>91.6</cell><cell>89.6</cell></row><row><cell>3 -6</cell><cell>18.3%</cell><cell>75.7</cell><cell>74.9</cell></row><row><cell>7 -10</cell><cell>3.9%</cell><cell>60.0</cell><cell>66.9</cell></row><row><cell>11 -14</cell><cell>1.4%</cell><cell>60.3</cell><cell>66.0</cell></row><row><cell>15 -18</cell><cell>0.6%</cell><cell>38.1</cell><cell>55.1</cell></row><row><cell>0-18 (all classes)</cell><cell>100%</cell><cell>63.8</cell><cell>69.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table E :</head><label>E</label><figDesc>Ablation study on the impact of LabelMix and CutMix for consistency regularization (CR) in OASIS on Cityscapes. Bold denotes the best performance.</figDesc><table><row><cell cols="4">Transformation FID↓ mIoU ↑</cell></row><row><cell cols="2">No CR</cell><cell>51.5</cell><cell>66.3</cell></row><row><cell cols="2">CutMix</cell><cell>52.1</cell><cell>67.4</cell></row><row><cell cols="2">LabelMix</cell><cell>47.7</cell><cell>69.3</cell></row><row><cell>Table F: OASIS on Cityscapes.</cell><cell></cell><cell cols="2">Table G: SPADE+ on ADE20K.</cell></row><row><cell cols="2">Bold denotes the best performance.</cell><cell cols="3">Bold denotes the best performance.</cell></row><row><cell cols="2">VGG FM FID↓ mIoU↑</cell><cell></cell><cell cols="2">VGG FM FID↓ mIoU↑</cell></row><row><cell>47.7</cell><cell>69.3</cell><cell></cell><cell>60.7</cell><cell>21.0</cell></row><row><cell>48.5</cell><cell>69.1</cell><cell></cell><cell>49.7</cell><cell>32.5</cell></row><row><cell>46.1</cell><cell>72.0</cell><cell></cell><cell>32.9</cell><cell>42.5</cell></row><row><cell>46.5</cell><cell>70.9</cell><cell></cell><cell>32.6</cell><cell>42.9</cell></row><row><cell cols="4">A.6 ABLATION ON USING MORE THAN ONE OASIS DISCRIMINATOR</cell></row></table><note>A major difference between SPADE and OASIS is that OASIS employs only one discriminator, while SPADE uses two PatchGAN discriminators at different scales. Naturally, the question arises how OASIS performs with two discriminators at different scales, as in SPADE. For this,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table H :</head><label>H</label><figDesc>Comparison of using 1 and 2 discriminators at different scales for OASIS on Cityscapes. Bold denotes the best performance.</figDesc><table><row><cell># of OASIS D</cell><cell cols="2">FID↓ mIoU↑</cell></row><row><cell>1 discriminator</cell><cell>47.7</cell><cell>69.3</cell></row><row><cell cols="2">2 discriminators at different scales (1 and 0.5) 48.7</cell><cell>68.8</cell></row><row><cell cols="2">A.7 ABLATION ON NOISE SAMPLING STRATEGIES DURING TRAINING</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table I :</head><label>I</label><figDesc>Different noise sampling strategies during training. Bold denotes the best performance.</figDesc><table><row><cell>Sampling</cell><cell cols="6">Cityscapes FID↓ mIoU↑ MS-SSIM↓ FID↓ mIoU↑ MS-SSIM↓ ADE20K</cell></row><row><cell>image-level</cell><cell>47.7</cell><cell>69.3</cell><cell>0.64</cell><cell>28.3</cell><cell>48.8</cell><cell>0.65</cell></row><row><cell cols="2">region-level 48.1</cell><cell>69.7</cell><cell>0.62</cell><cell>28.8</cell><cell>48.1</cell><cell>0.58</cell></row><row><cell>pixel-level</cell><cell>50.9</cell><cell>65.5</cell><cell>0.84</cell><cell>28.6</cell><cell>34.0</cell><cell>0.68</cell></row><row><cell>mix</cell><cell>46.4</cell><cell>70.9</cell><cell>0.68</cell><cell>28.5</cell><cell>47.6</cell><cell>0.66</cell></row><row><cell cols="4">A.8 ADDITIONAL EXPERIMENTS ON COCO-STUFF</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table J :</head><label>J</label><figDesc>Performance on COCO-stuff. Bold denotes the best perfromance.</figDesc><table><row><cell cols="3">Model VGG 3D noise FID↓ mIoU↑ MS-SSIM↓</cell></row><row><cell>SPADE</cell><cell>22.6 37.4</cell><cell>1.0</cell></row><row><cell>SPADE+</cell><cell>99.1 16.1</cell><cell>1.0</cell></row><row><cell>SPADE+</cell><cell>21.7 38.8</cell><cell>1.0</cell></row><row><cell>OASIS</cell><cell>16.7 45.5</cell><cell>1.0</cell></row><row><cell>OASIS</cell><cell>18.0 44.2</cell><cell>1.0</cell></row><row><cell>OASIS</cell><cell>17.0 44.1</cell><cell>0.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table K :</head><label>K</label><figDesc>Comparison of the precision and recall metric between SPADE+ and OASIS. Bold denotes the best performance.</figDesc><table><row><cell cols="2">B QUALITATIVE RESULTS</cell><cell></cell><cell></cell></row><row><cell cols="3">B.1 COMPARISON TO OTHER METHODS</cell><cell></cell></row><row><cell>Model</cell><cell>ADE20K P↑ R↑</cell><cell>ADE-outd. P↑ R↑</cell><cell>Cityscapes COCO-Stuff P↑ R↑ P↑ R↑</cell></row><row><cell cols="4">SPADE+ 0.71 0.52 0.62 0.51 0.54 0.34 0.63 0.56</cell></row><row><cell>OASIS</cell><cell cols="3">0.77 0.57 0.77 0.56 0.58 0.55 0.67 0.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table M :</head><label>M</label><figDesc>The OASIS generator. N refers to the number of semantic classes, z is noise sampled from a unit Gaussian, y is the label map, interp interpolates a given input to the appropriate spatial dimensions of the current layer.</figDesc><table><row><cell>Operation</cell><cell>Input</cell><cell>Size</cell><cell cols="2">Output Size</cell></row><row><cell>Concatenate</cell><cell>z 3D y</cell><cell>(64,256,256) (N,256,256)</cell><cell>z y</cell><cell>(64+N,256,256)</cell></row><row><cell>Conv2D</cell><cell>interp(z y)</cell><cell>(64+N,8,8)</cell><cell>x</cell><cell>(1024,8,8)</cell></row><row><cell>SPADE-ResBlock</cell><cell>x interp(z y)</cell><cell>(1024,8,8) (64+N,8,8)</cell><cell>up 1</cell><cell>(1024,16,16)</cell></row><row><cell>SPADE-ResBlock</cell><cell>up 1 interp(z y)</cell><cell>(1024,16,16) (64+N,16,16)</cell><cell>up 2</cell><cell>(512,32,32)</cell></row><row><cell>SPADE-ResBlock</cell><cell>up 2 interp(z y)</cell><cell>(512,32,32) (64+N,32,32)</cell><cell>up 3</cell><cell>(256,64,64)</cell></row><row><cell>SPADE-ResBlock</cell><cell>up 3 interp(z y)</cell><cell>(256,64,64) (64+N,64,64)</cell><cell>up 4</cell><cell>(128,128,128)</cell></row><row><cell>SPADE-ResBlock</cell><cell>up 4 interp(z y)</cell><cell>(128,128,128) (64+N,128,128)</cell><cell>up 5</cell><cell>(64,256,256)</cell></row><row><cell cols="2">Conv2D, LeakyRelu, TanH up 5</cell><cell>(64,256,256)</cell><cell>x</cell><cell>(3,256,256)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">github.com/NVlabs/SPADE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGEMENT Jürgen Gall has been supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -EXC 2070 -390732324.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Disentangled image generation through structured noise injection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazeed</forename><surname>Alharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-resolution with deep convolutional sufficient statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPs)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5228" to="5237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A u-net based discriminator for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Schönfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How good is my gan?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhentao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02867</idno>
		<title level="m">Rethinking spatially-adaptive normalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual attention gans for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Edge guided gans with semantic preserving for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13898</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Local class-specific and global imagelevel generative adversarial networks for semantic-guided scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conference on Signals, Systems &amp; Computers</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The unusual effectiveness of averaging in gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Sheng</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Piliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chandrasekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PA-GAN: Improving GAN training by progressive augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">StackGAN++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">In the following, we describe in detail our proposed changes to the discriminator and the generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Architectural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Training Details The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">C.1 DISCRIMINATOR ARCHITECTURE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The increased capacity of the OASIS discriminator allows it to learn a more powerful representation and provide more informative feedback to the generator. Table L: The OASIS discriminator. N refers to the number of semantic classes. Operation Input Size Output Size ResBlock-Down image (3,256,256) down 1 (128,128,128) ResBlock-Down down 1 (128,128,128) down 2 (128,64,64) ResBlock-Down down 2 (128,64,64) down 3 (256</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The architecture of the OASIS discriminator is outlined in Table L. It has in total 22M learnable parameters and is bigger than the multi-scale PatchGAN discriminator (5.5M) used by SPADE Park et</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ResBlock-Up cat</note>
	<note>ResBlock-Up. down 1) (256,128,128) up 6 (64,256,256) Conv2D up 6 (64,256,256) out (N+1,256,256</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">GENERATOR ARCHITECTURE</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
