<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging the Gap Between Spectral and Spatial Domains in Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LITIS Lab</orgName>
								<orgName type="institution">University of Rouen Normandy Rouen</orgName>
								<address>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Renton</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LITIS Lab</orgName>
								<orgName type="institution">University of Rouen Normandy Rouen</orgName>
								<address>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Héroux</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LITIS Lab</orgName>
								<orgName type="institution">University of Rouen Normandy Rouen</orgName>
								<address>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Gaüzère</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LITIS Lab</orgName>
								<orgName type="institution">University of Rouen Normandy Rouen</orgName>
								<address>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Adam</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LITIS Lab</orgName>
								<orgName type="institution">University of Rouen Normandy Rouen</orgName>
								<address>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Honeine</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LITIS Lab</orgName>
								<orgName type="institution">University of Rouen Normandy Rouen</orgName>
								<address>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging the Gap Between Spectral and Spatial Domains in Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Graph Convolutional Neural Networks, Spectral Graph Filter</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper aims at revisiting Graph Convolutional Neural Networks by bridging the gap between spectral and spatial design of graph convolutions. We theoretically demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial or the spectral domain. The obtained general framework allows to lead a spectral analysis of the most popular ConvGNNs, explaining their performance and showing their limits. Moreover, the proposed framework is used to design new convolutions in spectral domain with a custom frequency profile while applying them in the spatial domain. We also propose a generalization of the depthwise separable convolution framework for graph convolutional networks, what allows to decrease the total number of trainable parameters by keeping the capacity of the model. To the best of our knowledge, such a framework has never been used in the GNNs literature. Our proposals are evaluated on both transductive and inductive graph learning problems. Obtained results show the relevance of the proposed method and provide one of the first experimental evidence of transferability of spectral filter coefficients from one graph to another. Our source codes are publicly available at: https://github.com/balcilar/Spectral-Designed-Graph-Convolutions</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past decade, Deep Learning, and more specifically Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), had a strong impact in various applications of machine learning, such as image recognition <ref type="bibr" target="#b0">[1]</ref> and speech analysis <ref type="bibr" target="#b1">[2]</ref>. These successes have mostly been achieved on sequences or images, i.e. on data defined on grid structures which benefit from linear algebra operations in Euclidean spaces. However, there are many domains where data (e.g. social networks, molecules, knowledge graph) cannot be trivially encoded into an Euclidean domain, but can be naturally represented as graphs.</p><p>This explains the recent challenge tackled by the machine learning community which consists in transposing the deep learning paradigm into the world of graphs. The objective is to revisit Neural Networks to operate on graph data, in order to benefit from the representation learning ability. In this context, many Graph Neural Networks (GNNs) have been recently proposed in the literature of geometric learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. GNNs are Neural Networks that rely on the computation of hidden representations of nodes using information carried by the whole graph. In contrast to conventional Neural Network, where the architecture of the network is related to the known and invariant topology of the data (e.g. a 2-D grid for images), the node features of GNNs are propagated according to the graph topology.</p><p>Among GNNs, Convolutional GNNs (ConvGNNs) aim to mimic the simple and efficient solution provided by CNN to extract features through a weight-sharing strategy along the presented data. In images, a convolution relies on the computation of a weighted sum of neighbor's features and weight-sharing is possible thanks to the neighbor relative positions. With graph-structured data, designing such a convolution process is not straightforward. First, there is a variable and unbounded number of neighbors, avoiding the use of a fixed sized window to compute the convolution. Second, no order exists on node neighborhood. As a consequence, one may first redefine the convolution operator to design a ConvGNN.</p><p>As in images, a graph convolution process corresponds to the multiplication of a convolution kernel with the corresponding node feature vectors, followed by a sum or a mean rule. In the literature, there are some instances of trainable and non-trainable convolution kernels for graphs. Regardless if the convolution kernels are trainable or not, and according to the convolution theorem, two strategies have been investigated to design filter kernels, based either on the spectral or the spatial domains.</p><p>Spectral-based convolution filters are defined from a graph signal processing point of view. In a nutshell, a basis is defined by the eigendecomposition of the graph Laplacian matrix. This allows to define the graph Fourier transform, and thus the graph filtering operators. The original form of the spectral graph convolution (non-parametric) can be defined by a function of frequency (eigenvalues). Therefore, this method can theoretically extract information on any frequency. However, despite the solid mathematical foundations borrowed from the signal processing literature, such approaches suffer from (i) a large computational burden induced by the forward/inverse graph Fourier transform, (ii) being spatially non-localized and (iii) the transferability problem, i.e., filters designed using a given graph cannot be applied on other graphs. To alleviate these issues, some approaches based on parameterization using B-spline <ref type="bibr" target="#b6">[7]</ref>, Chebyshev polynomials <ref type="bibr" target="#b7">[8]</ref> and Cayley polynomials <ref type="bibr" target="#b8">[9]</ref> have been proposed. However, these approaches cannot use custom designed frequency response convolution, but only the one determined by B-spline, Chebyshev or Cayley polynomials. That means these methods cannot extract information on some custom band.</p><p>The second strategy is the spatial-based convolution, which is an extension of the conventional Euclidean convolution (e.g. 2D convolution in CNN), by aggregating nodes neighborhood information. Such convolutions have been very attractive due to their less computational complexity, their localized property and their transferability. Spatial-designed graph convolutions have to be a function of some spatial properties of graphs, such as adjacency, Laplacian or degree matrix combined with feature of connected nodes, and edge features. However, since they are designed in the spatial domain, their spectral behavior is not taken into account. We will show in the following that most of the existing spatial-designed convolutions are essentially low-pass filters. As a consequence, they do not have the ability to extract useful information on high frequency or some certain frequency bands. Yet, considering high-frequency information may be intuitively useful for some real-world problems where information localized on particular nodes have a strong influence on graph's property. For instance, molecular toxicity can be induced by some pharmacophores, i.e., particular subparts of molecule, which can consist in only one atom. Using only low-pass filters on such molecules will diffuse this discriminant information within the whole graph whereas a high-pass filter may help to highlight this useful difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>In this paper, we bridge the gap between spectral and spatial domains for ConvGNNs. Our first contribution consists in demonstrating the equivalence of convolution processes regardless if they are designed in the spatial or the spectral domain. Taking advantage of this result, our second contribution is to provide a spectral analysis of existing graph convolutions for four popular ConvGNNs, known as GCN <ref type="bibr" target="#b9">[10]</ref>, ChebNet <ref type="bibr" target="#b7">[8]</ref>, CayleyNet <ref type="bibr" target="#b8">[9]</ref> and Graph Attention Networks (GAT) <ref type="bibr" target="#b10">[11]</ref>. Using these results, our third contribution is to design new convolutions in the spectral domain with a custom frequency profile that provides a better convolution process. In this context, we also propose a spectraldesigned multi-convolution method under the depthwise separable convolution framework. To the best of our knowledge, such a framework has never been used in the GNNs literature. It allows to decrease the total number of trainable parameters by keeping the variability capacity of the model at a maximum level.</p><p>Our proposal is assessed on both transductive and inductive learning problems <ref type="bibr" target="#b11">[12]</ref>. In both settings, we show the relevance of the proposed method on well-known public benchmark datasets. Especially, the success of the proposed method on inductive problems provides one of the first experimental evidence of transferability of spectral filter coefficients from one graph to another.</p><p>The remainder of this paper is organized as follows. In Section 2, we introduce ConvGNNs and we review existing approaches. Then, Section 3 describes the three main contributions mentioned above. Section 4 presents a series of experiments and results which validate our propositions. Finally, Section 5 is dedicated to the conclusion. Let G be a set of graphs, where each graph G (k) has n k nodes and an arbitrary number of edges. Node-to-node connectivity in G (k) is given by the adjacency matrix A (k) . For unweighted graphs, A (k) ∈ {0, 1} n k ×n k , while for weighted graphs, A (k) ∈ R n k ×n k . In this paper, we consider undirected attributed graphs. Hence, A (k) is symmetric and features are defined on nodes by X (k) ∈ R n k ×f0 , with f 0 the length of feature vectors.</p><p>In the literature, there are three different types of learning problems on graphs. The first one is the single graph node classification or regression problem. In this case, G is reduced to a single graph denoted G, with n nodes. Some of the nodes are labeled for training and the task is to predict the labels of unlabeled nodes. For a classification problem, the output would be represented by Y ∈ {0, 1} n×nc , i.e., a one-hot class encoding of the n c possible classes for each node. For a node regression problem, the output would be Y ∈ R n . The second type of problems is multi-graph node classification or regression problem. In such cases, the output is defined as a set of Y (k) ∈ {0, 1} n k ×nc for classification or Y (k) ∈ R n k for regression. The last type is the entire graph classification or regression problem, in which case the output must be Y (k) ∈ {0, 1} nc or Y (k) ∈ R for classification and regression problems, respectively.</p><p>Problems of the first type are transductive problems, while problems of the two last types are inductive since test data are completely unknown during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Literature review</head><p>For reviewing ConvGNNs, we use the classical "spectral vs. spatial" dichotomy <ref type="bibr" target="#b12">[13]</ref>. Beyond, we propose for this review a third category called Spectral-Rooted Spatial Convolutions which gathers recent and efficient methods that take their foundations in the spectral domain, but apply them in the spatial one, without computing the graph Fourier transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Spectral ConvGNN</head><p>Spectral ConvGNNs rely on the spectral graph theory <ref type="bibr" target="#b13">[14]</ref>. In this framework, signal on graphs are filtered using eigendecomposition of graph Laplacian <ref type="bibr" target="#b14">[15]</ref>. A graph Laplacian is defined by L = D − A (or L = I − D −1/2 AD −1/2 for the normalized version), where A is the adjacency matrix, D ∈ R n k ×n k is the diagonal degree matrix with entries D i,i = j A j,i and I is the identity matrix. Since the Laplacian is positive semidefinite, it can be decomposed into L = U ΣU T where U is the eigenvectors matrix and Σ = diag(λ) where λ denotes the vector of the positive eigenvalues. The graph Fourier transform of any unidimensional signal on graph is defined by x f t = U x and its inverse is given by x = U x f t . By transposing the convolution theorem to graphs, the spectral filtering in the frequency domain can be defined by</p><formula xml:id="formula_0">x f iltered = U diag( (λ))U x,<label>(1)</label></formula><p>where (λ) is the desired filter function applied to the eigenvalues λ. As a consequence, a graph convolution layer in spectral domain can be written by a sum of filtered signals followed by an activation function as in <ref type="bibr" target="#b6">[7]</ref>, namely</p><formula xml:id="formula_1">H (l+1) j = σ f l i=1 U diag(F i,j,l )U H (l) i ,<label>(2)</label></formula><p>for all j ∈ {1, . . . , f l+1 }. Here, σ is the activation function such as RELU (REctified Linear Unit), H</p><p>i is the i-th feature vector of the l-th layer, F i,j,l ∈ R n is the corresponding weight vector whose size is the number of eigenvectors (also n, the number of nodes). A spectral ConvGNN based on (2) seeks to tune the trainable parameters F i,j,l , as proposed in <ref type="bibr" target="#b15">[16]</ref> for the single-graph problem.</p><p>A first drawback is the necessity of Fourier and inverse Fourier transform by matrix multiplication of U and U T . Another drawback occurs when generalizing the approach to multi-graph learning problems. Indeed, the k-th element of the vector F i,j,l weights the contribution of the k-th eigenvector to the output. Those weights are not shareable between graphs of different sizes, which means a different length of F i,j,l is needed. Moreover, even though the graphs have the same number of nodes, their eigenvalues will be different if their structures differ. As a consequence, a given weight F i,j,l may correspond to different eigenvalues in different graphs.</p><p>To overcome these issues, a few spatially-localized filters have been defined such as cubic B-spline parameterization <ref type="bibr" target="#b6">[7]</ref> and polynomial parameterization <ref type="bibr" target="#b7">[8]</ref>. With such approaches, trainable parameters are defined by:</p><formula xml:id="formula_3">F i,j,l = B W (l,1) i,j , . . . , W (l,S) i,j ,<label>(3)</label></formula><p>where B ∈ R n×S is the initial designed matrix and W (l,s) is the trainable matrix for the l-th layer's s-th convolution kernel, W (l,s) i,j is the (i, j)-th entry of W (l,s) and S is the desired number of convolution kernels. Each column in B is designed as a function of eigenvalues, namely B i,j = ( j (λ i )). In the polynomial case, each column of B is power of eigenvalues starting at 0-th and ending at (S − 1)-th power. In the cubic B-spline case, the B matrix encodes the cubic B-spline coefficients <ref type="bibr" target="#b6">[7]</ref>. A very recent ConvGNN named CayleyNet parameterizes trainable coefficients by F i,j,l = [g i,j,l (λ 1 , h), ..., g i,j,l (λ n , h)] , where h is a scale parameter to be learned, λ n is the n-th eigenvalue, and g is a spectral filter function defined as follows in <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_4">g(λ, h) = c 0 + 2Re r k=1 c k hλ − i hλ + i k<label>(4)</label></formula><p>where i 2 = −1, Re(·) is the function returning the real part, c 0 is a real trainable coefficient, and for k = 1, . . . , r, c k are the complex trainable coefficients. The CayleyNet parameterization takes also the form (3), as shown in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Spatial ConvGNN</head><p>Spatial ConvGNNs can be generalized as propagation of node features to the neighborhood nodes followed by activation function, of the form</p><formula xml:id="formula_5">H (l+1) = σ s C (s) H (l) W (l,s) ,<label>(5)</label></formula><p>where H (l) ∈ R n×f l is the l-th layer's feature matrix with n nodes and f l features, s indexes the convolution kernels, C (s) is the convolution kernel that defines how the node features are propagated to the neighborhood nodes, W (l,s) ∈ R f l ×f l+1 is the trainable weight matrix that maps the f l -dimensional features into f l+1 dimensions. <ref type="figure" target="#fig_0">Figure 1</ref> provides a detailed schematic of graph convolution layer on a sample graph signal. The selection of convolution kernels defines the method in the literature. The vanilla version uses a single convolution kernel with C = A + I. Such a spatial ConvGNN has an effect of low-pass filtering, since it applies the same coefficients to all neighbors and to the node itself. High-pass filters can be obtained by differentiating the weight matrices used to compute neighbors and self-contributions <ref type="bibr" target="#b16">[17]</ref>. In such a case, the convolution process is given by C (1) = A and C (2) = I.</p><p>Some solutions have been proposed to overcome the limitations of using only low-pass and high-pass filters. If nodes have discrete labels (unless the node's degree can be used as discrete feature), weights can be shared by the neighbors whose labels are the same <ref type="bibr" target="#b17">[18]</ref>. Another method consists in defining an ordering on nodes included within the receptive field of convolution, and sharing the coefficients according to this reordering <ref type="bibr" target="#b18">[19]</ref>. The reordering process is called canonical node reordering. A similar sharing approach, based on reordered neighbors, was presented in <ref type="bibr" target="#b19">[20]</ref>. The difference is that the reordering is computed according to the absolute correlation of features to the center node. A different spatial-designed method proposed in <ref type="bibr" target="#b20">[21]</ref> considers a diffusion process on the graph using random walks. This allows to induce variability on output signal by applying random walks of different lengths to the different features.</p><p>All aforementioned spatial graph convolutions use fixed-design matrices C (s) and variability is induced by W (l,s) in <ref type="bibr" target="#b4">(5)</ref>. Other methods use trainable convolution kernels in order to make the convolutions more productive in terms of output signal frequency profiles, such as graph attention networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, MoNet <ref type="bibr" target="#b22">[23]</ref> and SplineCNN <ref type="bibr" target="#b23">[24]</ref>. The attention mechanism tunes each element of the convolution kernel of the l-th layer C (l,s) , which is defined as a function of connected nodes features and some trainable parameter</p><formula xml:id="formula_6">C (l,s) i,j = f H (l) i , H (l) j , W (l,s) AT ,<label>(6)</label></formula><p>where H (l) i ∈ R f l is the i-th node's feature vector for layer l, W (l,s)</p><p>AT encodes the trainable parameter of the s-th convolution kernel for layer l and f is some element-wise function to be selected. In this case, since convolution kernels are learned through the W (l,s) AT of (6), the trainable parameters W (l,s) of (5) can be defined as the identity matrix, or other trainable parameters that may be shared with W (l,s) AT . The most influential attention mechanism applied on graph data, called GAT <ref type="bibr" target="#b10">[11]</ref>, uses multi-attention weights (denoted as multi-support convolution kernels), with f H . Two convolution kernels C <ref type="bibr" target="#b0">(1)</ref> and C <ref type="bibr" target="#b1">(2)</ref> are used. This architecture has 12 trainable parameters, omitting biases.</p><formula xml:id="formula_7">(l) i , H (l) j , W (l,s) AT = softmax j σ(a[WH (l) i ||WH (l) j ]) ,<label>(7)</label></formula><p>where two linear transformations are considered by elements of general trainable parameter set W (l,s) AT = {a, W}, with a being a weight vector. The operator || is the concatenation operator, σ corresponds to the LeakyReLU function <ref type="bibr" target="#b24">[25]</ref> and softmax j is the normalized exponential function that uses all neighbors of i-th node to normalize edge of i-th to j-th node. In convolution layer's output calculation <ref type="bibr" target="#b4">(5)</ref>, GAT proposes to use the same parameters W. The main limitation of this method is the use of a very small context, limited to the features of the pair of nodes, to determine the intensity of the attention. Dual-Primal Graph CNN (DPGCNN) <ref type="bibr" target="#b25">[26]</ref> extends this approach by defining attention using new features computed from the neighborhood of each node of the pair, hence using a larger context.</p><p>Since the methods mentioned above are defined in the spatial domain, they do not provide any analysis of their frequency spectrum of filters. Moreover, their frequency responses will be different for different graphs. Besides, they need more multi-support (attention or sub-layer weights) to produce high variability output, which drastically increases the number of trainable parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Spectral-rooted Spatial Convolutions</head><p>As said before, some methods have recently been proposed to get rid of the computation burden of graph Fourier and inverse graph Fourier transforms, while still taking their foundations in the spectral domain. These solutions rely on the approximation of a spectral graph convolution proposed in <ref type="bibr" target="#b26">[27]</ref>, based on the Chebyshev polynomial expansion of the scaled graph Laplacian. Accordingly, the first two Chebyshev kernels are C (1) = I and C (2) = 2L/λ max − I and the remaining kernels are defined by</p><formula xml:id="formula_8">C (k) = 2C (2) C (k−1) − C (k−2) .<label>(8)</label></formula><p>Researchers have shown that any desired filter can be written as a linear combination of these kernels <ref type="bibr" target="#b26">[27]</ref>. ChebNet is the first method that used these kernels in ConvGNN <ref type="bibr" target="#b7">[8]</ref>.</p><p>One major extension and simplification of the Chebyshev polynomial expansion method is Graph Convolution Network (GCN) <ref type="bibr" target="#b9">[10]</ref>. GCN uses the subtraction of the second Chebyshev kernels from the first one under the assumption of λ max = 2 and L is the normalized graph Laplacian. However, instead of using this subtracted kernel, they used re-normalization trick and defined the final single kernel by:</p><formula xml:id="formula_9">C = D −1/2 A D −1/2 ,<label>(9)</label></formula><p>with D i,i = j A i,j and A = (A + I) the adjacency matrix with added self-connections. This approach influenced many other contributions. The method described in <ref type="bibr" target="#b27">[28]</ref> directly uses this convolution but changes the network architecture by adding a fully connected layer as the last layer. The MixHop algorithm <ref type="bibr" target="#b28">[29]</ref> uses the 2nd or 3rd powers of the same convolution.</p><p>The methods described in this section are quite different from pure spatial and pure spectral convolutions. They are not designed by using eigenvalues, but are implicitly designed as a function of structural information (adjacency, Laplacian) and perform convolution in spatial domain as how all spatial convolutions do. However, their frequency profiles are stable for different arbitrary graphs as how spectral convolutions do. This aspect will be theoretically and experimentally illustrated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bridging Spatial and Spectral ConvGNN</head><p>This section presents the main theoretical contributions of this paper. First, we provide a theoretical analysis demonstrating that parameterized spectral ConvGNNs can be implemented as spatial ConvGNNs when they use a fixed frequency profile matrix B. Then, using this result, some state-of-the-art GNNs described in the previous section are analyzed from a spectral point of view. This analysis provide a better understanding on these convolutions and reveal their problematic sides. Finally, we propose a new method that fully exploits spectral graph convolution capabilities, called Depthwise Separable Graph Convolution Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Theoretical analysis</head><formula xml:id="formula_10">Theorem 1. Spectral ConvGNN parameterized with fixed frequency profiles matrix B of entries B i,j = j (λ i ), defined as H (l+1) j = σ f l i=1 U diag B W (l,1) i,j , . . . , W (l,S) i,j U H (l) i ,<label>(10)</label></formula><p>is a particular case of spatial ConvGNN, defined as</p><formula xml:id="formula_11">H (l+1) = σ s C (s) H (l) W (l,s) ,<label>(11)</label></formula><p>with the convolution kernel set to</p><formula xml:id="formula_12">C (s) = U diag( s (λ))U ,<label>(12)</label></formula><p>where the columns of U are the eigenvectors of the studied graph, σ is the activation function,</p><formula xml:id="formula_13">H (l) ∈ R n×f l is the l-th layer's feature matrix with f l features, H (l)</formula><p>i is the i-th column of H (l) , B ∈ R n×S is an apriori designed matrix for each graph's eigenvalues, and s (λ) is the s-th column of B. Both W (l,s) and S are defined in (3).</p><p>Proof. First, let us expand the matrix B and rewrite it as the sum of its columns, denoted 1 (λ), . . . , S (λ) ∈ R n :</p><formula xml:id="formula_14">H (l+1) j = σ f l i=1 U diag S s=1 W (l,s) i,j s (λ) U H (l) i .<label>(13)</label></formula><p>Now, we distribute U and U over the inner summation:</p><formula xml:id="formula_15">H (l+1) j = σ S s=1 f l i=1 U diag W (l,s) i,j s (λ) U H (l) i .<label>(14)</label></formula><p>Then, we take out the scalars W (l,s) i,j of the diag operator:</p><formula xml:id="formula_16">H (l+1) j = σ S s=1 f l i=1 W (l,s) i,j U diag( s (λ))U H (l) i .<label>(15)</label></formula><p>Let us define a convolution operator C (s) ∈ R n×n as:</p><formula xml:id="formula_17">C (s) = U diag( s (λ))U .<label>(16)</label></formula><p>Using <ref type="formula" target="#formula_0">(15)</ref> and <ref type="formula" target="#formula_0">(16)</ref>, we have thus:</p><formula xml:id="formula_18">H (l+1) j = σ f l i=1 S s=1 W (l,s) i,j C (s) H (l) i .<label>(17)</label></formula><p>Then, each term of the sum over s corresponds to a matrix H (l+1) ∈ R n×f l+1 with</p><formula xml:id="formula_19">H (l+1) = σ C (1) H (l) W (l,1) + · · · + C (S) H (l) W (l,S) ,<label>(18)</label></formula><p>with</p><formula xml:id="formula_20">H (l) = [H (l) 1 , . . . , H (l) f l ].</formula><p>We get by grouping the terms:</p><formula xml:id="formula_21">H (l+1) = σ S s=1 C (s) H (l) W (l,s) ,<label>(19)</label></formula><p>which corresponds to <ref type="bibr" target="#b10">(11)</ref>. Therefore, (10) corresponds to <ref type="bibr" target="#b10">(11)</ref> with C (s) defined as <ref type="bibr" target="#b15">(16)</ref>.</p><p>This theorem is general, since it covers many well-known spectral ConvGNNs, such as non-parametric spectral graph convolution <ref type="bibr" target="#b15">[16]</ref>, polynomial parameterization <ref type="bibr" target="#b7">[8]</ref>, cubic B-spline parameterization <ref type="bibr" target="#b6">[7]</ref> and CayleyNet <ref type="bibr" target="#b8">[9]</ref>.</p><p>From Theorem 1, designing a graph convolution either in spatial or in spectral domain is equivalent. Therefore, Fourier calculations are not necessary when convolutions are parameterized by an initially designed matrix B. Using that relation, it is not difficult to show the spatial equivalence of non-parametric spectral graph convolution defined in <ref type="bibr" target="#b1">(2)</ref>. It can be written in spatial domain with B = I in (3). It thus corresponds to <ref type="bibr" target="#b10">(11)</ref> where each convolution kernel is defined by</p><formula xml:id="formula_22">C (s) = U s U s , where U s is the s-th eigenvector.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spectral Analysis of Existing Graph Convolutions</head><p>This section aims at providing a deeper understanding of the graph convolution process through an analysis of existing GNNs in the spectral domain. To the best of our knowledge, no one has led such an analysis concerning graph convolutions in the literature. In this section, we show how it can be done on four well-known graph convolutions: ChebNet <ref type="bibr" target="#b7">[8]</ref>, CayleyNet <ref type="bibr" target="#b8">[9]</ref>, GCN <ref type="bibr" target="#b9">[10]</ref> and GAT <ref type="bibr" target="#b10">[11]</ref>. This analysis is led using the following corollary of Theorem 1.</p><p>Corollary 1.1. The frequency profile of any given graph convolution kernel C (s) can be defined in spectral domain by the vector</p><formula xml:id="formula_23">s (λ) = diag −1 (U C (s) U ).<label>(20)</label></formula><p>Proof. By using (12) from Theorem 1, we can obtain a spatial convolution kernel C (s) whose frequency profile is s (λ). Since the eigenvector matrix is orthonormal (i.e., U −1 = U ), we can extract s (λ), which yields <ref type="bibr" target="#b19">(20)</ref>.</p><p>We denote the matrix s = U C (s) U as the full frequency profile of the convolution kernel C (s) , and s (λ) = diag( s ) as the standard frequency profile of the convolution kernel. The full frequency profile includes all eigenvectorto-eigenvector pairs contributions. Standard frequency profile just includes each eigenvector's self-contribution.</p><p>To show the frequency profiles of some well-known graph convolutions, we used three graphs. The first one corresponds to a 1D signal encoded as a regular circular line graph with 1001 nodes. The second and third ones are the Cora and Citeseer reference datasets, which consist of one single graph with respectively 2708 and 3327 nodes <ref type="bibr" target="#b11">[12]</ref>. Basically, each node of these graphs is labeled by a vector, and edges are unlabeled and undirected. These two graphs will be described in details in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChebNet</head><p>After computing the kernels of ChebNet by <ref type="formula" target="#formula_8">(8)</ref>, Corollary 1.1 can be used to obtain their frequency profiles. As shown in Appendix A, the first two kernel frequency profiles of ChebNet are 1 (λ) = 1 and 2 (λ) = 2λ/λ max − 1, where 1 is the vector of ones. Since λ max = 2 for all three graphs, we get 2 (λ) = λ − 1. The third one and following kernel frequency profiles can also be computed using k (λ) = 2 2 (λ) k−1 (λ) − k−2 (λ), leading to 3 (λ) = λ 2 − 4λ + 1 for example for the third kernel. The resulting 5 frequency profiles are shown in <ref type="figure" target="#fig_1">Figure 2</ref> (in absolute value). Since the full frequency profiles consist of zeros outside the diagonal, they are not illustrated.</p><p>Analyzing the frequency profile of ChebNet, one can argue that the convolutions mostly cover the spectrum. However, none of the kernels focuses on some certain parts of the spectrum. As an example, the second kernel is mostly a low-pass and high-pass filter and stops the middle band, while the third one passes very high, very low and middle bands, but stops almost first and third quarter of the spectrum. Therefore, if the relation between input-output pairs can be figured out by just a low-pass, high-pass or some specific band-pass filter, a high number of convolution kernels is needed. However, in the literature, only 2 or 3 kernels are generally used for experiments <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CayleyNet</head><p>CayleyNet uses spectral graph convolutions whose frequency profiles can be changed by scaling eigenvalues <ref type="bibr" target="#b8">[9]</ref>. The frequency profile is defined by a complex rational function of eigenvalues, scaled by a trainable parameter h in (4).</p><p>As proven in Appendix B, CayleyNet can be defined through the frequency profile matrix B. Using this representation, CayletNet can be seen as multi-kernel convolutions with real-valued trainable coefficients. According to this analysis, CayleyNet uses 2r + 1 graph convolution kernels, with r being the number of complex coefficients <ref type="bibr" target="#b8">[9]</ref>. The first 7 kernel's frequency profiles are illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. The scale parameter h affects the x-axis scaling but does not change the global shape. When h = 1, frequency profiles can be defined within the range [0, 2] (because λ max = 2 in all three test graphs). If h = 1.5, the frequency profile can be defined till 1.5λ max = 3 in <ref type="figure" target="#fig_2">Figure 3</ref> and rescale axis label from [0, 3] to [0, 2] in original range.</p><p>Learning the scaling of eigenvalues may seem advantageous. However, it induces extra computational cost in order to calculate the new convolution kernel. To limit this cost, an approximation is computed using a fixed number of Jacobi iterations <ref type="bibr" target="#b8">[9]</ref>.</p><p>In addition, similarly to ChebNet, CayleyNet does not have any band specific convolutions, even when considering different scaling factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN</head><p>As for ChebNet, a theoretical analysis of frequency profiles of GCN convolution is carried out in Appendix C. It shows that GCN frequency profile can be approximated according to (λ) ≈ 1 − λd/(d + 1), where d is the average node degree. Therefore, the cut-off frequency of the GCN convolution is λ cut ≈ (1 + d)/d. Theoretically, if all nodes degree are different, standard frequency profile will not be smooth and will include some perturbations. In addition, full frequency profile will be composed of non-zero components.</p><p>Analyzing experimentally the behavior of GCN <ref type="bibr" target="#b9">[10]</ref> in the spectral domain first implies to compute the convolution kernel as given in <ref type="bibr" target="#b8">(9)</ref>. Then, the spectral representation of the obtained convolution matrix can be back-calculated using Corollary 1.1. This result leads to the frequency profiles illustrated in <ref type="figure" target="#fig_3">Figure 4</ref> for the three different graphs. The three standard frequency profiles have almost the same low-pass filter shape corresponding to a function composed of a decreasing part on the three first quarters of the eigenvalues range, followed by an increasing part on the remaining range. This observation is coherent with the theoretical analysis. Hence, kernels used in GCN are transferable across the three graphs at hand. In <ref type="figure" target="#fig_3">Figure 4</ref>, the cut-off frequency of the 1-D linear circular graph is exactly 1.5, while it is about 1.35 for Citeseer. This observation can be explained by the fact that when considering a 1-D linear circular graph, all nodes have a degree equal to 2, hence λ cut = 1.5. Since the average node degree in Citeseer is 2.77, therefore λ cut ≈ 1.36.</p><p>Concerning the full frequency profiles, there is no contribution outside the diagonal for the regular line graph <ref type="figure" target="#fig_3">(Figure 4</ref> b). Conversely, some off-diagonal values are not null for Citeseer and Cora. Again, this observation confirms the theoretical analysis.</p><p>Since GCN frequency profile does not cover the whole spectrum, such an approach is not able to learn relations that can be represented by high-pass or band-pass filtering. Hence, even though it gives very good results on a single graph node classification problem in <ref type="bibr" target="#b9">[10]</ref>, it may fail for problems where discriminant information lies in particular frequency bands. Therefore, such an approach can be considered as problem specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAT</head><p>Graph attention networks (GATs) rely on trainable convolutions kernels <ref type="bibr" target="#b10">[11]</ref>. For this reason, frequency profiles cannot be directly computed similarly to GCN or ChebNet ones. Thus, instead of back-calculating the kernels, we perform simulations and evaluate the potential kernels of attention mechanism for given graphs. Hence, we show the frequency profiles of those simulated potential kernels.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, 8 different attention heads are used. Assuming that each attention head matrix is a convolution kernel, multi-attention systems can be seen as multi-kernel convolutions. The difference is that convolution kernels are not a priori defined but are functions of node feature vectors and trainable parameters a and W; see <ref type="bibr" target="#b6">(7)</ref>. To show the potential output of GATs on the Cora graph (1433 features for each node), we produce 250 random pairs of W ∈ R 1433×8 and a ∈ R 16×1 , which correspond to the convolution kernels trained by GATs. The σ function in <ref type="formula" target="#formula_7">(7)</ref> is a LeakyReLU activation with a 0.2 negative slope as in <ref type="bibr" target="#b10">[11]</ref>. The mean and standard deviation of the frequency profiles for these simulated GAT kernels are shown in <ref type="figure">Figure 5</ref>. As one can see, the mean standard frequency profile has a similar shape as those of GCN ( <ref type="figure" target="#fig_3">Figure 4</ref>). However, variations on the frequency profile induce more variations on output signal when compared to GCN.</p><p>The full frequency profile is not symmetric. According to <ref type="figure">Figure 5</ref>, variations are mostly on the right side of the diagonal in the full frequency profile. This is related to the fact that these convolution kernels are not symmetric. However, the variation on frequency profile might not be sufficient in problems that need some specific band-pass filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>This section has shown that most influential graph convolutions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> operate as low-pass filters. Interestingly, while being restricted to low-pass filters, they still obtain state-of-the-art performance on particular node classification problems such as Cora and Citeseer <ref type="bibr" target="#b11">[12]</ref>. These results on these particular problems are induced by the nature of the graphs to be processed. Indeed, citation network problems are inherently low-pass filtering problems, similarly to image segmentation problems, which are efficiently tackled by low-pass filtering.</p><p>It is worth noting that, if we use enough convolution kernels, the frequency response of ChebNet kernels <ref type="bibr" target="#b7">[8]</ref> covers nearly all frequency profiles. However, these frequency responses are not specific to special bands of frequency. It means that they can act as high-pass filters, but not as Gabor-like special band-pass filters.</p><p>As a conclusion, we claim that graph convolutions presented in this section are problem specific and not problem agnostic. Experiments conducted in Section 4 provide empirical results to validate the theoretical analysis conducted in this section. . There are three shared coefficients. Each coefficient encodes the contribution of corresponding frequency profiles. First row refers mostly to high frequencies, middle row to middle frequencies and last row to low frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Depthwise Separable Graph Convolutions</head><p>Instead of designing the spatial convolution kernels C (s) of (5) by functions of graph adjacency and/or graph Laplacian, we propose in this section to use S convolution kernels that have custom-designed standard frequency profiles. These designed frequency profiles are a function of eigenvalues, such as [ 1 (λ), . . . , S (λ)]. In this proposal, the number of kernels and their frequency profiles are hyperparameters. Then, we can back-calculate corresponding spatial convolution matrices using <ref type="bibr" target="#b11">(12)</ref> in Theorem 1.</p><p>To obtain problem-agnostic graph convolutions, the sum of all designed convolutions' frequency profiles has to cover most of the possible spectrum and each kernel's frequency profile must focus on some certain ranges of frequencies. As a didactic example, we show in <ref type="figure">Figure 6</ref> an example of desired spectral convolutions frequency profiles for S = 3 and its application on two different graphs.</p><p>In order to figure out arbitrary relations of input-output pairs, multiple convolution kernels have to be efficiently designed. However, increasing the number S of convolution kernels increases the number of trainable parameters linearly. Hence, the total number of multi-support ConvGNN is given by S L i=0 f i f i+1 where L is the number of layers and f i is the feature length of the i-th layer.</p><p>To overcome this issue, we propose to use Depthwise Separable Graph Convolution Network (DSGCN). Depthwise Separable Convolution framework has already been used in computer vision problems to reduce the model size and its complexity <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. To the best of our knowledge, depthwise separable graph convolution has never been proposed in the literature. . Here, two convolution kernels are used, denoted by C <ref type="bibr" target="#b0">(1)</ref> and C <ref type="bibr" target="#b1">(2)</ref> . Convoluted signals are multiplied by trainable weight w and are summed to obtain interlayer signals. To obtain the 3 next layer features, a weighted sum is computed using the other trainable parameter W .</p><p>Instead of filtering all input features for each output feature, DSGCN consists in filtering each input feature once. Then, filtered signals are merged into the desired number of output features through 1×1 convolutions with different contribution coefficients. Detailed illustration of the proposed depthwise separable graph convolution process is presented in <ref type="figure" target="#fig_5">Figure 7</ref>.</p><p>Mathematically, forward calculation of each layer of DSGCN is defined by:</p><formula xml:id="formula_24">H (l+1) = σ S s=1 w (s,l) (C (s) H (l) ) W (l) .<label>(21)</label></formula><p>In this expression, the notation denotes the element-wise multiplication operator. Note that there is only one trainable matrix W in each layer. Other trainable variables w (s,l) ∈ R 1×f l encode feature contributions for each convolution kernel and layer. The number of trainable parameters for this case becomes L i=0 Sf i + f i f i+1 . Previously, adding a new kernel increases the number of parameters by L i=0 f i f i+1 . Using separable convolutions, this number is only increased by L i=0 f i . This modification is particularly interesting when the number of features is high. On the other hand, the variability of the model also decreases. If the data has a smaller number of features, using this approach might not be optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental evaluation</head><p>In this section, we describe the experiments carried out to evaluate the proposed approach on both transductive and inductive problems. In the first case, we target a single graph node classification task while in the second case, both multi-graph node classification task and entire graph classification task are considered (see Section 2.1). For all the experiments, we compare our algorithm to state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transductive Learning Problem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>Experiments on transductive problems were led on the three datasets summarized in <ref type="table" target="#tab_1">Table 1</ref>. These datasets are well-known paper citation graphs. Each node corresponds to a paper. If one paper cites another one, there is an unlabeled and undirected edge between the corresponding nodes. Binary features on the nodes indicate the presence of specific keywords in the corresponding paper. The task is to attribute a class to each node (i.e., paper) of the graph using for training the graph itself and a very limited number of labeled nodes. Labeled data ratio is 5.1%, 3.6% and 0.3% for Cora, Citeseer and PubMed respectively. We use predefined train, validation and test sets as defined in <ref type="bibr" target="#b11">[12]</ref> and follow the test procedure of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Models</head><p>To evaluate the performance of convolutions designed in the spectral domain independently from the architecture design, a single hidden layer is used for all models, as in <ref type="bibr" target="#b9">[10]</ref> for GCN. This choice, even sub-optimal, enables a deep understanding of the convolution kernels. For these evaluations, a set of convolution kernels is experimented:</p><p>• A low-pass filter defined by 1 (λ) = (1 − λ/λ max ) η where η impacts the cut-off frequency</p><p>• A high-pass filter defined by 2 (λ) = λ/λ max • Three band-pass filters defined by:</p><formula xml:id="formula_25">-3 (λ) = exp(−γ(0.25λ max − λ) 2 ) -4 (λ) = exp(−γ(0.5λ max − λ) 2 ) -5 (λ) = exp(−γ(0.75λ max − λ) 2 )</formula><p>• An all-pass filter defined by 6 (λ) = 1</p><p>We firstly consider a model composed of only 1 . This choice comes from the fact that state-of-the-art GNNs are sort of low-pass filters (see Section 3.2) and perform well on the datasets of <ref type="table" target="#tab_1">Table 1</ref>. Hence, it is interesting to evaluate our framework with 1 . For the experiments, the value of η are tuned for each dataset, using the validation loss value and accuracy, yielding η = 5 for Cora and Citeseer, and η = 3 for PubMed. Details concerning this tuning can be found in <ref type="table" target="#tab_1">Table A1</ref> in Appendix D. Since there is only one convolution kernel, depthwise separable convolutions are not necessary for this model. Therefore, this model can be seen as similar to those from <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> but using a different convolution kernel. This approach is denoted as LowPassConv in the results section (Section 4.1.3).</p><p>Beyond this low-pass model, we also evaluate different combinations of the i (λ) through the depthwise separable schema defined in Section 3.3. For experiments involving { 3 (λ), 4 (λ), 5 (λ)}, the bandwidth parameter γ was tuned using train and validation sets. <ref type="table" target="#tab_2">Table 2</ref> details the best models found on the validation set. As an example, for Cora dataset, 4 kernels are used by a DSGCN with 160 neurons: 1 (λ), 3 (λ), 4 (λ), 5 (λ). As an illustration, <ref type="figure" target="#fig_6">Figure 8</ref> provides the standard frequency profiles of this designed convolution on Cora dataset. The models of <ref type="table" target="#tab_2">Table 2</ref> are denoted as DSGCN in the following.</p><p>The training hyperparameters were tuned over a grid search using a cross-validation procedure. Hyperparameter values can be found in <ref type="table" target="#tab_2">Table A2</ref> of Appendix D. Other protocol details are also given in this appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Results</head><p>Obtained results on transductive learning are given in <ref type="table" target="#tab_3">Table 3</ref>. We compare the performance of the proposed LowPass-Conv and DSGCN to state-of-the-art methods.</p><p>We first can see that our low-pass convolution kernel (LowPassConv) obtains comparative performance with existing methods. This result confirms our theoretical analysis which states that GCN and GAT mostly correspond to low-pass  filters (Section 3.2). Second, DSGCN outperforms state-of-the-art methods thanks to the flexibility provided by the different filters. It is worth noting that the good results obtained by low-pass approaches show that these three classification tasks are mainly low-pass specific problems. Differences in accuracies may be significantly bigger for band-pass or high-pass based problems.</p><formula xml:id="formula_26">1 (λ) = (1 − λ/λ max ) 5 3 (λ) = exp(−0.25(0.25λ max − λ) 2 ) Cora 4 (λ) = exp(−0.25(0.5λ max − λ) 2 ) 5 (λ) = exp(−0.25(0.75λ max − λ) 2 ) DSG160-DSG7 Citeseer 1 (λ) = (1 − λ/λ max ) 5 , 6 (λ) = 1 DSG160-DSG6 Pubmed 1 (λ) = (1 − λ/λ max ) 3 , 2 (λ) = λ/λ max DSG16-DSG3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inductive Learning Problem</head><p>Inductive Learning problems are common in chemoinformatics and bioinformatics. In an inductive setting, a given instance is represented by a single graph. Thus, models are trained and tested on different graph sets.</p><p>In the graph neural networks literature, there is a controversy concerning the transferability of spectral designed convolutions from learning graphs to unseen graphs. Some authors consider that convolutions cannot be transferred <ref type="bibr" target="#b22">[23]</ref>, while very recent theoretical <ref type="bibr" target="#b31">[32]</ref> and empirical <ref type="bibr" target="#b32">[33]</ref> works show the contrary. In this subsection, we target to bring an answer to this controversy by experimenting our proposal on inductive learning problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Datasets</head><p>Inductive experiments are led on 3 datasets (see <ref type="table" target="#tab_4">Table 4</ref> for a summary): a multi-graph node classification dataset called Protein-to-Protein Interaction (PPI) <ref type="bibr" target="#b33">[34]</ref> and on two graph classification datasets called PROTEINS and ENZYMES <ref type="bibr" target="#b34">[35]</ref>. The protocols used for the evaluations are those defined in <ref type="bibr" target="#b10">[11]</ref> for PPI and <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> for PROTEINS and ENZYMES datasets.  <ref type="bibr" target="#b11">[12]</ref> 0.757 0.647 0.744 MoNet <ref type="bibr" target="#b22">[23]</ref> 0.817 ± 0.005 -0.788 ± 0.003 ChebNet <ref type="bibr" target="#b7">[8]</ref> 0.812 0.698 0.744 CayleyNet <ref type="bibr" target="#b8">[9]</ref> 0.819 ± 0.007 --DPGCNN <ref type="bibr" target="#b25">[26]</ref> 0.833 ± 0.005 0.726 ± 0.008 -GCN <ref type="bibr" target="#b9">[10]</ref> 0.819 ± 0.005 0.707 ± 0.004 0.789 ± 0.003 GAT <ref type="bibr" target="#b10">[11]</ref> 0.830 ± 0.007 0.725 ± 0.007 0.790 ± 0.007</p><p>LowPassConv 0.827 ± 0.006 0.717 ± 0.005 0.794 ± 0.005 DSGCN 0.842 ± 0.005 0.733 ± 0.008 0.819 ± 0.003 The PPI dataset is a multi-label node classification problem on multi-graphs. Each node has to be classified either True or False for 121 different criteria. All the nodes are described by a 50-length continuous feature vector. The PPI dataset includes 24 graphs, with a train/validation/test standard splitting.</p><p>The PROTEINS and ENZYMES datasets are graph classification datasets. There are 2 classes in PROTEINS and 6 classes in ENZYMES. In PROTEINS dataset, there are three different types of nodes and one continuous feature. But we do not use this continuous feature on nodes. In ENZYMES dataset, there are 18 continuous node features and three different kinds of node types. In the literature, some methods use all provided continuous node features while others use only node label. This is why ENZYMES results are given using either all features (denoted by ENZYMES-allfeat) or only node labels (denoted by ENZYMES-label).</p><p>Since there is no standard train, validation and test sets split for PROTEINS and ENZYMES, the results are given using a 10-fold cross-validation (CV) strategy under a fixed predefined epoch number. The CV only uses training and validation set. Specifically, after obtaining 10 validation curves corresponding to 10 folds, we first take average of validation curves across the 10 folds and then select the single epoch that achieved the maximum averaged validation accuracy. This procedure is repeated 20 times with random seeds and random division of dataset. Mean accuracy and standard deviation are reported. This is the same protocol than <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Models</head><p>For PPI, 7 depthwise graph convolution layers compose the model. Each layer has 800 neurons, except the output layer which has 121 neurons, each one classifying the node either True or False. All layers use a ReLU activation except the output layer, which is linear. No dropout or regularization of the binary cross-entropy loss function is used. All <ref type="table">Table 5</ref>: Kernels frequency profiles and model architecture for each inductive dataset. meanmax refers to global mean and max pooling layer. Same legend as <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Dataset Architecture</p><formula xml:id="formula_27">1 (λ) = exp(−λ/10) PPI 2 (λ) = λ/λ max , 3 (λ) = 1 DSG800-DSG800-DSG800-DSG800- DSG800-DSG800-DSG121 PROTEINS 1 (λ) = 1 − λ/λ max , 2 (λ) = λ/λ max G200-G200-meanmax-D100-D2 1 (λ) = 1, 2 (λ) = λ s − 1 ENZYMES-label 3 (λ) = 2λ 2 s − 4λ s + 1, λ s = 2λ/λ max G200-G200-G200-G200-meanmax-D6 1 (λ) = 1, 2 (λ) = exp(−λ 2 ) ENZYMES-allfeat 3 (λ) = exp(−(λ − 0.5λ max ) 2 ) 4 (λ) = exp(−(λ − λ max ) 2 ) G200-G200-meanmax-D100-D6</formula><p>graph convolutions use three spectral designed convolutions: a low-pass convolution given by 1 (λ) = exp(−λ/10), a high-pass one given by 2 (λ) = λ/λ max and an all-pass filter given by 3 (λ) = 1.</p><p>For graph classification problems (PROTEINS and ENZYMES), depthwise graph convolution layers are not needed since these datasets have a reduced number of features. Thus, it is tractable to use all multi-support graph convolution layers instead of the depthwise schema. In these cases, our models firstly consist of a series of graph convolution layers. Then, a global pooling (i.e., graph readout) is applied in order to aggregate extracted features at graph level. For this pooling, we use a concatenation of mean and max global pooling operator, as used in <ref type="bibr" target="#b36">[37]</ref>. Finally, a dense layer (except for ENZYMES-label) is applied, before the output layer as in <ref type="bibr" target="#b38">[39]</ref>.</p><p>All details about the architecture and designed convolutions can be found in <ref type="table">Table 5</ref>. The hyperparameters used in best models can be found on <ref type="table" target="#tab_2">Table A2</ref> in Appendix D. <ref type="table" target="#tab_5">Table 6</ref> compares the results obtained by the models described above and state-of-the-art methods. A comparison with the same models but without graph information, a Multi-Layer Perceptron (MLP) that corresponds to C (1) = I is also provided to discuss if structural data include information or not. To the best of our knowledge, such an analysis is not provided in the literature. Finally, results obtained by the same architecture with GCN kernel is also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Results</head><p>As one can see in <ref type="table" target="#tab_5">Table 6</ref>, the proposed method obtains competitive results on inductive datasets. For PPI, DSGCN clearly outperforms state-of-the-art methods with the same protocol, reaching a micro-F1 percentage of 99.09 and an accuracy of 99.45%. For this dataset, MLP accuracy is low since the percentage of micro-F1 is 46.2 (random classifier's micro-F1 being 39.6%). This means that the problem includes significant structural information. Using the GCN kernel, which operates as low-pass convolution (see Section 3.2), the accuracy increases to 0.592, but again not comparable with state-of-the-art accuracy.</p><p>For the PROTEINS dataset, one can see that MLP (C (1) = I) reaches an accuracy that is quite comparable with state-of-the-art GNN methods. Hence, MLP reaches a 74.03% validation accuracy while the proposed DSGCN reaches 77.28%, which is the best performance among GNNs. This means that PROTEINS problem includes very few structural information to be exploited by GNNs.</p><p>ENZYMES dataset results are very interesting in order to understand the importance of continuous features and their processing through different convolutions. As one can see in <ref type="table" target="#tab_5">Table 6</ref>, there are important differences of performance between the results on ENZYMES-label and ENZYMES-allfeat. When node labels are used alone, without features, MLP accuracy is very poor and nearly acts as a random classifier. When using all features, MLP outperforms GCN and even some state-of-the-art methods. A first explanation is that methods are generally optimized for just node label but not for continuous features. Another one is that the continuous features already include information related to the graph structure since they are experimentally measured. Hence, their values are characteristic of the node when it is included in the given graph. Since GCN is just a low-pass filter, it removes some important information on higher frequency and decreases the accuracy. Thanks to the multiple convolutions proposed in this paper, our GNN DSGCN clearly outperforms other methods on the ENZYMES dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The success of convolutions in neural network strongly depends on the capability of defined convolution kernels on producing outputs as different as possible. While this has been widely investigated for CNNs, there has not been any study for ConvGNNs with graph convolution, to the best of our knowledge. This paper proposed to fill this gap, by examining the graph convolutions as custom frequency profiles and taking advantage of using optimized multi-frequency profile convolutions. By this way, we significantly increased the performance on reference datasets.</p><p>Nevertheless, the proposed approach has some drawbacks. First, it needs eigenvalues and eigenvectors of the graph Laplacian. If the graph has more than 20k nodes, computing these values is not tractable. Second, we did not propose yet any automatic procedure to select the best frequency profile of convolution. Hence, the proposed approach needs expertise to find the appropriate graph kernels. Third, although our theoretic complexity is the same than GCN or ChebNet, in practice our convolutions are more dense than GCN, which makes it slower in practice since it cannot take advantage of sparse matrix multiplications. Last, if edge type can be handled by designing convolution for each type, the proposed method does not handle continuous edge features and directed edges.</p><p>Our future work will target the automatic design of graph convolutions in spectral domain. It may be done by unsupervised manner as preprocessing step. Another future work will be on handling given continuous edge features and directed edge in our framework. Also, we have a plan to design convolution frequencies not by function of eigenvalues but through linear combination of Chebyshev kernels in order to skip the necessity of eigenvalue calculations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDICES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Theoretical Analysis of Chebyshev Kernels Frequency Profile</head><p>In this appendix, we provide the expressions of the full and standard frequency profiles of the Chebyshev convolution kernels.</p><p>Theorem 2. The frequency profile of the first Chebyshev convolution kernel for any undirected arbitrary graph defined by C (1) = I can be defined by</p><formula xml:id="formula_28">1(λ) = 1,<label>(22)</label></formula><p>where 1 denotes the vector of ones of appropriate size.</p><p>Proof. When the identity matrix is used as convolution kernel, it just directly transmits the inputs to the outputs without any modification. This process is called all-pass filter. Mathematically, we can calculate the full frequency profile for kernel I by using Corollary 1.1, namely</p><formula xml:id="formula_29">1 = U IU = U U = I,<label>(23)</label></formula><p>since the eigenvectors are orthonormal. Therefore, we can parameterize the diagonal of the full frequency profile by λ and reach the standard frequency profile as follows:</p><formula xml:id="formula_30">1(λ) = diag(I) = 1.<label>(24)</label></formula><p>Theorem 3. The frequency profile of the second Chebyshev convolution kernel for any undirected arbitrary graph given by C (2) = 2L/λmax − I can be defined by</p><formula xml:id="formula_31">2(λ) = 2λ λmax − 1.<label>(25)</label></formula><p>Proof. We can compute the C (2) kernel full frequency profile using Corollary 1.1:</p><formula xml:id="formula_32">2 = U 2 λmax L − I U.<label>(26)</label></formula><p>Since U IU = I, (26) can be rearranged as</p><formula xml:id="formula_33">2 = 2 λmax U LU − I.<label>(27)</label></formula><p>Since λ = [λ1, . . . , λn] are the eigenvalues of the graph Laplacian L, those must conform to the following condition:</p><formula xml:id="formula_34">LU = U diag(λ);<label>(28)</label></formula><formula xml:id="formula_35">U LU = diag(λ).<label>(29)</label></formula><p>Replacing <ref type="bibr" target="#b28">(29)</ref> into <ref type="formula" target="#formula_1">(27)</ref>, we get</p><formula xml:id="formula_36">2 = 2 λmax diag(λ) − I.<label>(30)</label></formula><p>This full frequency profile consists of two parts, a diagonal matrix and the negative identity matrix. Therefore, we can parameterize the full frequency matrix diagonal to show the standard frequency profile as follows:</p><formula xml:id="formula_37">2(λ) = diag( 2) = 2λ λmax − 1.<label>(31)</label></formula><p>Theorem 4. The frequency profile of third and followings Chebyshev convolution kernels for any undirected arbitrary graph can be defined by</p><formula xml:id="formula_38">k = 2 2 k−1 − k−2 ,<label>(32)</label></formula><p>and their standard frequency profiles by</p><formula xml:id="formula_39">k (λ) = 2 2(λ) k−1 (λ) − k−2 (λ).<label>(33)</label></formula><p>Proof. Given the third and following Chebyshev kernels defined by C (k) = 2C (2) C (k−1) − C (k−2) and using Corollary 1.1, the corresponding frequency profile is</p><formula xml:id="formula_40">k = U 2C (2) C (k−1) − C (k−2) U.<label>(34)</label></formula><p>By expanding (34), we get</p><formula xml:id="formula_41">k = 2U C (2) C (k−1) U − U C (k−2) U.<label>(35)</label></formula><p>Since U U = I, we can insert the product U U into <ref type="bibr" target="#b34">(35)</ref>. Thus, we have</p><formula xml:id="formula_42">k = 2U C (2) U U C (k−1) U − U C (k−2) U (36) k = 2 U C (2) U U C (k−1) U − U C (k−2) U.<label>(37)</label></formula><p>Since k = U C (k ) U for any k , it follows that <ref type="bibr" target="#b36">(37)</ref> and (32) are identical.</p><p>Hence 1 and 2 are diagonal matrices, and the rest of the kernels frequency profiles become diagonal matrices in <ref type="bibr" target="#b31">(32)</ref>. Therefore, we can write the corresponding standard frequency profiles of third and followings Chebyshev convolution kernels as follows:</p><formula xml:id="formula_43">k (λ) = 2 2(λ) k−1 (λ) − k−2 (λ).<label>(38)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Theoretical Analysis of CayleyNet Frequency Profile</head><p>CayleyNet uses in (2) the weight vector parametrization F i,j,l = [g i,j,l (λ1, h), ..., g i,j,l (λn, h)] , where the function g(·, ·) is defined in <ref type="bibr" target="#b8">[9]</ref> by</p><formula xml:id="formula_44">g(λ, h) = c0 + 2Re r k=1 c k hλ − ı hλ + i k ,<label>(39)</label></formula><p>where i 2 = −1, Re(·) is the function that returns the real part of a given complex number, c0 is a trainable real coefficient, and c1, . . . , cr are complex trainable coefficients. We can write hλ − i in Euler form by √ h 2 λ 2 + 1.e i atan2(−1,hλ) and for hλ + i by √ h 2 λ 2 + 1.e i atan2 <ref type="bibr">(1,hλ)</ref> . By this substitution, <ref type="bibr" target="#b38">(39)</ref> becomes</p><formula xml:id="formula_45">g(λ, h) = c0 + 2Re r k=1 c k e ik(atan2(−1,hλ)−atan2(1,hλ)) .<label>(40)</label></formula><p>where atan2(y, x) is the inverse tangent function, which finds the angle (in range of [−π, π]) of a point given its y and x coordinates. For further simplification, let us introduce the θ(·) function defined by</p><formula xml:id="formula_46">θ(x) = atan2(−1, x) − atan2(1, x).<label>(41)</label></formula><p>Since the c k s are complex numbers, we can write them as a sum of real and imaginary parts, c k = a k /2 + ib k /2 (the scale factor 2 is added for convenience). Thus, <ref type="bibr" target="#b39">(40)</ref> can be rewritten as follows:</p><formula xml:id="formula_47">g(λ, h) = c0 + Re r k=1 (a k + ib k )e ikθ(hλ) .<label>(42)</label></formula><p>We can replace e ikθ(hλ) with its polar coordinate equivalence form cos(kθ(hλ)) + i sin(kθ(hλ)). When we remove the imaginary components because of Re(·) function, (42) becomes g(λ, h) = c0 + r k=1 a k cos(kθ(hλ)) − b k sin(kθ(hλ)).</p><p>In this definition, there is no complex coefficient, but only real coefficients (c0, a k and b k for k = 1, . . . , r) to be tuned by training. By using the form in (43), we can parametrize CayleyNet by the parametrization matrix B ∈ R n×2r+1 , as in <ref type="formula" target="#formula_3">(3)</ref> </p><p>The s-th column vector of matrix B, denotes Bs, must fulfill the following conditions:</p><formula xml:id="formula_50">Bs = s(λ) =    1 if s = 1 cos( s 2 θ(hλ)) if s ∈ {2, 4, . . . , 2r} − sin( s−1 2 θ(hλ)) if s ∈ {3, 5, . . . , 2r + 1}<label>(45)</label></formula><p>We can see CayleyNet as a spectral graph convolution that uses 2r + 1 convolution kernels. The first kernel is an all-pass filter, and the frequency profiles of remaining 2r kernels ( s(λ)) are created using sine and cosine functions, with a parameter h used to scale the eigenvalues in (45). Considering <ref type="bibr" target="#b11">(12)</ref> in Theorem 1, we can write CayleyNet's convolutions (C (s) ) in spatial domain. CayleyNet includes the tuning of this scaling parameter in the training pipeline. Note that because of the function definition in (41), θ(hλ) is not linear in λ. Therefore, s cannot be a perfect sinusoidal in λs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Theoretical Analysis of GCN Frequency Profile</head><p>In this appendix, we study the GCN and its convolution kernel. We start by deriving the expression of its frequency profile.</p><p>Theorem 5. The frequency profile of GCN convolution kernel is defined by</p><formula xml:id="formula_51">CGCN = D −1/2 A D −1/2 ,<label>(46)</label></formula><p>and can be written as</p><formula xml:id="formula_52">GCN (λ) = 1 − p p + 1 λ,<label>(47)</label></formula><p>where λ is the eigenvalues of the normalized graph Laplacian and the given graph is an undirected regular graph whose node degrees are all equal to p.</p><p>Proof. Since Di,i = j Ai,j and A = (A + I), we can rewrite (46) as:</p><formula xml:id="formula_53">CGCN = (D + I) −1/2 (A + I)(D + I) −1/2 .<label>(48)</label></formula><p>Under the assumption that all node degrees are equal to p, we can write the diagonal degree matrix by D = pI. Then, (48) can be rewritten as</p><formula xml:id="formula_54">CGCN = ((p + 1)I) −1/2 (A + I)((p + 1)I) −1/2 ,<label>(49)</label></formula><p>which is equivalent to</p><formula xml:id="formula_55">CGCN = A + I p + 1 .<label>(50)</label></formula><p>Using Corollary 1.1, we can express the frequency profile of CGCN in matrix form by</p><formula xml:id="formula_56">GCN = 1 p + 1 U AU + 1 p + 1 I.<label>(51)</label></formula><p>Since λ = [λ1, . . . , λn] are the eigenvalues of the normalized graph Laplacian L = I − D −1/2 AD −1/2 , they must conform to the following condition:</p><formula xml:id="formula_57">I − D −1/2 AD −1/2 U = U diag(λ).<label>(52)</label></formula><p>According to D = pI, it conforms to D −1/2 AD −1/2 = A/p. Thus, (52) can be written as</p><formula xml:id="formula_58">U − AU p = U diag(λ).<label>(53)</label></formula><p>Then AU is expressed as</p><formula xml:id="formula_59">AU = pU − pU diag(λ)<label>(54)</label></formula><p>Replacing AU in (51), we obtain</p><formula xml:id="formula_60">GCN = 1 p + 1 U (pU − pU diag(λ)) + 1 p + 1 I.<label>(55)</label></formula><p>Since U U = I, then we have</p><formula xml:id="formula_61">GCN = pI − pdiag(λ) + I p + 1 .<label>(56)</label></formula><p>This expression can be simplified to</p><formula xml:id="formula_62">GCN = I − p p + 1 diag(λ),<label>(57)</label></formula><p>which is equal to the matrix form defined in (47) since GCN (λ) = diag( GCN ).</p><p>This demonstration shows that the GCN frequency profile acts as a low-pass filter. When the given graph is a circular undirected graph, all node degrees are equal to p = 2, leading to a frequency profile defined by 1 − 2λ/3. Since the normalized graph Laplacian eigenvalues are in the range [0, 2], the filter magnitude linearly decreases until the third quarter of the spectrum (cut-off frequency) where it reaches zero. Then it linearly increases until the end of the spectrum. This explains the shape of the frequency profile of GCN convolutions for 1D regular graph observed in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>However, this conclusion cannot explain the perturbations on the GCN frequency profile. To analyse this point, we relax the assumption D = pI and rewrite (48) as</p><formula xml:id="formula_63">CGCN = (D + I) −1 + (D + I) −1/2 A(D + I) −1/2 .<label>(58)</label></formula><p>We can see that the GCN kernel consists of two parts, CGCN = c1 + c2, where first part is given by c1 = (D + I) −1 and the second one is c2 = (D + I) −1/2 A(D + I) −1/2 .</p><p>For the second part (c2), we can write it using the element-wise multiplication operator (Hadamard multiplication)</p><formula xml:id="formula_64">c2 = A 1/(d + 1) · 1/(d + 1) ,<label>(59)</label></formula><p>where d is the column degree vector d = diag(D) and the division and square-root are also element-wise (Hadamard) operations. With the same notation, we can rewrite the Chebyshev second kernel, assuming that λmax = 2,</p><formula xml:id="formula_65">C (2) = −A 1/d · 1/d .<label>(60)</label></formula><p>The two expressions (59) and (60) show that negative c2 is an approximation of the second Chebyshev kernel if vector d consists of same values, as it was assumed in Theorem 5. When the vector d is composed of different values, the two matrices 1/d. 1/d and 1/(d + 1). 1/(d + 1) are not proportional for each coordinate (i.e., entry). To obtain c2 from C (2) , we need to use different coefficients for each coordinate of the kernel. If the difference between node degrees is important, these coefficients have the strong influence, and c2 may be very different from C <ref type="bibr" target="#b1">(2)</ref> . Conversely, if the node degrees are quite uniform, these coefficients may be neglected. This phenomenon is the first cause of perturbation on GCN frequency profile.</p><p>The first part (c1) of the GCN kernel in (58) is more interesting. Actually, it is a diagonal matrix that shows the contribution of each node in the convolution process. Instead of looking for some approximations of known frequency profiles such as those of Chebyshev kernels, we can write its frequency profile directly. Using Corollary 1.1, we can express the frequency profile of c1 in matrix form by</p><formula xml:id="formula_66">c 1 = (U c1 U ),<label>(61)</label></formula><p>where U is the eigenvectors matrix. By taking advantage of having a diagonal kernel c1, we can express each component of full frequency profile as</p><formula xml:id="formula_67">c 1 (i, j) = n k=1 1 1 + d k U i,k U j,k ,<label>(62)</label></formula><p>where n is the number of nodes in the graph, d k is degree of the k-th node, U i,k is the k-th element of i-th eigenvector. As eigenvectors Ui and Uj are orthogonal for i = j, their scalar product is null. However, in (62), the weighting coefficient 1 1+d k is not constant over all the dimensions of the eigenvectors. Therefore, there is no guarantee that c 1 (i, j) is null. This is another reason that explains that the GCN frequency profile has many non-zero elements outside of the diagonal.</p><p>In addition, it is also clear that the standard frequency profile of c1 (diagonal of c 1 , i.e., c 1 (i, i) in (62)) is not smooth. Indeed, the diagonal elements of c 1 can be written as a weighted sum of squared eigenvalues elements, which again is weighted by 1/ <ref type="figure" target="#fig_0">(1 + d k )</ref>. If the latter is constant for all k, the sum of squared eigenvectors elements has to be 1 since the eigenvectors have unit L2-norm. But in the general case where 1/(1 + d k ) are not necessarily constant over all the dimensions of eigenvectors, the diagonal of the matrix may have some perturbations. This point constitutes another explanation on the fact that the GCN standard frequency profile is not smooth.</p><p>On the other hand, under the assumption that the node degrees distribution is uniform, we can derive the following approximation:</p><formula xml:id="formula_68">p ≈ d = 1 n n k=1 d k .<label>(63)</label></formula><p>We can then write an approximation of the GCN frequency profile as a function of the average node degree by replacing p with d in (54) and obtain the final approximation:</p><formula xml:id="formula_69">GCN (λ) ≈ 1 − d d + 1 λ.<label>(64)</label></formula><p>We can theoretically show the cut-off frequency where GCN kernel's frequency profile reach 0 by</p><formula xml:id="formula_70">λcut ≈ d + 1 d .<label>(65)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Application Details</head><p>This section presents some additional details on the experimental settings and parameter tuning.</p><p>In the experiments of transductive learning problems, we tuned single convolution low-pass filter's parameter η in 1(λ) = (1 − λ/λmax) η . <ref type="table" target="#tab_1">Table A1</ref> shows searching space of η and their loss and accuracy performance over different datasets. One can see that we decided to use η = 5 for Cora and Citeseer, and η = 3 for PubMed dataset where it maximizes the validation set performance.</p><p>In depthwise separable graph convolution layer, the initialization of the trainable parameters w (s,l) affects the performance. If designed convolutions are supposed to have equal effect on the model, these parameters can be initialized randomly. But, if one is supposed to have more effect on the model, the important convolution kernel's correspondence weights can be initialized by 1, the rest of them initialized by 0. In our model, we assumed the first kernel is always the most important kernel. Thus, we initialized the first kernel's depthwise separable weights as w (1,l) = 1, and the rest of the kernel's depthwise separable weights w (s,l) = 0 when s &gt; 1. In this way, the model starts training as there is only kernel, which is supposed to be the most important one.</p><p>The used hyperparameters in our experiments are presented in <ref type="table" target="#tab_2">Table A2</ref>. We applied softmax to the output of the models and calculate cross entropy loss function for all problems expect PPI dataset. Since PPI is two class classification problem and we  coded output by one neuron, we applied tansig to the output of the PPI model and used binary cross entropy as loss function. In our models we did not consider any regularization on the bias parameter, but we applied the L2-loss to the trainable weights. In the depthwise separable layer, there are two different kinds of weights where additional one is depthwise weights. That is why in <ref type="table" target="#tab_2">Table A2</ref>, there is two different weight decays. We always used ReLU activation on the hidden layers and the Linear for output layers.</p><p>The table also provides if bias values are used in the hidden and output layers. In our model, we used two different types of dropout: the dropout applied on the inputs of the layer as usually used in the literature, and the dropout applied on the convolution kernel, which was first used in ? according to the best of our knowledge. Since Cora, Citeseer and PubMed datasets consist of one single graph, batch size is 1 for these problems. For the PPI dataset of only 24 graphs, we still prefer to update the model for each training graph. But for PROTEINS and ENZYMES datasets, we update the model 3 times in each epoch. Since in PROTEINS there are 1113 graphs and in each fold there are 1000 graphs in the train set, we used a 333 batch size. As the same in ENZYMES there is 540 graphs in each train fold, we used a 180 batch size to update the model 3 times in a single epoch. We used the Adam optimization and a fixed learning-coefficient in all models. The used learning coefficient and the maximum epoch number can be found in the <ref type="table" target="#tab_2">Table A2</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic of the GCN layer defined in<ref type="bibr" target="#b4">(5)</ref>. The graph has 12 nodes and 12 edges. Each node has a 2-length feature vector H colors. The second layer has a 3-length feature vector, denoted H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Standard frequency profiles of first 5 Chebyshev convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Standard frequency profiles of first 7 CayleyNet convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) Standard frequency profiles (b) Full frequency profile on 1D regular line graph (c) Full frequency profile on Cora (d) Full frequency profile on Citeseer Frequency profiles of GCN on different graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 : 3 and 3</head><label>5633</label><figDesc>Standard frequency profile (b) Mean of full frequency profile (c) Standard deviation of full frequency profile Frequency profiles of randomly generated 250 GAT convolutions using Cora graph. Three designed convolution kernel frequency profiles as a function of graph eigenvalues (λ) of two sample graphs G (i) and G (j) by 1 (λ) = λ 6 , 2 (λ) = 1− |λ−3| (λ) = 1− λ 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Detailed schematic of Depthwise Separable Graph Convolution Layer. Each node has a 2-length feature vector, indicated as H (l) 1 and H (l) 2 with values represented by colors. The following layer has a 3-length feature vector, denoted H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Designed convolution's frequency profiles for Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>, by [g(λ0, h), . . . , g(λn, h)] = B[c0, a1, b1, . . . , ar, br] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>2</head><label></label><figDesc>ConvGNN: Problem Statement and State of the Art 2.1 Graph Learning Problems</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of the transductive datasets used in our experiments.Each dataset consists of one single graph</figDesc><table><row><cell></cell><cell cols="3">Cora Citeseer PubMed</cell></row><row><cell># Nodes</cell><cell>2708</cell><cell>3327</cell><cell>19717</cell></row><row><cell># Edges</cell><cell>5429</cell><cell>4732</cell><cell>44338</cell></row><row><cell># Features</cell><cell>1433</cell><cell>3703</cell><cell>500</cell></row><row><cell># Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell></row><row><cell># Training Nodes</cell><cell>140</cell><cell>120</cell><cell>60</cell></row><row><cell cols="2"># Validation Nodes 500</cell><cell>500</cell><cell>500</cell></row><row><cell># Test Nodes</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Used kernels frequency profiles and architecture of models for each transductive dataset. DSG refers to Depthwise Separable Graph convolution layer, G to Graph convolution layer, D to Dense layer Dataset Architecture</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of methods on the transductive learning problems using publicly defined train, validation and test sets. Accuracies on test set are reported with their standard deviations under 20 random runs.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>MLP</cell><cell>0.551</cell><cell>0.465</cell><cell>0.714</cell></row><row><cell>Planetoid</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Summary of inductive learning datasets used in this paper.</figDesc><table><row><cell></cell><cell>PPI</cell><cell>PROTEINS</cell><cell>ENZYMES</cell></row><row><cell>Type</cell><cell>Node Class.</cell><cell>Graph Class.</cell><cell>Graph Class.</cell></row><row><cell># Graph</cell><cell>24</cell><cell>1113</cell><cell>600</cell></row><row><cell># Avg.Nodes</cell><cell>2360.8</cell><cell>39.06</cell><cell>32.63</cell></row><row><cell># Avg.Edges</cell><cell>33584.4</cell><cell>72.82</cell><cell>62.14</cell></row><row><cell># Features</cell><cell>50</cell><cell>3 label</cell><cell>3 label + 18 cont.</cell></row><row><cell># Classes</cell><cell>2 (121 criterias)</cell><cell>2</cell><cell>6</cell></row><row><cell># Training</cell><cell>20 graphs</cell><cell>9-fold</cell><cell>9-fold</cell></row><row><cell># Validation</cell><cell>2 graphs</cell><cell>1-fold</cell><cell>1-fold</cell></row><row><cell># Test</cell><cell>2 graphs</cell><cell>None</cell><cell>None</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of methods on inductive learning problems using publicly defined data split for PPI dataset and 10-fold CV for PROTEINS and ENZYMES datasets. PPI results are the test set results reported by micro-F1 metric percentage. Others are CV results reported by accuracy percentage. Results denoted by * were reproduced from original source codes but denoted feature set.</figDesc><table><row><cell>Method</cell><cell>PPI</cell><cell>PROTEINS</cell><cell cols="2">ENZYMES</cell></row><row><cell></cell><cell>All Features</cell><cell>Node Label</cell><cell>Node Label</cell><cell>All Features</cell></row><row><cell>GraphSAGE [22]</cell><cell>76.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GAT [11]</cell><cell>97.3 ± 0.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GaAN [40]</cell><cell>98.7 ± 0.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Hierarchical Pooling [37]</cell><cell>-</cell><cell>75.46</cell><cell>64.17</cell><cell>-</cell></row><row><cell>Diffpool [36]</cell><cell>-</cell><cell>76.30</cell><cell>62.50</cell><cell>66.66  *</cell></row><row><cell>ChebNet [33]</cell><cell>-</cell><cell>75.50 ± 0.40</cell><cell>58.00 ± 1.40</cell><cell>-</cell></row><row><cell>Multigraph [33]</cell><cell>-</cell><cell>76.50 ± 0.40</cell><cell cols="2">61.70 ± 1.30 68.00 ± 0.83</cell></row><row><cell>GIN [39]</cell><cell>-</cell><cell>76.20 ± 0.86</cell><cell>-</cell><cell>-</cell></row><row><cell>GFN [38]</cell><cell>-</cell><cell cols="3">76.56 ± 0.30  0.86</cell></row><row><cell>MLP (C (1) = I)</cell><cell>46.2 ± 0.56</cell><cell>74.03 ± 0.92</cell><cell cols="2">27.83 ± 2.51 76.11 ± 0.87</cell></row><row><cell>GCN (9)</cell><cell>59.2 ± 0.52</cell><cell>75.12 ± 0.82</cell><cell cols="2">51.33 ± 1.23 75.16 ± 0.65</cell></row><row><cell>DSGCN</cell><cell cols="2">99.09 ± 0.03 77.28 ± 0.38</cell><cell cols="2">65.13 ± 0.65 78.39 ± 0.63</cell></row></table><note>* 60.23 ± 0.92* 70.17 ±</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A1 :</head><label>A1</label><figDesc>Minimum validation set loss value and maximum validation set accuracy over different low-pass filters. − λ/λ max ) 1 1.116 80.4 1.12 73.0 0.654 77.1 (1 − λ/λ max ) 3 0.745 81.8 1.02 72.6 0.572 81.1 (1 − λ/λ max ) 5 0.705 81.8 1.02 73.8 0.592 80.5 (1 − λ/λ max ) 10 0.752 81.2 1.01</figDesc><table><row><cell>Convolution</cell><cell>Cora</cell><cell cols="2">Citeseer</cell><cell cols="2">PubMed</cell></row><row><cell>1 (λ)</cell><cell>Loss</cell><cell>Acc Loss</cell><cell>Acc</cell><cell cols="2">Loss Acc</cell></row><row><cell cols="4">(1 72.2</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">(1 − λ/λ max ) 20 0.792 80.8 1.01</cell><cell>71.2</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A2 :</head><label>A2</label><figDesc>Used hyperparameters.</figDesc><table><row><cell>Hyperparameters</cell><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed</cell><cell>PPI</cell><cell>PROTEINS</cell><cell>ENZYMES-label</cell><cell>ENZYMES-allfeat</cell></row><row><cell>Hidden Activations</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell>Output Activation</cell><cell>Linear</cell><cell>Linear</cell><cell>ReLU</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell></row><row><cell>Hidden Biases</cell><cell>False</cell><cell>False</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>False</cell><cell>False</cell></row><row><cell>Output Bias</cell><cell>True</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>True</cell><cell>True</cell><cell>True</cell></row><row><cell>Input Dropout</cell><cell>0.75</cell><cell>0.75</cell><cell>0.25</cell><cell>0</cell><cell>0</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Kernel Dropout</cell><cell>0.75</cell><cell>0.75</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Weight Decay</cell><cell>3e-4</cell><cell>3e-4</cell><cell>5e-4</cell><cell>0</cell><cell>0</cell><cell>1e-4</cell><cell>1e-4</cell></row><row><cell>Weight Decay on DSG</cell><cell>3e-3</cell><cell>3e-3</cell><cell>5e-3</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Learning Coeff</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.0005</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>Batch Size</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>333</cell><cell>180</cell><cell>180</cell></row><row><cell>Epoch</cell><cell>400</cell><cell>100</cell><cell>250</cell><cell>500</cell><cell>100</cell><cell>500</cell><cell>500</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was partially supported by the ANR grant APi (ANR-18-CE23-0014), the Normandy Region project AGAC and the PAUSE Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural message passing from quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cayleynets:Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the33rd International Conference on International Conference on Machine Learning, ICML&apos;16</title>
		<meeting>the33rd International Conference on International Conference on Machine Learning, ICML&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<title level="m">Spectral graph theory</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs:Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A generalization of convolutional neural networks to graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hechtlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08165</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Splinecnn:Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dual-primal graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunnemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Waveletson graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixhop:Higher-order graph convolution architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning(ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">in2018IEEE Conference on Computer Vision and Pattern Recognition, CVPR2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On the transferability of spectral graph filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Isufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10524</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09595</idno>
		<title level="m">Spectral multigraph networks for discovering and fusing relationships in molecules</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Predicting multi cellular function through multi-layer tissue networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Towards sparse hierarchical graph classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jovanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01287</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dissecting graph neural networks on graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ting Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gaan:Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">UAI</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
