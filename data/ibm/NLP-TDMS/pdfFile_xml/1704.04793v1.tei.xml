<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harvesting Multiple Views for Marker-less 3D Human Pose Annotations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ryerson University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Harvesting Multiple Views for Marker-less 3D Human Pose Annotations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances with Convolutional Networks (ConvNets) have shifted the bottleneck for many computer vision tasks to annotated data collection. In this paper, we present a geometry-driven approach to automatically collect annotations for human pose prediction tasks. Starting from a generic ConvNet for 2D human pose, and assuming a multi-view setup, we describe an automatic way to collect accurate 3D human pose annotations. We capitalize on constraints offered by the 3D geometry of the camera setup and the 3D structure of the human body to probabilistically combine per view 2D ConvNet predictions into a globally optimal 3D pose. This 3D pose is used as the basis for harvesting annotations. The benefit of the annotations produced automatically with our approach is demonstrated in two challenging settings: (i) fine-tuning a generic ConvNet-based 2D pose predictor to capture the discriminative aspects of a subject's appearance (i.e.,"personalization"), and (ii) training a ConvNet from scratch for single view 3D human pose prediction without leveraging 3D pose groundtruth. The proposed multi-view pose estimator achieves state-of-the-art results on standard benchmarks, demonstrating the effectiveness of our method in exploiting the available multi-view information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Key to much of the success with Convolutional Networks (ConvNets) is the availability of abundant labeled training data. For many tasks though this assumption is unrealistic. As a result, many recent works have explored alternative training schemes, such as unsupervised training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45]</ref>, auxiliary tasks that improve learning representations <ref type="bibr" target="#b41">[42]</ref>, and tasks where groundtruth comes for free, or is very easy to acquire <ref type="bibr" target="#b30">[31]</ref>. Inspired by these works, this paper proposes a geometry-driven approach to automatically gather a high-quality set of annotations for human pose estimation tasks, both in 2D and 3D.</p><p>ConvNets have had a tremendous impact on the task of 2D human pose estimation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b26">27]</ref>. A promising research direction to improve performance is to automatically</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-view Setup</head><p>Generic 2D pose ConvNets 3D Pictorial Structure 3D Annotations Heatmaps <ref type="figure">Figure 1</ref>: Overview of our approach for harvesting pose annotations. Given a multi-view camera setup, we use a generic ConvNet for 2D human pose estimation <ref type="bibr" target="#b26">[27]</ref>, and produce single-view pose predictions in the form of 2D heatmaps for each view. The single-view predictions are combined optimally using a 3D Pictorial Structures model to yield 3D pose estimates with associated per joint uncertainties. The pose estimate is further probed to determine reliable joints to be used as annotations. adapt (i.e., "personalize") a pretrained ConvNet-based 2D pose predictor to the subject under observation <ref type="bibr" target="#b10">[11]</ref>. In contrast to its 2D counterpart, 3D human pose estimation suffers from the difficulty of gathering 3D groundtruth. While gathering large-scale 2D pose annotations from images is feasible, collecting corresponding 3D groundtruth is not. Instead, most works have relied on limited 3D annotations captured with motion capture (MoCap) rigs in very restrictive indoor settings. Ideally, a simple, marker-less, multicamera approach could provide reliable 3D human pose estimates in general settings. Leveraging these estimates as 3D annotations of images would capture the variability in users, clothing, and settings, which is crucial for ConvNets to properly generalize.</p><p>Towards this goal, this paper proposes a geometry-driven approach to automatically harvest reliable annotations from multi-view imagery. <ref type="figure">Figure 1</ref> provides an overview of our approach to automatically harvest reliable joint annotations. Given a set of images captured with a calibrated multi-view setup, a generic ConvNet for 2D human pose <ref type="bibr" target="#b26">[27]</ref> pro-duces single-view confidence heatmaps for each joint. The heatmaps in each view are backprojected to a common discretized 3D space, functioning as unary potentials of a 3D pictorial structure <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>, while a tree graph models the pairwise relations between joints. The marginalized posterior distribution of the 3D pictorial structures model for each joint is used to identify which estimates are reliable. These reliable keypoints are used as annotations.</p><p>Besides achieving state-of-the-art performance as compared to previous multi-view human pose estimators, our approach provides abundant annotations for pose-related learning tasks. In this paper, we consider two tasks. In the first task, we project the 3D pose annotations to the 2D images to create "personalized" 2D groundtruth, which is used to adapt the generic 2D ConvNet to the particular test conditions <ref type="figure" target="#fig_0">(Figure 2a</ref>). In the second task, we use the 3D pose annotations to train from scratch a ConvNet for single view 3D human pose estimation that is on par with the current state-of-the-art. Notably, in training our pose predictor, we limit the training set to the harvested annotations and do not use the available 3D groundtruth ( <ref type="figure" target="#fig_0">Figure 2b</ref>).</p><p>In summary, our four main contributions are as follows:</p><p>• We propose a geometry-driven approach to automatically acquire 3D annotations for human pose without 3D markers;</p><p>• the harvested annotations are used to fine-tune a pretrained ConvNet for 2D pose prediction to adapt to the discriminative aspects of the appearance of the subject under study, i.e., "personalization"; we empirically show significant performance benefits;</p><p>• the harvested annotations are used to train from scratch a ConvNet that maps an image to a 3D pose, which is on par with the state-of-the-art, even though none of the available 3D groundtruth is used;</p><p>• our approach for multi-view 3D human pose estimation achieves state-of-the-art results on standard benchmarks, which further underlines the effectiveness of our approach in exploiting the available multiview information.  Instead of creating synthetic examples, or bypassing the missing data, the focus of our approach is different. In particular, our goal is to gather images with corresponding 2D and 3D automatically generated annotations and use them to train a ConvNet. This way we employ images with statistics similar to those found in-the-wild, which have been proven to be of great value for ConvNet-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>2D human pose: Until recently, the dominant paradigm for 2D human pose involved local appearance modeling of the body parts coupled with the enforcement of structural constraints with a pictorial structures model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b31">32]</ref>. Lately though, end-to-end approaches using ConvNets have become the standard in this domain. The initial work of Toshev and Szegedy <ref type="bibr" target="#b39">[40]</ref> regressed directly the x, y coordinates of the joints using a cascade of ConvNets. Tompson et al. <ref type="bibr" target="#b38">[39]</ref> proposed the regression of heatmaps to improve training. Pfister et al. <ref type="bibr" target="#b29">[30]</ref> proposed the use of intermediate supervision, with Wei et al. <ref type="bibr" target="#b40">[41]</ref> and Carreira et al. <ref type="bibr" target="#b9">[10]</ref> refining iteratively the network output. More recently, Newell et al. <ref type="bibr" target="#b26">[27]</ref> built upon previous work to identify the best practices for human pose prediction and propose an hourglass module consisting of ResNet components <ref type="bibr" target="#b18">[19]</ref>, and iterative processing to achieve state-of-the-art performance on standard benchmarks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref>. In this work, we employ the hourglass architecture as our starting point for generating automatic 3D human pose annotations.</p><p>Single view 3D human pose: 3D human pose estimation from a single image has been typically approached by applying more and more powerful discriminative methods on the image and combining them with expressive 3D priors to recover the final pose <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7]</ref>. As in the 2D pose case, ConvNets trained end-to-end have grown in prominence. Li and Chan <ref type="bibr" target="#b23">[24]</ref> regress directly the x, y, z spatial coordinates for each joint. Tekin et al. <ref type="bibr" target="#b37">[38]</ref> additionally use an autoencoder to learn and enforce structural constraints on the output. Pavlakos et al. <ref type="bibr" target="#b28">[29]</ref> instead propose the regression of 3D heatmaps instead of 3D coordinates. Li et al. <ref type="bibr" target="#b24">[25]</ref> follow a nearest neighbor approach between color images and pose candidates. Rogez and Schmid <ref type="bibr" target="#b33">[34]</ref> use a classification approach, where the classes represent a sample of poses. To demonstrate the quality of our harvested 3D annotations, we also regress the x, y, z joint coordinates <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref>, while employing a more recent architecture <ref type="bibr" target="#b26">[27]</ref>.</p><p>Multi-view 3D human pose: Several approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> have extended the pictorial structures model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref> to reason about 3D human pose taken from multiple (calibrated) viewpoints. Earlier work proposed simultaneously reasoning about 2D pose across multiple views, and triangulating 2D estimates to realize actual 3D pose estimates <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1]</ref>. Recently, Elhayek et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> used ConvNet pose detections for multi-view inference, but with a focus on tracking rather than annotation harvesting, as pursued here. Similar to the current paper, 3D pose has previously been directly modelled in 3D space <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. A straightforward application of the basic pictorial structures model to 3D is computationally expensive due to the six degrees of freedom for the part parameterization. Our parameterization instead models only the 3D joint position, something that has also been proposed in the context of single view 3D pose estimation <ref type="bibr" target="#b22">[23]</ref>. This instantiation of the pictorial structure makes inference tractable since we deal with three degrees of freedom rather than six.</p><p>Personalization: Consideration of pose in video presents an opportunity to tune the appearance model to the discriminative appearance aspects of the subject and thus improve performance. Previous work <ref type="bibr" target="#b32">[33]</ref> leveraged this insight by using a generic pose detector to initially identify a set of high-precision canonical poses. These detections are then used to train a subject-specific detector. Recently, Charles et al. <ref type="bibr" target="#b10">[11]</ref> extended this idea using a generic 2D pose ConvNet to identify a select number of high precision annotations. These annotations are propagated across the video sequence based on 2D image evidence, e.g., optical flow. Regarding identifying confident predictions, the work of Jammalamadaka et al. <ref type="bibr" target="#b20">[21]</ref> is related, where they extract features from the image and the output and train an evaluator to estimate whether the predicted pose is correct. In our work, rather than using 2D image cues to identify reliable annotations, our proposed approach leverages the rich 3D geometry presented by the multi-view setting and the constraints of 3D human pose structure, to combine and consolidate single view information. Such cues are highly reliable and complementary to image-based ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical approach</head><p>The following subsections describe the main components of our proposed approach. Section 3.1 gives a brief description of the generic ConvNet used for 2D pose predictions. Section 3.2 describes the 3D pictorial structures model used to aggregate multi-view image-driven keypoint evidence (i.e., heatmaps) provided as output by a ConvNetbased 2D pose predictor with 3D geometric information from a human skeleton model. Section 3.3 describes our annotation selection scheme that identifies reliable keypoint estimates based on the marginalized posterior distribution of the 3D pictorial structures model for each keypoint. The proposed uncertainty measure inherently integrates image evidence across all viewpoints and geometry. Finally, Sections 3.4 and 3.5 present two applications of our annotation harvesting approach. Section 3.4 describes the use of the harvested annotations to fine-tune an existing 2D pose ConvNet predictor. The resulting adapted predictor is sensitive to the discriminative aspects of the appearance of the subject under consideration, i.e., "personalization". Section 3.5 describes how we use the harvested annotations to train from scratch a 3D pose ConvNet predictor that maps a single image to 3D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generic ConvNet</head><p>The initial component of our approach is a generic ConvNet for 2D human pose estimation that provides the initial set of noisy predictions for single view images. Since our approach is agnostic to the particular network architecture, any of the top-performing ConvNets is sufficient for this step, e.g., <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27]</ref>. Here, we adopt the state-of-theart stacked hourglass design <ref type="bibr" target="#b26">[27]</ref>. The main architectural component of this network is the hourglass module which consists of successive convolutional and pooling layers, followed by convolutional and upsampling layers, leading to a symmetric hourglass design. Stacking multiple hourglasses together allows for iterative processing of the image features. Best performance is achieved by the use of intermediate supervision, forcing the network to produce one set of predictions at the end of each hourglass.</p><p>The prediction of the network is in the form of 2D heatmaps for each joint. The entire heatmap output includes useful information regarding the confidence of predictions, and can be considered as a 2D distribution of the joint locations. To take advantage of the entire heatmap prediction, we backproject the 2D distributions of the joints in a discretized 3D cube. This is used to accommodate the predictions for all the views and serves as the inference space for 3D pictorial structures model, described in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-view optimization</head><p>The pose model used to aggregate information across views is based on a 3D generalization of the classical pic-torial structures model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>. A major departure of the current work from prior 3D instantiations of multi-view approaches (e.g., <ref type="bibr" target="#b8">[9]</ref>) is the use of a joint representation, S = {s i |i = 1, . . . , N }, where s i ∈ R 3 encodes the 3D position of each joint, rather than the 3D configuration of parts, i.e., limbs. The simplified parameterization and tree structure for the pairwise terms admit efficient 3D joint configuration inference via dynamic programming, i.e., the sum-product algorithm.</p><p>Articulation constraints: The pairwise relation between joints is modelled by a tree structure of the human skeleton. The edge set is denoted by E and the edge (i.e., limb) lengths by {L ij |(i, j) ∈ E}. The prior distribution is given by the articulation constraints and can be written as</p><formula xml:id="formula_0">p(S) ∝ (i,j)∈E p(s i , s j ).<label>(1)</label></formula><p>The pairwise terms, p(s i , s j ), constrain the lengths of the human limbs L ij :</p><formula xml:id="formula_1">p(s i , s j ) = 1, if L ij − ε ≤ s i − s j ≤ L ij + ε 0, otherwise ,<label>(2)</label></formula><p>where ε = 1 is used as a tolerance for the variability from the expected limb length L ij of the subject. More sophisticated pairwise terms can also be adopted if MoCap data are available, e.g., <ref type="bibr" target="#b22">[23]</ref>.</p><p>Data likelihood: Given a 3D pose, the likelihood of seeing M synchronized images from M calibrated cameras is modeled as</p><formula xml:id="formula_2">p(I|S) ∝ M k=1 N i=1 p(I k |π k (s i )),<label>(3)</label></formula><p>where π k (s i ) denotes the 2D projection of s i in the k-th view given the camera parameters. The data likelihood, p(I k |π k (s i )), is modelled by the multi-channel heatmap outputs of the ConvNet (Sec. 3.1).</p><p>Inference: Finally, the posterior distribution of a 3D pose given 2D images from different views is given by:</p><formula xml:id="formula_3">p(S|I) ∝ M k=1 N i=1 p(I k |π k (s i )) (i,j)∈E p(s i , s j ). (4)</formula><p>The solution space of the 3D joint position is restricted to a 3D bounding volume around the subject and quantized by a 64 × 64 × 64 grid. Pose estimates are computed as the mean of the marginal distribution of each joint given the multi-view images. The marginal distribution of the discrete variables is efficiently computed by the sum-product algorithm <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Annotation selection</head><p>The 3D reconstructions provided by the multi-view optimization offer a very rich but noisy set of annotations. We are effectively equipped with automatic 3D annotations for all the images of the multi-view setup. Moreover, these annotations integrate appearance cues from each view (2D pose heatmaps), geometric constraints from the multiple views (backprojection in a common 3D space), as well as constraints from the articulated structure (3D pictorial structure). This allows us to capitalize on the available information from the images and the 3D geometry to provide a robust set of annotations.</p><p>For further benefits, we proceed to a selection step over the annotations provided from the 3D reconstruction. A useful property of our multi-view optimization using the pictorial structures model is that the marginalized distribution of each joint offers a measure of the prediction's uncertainty. This means that we are provided with a selection cue for free. For example, the determinant of the 3D covariance matrix for each joint's marginalized distribution can be used as a confidence measure to decide whether the joint will be used as an annotation. In our experiments, we identify as reliable annotations the 70% most confident predictions for each joint in terms of the determinant of the 3D covariance matrix, although other measures are also possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">"Personalizing" 2D pose ConvNet</head><p>The goal of "personalization" is to adapt the original detector such that it captures the discriminative appearance aspects of the subject of interest, such as clothing. Both Ramanan et al. <ref type="bibr" target="#b32">[33]</ref> and Charles et al. <ref type="bibr" target="#b10">[11]</ref> proposed methods to "personalize" a detector using 2D evidence (e.g., optical flow) from monocular video. Instead, our proposed approach focuses on cues provided by image evidence, geometric properties of the multi-view setup, and structural constraints of the human body.</p><p>Given the set of selected annotations, we use them to fine-tune a generic 2D pose ConvNet with backpropagation, such that it adapts to the testing conditions of interest. The procedure is very similar to the one used to train the ConvNet in the first place, with the difference that we leverage our automatically generated annotations as targets for the available images. The target heatmaps consist of a 2D Gaussian with a standard deviation σ = 1 pixel, centered on the annotation location of the joint. A separate heatmap is synthesized for each joint. During training, we use a Mean Squared-Error loss between the predicted and the target heatmaps. If the joint is not within the selected annotation set (i.e., the localization is not confident), we simply ignore the loss incurred by it during optimization. We terminate refinement after four epochs through our autoannotated data to avoid overfitting on the given examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">3D pose ConvNet training</head><p>For 3D human pose estimation, we train a ConvNet from scratch that takes a single image as input and predicts the 3D pose. Our formulation follows the coordinate regression paradigm <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref>, but more sophisticated methods can also be employed, e.g., the volumetric representation for 3D pose <ref type="bibr" target="#b28">[29]</ref>. The target of the network is the x, y, z coordinates of N human body joints. For x, y we use pixel coordinates, while z is expressed in metric depth with respect to a specified root joint (here the pelvis is defined as the root). We organize the output in a single 3N -dimensional vector. The network is supervised with an L 2 regression loss:</p><formula xml:id="formula_4">L = N n=1 x n gt − x n pr 2 2 ,<label>(5)</label></formula><p>where x n gt is the groundtruth and x n pr is the predicted location for joint n. The architecture we use is a single hourglass module <ref type="bibr" target="#b26">[27]</ref> with the addition of a fully connected layer at the end to allow every output to have a connection with each activation of the previous feature volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical evaluation</head><p>This section is dedicated to the empirical evaluation of our proposed approach. First, we give a description of the datasets used (Section 4.1). Next, we briefly discuss the implementation details of our approach (Section 4.2). Finally, we present the quantitative (Sections 4.3 to 4.5) and the qualitative evaluations (Section 4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>For our quantitative evaluation we focused on two datasets that target human pose estimation and provide a multiple camera setup; (i) KTH Multiview Football II <ref type="bibr" target="#b8">[9]</ref>, a small-scale outdoor dataset with challenging visual conditions, and (ii) Human3.6M <ref type="bibr" target="#b19">[20]</ref>, a large-scale indoor dataset, with a variety of available scenarios.</p><p>KTH Multiview Football II <ref type="bibr" target="#b8">[9]</ref> contains images of professional footballers playing a match. Evaluation for 3D pose was performed using the standard protocol introduced with the dataset <ref type="bibr" target="#b8">[9]</ref> and used elsewhere <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5]</ref>, where Sequence 1 of "Player 2" is used for testing. Reported results are based on the percentage of correct parts (PCP) to measure 3D part localization using the two and three camera setups. Additional evaluation for 2D pose was performed using Sequence 2 of "Player 2" for testing, where reported results are based on the percentage of correct parts in 2D.</p><p>Human3.6M <ref type="bibr" target="#b19">[20]</ref> is a recent large-scale dataset for 3D human sensing captured in a lab setting. It includes 11 subjects performing 15 actions, such as walking, sitting, and phoning. Following previous work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b46">47]</ref>, we use two subjects for testing (S9 and S11), and report results based on the average 3D joint error.  <ref type="table">Table 1</ref>: Quantitative comparison of multi-view pose estimation methods on KTH Multiview Football II. The numbers are the percentage of correct parts (PCP) in 3D using two and three cameras. Baseline numbers are taken from the respective papers. In constrast to the compared methods, no training data from this dataset was used for our approach.</p><p>It is crucial to mention that in the experiments presented below, no groundtruth data was leveraged for training from the respective datasets. We relied solely on the generic 2D ConvNet (trained on MPII <ref type="bibr" target="#b1">[2]</ref>) and the knowledge of the geometry from the calibrated camera setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>For the generic 2D pose ConvNet, we use a publicly available model <ref type="bibr" target="#b26">[27]</ref>, which is trained on the MPII human pose dataset <ref type="bibr" target="#b1">[2]</ref>. To "personalize" a given 2D pose ConvNet through fine-tuning, we maintain the same training details as the ones described in the original work. The learning rate is set to 2.5e-4, the batch size is 4, rmsprop is used for optimization and data augmentation is used, that includes rotation (±30 o ), scale (±0.25), and left-right flipping.</p><p>To train the 3D pose ConvNet, we employ the same architecture, but we use only one hourglass component and add a fully connected layer at the end, to regress the N joints coordinates. The training details with respect to optimization and data augmentation are the same as for the initial network, but training is done from scratch (we do not use a pretrained model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-view pose estimation</head><p>First of all, we need to assess the accuracy of the annotations provided from our multi-view optimization scheme. Since our ConvNets are not trained using groundtruth data from the aforementioned datasets, we heavily rely on the quality of these automatic annotations. Therefore, we evaluate multi-view pose estimation using our approach, described in Section 3.2</p><p>First, we report results of our approach on the smallscale, yet challenging KTH dataset. Even though relevant methods train specialized 2D detectors for pose estimation, they are all outperformed by our approach using only a generic ConvNet for 2D joint prediction. The relative improvement is illustrated in <ref type="table">Table 1</ref>.</p><p>For Human3.6M we apply the same method to multi-  <ref type="table">Table 2</ref>: Quantitative evaluation of our approach on Human3.6M. The numbers are the average 3D joint errors (mm). Baseline numbers are taken from the respective papers. Note that Zhou et al. <ref type="bibr" target="#b46">[47]</ref> use video, while our proposed method is multi-view.</p><p>view pose estimation. Since this dataset was published very recently, there are no reported results for multi-view pose estimation methods. It is interesting though to compare with the top-performing works for single view 3D pose such that we can quantify the current gap between single view and multi-view estimation. The full results are presented in <ref type="table">Table 2</ref>. Our approach reduces the error of the state-of-the-art single view approach of Zhou et al. <ref type="bibr" target="#b45">[46]</ref> by almost a half. We note that Zhou et al. <ref type="bibr" target="#b46">[47]</ref> use video instead of prediction from a single frame. We do not include results from Bogo et al. <ref type="bibr" target="#b6">[7]</ref> and Sanzari et al. <ref type="bibr" target="#b34">[35]</ref> which report average errors of 82.3mm and 93.15mm, respectively, since they use a rigid alignment between the estimated pose and the groundtruth, making it not comparable with the other methods. Moreover, as a weak multi-view baseline, we averaged the per view 3D estimates from one of the state-of-the-art approaches <ref type="bibr" target="#b46">[47]</ref>. This naive combination achieves an average error of 103.10mm which is a minimal improvement compared to the original error of 113.01mm for the corresponding single view approach. This demonstrates that handling the views independently and combining the single view 3D pose results in a late stage does not leverage the rich 3D geometric constraints available and significantly underperforms compared to our multi-view optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">"Personalizing" 2D pose ConvNet</head><p>Having validated the accuracy of our proposed multiview optimization scheme, the next step is to actually leverage the automatic annotations for learning purposes. The most immediate benefit comes from using them to refine the generic ConvNet and adapt it to the particular test conditions. This can be considered as an application of "personalization", similar in nature to the goal of Charles et al. <ref type="bibr" target="#b10">[11]</ref>, where significant pose estimation gains in terms of accuracy were reported.</p><p>For KTH we use the two available sequences from "Player 2" to evaluate the online adaptation of our network.  Since our focus is to purely evaluate the quality of the 2D predictions before and after refinement, we report 2D PCP results in <ref type="table" target="#tab_4">Table 3</ref>. We observe performance improvement across all parts of the subject. Moreover, for the second sequence which is considerably more challenging, the benefit from our refinement is even greater. This underlines the importance of refinement when the original detector fails. For Human3.6M we evaluate the quality of 2D heatmaps through their impact on the multi-view optimization. Achieving better results for 2D pose estimation is definitely desirable, but ideally, the predicted heatmaps should benefit other post-processing steps as well, e.g., our multi-view optimization. In <ref type="table">Table 4</ref>, we provide a more detailed ablative study comparing different sets of annotations for refinement. Starting with the "Generic" ConvNet, one naive approach we compare against is using the heatmap maximum predictions as annotations ("HM"), or a subset of the most confident of those predictions ("HM+sel"). For "HM+sel" we use the heatmap value to indicate detection confidence, and identify only the top 70% for each joint as reliable 2D annotations. These serve as baselines for refining the ConvNet. We also employ the complete annotation set that is provided from our multi-view optimization  <ref type="table">Table 4</ref>: Quantitative comparison of multi-view optimization after fine-tuning the ConvNet with different annotation sets and evaluating on Human3.6M. We present results for the three most challenging actions (based on <ref type="table">Table 2</ref>), along with the average across all actions. The numbers are the average 3D joint error (mm). "Generic", "HM", "HM+sel", "PS" and "PS+sel" are defined in Section 4.4.</p><p>("PS"), and a high quality version of this by selecting the most confident joint predictions only (denoted as "PS+sel" and described in Section 3.3). The reported results include both the average performance across all 15 actions, as well as the performance on the three actions with the highest error, according to <ref type="table">Table 2</ref>, namely, Purchases, Sitting, and Sitting Down. Again, the performance benefits are greater for more challenging actions, which justifies the use of our method to overcome dataset bias and adapt to the scenario of interest. Also, the naive approach to recover more 2D annotations and bootstrapping on the output of the generic ConvNet ("HM" and "HM+sel") is only marginally helpful on average, which underlines the benefit of the rich geometrical information we employ to recover annotations. Finally, the proposed selection scheme ("PS+sel") outperforms the model that uses all annotations of the multi-view optimization ("PS") which exemplifies the importance of selecting only a high-quality subset of the annotations for refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training a 3D pose ConvNet</head><p>A great challenge, but also a very interesting application of our method is to use the automatically generated annotations to train a ConvNet for 3D pose estimation. Since KTH is a small-scale dataset, we focus on Human3.6M. We leverage the high-quality annotations from the multiview optimization scheme, and train the network described in Section 3.5 from scratch. The results are presented in <ref type="table">Table 5</ref>, along with other approaches. Even though we only use the noisy annotations recovered by our approach for training and ignored the groundtruth from the dataset, the final trained ConvNet is on par with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative results</head><p>For "personalization", <ref type="figure" target="#fig_2">Figures 3 and 4</ref> show qualitative sample results of the proposed approach with and without fine-tuning on annotations recovered from the input imagery on KTH Multiview Football II and Human3.6M, re-Average (6 actions) Average (15 actions) Li et al. <ref type="bibr" target="#b24">[25]</ref> 121.31 -Tekin et al. <ref type="bibr" target="#b37">[38]</ref> 116.77 -Park et al. <ref type="bibr" target="#b27">[28]</ref> 111.12 117.34 Zhou et al. <ref type="bibr" target="#b45">[46]</ref> 104.73 107.26 Rogez et al. <ref type="bibr" target="#b33">[34]</ref> -121.2 Ours 113.65 118.41 <ref type="table">Table 5</ref>: Quantitative comparison of single image approaches on Human3.6M. The numbers are the average 3D joint errors (mm). Baseline numbers are taken from the respective papers. In contrast to the other works, we do not use 3D groundtruth for training, instead we rely solely on the harvested 3D annotations. Despite that, our performance is on par with the state-of-the-art.</p><p>spectively. Despite the generic ConvNet being quite reliable, it might fail for the most challenging poses which are underrepresented in the original generic training set. The benefit from the "personalized" ConvNet is greater in these cases since it adapts to the discriminative appearance of the user and recovers the pose successfully. For the 3D pose ConvNet trained from scratch, we present example 3D reconstructions in <ref type="figure" target="#fig_4">Figure 5</ref>. Notice the challenging poses of the subject and the very accurate poses predicted by the ConvNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>This paper presented an automatic way to gather 3D annotations for human pose estimation tasks, using a generic ConvNet for 2D pose estimation and recordings from a multi-view setup. The automatically generated annotations were used to adapt a generic ConvNet to the particular task, demonstrating important performance benefits from this "personalization". Additionally, we trained a ConvNet for 3D pose estimation which performs on par with the current state-of-the-art, even though we only used automatically harvested annotations, and ignored the provided groundtruth.</p><p>One promising direction for future work is using the automatic annotation setup in an outdoor environment, (where MoCap systems and depth sensors are not applicable) to collect 3D annotations for in-the-wild images. This would allow us to train a generic 3D human pose ConvNet, similar to the 2D counterparts, by overcoming the bottleneck of limited color images with 3D groundtruth.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The quality of the harvested annotations is demonstrated in two applications: (a) projecting the 3D estimates into the 2D imagery and using them to adapt ("personalize") a generic 2D pose ConvNet to the discriminative appearance aspects of the subject, (b) training a ConvNet that predicts 3D human pose from a single color image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Project Page: https://www.seas.upenn.edu/˜pavlakos/ projects/harvesting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples on KTH Multiview Football II showing the performance gain from "personalization". For each pair of images, pose estimation results are presented from the generic (left) and the "personalized" ConvNet (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Examples on Human3.6M showing the performance gain from "personalization". For each pair of images, pose estimation results are presented from the generic (left) and the "personalized" ConvNet (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Example predictions on Human3.6M from the ConvNet trained to estimate 3D pose from a single image. For each example, we present (left-to-right) the input image, the predicted 3D pose from the original view, and a novel view. Red and green indicate left and right, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Ours [9] [22] [4] [5] Ours Upper arms 53 64 96 98 60 89 68 98 100 Lower arms 28 50 68 92 35 68 56 72 100 Upper legs 88 75 98 99 100 100 78 99 100 Lower legs 82 66 88 97 90 99 70 92 100 Average 62.7 63.8 87.5 96.5 71.2 89.0 68.0 90.3 100</figDesc><table><row><cell>Two cameras</cell><cell>Three cameras</cell></row><row><cell>[9] [4] [5]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison of the generic ConvNet versus the refined version for the two sequences of "Player 2" from KTH Multiview Football II. The numbers are percentage of correct parts (PCP) in 2D. Performance improvement is observed across all parts.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We gratefully appreciate support through the following grants: NSF-DGE-0966142 (IGERT), NSF-IIP-1439681 (I/UCRC), NSF-IIS-1426840, ARL MAST-CTA W911NF-08-2-0004, ARL RCTA W911NF-10-2-0016, ONR N00014-17-1-2093, an ONR STTR (Robotics Research), NSERC Discovery, and the DARPA FLA program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiview pictorial structures for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<title level="m">3D pictorial structures revisited: Multiple human pose estimation. PAMI</title>
		<imprint>
			<date type="published" when="2105" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A study of parts-based object class detection using complete graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bergtholdt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Personalizing human video pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinsk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient ConvNet-based marker-less motion capture in general scenes with a low number of cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MARCOnI -ConvNet-based MARker-less motion Capture in Outdoor and Indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="501" to="514" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TC, C</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning camera viewpoint using CNN to improve 3D body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Ghezelghieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Has my algorithm succeeded? An evaluator for human pose estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jammalamadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiview body part recognition with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3D human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3D human pose estimation using convolutional neural networks with 2D pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flowing ConvNets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The curious robot: Learning visual representations via physical interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tracking people by learning their appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MoCap-guided data augmentation for 3D pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bayesian image based 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ntouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MODEC: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Single image 3D human pose estimation from noisy observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alenyà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structured prediction of 3D human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Single image 3D interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3D human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
