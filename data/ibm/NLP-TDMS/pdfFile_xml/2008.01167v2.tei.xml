<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reducing Label Noise in Anchor-Free Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nermin</forename><surname>Samet</surname></persName>
							<email>nermin@ceng.metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">Middle East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Hicsonmez</surname></persName>
							<email>samethicsonmez@hacettepe.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="institution">Hacettepe University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Middle East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reducing Label Noise in Anchor-Free Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>SAMET, HICSONMEZ, AKBAS: PPDET 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current anchor-free object detectors label all the features that spatially fall inside a predefined central region of a ground-truth box as positive. This approach causes label noise during training, since some of these positively labeled features may be on the background or an occluder object, or they are simply not discriminative features. In this paper, we propose a new labeling strategy aimed to reduce the label noise in anchor-free detectors. We sum-pool predictions stemming from individual features into a single prediction. This allows the model to reduce the contributions of non-discriminatory features during training. We develop a new one-stage, anchor-free object detector, PPDet, to employ this labeling strategy during training and a similar prediction pooling method during inference. On the COCO dataset, PPDet achieves the best performance among anchorfree top-down detectors and performs on-par with the other state-of-the-art methods. It also outperforms all major one-stage and two-stage methods in small object detection (AP S 31.4). Code is available at https://github.com/nerminsamet/ppdet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Early deep learning based object detectors were two-stage, proposal driven methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>. In the first stage, a sparse set of object proposals are generated and a convolutional neural network (CNN) categorizes them in the second stage. Later, the idea of unified detection in a single stage has gained increasing attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>, where proposals were replaced with predefined anchors. On the one hand, anchors have to cover the image densely (in terms of location, shape and scale) so as to maximize recall; on the other hand, their number should be kept at a minimum to reduce both the inference time and the imbalance problems <ref type="bibr" target="#b18">[19]</ref> they create during training.</p><p>A considerable amount of effort has been spent on addressing the drawbacks of anchors: several methods have been proposed to improve the quality of anchors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>, to address the extreme foreground-background imbalance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>, and recently, one-stage anchorfree methods have been developed. There are two main groups of prominent approaches in anchor-free object detection. The first group is keypoint based, bottom-up methods, popularized after the pioneering work CornerNet <ref type="bibr" target="#b10">[11]</ref>. These detectors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref> first detect c 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:2008.01167v2 [cs.CV] 13 Aug 2020 The colored dots show the locations whose predictions are pooled to generate the final detection shown in the green bounding box. The color denotes the contribution weight. Highest contributions are coming from the objects and not occluders or background areas. Images are from COCO val2017 set.</p><p>keypoints (e.g. corners, center and extreme points) of objects, and then group them to yield whole-object detections. The second group of anchor-free object detectors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref> follow a top-down approach, and directly predict class and bounding box coordinates at each location in the final feature map(s).</p><p>One important aspect of object detector training is the strategy used to label object candidates, which could be proposals, anchors or locations (i.e. features) in the final feature map. In order to label a candidate 'positive' (foreground) or 'negative' (background) during training, a variety of strategies have been proposed, based on Intersection over Union (IoU) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref>, keypoints <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref> and relative location to a ground-truth box <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Specifically in top-down anchor-free object detectors, after the input image is passed through the backbone feature extractor and the FPN <ref type="bibr" target="#b12">[13]</ref>, features that spatially fall inside a ground-truth box are labeled as positive and others as negative -there is also an "ignore" region in between. Each of these positively-labeled features contributes to the loss function as a separate prediction. The problem with this approach is that some of these positive labels might be plain-wrong or of poor quality, hence, they inject label noise during training. Noisy labels come from (i) non-discriminatory features that are on the object, (ii) background features within the ground-truth box, and (iii) occluders ( <ref type="figure" target="#fig_0">Fig. 1</ref>). In this paper, we propose an anchor-free object detection method, which relaxes the positive labeling strategy so that the model is able to reduce the contributions of non-discriminatory features during training. In accordance with this training strategy, our object detector employs an inference method where highly-overlapping predictions enforce each other.</p><p>In our method, during training, we define a "positive area" within a ground-truth (GT) box, which is co-centric and has the same shape with the GT box. We experimentally adjust the size of the positive area relative to the GT box. As this is an anchor-free method, each feature (i.e. location in the final feature maps) predicts a class probability vector and bounding box coordinates. The class predictions from the positive area of a GT box get pooled together and contribute to the loss as a single prediction. This sum-pooling alleviates the noisy-labels problem mentioned above since the contributions of features from non-object (background or occluded) areas, and non-discriminatory features are automatically down weighted during training. At inference, class probabilities of highly overlapping boxes are again pooled together to obtain the final class probabilities. We name our method as "PPDet", which is short for "prediction pooling detector."</p><p>Our contributions with this work are two fold: (i) a relaxed labelling strategy, which allows the model to reduce the contribution of non-discriminatory features during training, and (ii) a new object detection method, PPDet, which uses this strategy for training and a new inference procedure based on prediction pooling. We show the effectiveness of our proposal on the COCO dataset. PPDet outperforms all anchor-free top-down detectors and performs on-par with the other state-of-the-art methods. PPDet is especially effective for detecting small objects (31.4 AP S , better than state-of-the-art).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Apart from the classical one-stage <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref> vs. two-stage <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref> categorization of object detectors, we can also categorize the current approaches into two: anchor-based and anchor-free. Top-down anchor-free object detectors simplify the training process by eliminating complex IoU operations and focus on identifying the regions that may contain objects. In that sense, FCOS <ref type="bibr" target="#b24">[25]</ref>, FSAF <ref type="bibr" target="#b32">[33]</ref> and FoveaBox <ref type="bibr" target="#b9">[10]</ref> first map GT boxes onto the FPN levels, then label the locations, i.e. features, as positive or negative based on whether they are inside a GT box. Bounding box prediction is only for positively-labeled locations. FoveaBox <ref type="bibr" target="#b9">[10]</ref> and FSAF <ref type="bibr" target="#b32">[33]</ref> define three areas for each object instance; positive area, ignore area and negative area. FoveaBox defines the positive (fovea) area as the region which is co-centric with the GT box, and whose dimensions are scaled by a (shrink) factor 0.3. All locations within this positive area are labeled as positive. Similarly, another area is obtained using a shrink factor of 0.4. Any location that is outside this area is labeled as negative. If a location is neither positive nor negative, it is ignored during training. FSAF follows the same approach and uses shrink factors 0.2 and 0.5, respectively. Instead of having pre-defined discrete areas as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>, FCOS down-weights the features based on their distance to the center using a centerness branch. FCOS and FoveaBox implement static feature-pyramid level selection where they assign objects to levels based on GT box scale and GT box regression distance, respectively. Unlike them, FSAF relaxes the feature selection step and dynamically assigns each object to the most suitable feature-pyramid level.</p><p>Bottom-up anchor-free object detection methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref> aim to detect certain keypoints of objects, such as corners and the center. Their labeling strategy uses heatmaps, and in this sense, it is considerably different from that of top-down anchor-free methods. More recently, HoughNet, a novel, bottom-up voting-based method that can utilize both near and long-range evidence to detect object centers, has shown comparable performance with major one-stage and two-stage top-down methods <ref type="bibr" target="#b22">[23]</ref>.</p><p>In the anchor-based approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>, objects are predicted from regressed anchor boxes. During training, the label of an anchor box is determined based on its intersection over union (IoU) with a GT box. Different detectors use different criteria, e.g. Faster RCNN <ref type="bibr" target="#b21">[22]</ref> labels an anchor as positive if IoU &gt; 0.7, and negative if IoU &lt; 0.3; R-FCN <ref type="bibr" target="#b2">[3]</ref>, SSD <ref type="bibr" target="#b15">[16]</ref> and Retinanet <ref type="bibr" target="#b13">[14]</ref> use IoU &gt; 0.5 for positive labeling but slightly different criterias for negative labeling. There are two prominent anchor-based methods which directly address the labeling problem. Guided Anchoring <ref type="bibr" target="#b25">[26]</ref> introduces a new adaptive anchoring scheme that learns arbitrary shaped boxes instead of dense and predefined ones. Similar to FSAF <ref type="bibr" target="#b32">[33]</ref>, FoveaBox <ref type="bibr" target="#b9">[10]</ref> and our method PPDet, Guided Anchoring follows region based labelling and defines three types of regions for each ground-truth object; center region, ignore region and outside region, and labels the generated anchors positive if it resides inside the center region, negative if in outside region and ignores the rest. On the other hand, FreeAnchor <ref type="bibr" target="#b30">[31]</ref> applies the idea of relaxing positive labels for anchor-based detectors. This is the most similar method to ours. It replaces hand-crafted anchor assignment with a maximum likelihood estimation procedure, where anchors are set free to choose their GT box. Since FreeAnchor is optimizing object-anchor matching using a customized loss function, it can not be directly applied to anchor-free object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Labeling strategy and training. Anchor-free detectors limit prediction of GT boxes by assigning them to appropriate FPN levels based on their scales <ref type="bibr" target="#b9">[10]</ref> or target regression distances <ref type="bibr" target="#b24">[25]</ref>. Here, we follow the scale-based assignment strategy <ref type="bibr" target="#b9">[10]</ref> since it is a way of naturally associating GT boxes with feature pyramid levels. Then, we construct two different regions for each GT box. We define the "positive area" as the region that is co-centric with the GT box and having the same shape as the GT box. We experimentally set the size of the "positive area". Then, we identify all the locations (i.e. features) that spatially fall inside the "positive area" of a GT box as "positive (foreground)" features and the rest as "negative (background)" features. Each positive feature is assigned to the ground-truth box that contains it. In <ref type="figure" target="#fig_1">Figure 2</ref>, blue and red cells represent foreground cells and the rest (empty or white) are background cells. The blue cells are assigned to the frisbee object and the red cells to the person object. To obtain the final detection score for an object instance, we pool the classification scores of all the features that are assigned to that object, by adding them together to obtain a final C-dimensional vector where C is the number of the classes. All features except the positively labelled ones are negatives. Each negative feature contributes individually to the loss (i.e. no pooling). This final prediction vector is fed to the focal loss (FL). For example, suppose {p i |i = 1, 2, . . . , N} represent the red, foreground features that are assigned to the person object in <ref type="figure" target="#fig_1">Figure 2</ref>. Let y be the ground-truth, one-hot vector for the person class. Then, this particular object instance contributes "FL(∑ i p i , y)" to the loss function in training. Each object instance is represented with a single prediction.</p><p>By default, we assign positive features to the object instance of the box they are in. At this point, assignment of features in the intersection areas of different GT boxes is an issue to be handled. In such cases, we assign those features to the GT box with the smallest distance to their centers. Similar to other anchor-free methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, in our model each foreground feature assigned to an object is trained to predict the coordinates of its object's GT box.  We use the focal loss <ref type="bibr" target="#b13">[14]</ref> (α = 0.4 and γ = 1.5) for the classification branch and smooth L 1 loss <ref type="bibr" target="#b6">[7]</ref> for the regression branch.</p><p>Inference. Inference pipeline of PPDet is given in <ref type="figure" target="#fig_2">Figure 3</ref>. First, the input image is fed to a backbone neural network model (described in the next section) which produces the initial set of detections. Each detection is associated with (i) a bounding box, (ii) an object class (chosen as the class with maximum probability) and (iii) a confidence score. Within these detections, those labeled with the background class are eliminated. We consider each remaining detection at this stage as a vote for the object that it belongs to, where the box is an hypothesis for the location of the object and the confidence score is the strength of the vote. Next, these detections are pooled together as follows. If two detections belonging to the same object class overlap more than a certain amount (i.e. intersection over union (IoU) &gt; 0.6), then we consider them as voting for the same object and the score of each detection is increased by k (IoU−1.0) times the score of the other detection, where k is a constant. The more the IoU, the higher the increase. After applying this process to every pair of detections, we obtain the scores for final detections. This step is followed by the class aware non-maxima suppression (NMS) operation which yields the final detections.</p><p>Note that although the prediction pooling used in inference might seem to be different from the pooling employed in training, in fact, they are the same process. The pooling used in training makes the assumption that the bounding boxes predicted by the features in the positive area overlap among each other perfectly (i.e. IoU=1).</p><p>Network architecture. PPDet uses the network model of RetinaNet <ref type="bibr" target="#b13">[14]</ref> which consists of a backbone convolutional neural network (CNN) followed by a feature pyramid network (FPN) <ref type="bibr" target="#b12">[13]</ref>. The FPN computes a multi-scale feature representation and produces feature maps at five different scales. There are two separate, parallel networks on the top of each FPN layer, namely classification network and regression network. The classification network outputs a W × H ×C tensor where W and H are spatial dimensions (width and height, respectively) and C is the number of the classes. Similarly, the regression network outputs a W × H × 4 tensor where 4 is the number of bounding box coordinates. We refer to each pixel in these tensors as a feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section describes the experiments we conducted to show the effectiveness of our proposed method. First, we present ablation experiments to find the optimal relative area of the positive region within GT boxes and the regression loss weight. Next, we present several performance comparisons on the COCO dataset. Finally, we provide sample heatmaps which show the GT box relative locations of features responsible for correct detections. Implementation Details. We use Feature Pyramid Network (FPN) <ref type="bibr" target="#b12">[13]</ref> on top of ResNet <ref type="bibr" target="#b8">[9]</ref> and ResNeXt <ref type="bibr" target="#b27">[28]</ref> as our backbone networks for ablations and state of the art comparison, respectively. For all experiments, we resize the images such that their shorter side is 800 pixels and longer side is maximum 1300 pixels. The constant k used in vote aggregation (i.e., k IoU−1 ) was set to 40 experimentally. We trained all of the experiments on 4 Tesla V100 GPUs, and tested using a single Tesla V100 GPU. We used MMDetection <ref type="bibr" target="#b1">[2]</ref> framework with Pytorch <ref type="bibr" target="#b19">[20]</ref> to implement our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Experiments</head><p>Unless stated otherwise, in ablation experiments we used ResNet-50 with FPN backbone. They are trained with a batch size of 16 for 12 epochs using stochastic gradient descent (SGD) with weight decay of 0.0001 and momentum of 0.9. Initial learning rate 0.01 was dropped 10× at epochs 8 and 11. All ablation models are trained on COCO <ref type="bibr" target="#b11">[12]</ref> train2017 dataset and tested on val2017 set.</p><p>Size of the "positive area". As explained before, we define the "positive area" as the region that is co-centric with the GT box and that has the same shape as the GT box. We adjust the size of this "positive area" by multiplying its width and height with a shrink factor. We experimented with shrink factors between 1.0 and 0.2. Performance results are presented in <ref type="table" target="#tab_2">Table 1</ref>. From shrink factor 1.0 to 0.4, AP increases, however, after that point performance degrades dramatically. Based on this ablation, we set the shrink factor to 0.4 for the rest of our experiments.</p><p>Regression loss weight. To find the optimal balance between the classification and regression loss, we conducted ablation experiments on the regression loss weight. As shown in <ref type="table" target="#tab_3">Table 2</ref>, 0.75 yields the best results. We set the weight of the regression loss to 0.75 for the rest of our experiments.    Improvements. We also employed improvements used in other state-of-the-art object detectors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>. First, we trained our baseline model using ResNet-101 with FPN backbone. Later, we replaced the last convolution layer before class prediction in the classification branch with deformable convolutional layers. This modification improved the performance around 0.3 for all APs (see <ref type="table" target="#tab_5">Table 3</ref>). Later, on top of this modification, we add another one where we adopt group normalization after each convolution layer in the regression and classification branches. As seen in <ref type="table" target="#tab_5">Table 3</ref>, this modification increased AP by 0.6 and AP 50 by 1.1. In this table, we also provide results for the recently introduced moLRP [18] metric, which combines localization, precision and recall in a single metric. Lower values are better. Models are trained with a batch size of 16 for 24 epochs using stochastic gradient descent (SGD) with weight decay of 0.0001 and momentum of 0.9. Initial learning rate 0.01 was dropped 10× at epochs 16 and 22. We include these two modifications in our final model.</p><p>Class imbalance. PPDet sum-pools predictions into a single prediction per object instance which reduces the number of positives during training. One may think that it exacerbates the class imbalance <ref type="bibr" target="#b18">[19]</ref> even more. To analyse the issue, we calculated the average number of positives per image, which is 7 for PPDet, 41 for FoveBox and 165 for RetinaNet. PPDet considerably decreases the number of positives. However, this is still small compared to the number of negatives (tens of thousands), hence, it does not exacerbate the existing class imbalance problem. We use focal loss to tackle the imbalance.    <ref type="table" target="#tab_6">Table 4</ref> presents performances of PPDet and several established state-of-the-art detectors. FSAF <ref type="bibr" target="#b32">[33]</ref> and FoveaBox [10] use a similar approach to ours to build the "positive area".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>While single scale testing performance of PPDet is comparable with that of FSAF on the same ResNeXt-101-64x4d with FPN backbone, PPDet's multi-scale testing performance is 1.7 AP points better than that of FSAF's. Our both models with single-scale testing get slightly better results than FoveaBox while outperforming it on small objects by more than 1.0. The results of our multi-scale testing outperforms FoveaBox by 1 AP on the same ResNet-101 with FPN backbone. Our multi-scale performance is the best among all the anchor-free top-down methods. Moreover, our multi-scale performance on small objects (i.e. AP S ) sets the new state-of-theart among all detectors in <ref type="table" target="#tab_6">Table 4</ref>.</p><p>We conducted experiments to analyse the effect of the prediction pooling for training and inference. When we removed the prediction pooling from the inference pipeline of our ResNet-101-FPN backbone model, we observed that AP goes down by 2.5 points on val2017 set. To analyse the effect of prediction pooling for training, we added prediction pooling to RetinaNet <ref type="bibr" target="#b13">[14]</ref> and FoveaBox <ref type="bibr" target="#b9">[10]</ref> only during inference (so, no PP in training). This resulted in 0.5 and 2.8 points drop in AP for RetinaNet and FoveaBox, respectively.</p><p>We also conducted another experiment to test the effectiveness of sum-pooling over maxpooling. For max-pooling, we identified the feature within the positive area, whose predicted box overlaps the most with the GT box. Then, only this feature is included in focal loss to represent its GT box during training. This strategy dropped AP by more than 2 points, yielding 38.4 with ResNet101 with FPN backbone.</p><p>As an additional result, we present the performance of PPDet on the PASCAL VOC dataset <ref type="bibr" target="#b4">[5]</ref>. For training, we used the union set of PASCAL VOC 2007 trainval and VOC 2012 trainval images ("07+12"). For testing, we used the test set of PASCAL VOC 2007. Our PPDet model achieves 77.8 mean average precision (mAP) outperforming FoveaBox <ref type="bibr" target="#b9">[10]</ref> at 76.6 mAP, which we consider as a baseline here, when both use the ResNet-50 backbone. <ref type="figure">Figure 4</ref> shows the heatmap of cell centers relative to the ground-truth box, which are responsible for detection. The heatmaps of RetinaNet are concentrated at the center of the ground-truth object boxes. In contrast, PPDet's final detections are formed from a relatively wider area verifying its dynamic and automatic characteristics on assigning weights to the features in the positive area. In addition to the detections coming from the center of the ground-truth box, they may heavily come from the different parts of the ground-truth box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we introduced a novel labeling strategy for the training of anchor-free object detectors. While current anchor-free methods force positive labels on all the features that are spatially inside a predefined central region of a ground-truth box, our labeling strategy relaxes this constraint by sum-pooling predictions stemming from individual features into a single prediction. This allows the model to reduce the contributions of non-discriminatory features during training. We developed PPDet, a one-stage, anchor-free object detector which employs the new labeling strategy during training and a new inference method based on pooling predictions. We analyzed our idea by conducting several ablation experiments. We reported results on COCO test-dev and show that PPDet performs on par with the state-of-the-art and achieves state-of-the-art results on small objects (AP S 31.4). We further validated the effectiveness of our method through visual inspections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person</head><p>Bicycle Boat Bench Tie Skis <ref type="figure">Figure 4</ref>: Feature locations that are responsible for detection during inference, relative to the ground-truth box (blue rectangle). To bring different ground-truth boxes into the same plot, we normalized each ground-truth box to a canonical size. Relative locations of responsible features were normalized accordingly. Top row shows the heatmap of responsible feature locations for anchor-based RetinaNet. Second row shows the same for anchor-free object detector FoveaBox. Bottom row shows the same for PPDet. Heatmaps were obtained on COCO val2017 images with ResNet-101 with FPN backbone. RetinaNet detects objects mostly with center cells. In terms of peakyness, FoveaBox's heatmaps are similar to RetinaNet's. PPDet detects objects from a wider area, also from outside of the object box.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Three sample detections by PPDet, from left to right: surfboard, laptop and racket.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Prediction pooling during training of PPDet. For simplicity, it is illustrated on a single FPN level and the bounding box regression branch is not shown. Blue and red cells are foreground cells. Same color foreground cells, each of which is a C-dimensional vector, are pooled, i.e. summed together, to form the final prediction score for the corresponding object. These pooled scores (i.e., p person , p frisbee ), are fed to the loss function (i.e., focal loss).(Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(Left) Illustration of PPDet's inference pipeline. Predicted boxes for person and snowboard are shown in red and blue, respectively. Red and blue boxes vote for each other among themselves. See text for details. (Right) A pooling example. The dashed-boundary red boxes vote for the solid red box and the dashed-boundary blue boxes vote for the solid blue box. Final scores (after aggregation) of solid boxes are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Shrink</head><label></label><figDesc>Factor AP AP 50 AP 75 AP S AP M AP L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Experiments to determine the best shrink factor which defines the relative size of the "positive area" with respect to the GT box. Models were trained on train2017 and results were obtained on val2017.RL weight AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell>1.00</cell><cell>36.0 53.6 39.0 20.4 39.6 46.6</cell></row><row><cell>0.90</cell><cell>36.0 53.9 39.3 20.1 39.6 47.2</cell></row><row><cell>0.75</cell><cell>36.3 54.3 39.5 21.1 39.5 47.5</cell></row><row><cell>0.60</cell><cell>36.2 54.6 39.5 21.0 40.1 47.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Experiments on regression loss (RL) weight. Models were trained on train2017 and results were obtained on val2017.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>AP AP 50 AP 75 AP S AP M AP L moLRP ↓</figDesc><table><row><cell>Baseline</cell><cell>39.6 58.0 43.4 23.9 44.1 51.0</cell><cell>68.9</cell></row><row><cell cols="2">+ Deform. Conv. 39.9 58.4 43.7 24.2 44.4 51.3</cell><cell>68.7</cell></row><row><cell>+ Group Norm.</cell><cell>40.5 59.5 44.2 25.4 44.7 52.3</cell><cell>67.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Train size</cell><cell>Test size</cell><cell cols="3">AP AP 50 AP 75 AP S AP M AP L</cell><cell>FPS</cell></row><row><cell>Two-stage detectors:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R-FCN [3]</cell><cell>ResNet-101</cell><cell>800×800</cell><cell cols="2">600×600 29.9 51.9</cell><cell>-</cell><cell>10.8 32.8 45.0</cell><cell>5.9</cell></row><row><cell>CoupleNet [34]</cell><cell>ResNet-101</cell><cell>ori.</cell><cell>ori.</cell><cell cols="3">34.4 54.8 37.2 13.4 38.1 50.8</cell><cell>-</cell></row><row><cell>Faster R-CNN+++ [9]</cell><cell>ResNet-101</cell><cell cols="5">1000×600 1000×600 34.9 55.7 37.4 15.6 38.7 50.9</cell><cell>-</cell></row><row><cell>Faster R-CNN [13]</cell><cell>ResNet-101-FPN</cell><cell cols="5">1000×600 1000×600 36.2 59.1 39.0 18.2 39.0 48.2</cell><cell>5.0</cell></row><row><cell>Mask R-CNN [8]</cell><cell cols="7">ResNeXt-101-FPN 1300×800 1300×800 39.8 62.3 43.4 22.1 43.2 51.2 11.0</cell></row><row><cell>Cascade R-CNN [1]</cell><cell>ResNet-101</cell><cell>-</cell><cell>-</cell><cell cols="4">42.8 62.1 46.3 23.7 45.5 55.2 12.0</cell></row><row><cell>PANet [15]</cell><cell>ResNeXt-101</cell><cell cols="5">1400×840 1400×840 47.4 67.2 51.8 30.1 51.7 60.0</cell><cell>-</cell></row><row><cell>One-stage, anchor-based:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD [16]</cell><cell>VGG-16</cell><cell>512×512</cell><cell cols="4">512×512 28.8 48.5 30.3 10.9 31.8 43.5</cell><cell>-</cell></row><row><cell>YOLOv3 [21]</cell><cell>Darknet-53</cell><cell>608×608</cell><cell cols="5">608×608 33.0 57.9 34.4 18.3 35.4 41.9 20.0</cell></row><row><cell>DSSD513 [6]</cell><cell>ResNet-101</cell><cell>513×513</cell><cell cols="4">513×513 33.2 53.3 35.2 13.0 35.4 51.1</cell><cell>-</cell></row><row><cell>RefineDet (SS) [30]</cell><cell>ResNet-101</cell><cell>512×512</cell><cell cols="4">512×512 36.4 57.5 39.5 16.6 39.9 51.4</cell><cell>-</cell></row><row><cell>RetinaNet [14]</cell><cell>ResNet-101-FPN</cell><cell cols="6">1300×800 1300×800 39.1 59.1 42.3 21.8 42.7 50.2 10.9  *</cell></row><row><cell>RetinaNet [14]</cell><cell cols="7">ResNeXt-101-FPN 1300×800 1300×800 40.8 61.1 44.1 24.1 44.2 51.2 7.0  *</cell></row><row><cell>RefineDet (MS) [30]</cell><cell>ResNet-101</cell><cell>512×512</cell><cell>≤2.25×</cell><cell cols="3">41.8 62.9 45.7 25.6 45.1 54.1</cell><cell>-</cell></row><row><cell>GA-RetinaNet [26]  *</cell><cell>ResNet-101</cell><cell cols="5">1300×960 1300×800 41.9 62.2 45.3 24.0 45.3 53.8</cell><cell>-</cell></row><row><cell>FreeAnchor (SS) [31]</cell><cell cols="7">ResNeXt-101-FPN 1300×960 1300×960 44.9 64.3 48.5 26.8 48.3 55.9 8.4  *</cell></row><row><cell>FreeAnchor (MS) [31]</cell><cell cols="2">ResNeXt-101-FPN 1300×960</cell><cell>∼≤2.0×</cell><cell cols="3">47.3 66.3 51.5 30.6 50.4 59.0</cell><cell>-</cell></row><row><cell>Anchor-free, bottom-up:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ExtremeNet (SS) [17]</cell><cell>Hourglass-104</cell><cell>511×511</cell><cell>ori.</cell><cell cols="3">40.2 55.5 43.2 20.4 43.2 53.1</cell><cell>3.1</cell></row><row><cell>CornerNet (SS) [11]</cell><cell>Hourglass-104</cell><cell>511×511</cell><cell>ori.</cell><cell cols="3">40.5 56.5 43.1 19.4 42.7 53.9</cell><cell>4.1</cell></row><row><cell>CornerNet (MS) [11]</cell><cell>Hourglass-104</cell><cell>511×511</cell><cell>≤1.5×</cell><cell cols="3">42.1 57.8 45.3 20.8 44.8 56.7</cell><cell>-</cell></row><row><cell>CenterNet (SS) [32]</cell><cell>Hourglass-104</cell><cell>512×512</cell><cell>ori.</cell><cell cols="3">42.1 61.1 45.9 24.1 45.5 52.8</cell><cell>7.8</cell></row><row><cell>HoughNet (SS) [23]</cell><cell>Hourglass-104</cell><cell>512×512</cell><cell>≤ ori.×</cell><cell cols="3">43.1 62.2 46.8 24.6 47.0 54.4</cell><cell>6.4</cell></row><row><cell>ExtremeNet (MS) [17]</cell><cell>Hourglass-104</cell><cell>511×511</cell><cell>≤1.5×</cell><cell cols="3">43.7 60.5 47.0 24.1 46.9 57.6</cell><cell>-</cell></row><row><cell>CenterNet (SS) [4]</cell><cell>Hourglass-104</cell><cell>511×511</cell><cell>ori.</cell><cell cols="3">44.9 62.4 48.1 25.6 47.4 57.4</cell><cell>3.0</cell></row><row><cell>CenterNet (MS) [32]</cell><cell>Hourglass-104</cell><cell>512×512</cell><cell>≤1.5×</cell><cell cols="3">45.1 63.9 49.3 26.6 47.1 57.7</cell><cell>-</cell></row><row><cell>HoughNet (MS) [23]</cell><cell>Hourglass-104</cell><cell>512×512</cell><cell>≤1.8×</cell><cell cols="3">46.4 65.1 50.7 29.1 48.5 58.1</cell><cell>-</cell></row><row><cell>CenterNet (MS) [4]</cell><cell>Hourglass-104</cell><cell>511×511</cell><cell>≤1.8×</cell><cell cols="3">47.0 64.5 50.7 28.9 49.9 58.9</cell><cell>-</cell></row><row><cell>Anchor-free, top-down:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FoveaBox [10] (SS)</cell><cell>ResNet-101-FPN</cell><cell cols="5">1300×800 1300×800 40.6 60.1 43.5 23.3 45.2 54.5</cell><cell>-</cell></row><row><cell>FoveaBox [10] (SS)</cell><cell cols="6">ResNeXt-101-FPN 1300×800 1300×800 42.1 61.9 45.2 24.9 46.8 55.6</cell><cell>-</cell></row><row><cell>FSAF (SS) [33]</cell><cell cols="6">ResNeXt-101-FPN 1300×800 1300×800 42.9 63.8 46.3 26.6 46.2 52.7</cell><cell>2.7</cell></row><row><cell>FoveaBox [10] (MS)  †</cell><cell>ResNet-101-FPN</cell><cell cols="5">1300×800 1300×800 44.2 65.4 47.8 28.8 46.7 53.7</cell><cell>-</cell></row><row><cell>FSAF (MS) [33]</cell><cell cols="2">ResNeXt-101-FPN 1300×800</cell><cell>∼≤2.0×</cell><cell cols="3">44.6 65.2 48.6 29.7 47.1 54.6</cell><cell>-</cell></row><row><cell>FCOS [25]</cell><cell cols="7">ResNeXt-101-FPN 1300×800 1300×800 44.7 64.1 48.4 27.6 47.5 55.6 7.0  *</cell></row><row><cell>PPDet (SS)</cell><cell>ResNet-101-FPN</cell><cell cols="5">1300×800 1300×800 40.7 60.2 44.5 24.5 44.4 49.7</cell><cell>7.5</cell></row><row><cell>PPDet (SS)</cell><cell cols="6">ResNeXt-101-FPN 1300×800 1300×800 42.3 62.0 46.3 26.2 46.0 51.9</cell><cell>4.1</cell></row><row><cell>PPDet (MS)</cell><cell>ResNet-101-FPN</cell><cell>1300×800</cell><cell>∼≤2.0×</cell><cell cols="3">45.2 63.5 50.3 30.0 48.6 54.7</cell><cell>-</cell></row><row><cell>PPDet (MS)</cell><cell cols="2">ResNeXt-101-FPN 1300×800</cell><cell>∼≤2.0×</cell><cell cols="3">46.3 64.8 51.6 31.4 49.9 56.4</cell><cell>-</cell></row></table><note>Experiments on improvements. Using deformable convolution in the classification branch and group normalization layers further improve detection performances. Models are trained on train2017 and tested on val2017 set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Detection performances on COCO test-dev set. The methods are divided into three groups: two-stage, one-stage anchor-based and one-stage anchor-free. The best results are boldfaced separately for each group. PPDet achieves state-of-the-art results on the AP S metric among all the detectors. * results are taken from MMDetection. † MS test for FoveaBox is implemented by us on top of the original code.</figDesc><table><row><cell>4.2 State-of-the-art comparison</cell></row><row><cell>To compare our model with the state-of-the-art methods, we used ResNet-101 with FPN and</cell></row><row><cell>ResNeXt-101-64x4d with FPN backbones. They are trained with batch sizes of 16 and 8</cell></row><row><cell>for 24 and 16 epochs, respectively, using SGD with weight decay of 0.0001 and momentum</cell></row><row><cell>of 0.</cell></row></table><note>9. For the ResNet backbone, initial learning rate 0.01 was dropped 10× at epochs 16 and 22. For the ResNeXt backbone, initial learning rate 0.005 was dropped 10× at epochs 11 and 14. The models are trained on COCO [12] train2017 dataset and tested on test-dev set. We used (800, 480), (1067, 640), (1333, 800), (1600, 960), (1867, 1120), (2133, 1280) scales for multi-scale testing.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported by the Scientific and Technological Research Council of Turkey (TÜBİTAK) through the project titled "Object Detection in Videos with Deep Neural Networks" (grant #117E054). The numerical calculations reported in this paper were partially performed at TÜBİTAK ULAKBİM, High Performance and Grid Computing Center (TRUBA resources). We also gratefully acknowledge the support of the AWS Cloud Credits for Research program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="7389" to="7398" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Localization recall precision (LRP): A new performance metric for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imbalance Problems in Object Detection: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Baris Can Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HoughNet: Integrating near and long-range evidence for bottom-up object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nermin</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Hicsonmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for singleshot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Couplenet: Coupling global structure with local parts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4126" to="4134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
