<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Interactive Object Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
							<email>ningxu2@illinois.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
							<email>bprice@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
							<email>scohen@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
							<email>jimyang@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Interactive Object Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interactive object selection is a very important research problem and has many applications. Previous algorithms require substantial user interactions to estimate the foreground and background distributions. In this paper, we present a novel deep-learning-based algorithm which has a much better understanding of objectness and thus can reduce user interactions to just a few clicks. Our algorithm transforms user-provided positive and negative clicks into two Euclidean distance maps which are then concatenated with the RGB channels of images to compose (image, user interactions) pairs. We generate many of such pairs by combining several random sampling strategies to model users' click patterns and use them to finetune deep Fully Convolutional Networks (FCNs). Finally the output probability maps of our FCN-8s model is integrated with graph cut optimization to refine the boundary segments. Our model is trained on the PASCAL segmentation dataset and evaluated on other datasets with different object classes. Experimental results on both seen and unseen objects clearly demonstrate that our algorithm has a good generalization ability and is superior to all existing interactive object selection approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Interactive object selection (also known as interactive segmentation) has become a very popular research area over the past years. It enables users to select objects of interest accurately by interactively providing inputs such as strokes and bounding boxes. The selected results are useful for various applications such as localized editing and image/video composition.</p><p>There are many algorithms proposed to solve this problem. One of the most famous algorithms is proposed by Boykov and Jolly <ref type="bibr" target="#b1">[2]</ref> where they formulate interactive seg-mentation as the graph cut optimization and solve it via max-flow/min-cut energy minimization. Rother et al. <ref type="bibr" target="#b18">[19]</ref> extend graph cut by using a more powerful, iterative version of optimization. Bai and Sapiro <ref type="bibr" target="#b0">[1]</ref> present a new algorithm that computes weighted geodesic distances to the userprovided scribbles. Grady <ref type="bibr" target="#b7">[8]</ref> uses the graph theory to estimate the probabilities of random walks from unlabeled pixels to labeled pixels. In order to get accurate segmentation, all these algorithms require substantial user interactions to have a good estimation of the foreground/background distributions. In contrast, our approach simplifies user interactions to a few clicks, with one or two clicks usually giving reasonably good results. The advantage of our approach over the others is the capability to understand objectness and semantics by leveraging deep learning techniques. To our best knowledge, this is the first work that solves interactive segmentation in the framework of deep learning.</p><p>Our approach is inspired by recent successes of deep fully convolutional neural networks (FCNs) on the semantic segmentation problem <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>. Long et al. <ref type="bibr" target="#b14">[15]</ref> adapt popular deep classification networks into FCNs for semantic segmentation and improve the architecture with multi-resolution layer combinations. Built upon this, Chen et al. <ref type="bibr" target="#b2">[3]</ref> combine the outputs of FCNs with Conditional Random Field (CRF) while Zheng et al. <ref type="bibr" target="#b25">[26]</ref> formulate mean-field approximate inference as Recurrent Neural Network (RNN) and plug it on top of FCNs to get finer results.</p><p>A seemingly plausible transformation of those approaches to interactive segmentation is that we first perform semantic segmentation on the whole image and then select the connected components which contain user-provided selections. However, there exists at least three problems with this approach. First, it is not always clear how to response to use inputs. For example, if the user places a foreground click and background click inside the same class label, this approach cannot response to that. Second, current semantic segmentation methods do not support instance-level seg-mentation while that is often the user's desire. Last but not the least, current semantic segmentation approaches do not generalize to unseen objects. This means that we have to train a model for every possible object in the world, which is obviously impractical.</p><p>In this paper, we present a novel algorithm for interactive object selection ( <ref type="figure">Fig. 1</ref>). To select an object in an image, users provide positive and negative clicks which are then transformed into separate Euclidean distance maps and concatenated with the RGB channels of the image to compose a (image, user interactions) pair. FCN models are fine tuned on many of these pairs generated by random sampling. Moreover, graph cut optimization is combined with the outputs of our FCN models to get satisfactory boundary localization. The key contributions of this paper are summarized as follows:</p><p>• We propose an effective transformation to incorporate user interaction with current deep learning techniques.</p><p>• We propose several sampling strategies which can represent users' click behaviors well and obtain the required training data inexpensively.</p><p>• Our interactive segmentation system is real time given a high-end graphics processing units (GPU).</p><p>The rest of the paper is organized as follows. Section 2 gives a brief review of related works. The proposed algorithm is elaborated in Section 3. Experimental results are presented in Section 4 and finally we conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Interactive segmentation has been studied for many years. There are many interactive approaches, such as contour-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10]</ref> and bounding box methods <ref type="bibr" target="#b18">[19]</ref>. Stroke-based methods are popular, and use a number of underlying algorithms, including normalized cuts <ref type="bibr" target="#b19">[20]</ref>, graph cut <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>, geodesics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>, the combination of graph cut and geodesics <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref> and random walks <ref type="bibr" target="#b7">[8]</ref>. However, all these previous algorithms estimate the foreground/background distributions from low-level features. Unfortunately, low-level features are insufficient at distinguishing the foreground and background in many cases, such as in images with similar foreground and background appearances, complex textures and appearances, and difficult lighting conditions. In such cases, these methods struggle and require excessive user interaction to achieve desirable results. In contrast, our FCN model is trained end-toend and has a high level understanding of objectness and semantics, therefore simplifying user interactions to just a few clicks.</p><p>The task of semantic segmentation is closely related to interactive segmentation. Many algorithms have been proposed in the past <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. Due to the great improvements on image classification and detection by deep neural networks especially the convolutional neural networks (CNNs), many researchers have recently applied CNNs to the problem of semantic segmentation. Farabet et al. <ref type="bibr" target="#b5">[6]</ref> use a multi-scale convolutional network trained from raw pixels for scene labeling. Girshick et al. <ref type="bibr" target="#b6">[7]</ref> apply CNNs to bottom-up regions proposals for object detection and segmentation and improve over previous low-level-featurebased approaches greatly. Long et al. <ref type="bibr" target="#b14">[15]</ref> adapt highcapacity CNNs to FCNs which can be trained end-to-end, pixels-to-pixels and leverage a skip architecture which combines model responses at multiple layers to get finer results. However, as explained in the introduction, semantic segmentation is not directable for interactive segmentation. Our model is based on FCNs but different from <ref type="bibr" target="#b14">[15]</ref> in mainly two points. 1) Our model is trained on randomly generated (image, user interactions) pairs which are the concatenations of RGB channels and transformed Euclidean distance maps. 2) Our model has only two labels -"object" and "background".</p><p>Other work has looked at improving the boundary localization of CNN semantic segmentation approaches. Chen et al. <ref type="bibr" target="#b2">[3]</ref> combine the outputs of FCNs with fully connected CRF. Zheng et al. <ref type="bibr" target="#b25">[26]</ref> formulate mean-field approximate inference as RNNs and train with FCNs end-to-end. They improve the mean intersection over union (IU) accuracy of FCNs from 62.2% to 71.6% and 72% respectively. Although our FCN models are quite general to be combined with their approaches, their segmentation results are far less acceptable for the interactive segmentation task. Therefore, we propose a simple yet effective approach that combine graph cut optimization with our FCN output maps, which enables our algorithm achieve high IU accuracy with even a single click.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed algorithm</head><p>We propose a deep-learning-based algorithm for interactive segmentation. User interactions are first transformed into Euclidean distance maps and then concatenated with images' RGB channels to fine tune FCN models. After the models are trained, graph cut optimization is combined with the probability maps of FCN-8s to get the final segmentation results. <ref type="figure">Figure 1</ref> illustrates the framework of how we train our FCN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transforming user interactions</head><p>In our approach, a user can provide positive and negative clicks (or strokes) sequentially in order to segment objects of interest. A click labels a particular location as being either "object" or "background". A sequence of user inter-FCN <ref type="figure">Figure 1</ref>: The framework of learning our FCN models. Given an input image and user interactions, our algorithm first transforms positive and negative clicks (denoted as green dots and red crosses respectively) into two separate channels, which are then concatenated (denoted as ⊕) with the image's RGB channels to compose an input pair to the FCN models. The corresponding output is the ground truth mask of the selected object.</p><p>actions S includes a positive click set S 1 which contains all user-provided positive clicks and a negative click set S 0 which contains all user-provided negative clicks. Our algorithm uses a Euclidean distance transformation to transform S 1 and S 0 to separate channels U 1 and U 0 respectively. Each channel is a 2D matrix with the same height and width as the original image. To calculate the pixel value u t ij at the location (i, j), t ∈ {0, 1}, let us first define an operator f such that given a set of points p ij ∈ A where (i, j) is the point location, then for any point p mn ,</p><formula xml:id="formula_0">f (p mn |A) = min ∀pij ∈A (m − i) 2 + (n − j) 2 .</formula><p>In other words, the operator f calculates the minimum Euclidean distance between a point and a set of points. Then,</p><formula xml:id="formula_1">u t ij = f (p ij |S t ), t ∈ {0, 1}<label>(1)</label></formula><p>For the efficiency of data storage, we truncate u t ij to 255. It should be noted that it is possible that S 0 is a empty set since in many scenarios our algorithm has perfect segmentation results with even one single positive click. In this case, all u 0 ij are set to 255. Then we concatenate the RGB channels of the image with U 1 , U 0 to compose a (image, user interaction) pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Simulating user interactions</head><p>It should be noted that different users tend to have different interaction sequences for selecting the same object. Therefore our FCN models need a lot of such training pairs to learn this. However, it is too expensive to collect many interaction sequences from real users. We thus use random sampling to automatically generate those pairs. Let O be the set of ground truth pixels of the object and let us define</p><formula xml:id="formula_2">a new set G = {p ij |p ij ∈ O or f (p ij |O) ≥ d}. Let G c denote the complementary set of G.</formula><p>It is easy to see that the pixels in G c have two properties: 1) they are background pixels and 2) they are within a certain distance range to the object. To sample positive clicks, we randomly select n pixels in O where n ∈ [1, N pos ]. The pixels in O are actually filtered in the way that 1) any two pixels are at least d step pixels away from each other and 2) any pixel is at least d margin pixels away from the object boundaries.</p><p>To sample negative clicks, we combine several sampling strategies to model the complexity of users' click patterns.</p><p>• Strategy 1: n negative clicks are randomly sampled in the set G c , where n ∈ [0, N neg1 ]. G c is filtered in the same way as O.</p><p>• Strategy 2: n i negative clicks are randomly sampled on each negative object O i in the same image, where</p><formula xml:id="formula_3">n i ∈ [0, N neg2 ].</formula><p>Each O i is filtered in the same way as O.</p><p>• Strategy 3: N neg3 negative clicks are sampled to cover the outside object boundaries as much as possible. In detail, the first negative click is randomly sampled in G c . Then the following clicks are obtained sequentially by</p><formula xml:id="formula_4">p next = arg max pij ∈G c f (p ij |S 0 ∪ G)<label>(2)</label></formula><p>where S 0 includes all previously sampled negative clicks. <ref type="figure" target="#fig_0">Figure 2</ref> presents an example of the three strategies. The sampled negative clicks from Strategy 1 or 2 alone do not always follow users' typical click patterns, therefore making them harder for our models to learn. The sampled negative clicks from Strategy 3 surround the object evenly, which has a strong pattern but is easy to learn. We find that using all three stategies provides better results than relying on any one strategy, therefore we combine them together. Specifically, for each object in an image we randomly sample N pairs training pairs of (image, user interactions). Each pair is generated by one of the sampling strategies with an equal probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fine tuning FCN models</head><p>We leverage FCNs to learn the interactive segmentation task. The training samples to our models are (image, user interactions) pairs and the labels are the binary masks of corresponding objects. We first fine tune a stride-32 FCN model (FCN-32s) from the stride-32 semantic segmentation model of <ref type="bibr" target="#b14">[15]</ref>. For the two extra channels of filters in the first convolutional layer, we use zero initialization. We also tried initialization with the mean value of those filter weights, but it shows no difference. After fine tuning FCN-32s, we continue to fine tune a stride-16 FCN (FCN-16s) from FCN-32s with the same training data. Finally we fine tune a stride-8 FCN (FCN-8s) model from FCN-16s. As suggested by <ref type="bibr" target="#b14">[15]</ref>, training finer-stride FCNs does not provide further benefits, which we also observed.</p><p>It takes approximately three days to fine tune FCN-32s and five days to fine tune FCN-16s and FCN-8s. By balancing the trade-offs between the performance and time, each FCN model is trained about 20 epochs. FCN-32s converges fast in the first two epochs while a longer training time gives finer segmentation results. We also find that FCN-16s has obvious improvements over FCN-32s especially in regions close to object boundaries, but the accuracy of FCN-16s and FCN-8s are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Graph cut optimization</head><p>From the outputs at the last layer of FCN-8s we can obtain a probability map Q, of which the entry q ij indicates how likely the pixel p ij is labeled as "object" (e.g. <ref type="figure" target="#fig_1">Figure  3b</ref>). Directly thresholding q ij at 0.5 gives us very coarse segmentation masks, which are not useful for interactive segmentation. Instead, we integrate Q into the graph cut optimization <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_5">E(L) = λ · R(L) + B(L)<label>(3)</label></formula><p>Where λ is a coefficient that specifies a relative importance between R(L) and B(L).</p><p>The first term R(L) = pij ∈P R pij (L pij ), where R pij (L pij ) estimates the penalty of assigning pixel p ij to label L pij . Our algorithm defines</p><formula xml:id="formula_6">R pij (L pij ) = − log(q ij ), if L pij = "object" − log(1 − q ij ), otherwise<label>(4)</label></formula><p>The second term B(L) = {pij ,pmn}∈N B {pij ,pmn} · δ(L pij , L pmn ), where B {pij ,pmn} comprises the properties of object boundaries. Our algorithm defines</p><formula xml:id="formula_7">B {pij ,pmn} ∝ exp(− (I pij − I pmn ) 2 2σ 2 )· 1 dist(p ij , p mn )<label>(5)</label></formula><p>Our algorithm solves Equation 3 via max-flow/min-cut energy minimization. <ref type="figure" target="#fig_1">Figure 3c</ref> illustrates the result after graph cut optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Evaluation and complexity</head><p>A user can provide positive and negative clicks sequentially to select objects of interest. Each time a new click is added, our algorithm recomputes the two distance maps U 1 and U 0 . Then the new (image, user interactions) pair is sent to our FCN-8s model and a new probability map Q is obtained. Graph cut uses Q to update the segmentation results without recomputing everything from scratch. To compare our algorithm with other approaches, we also design a method to automatically add a click given the current segmentation mask and the ground truth mask. The method places a seed at the mislabeled pixel that is farthest from the boundary of the current selection and the image boundaries, mimicing a user's behavior under the assumption that the user clicks in the middle of the region of greatest error.</p><p>Given high-end GPUs like NVIDIA Titan X, the computation of Q is very fast and less than 100 millisecond. Graph cut optimization is also very efficient on modern CPUs. Therefore our algorithm satisfies the speed requirement for the interactive segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>We fine tune our FCN models on the PASCAL VOC 2012 segmentation dataset <ref type="bibr" target="#b4">[5]</ref> which has 20 distinct object categories. We use its 1464 training images which have instance-level segmentation masks and their flipped versions to sample the (image, user interactions) pairs. The choices of some sampling hyper-parameters are: d is set to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(e) MS COCO unseen categories</head><p>Graph cut <ref type="bibr" target="#b1">[2]</ref> Geodesic matting <ref type="bibr" target="#b0">[1]</ref> Random walker <ref type="bibr" target="#b7">[8]</ref> Euclidean star convexity <ref type="bibr" target="#b8">[9]</ref> Geodesic star convexity <ref type="bibr" target="#b8">[9]</ref> Growcut <ref type="bibr" target="#b22">[23]</ref> Ours be 40, N pos is set to be 5, N neg1 , N neg2 , N neg3 are set to be 10, 5 and 10 respectively. N pairs is set to be 15. The total number of sampled training pairs is about 80k. 200 validation images are randomly sampled from the whole training set to control the learning of our models. We compare our algorithm to several popular interactive segmentation algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>. Since the other algorithms cannot estimate foreground/background distributions with a single click, we enlarge every click to a big dot with a radius 5 for them. We use such big dots for our graph cut refinement but only use single clicks for our FCN models. To evaluate, we record the updated IU accuracy of an object given sequential clicks which are automatically generated in the way described in Section 3.5. The maximum number of clicks on a single object is limited to 20. We also record how many clicks are required to achieve a certain IU accuracy for the object. If the IU accuracy cannot be achieved in 20 clicks, we will threshold it by 20. Finally, we average each metric over all objects in a dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We evaluate all the algorithms on four public datasets: Pascal VOC 2012 segmentation validation set, Grabcut <ref type="bibr" target="#b18">[19]</ref>, Berkeley <ref type="bibr" target="#b15">[16]</ref> and MS COCO <ref type="bibr" target="#b12">[13]</ref>. The quantitative results of the two metrics on different datasets are shown in <ref type="figure" target="#fig_3">Figure 4</ref> and <ref type="table" target="#tab_0">Table 1</ref> respectively.</p><p>Pascal: The validation set has 1449 images and many of them contain multiple objects. From <ref type="figure" target="#fig_3">Figure 4a</ref> we can see that our algorithm is better than all the other algorithms. Since the validation set contains 20 object categories which have been seen in our training set, we test our algorithm on other datasets with different objects to prove the generalization capability of our algorithm to unseen object classes.</p><p>Grabcut and Berkeley: These two datasets are benchmark datasets for interactive segmentation algorithms. On the Grabcut dataset <ref type="figure" target="#fig_3">(Figure 4b</ref>), our algorithm achieves better results with a few clicks and has a similar IU accuracy with Geodesic/Euclidean star convexity <ref type="bibr" target="#b8">[9]</ref> with more clicks. Since Grabcut only has 50 images and most images have distinct foreground and background distributions which can be handled well by low-level-feature-based algorithms, our advantage over other methods is smaller than it is on more challenging datasets. On the Berkeley dataset <ref type="figure" target="#fig_3">(Figure 4c</ref>), our algorithm achieves better IU accuracy at every step and increases the IU accuracy much faster than the others at the beginning of the interactive selection.  MS COCO: MS COCO is a large-scale segmentation dataset and has 80 object categories. 60 of them are distinct from the Pascal dataset. We randomly sample 10 images per categories and test all the algorithms on the 20 seen categories and 60 unseen categories separately. Our algorithm still consistently performs better than the other algorithms by a large margin in both cases.</p><p>Our algorithm also requires the least number of clicks to achieve a certain IU accuracy on all the datasets. <ref type="figure" target="#fig_3">Figure 4</ref> and <ref type="table" target="#tab_0">Table 1</ref> clearly demonstrate that 1) our algorithm achieves more accurate results with less interaction than other methods and 2) our algorithm has a good generaliza-tion ability to all kinds of objects. Given the same user interaction sequences, some segmentation results by different algorithms are illustrated in <ref type="figure" target="#fig_4">Figure 5 and 9</ref>. In many examples, our algorithm obtains very good results in just one single click while the others either only segment a part of the object or completely fail. This is because our FCN models have a high-level understanding of the objectness and semantics, in contrary to the other approaches simply relying on low-level features. We also show a failed segmentation result by our algorithm in <ref type="figure">Figure 9</ref>. The failure is because FCNs cannot capture thin structures and fine details very well. Therefore the output probabilities from our FCN-8s FCN CRF-RNN Ours Ground truth <ref type="figure">Figure 6</ref>: The segmentation results by the semantic segmentation algorithms (FCN and CRF-RNN) and our algorithm. The first row is a testing image from seen categories (i.e. "person"). The second row is a testing image from unseen categories (i.e. "banana").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation models MS COCO seen categories</head><p>MS COCO unseen categories FCN <ref type="bibr" target="#b14">[15]</ref> 42.37% 16.14% CRF RNN <ref type="bibr" target="#b25">[26]</ref> 47.01% 13.28% Ours 48.35% 42.94% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons to semantic segmentation approaches</head><p>Since all existing interactive segmentation algorithms are only based on low-level features, we should also compare our algorithm to some models that understand high-level semantics, such as FCNs <ref type="bibr" target="#b14">[15]</ref> and CRF-RNN <ref type="bibr" target="#b25">[26]</ref>. However, they neither support instance-level segmentation nor can respond to users' interactions. To make them comparable, we design a simple strategy such that the connected component of a given label that contains the user click is selected as foreground and the other areas are treated as background. It is not straightforward how to respond to negative clicks, therefore we only compare results by a single positive click.</p><p>The visual comparison results are shown in <ref type="figure">Figure 6</ref>. In the first example, since "person" is a known category to FCN and CRF-RNN, they are able to segment all the persons in the image. But they cannot segment the man in the middle who overlaps with other persons. In the second example, "banana" is a new category to FCN and CRF-RNN. Therefore they don't recognize it at all. <ref type="table" target="#tab_1">Table 2</ref> presents the mean IU accuracy with a single positive click on the MS COCO dataset, which demonstrates the limitations of <ref type="figure">Figure 7</ref>: The segmentation results of clothing parts by our algorithm on the Fashionista dataset. The clothing parts from the left image to the right image are "shirt", "skirt" and "jacket". semantic segmentation approaches directly applied to interactive segmentation. For seen categories, since many of the class instances are non-overlapping, we only have a modest improvement. However, remember that our algorithm was given only one click, and with more clicks we can greatly improve our results. For unseen classes, our algorithm performs significantly better, proving both our ability to generalize to new classes and the effectiveness of our algorithm in combining user interactions with deep learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Segmenting object parts</head><p>Previous results demonstrate that our algorithm performs very well on general objects. Moreover, although our FCN models are only trained on whole objects, our algorithm can still select their subparts. In <ref type="figure">Figure 7</ref> we show some segmentation results of clothing parts on the Fashionista dataset <ref type="bibr" target="#b23">[24]</ref>. This demonstrates the flexibility of our algorithm and the effectiveness of our learning framework that enables our models to understand users' intentions well. In addition, compared with the other interactive segmentation approaches, there is no doubt that they need many user interactions to achieve the results. Compared with automatic semantic segmentation methods like FCNs, they are trained to segment entire people and thus cannot get the subparts. This again shows the advantages of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Refinement by Graph Cut</head><p>We illustrate the differences of segmentation results before and after our graph cut refinement in <ref type="figure" target="#fig_5">Figure 8</ref>. The first row shows the output probability maps of our FCN-8s model thresholded at 0.5. We can see our model responds correctly to the user interactions and selects most parts of the bus. But the results along object boundaries are not very accurate. Therefore our algorithm leverages graph cut to refine the results. The second row shows the final results of our algorithm. Clearly the results are more satisfactory and have better boundary localization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The proposed algorithm is the first work that solves the interactive segmentation problem by combining user interactions with current deep learning models. Our algorithm transforms user interactions to Euclidean distance maps and trains FCN models to recognize "object" and "background" based on many synthesized training samples. Our algorithm also combines graph cut optimization with the output of the FCN-8s model to refine the segmentation results along object boundaries. Experimental results clearly demonstrate the superiority of the proposed deep algorithm over existing interactive methods using hand designed, low level features. Our method can achieve high quality segmentations with a small amount of user effort, often just a few clicks. <ref type="table">Geodesic  matting  Random  walker  Euclidean  star convexity  Geodesic  star convexity  Growcut  Ours  Ground  truth</ref> Seen categories</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph cut</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unseen categories</head><p>Failure case <ref type="figure">Figure 9</ref>: The segmentation results by different algorithms given the same user interaction sequences on the MS COCO dataset. The first to third rows are testing images from seen categories (i.e. "cow", "dog", "motorcycle"). The forth to sixth rows are testing images from unseen categories (i.e. "elephant", "apple", "teddy bear"). The last row is a failure example (i.e. "bicycle") by our algorithm. Each of the first seven columns represent the segmentation results by one algorithm and the rightmost column shows the ground truths. In each figure, green dots indicate positive clicks and red crosses indicate negative clicks. Background regions are faded to black and object boundaries are outlined in cyan.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) Strategy 1 (b) Strategy 2 (c) Strategy 3 A visual example of the three sampling strategies for negative clicks. The person is the foreground object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>An example of Section 3.4. (a) An testing image and user interactions. (b) The output probability map from FCN-8s. (c) The result after graph cut.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The mean IU accuracy vs. the number of clicks on the (a) Pascal (b) Grabcut (c) Berkeley (d) MS COCO seen categories and (e) MS COCO unseen categories datasets. The legend of these plots is shown in (f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The segmentation results by different algorithms given the same user interaction sequences. Each row is an testing image from one dataset. Each of the first seven columns represent the segmentation results by one algorithm and the rightmost column shows the ground truths. In each figure, green dots indicate positive clicks. Background regions are faded to black and object boundaries are outlined in cyan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>The sequential segmentation results before and after our graph cut refinement. The first row shows the results by thresholding the output probability maps of our FCN-8s model without using graph cut. The second row shows our final results after graph cut.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The mean number of clicks required to achieve a certain IU accuracy on different datasets by various algorithms. The IU accuracy for different datasets is indicated in the parentheses. The best results are emphasized in bold.</figDesc><table><row><cell cols="2">Segmentation models</cell><cell cols="2">Pascal (85% IU)</cell><cell cols="2">Grabcut (90% IU)</cell><cell>Berkeley (90% IU)</cell><cell>MS COCO seen categories (85% IU)</cell><cell>MS COCO unseen categories (85% IU)</cell></row><row><cell cols="3">Graph cut [2] Geodesic matting [1] Random walker [8] Euclidean start convexity [9] Geodesic start convexity [9] Growcut [23] Ours</cell><cell>15.06 14.75 11.37 11.79 11.73 14.56 6.88</cell><cell>11.10 12.44 12.30 8.52 8.38 16.74 6.04</cell><cell></cell><cell>14.33 15.96 14.02 12.11 12.57 18.25 8.65</cell><cell>18.67 17.32 13.91 13.90 14.37 17.40 8.31</cell><cell>17.80 14.86 11.53 11.63 12.45 17.34 7.82</cell></row><row><cell>Graph cut</cell><cell>Geodesic matting</cell><cell>Random walker</cell><cell cols="2">Euclidean star convexity</cell><cell cols="2">Geodesic star convexity</cell><cell>Growcut</cell><cell>Ours</cell><cell>Ground truth</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Grabcut example</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Berkeley example</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Pascal example</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The mean IU accuracy with a single positive click on the MS COCO seen and unseen categories. The best results are emphasized in bold. model are not accurate enough in those areas, which affects the performance of graph cut in producing our final result.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geodesic matting: A framework for fast interactive image and video segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="132" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Eighth IEEE International Conference on</title>
		<meeting>Eighth IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geos: Geodesic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2008</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="99" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geodesic star convexity for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lazy snapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="308" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01013</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02634</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A comparative evaluation of interactive segmentation algorithms. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Connor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="434" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intelligent scissors for image composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 22nd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geodesic graph cut for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3161" to="3168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finding things: Image parsing with regions and per-exemplar detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3001" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Growcut: Interactive multi-label nd image segmentation by cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Konouchine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of Graphicon</title>
		<meeting>of Graphicon</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3570" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Context driven scene parsing with attention to rare classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3294" to="3301" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03240</idno>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
