<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Monocular Depth and Ego-motion Learning with Structure and Semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
							<email>casser@google.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Robotics at Google</orgName>
								<orgName type="department" key="dep2">Google Brain</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
							<email>pirk@google.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Robotics at Google</orgName>
								<orgName type="department" key="dep2">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
							<email>rezama@google.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Robotics at Google</orgName>
								<orgName type="department" key="dep2">Google Brain</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
							<email>anelia@google.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Robotics at Google</orgName>
								<orgName type="department" key="dep2">Google Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Monocular Depth and Ego-motion Learning with Structure and Semantics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach which takes advantage of both structure and semantics for unsupervised monocular learning of depth and ego-motion. More specifically, we model the motion of individual objects and learn their 3D motion vector jointly with depth and egomotion. We obtain more accurate results, especially for challenging dynamic scenes not addressed by previous approaches. This is an extended version of Casser et al. <ref type="bibr" target="#b0">[1]</ref>. Code and models have been open sourced at: https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Predicting scene depth and agent ego-motion from input imagery is important for robot navigation, both for indoors and outdoors settings. While supervised dense depth prediction has been successful <ref type="bibr" target="#b2">[3]</ref>, we here consider joint learning of depth and ego-motion from monocular input videos only. Unsupervised monocular or stereo-based learning has also shown progress recently <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b7">7]</ref>, but prior work has not been successful at dynamic scenes.</p><p>We present an approach that explicitly models 3D motions of moving objects, together with camera ego-motion and scene depth, and adapts to new environments by learning with an online refinement of multiple frames <ref type="figure">(Figure 1</ref>). With the newly introduced motion handling and the proposed object size constraint, this approach is the first to effectively learn from highly dynamic scenes in a monocular setting. Our approach introduces both structure and semantics in the learning process by representing objects in 3D and modeling motion as SE3 transforms; this is trained from uncalibrated monocular videos in a fully differentiable manner. We further introduce an online refinement method * Work done while at Google Brain. <ref type="figure">Figure 1</ref>: Our method utilizes 3D geometry structure and semantics during learning by modeling motions of individual objects, ego-motion and scene depth in a principled way. Furthermore, a refinement approach adapts the model on the fly in an online fashion.</p><p>for domain transfer in this unsupervised learning setting, which can be applied independently of the base method. This work is an extended version of <ref type="bibr" target="#b0">[1]</ref>. We here present additionally new results on the challenging Cityscapes dataset with prevalent dynamic scenes and on ego-motion. Our algorithm yields significant improvements on two publicly available datasets and on both depth and ego-motion estimation, compared to the state-of-the-art, especially on dynamic scenes. Furthermore, we evaluate direct domain transfer, by training on one dataset and testing on another, without fine-tuning.</p><p>Setup: The main learning setup is unsupervised learning of depth and ego-motion from monocular video <ref type="bibr" target="#b22">[22]</ref>, where the only source of supervision is obtained from the video itself. No depth sensor supervision is used. Objects' masks are introduced from an off-the-shelf algorithm during training only. During inference, only a still input image is needed to predict depth, and two images to predict egomotion. Runtime: our model runs at 50 FPS and 30 FPS on a NVIDIA GeForce 1080Ti for batch 4 and 1, respectively. <ref type="figure">Figure 2</ref>: Schematic of the warping sequence for our method: first object masks are used to remove regions with movement; then object ego-motion is computed; after that individual object motion is computed, but this is done using the output of the image warped according to ego-motion. The final warped images both previous and next (with a validity mask) are compared in RGB space to the original image.</p><p>formation provided by a sensor, such as a LiDAR, is used as supervision. In parallel to supervised learning techniques, unsupervised image-to-depth learning has been proposed <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b13">13]</ref>, where the only supervision is obtained from a monocular video. The work of Garg et al. <ref type="bibr" target="#b3">[4]</ref> introduced joint learning of depth and ego-motion in a neural based framework. Zhou et al. <ref type="bibr" target="#b22">[22]</ref> proposed the first fully differentiable deep neural network approach for unsupervised learning of depth and ego-motion, and showed it outperforms prior approaches which used depth sensors as supervision. Many subsequent works have further improved the quality of depth and ego-motion <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b9">9]</ref>. Some of these approaches have successfully used stereo pair videos during training <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b5">6]</ref> to also produce a single image-based depth estimation. These methods tend to achieve better quality results, due to the extra camera input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Depth and Ego-motion Learning</head><p>We here present an approach which is able to model dynamic scenes by modeling object motion, and that can optionally adapt its learning strategy with an online refinement technique. Note that both ideas are tangential and can be used either separately or jointly. See <ref type="bibr" target="#b0">[1]</ref> for details. During training, the method operates on sequences of three consecutive RGB images 1 (I 1 , I 2 , I 3 ) ∈ R H×W ×3 , and camera intrinsics matrix K ∈ R 3×3 . Depth is predicted by learn-ing a depth function θ : R H×W ×3 → R H×W , which is a fully convolutional encoder-decoder neural network producing a dense depth map D i = θ(I i ) from a single RGB frame. Ego-motion is predicted by an ego-motion network ψ E : R 2×H×W ×3 → R 6 which takes a sequence of two RGB images as input and produces the SE3 transform between the frames, i.e. 6-dimensional transformation vector E 1→2 = ψ E (I 1 , I 2 ) specifying translation and rotation parameters between the frames.</p><p>Let us suppose that the depth network output is providing an adequate depth of the scene per frame, then using it, we can represent points in 3D space. Further, given the ego-motion between consecutive frames, we can transform the scene and project it to obtain the neighbouring frame. More specifically, by using a differentiable image warping operator φ(I i , D j , E i→j ) →Î i→j , we can inverse-warp any source RGB-image I i into I j given corresponding depth estimate D j and an ego-motion estimate E i→j . Here, φ performs the warping by reading from transformed image pixel coordinates, settingÎ xy</p><formula xml:id="formula_0">i→j = Ixŷ i , where [x,ŷ, 1] T = KE i→j (D xy j · K −1 [x, y, 1] T ).</formula><p>The latter construct succinctly denotes projecting the depth into a 3D point cloud, then transforming it according to E i→j and then projecting the transformed 3D point cloud into image space. In practice, we always warp the outer images towards the center frame within a sequence. The supervisory signal is then established using a photometric loss comparing the projected scene onto the next frameÎ i→j with the actual next frame I j image in RGB space, for example using a reconstruction loss: L rec = min( Î i→j − I j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motion Model</head><p>In order to handle highly dynamic scenes, we model motions of individual objects. Namely, we introduce a third component ψ M to the model, which is specialized to predicting motion of objects in 3D ( <ref type="figure">Figure 1</ref>). It uses the same network structure as the ego-motion network ψ E but trains to separate weights. The object motion model takes an RGB image sequence as input, complemented by precomputed instance segmentation masks. The motion model learns to predict the transformation vectors per each object in 3D-space, which create the observed object appearance in the respective target frame when applied to the camera. In the new model, the final warping result is a combination of the individual warping from moving objects, and the ego-motion. <ref type="figure">(Figure 2</ref>). In order to compute ego-motion, object motions are masked out of the images first, i.e. the static scene binary mask is applied to all images in the sequence by element-wise multiplication, before feeding the sequence to the ego-motion model. The static background is generated by a single warp based on ψ E , whereas all segmented objects are then added by their appearance being warped first according to ψ E and then ψ M . Outlines of potentially moving objects are provided by an off-the-shelf algorithm <ref type="bibr" target="#b8">[8]</ref> and are needed only for training (similar to prior work that use optical flow <ref type="bibr" target="#b17">[17]</ref> that is not trained on either of the datasets of interest). Our approach not only models objects in 3D but also learns their motion on the fly, together with scene depth and ego-motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Imposing Object Size Constraints</head><p>Previous work has pointed out a significant limitation for monocular methods <ref type="bibr" target="#b5">[6]</ref> [17] <ref type="bibr" target="#b16">[16]</ref> -that cars moving in front at roughly the same speed often get projected into infinite depth e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">17]</ref>. This is because the object in front shows no apparent motion, and if the network estimates it as being infinitely far away, the reprojection error is almost reduced to zero which is preferred to the correct case. Previously, only methods with stereo images as input were able to solve this problem. Instead, we propose a different approach. Since the main problem stems from the fact that if the model has no knowledge about object scales, it could explain the same object motion by placing an object very far away and predicting large motion, or placing it close and predicting small motion, we here let the model learn the scales of objects as part of the training process. Assuming a weak prior on the height of certain objects, we can get an approximate depth estimation for it given its segmentation mask and the camera intrinsics using D approx (p; h) ≈ f y p h where f y ∈ R is the focal length, p ∈ R our height prior in world units, and h ∈ N the height of the respective segmentation blob in pixels. A loss term on the scale of each object i (i = 1 . . . N ) is added to the main loss. Let t(i) : N → N define a category ID for any object i, and p j be a learnable height prior for each category ID j. Let D be a depth map estimation and S the corresponding object outline mask per object O i ( is the element-wise multiplication). Then the loss is:</p><formula xml:id="formula_1">L sc = N i=1 D O i (S) D − D approx (p t(i) ; h(O i (S))) D</formula><p>We scale by D, which is the mean estimated depth of the middle frame, to reduce a potential issue of trivial loss reduction by jointly shrinking priors and the depth prediction range. To our knowledge this is the first method to address common degenerative cases in a fully monocular training setup in 3D.</p><p>In addition to the above-mentioned losses, the full loss includes the photometric reconstruction loss, the SSIM loss, a depth smoothness loss <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b16">16]</ref>. The loss is also applied on 4 image resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Test Time Refinement Model</head><p>With the above-mentioned model, depth can be predicted from a single, still image during inference. If multi-frames are available during inference too, one may take advantage of that and learn on the fly. More specifically, we propose to further optimize the model weights during inference which allows the model to adapt to the environment online. Thus, the model will be training for a number of steps, while performing inference. In doing that, we also show that even with very limited temporal resolution (i.e., three-frame sequences), the quality of depth predictions, both qualitatively and quantitatively, improves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We conduct experiments on depth estimation, egomotion estimation and on transfer learning to new environments, using common metrics and protocols for evaluation adopted by prior methods. We report results on the two main benchmarks for depth and ego-motion evaluation: the KITTI dataset <ref type="bibr" target="#b4">[5]</ref> and the Cityscapes dataset <ref type="bibr" target="#b1">[2]</ref>.</p><p>Results on the KITTI Dataset. <ref type="figure" target="#fig_0">Figure 3</ref> visualizes the results of our method compared to the ground truth provided by a sensor and <ref type="table">Tables 1 and 2</ref> show quantitative results. Improvement over the baseline and over previous methods in the literature is observed. Our method outperforms competitive models that use motion <ref type="bibr" target="#b17">[17]</ref> and <ref type="bibr" target="#b20">[20]</ref>. Furthermore, our results which are trained in monocular setup, are close to methods which use stereo or a combination of stereo and monocular, e.g. <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b5">6]</ref>. More results can be seen in <ref type="bibr" target="#b0">[1]</ref>.</p><p>Experimental Results on the Cityscapes Dataset. In this section we evaluate our method on the Cityscapes dataset, whhich contains many dynamic scenes. <ref type="table" target="#tab_3">Table 3</ref> shows our experimental results when training on the   <ref type="table">Table 1</ref>: Evaluation of depth estimation of our method compared to the state-of-the-art. Separate results of the motion model (M), the online refinement one (R), and both (M+R) are presented. For the purple columns, lower is better, for the yellow ones higher is better. KITTI dataset.</p><p>Cityscapes data, and then evaluating on KITTI (without fine-tuning). We also show evaluation on the Cityscapes dataset itself, which contains many moving objects. These experiments clearly demonstrate the benefit of our method for dynamic scenes as we see significant improvements in depth estimation. We observe that the improvements are due to the appropriate depth learning of many moving objects ( <ref type="figure" target="#fig_1">Figure 4</ref>) enabled by the motion model. We further note that these are new results and training and testing on Cityscapes is not customarily done, as seen in the table, since the dataset is very challenging. Motion Model. We here further examine the effects of the motion model. <ref type="figure" target="#fig_1">Figure 4</ref> shows several examples of dynamic scenes from the Cityscapes dataset, which contain many moving objects. We note that our baseline, which is by itself a top performer on KITTI, is failing on moving objects. Our method makes a notable difference both qualitatively ( <ref type="figure" target="#fig_1">Figure 4</ref>) and quantitatively (see <ref type="table" target="#tab_3">Table 3</ref>). <ref type="figure">Figure 5</ref> further compares our results with previous monocular methods in the case of a moving vehicle in front of the egomotion vehicle. As seen our approach is the only one that can extract its depth. Another benefit provided by the motion model is that it learns to predict motions of individual objects in 3D, which can be available for inference if an object mask is specified <ref type="bibr" target="#b0">[1]</ref>. In the general case, object masks are not needed for depth or ego-motion inference.</p><p>Refinement Model. <ref type="figure">Figure 6</ref> shows results of the refinement method only. We can see improvements of the refinement model on both KITTI and Cityscapes datasets for a model trained on KITTI. As seen for both evaluating on KITTI or Cityscapes dataset the refinement is helpful in recovering the geometry structure better. Of note that in the case of Cityscape (left), this is testing across datasets. <ref type="figure" target="#fig_5">Figure 8</ref> further shows improvements per frame by the online refinement model. As seen, most frames benefit from refinement and improve their depth estimation. More online refinement results, demonstrated on an indoor dataset, collected by the Fetch robot are shown in <ref type="bibr" target="#b0">[1]</ref>.</p><p>Visual Odometry Results. The ego-motion results are shown in <ref type="table" target="#tab_4">Table 4</ref>. The experiments are conducted by a standard protocol adopted by prior work <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b5">6]</ref>   <ref type="table">Table 2</ref>: Evaluation of depth estimation of our method compared on the KITTI dataset, for 50 meters range cap. Methods marked with an asterik (*) use depth computed from disparities as ground truth, and are trained on stereo images. Results marked with † were computed by us using predictions that the authors provided. of-the-art methods, even the ones that use more temporal information. Handling of motion is the biggest contributing factor to improving the ego-motion estimation of our algorithm. <ref type="figure" target="#fig_3">Figure 7</ref> shows results on the KITTI sequence.</p><p>Experiments Discussion. As shown previously (in Tables 1, 2, and 3) our method benefits from both motion and online refinement, but each component works in different extents. For example, the motion net although extremely beneficial for Cityscapes, a dataset with many dynamic scenes, affects the metrics to moderate amounts in KITTI, which reflects the scarcity of motion in this dataset. Online refinement, on the other hand, is generally useful, but some confusion may arise when applying it solo on a data with a lot of motion, e.g., Cityscapes. When both online refinement and motion are applied we have much better results than the baselines. Notes on the Evaluation Procedure and Revised Results. As pointed out by Godard et al. <ref type="bibr" target="#b5">[6]</ref>, the evaluation code released by <ref type="bibr" target="#b22">[22]</ref> contains an inaccuracy where the depth ground truth on the KITTI dataset is computed with respect to the camera instead of the LIDAR. Fortunately, in practice, the effects of this are rather subtle as the displacement is not very large. We know that at least the results of Zhou et al. <ref type="bibr" target="#b22">[22]</ref>, Mahjourian et al. <ref type="bibr" target="#b13">[13]</ref>, Mon-oDepth <ref type="bibr" target="#b7">[7]</ref>, Pilzer et al. <ref type="bibr" target="#b14">[14]</ref> and GeoNet <ref type="bibr" target="#b20">[20]</ref> are affected, as they adopted the same evaluation code. To be able to better compare to these methods, all numbers reported in the main paper use the old evaluation code for both caps at 50m and 80m. We show results using the revised evaluation code in <ref type="table">Table 5</ref>. For all methods where we have raw predictions available, we recompute their scores and also include them in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper addresses the monocular depth and egomotion problem by modeling individual objects' motion in Input Godard et al. <ref type="bibr" target="#b5">[6]</ref> Mono GeoNet <ref type="bibr" target="#b20">[20]</ref> DDVO <ref type="bibr" target="#b16">[16]</ref> Baseline Ours (M) <ref type="figure">Figure 5</ref>: Example showing a common failure case for monocular methods, which is handled correctly by our motion model due to imposed size constraints. KITTI dataset.   3D, and an online refinement algorithm which is beneficial for transfering learned models to new environments. The algorithm allows application to videos with dynamic scenes and motion. Results on two major and challenging benchmarks datasets, KITTI and Cityscapes, for depth and egomotion prediction are presented. We also showed successful transfer across datasets.   <ref type="table">Table 5</ref>: Evaluation of depth estimation of our method using the revised evaluation code on KITTI, testing individual contributions of motion and online refinement components. We re-evaluate related methods if predictions are available, as marked with †. As before, our method outperforms every competing one.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Example results of depth estimation. Each column shows an input image, depth prediction of our method, and ground truth depth. KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Examples of depth estimation with the motion model (M) on highly dynamic scenes. Cityscapes dataset; from left to right: image input, baseline, ours, ground truth. A common failure case for dynamic scenes in monocular methods are objects moving with the camera itself. These objects are projected into infinite depth in prior work. Our method correctly estimates depth notably here, particularly on moving vehicles and persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 9</head><label>9</label><figDesc>shows several typical examples of failures for depth estimation for the motion-only model. These can generally happen in scenes which are considerably different than scenes seen during training, for example scenes in which the average depth is very low over the full image, or images with new objects, such as a tank truck or a bridge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Ego-motion results visualized as vehicle speed (in non-metric network units) and turn indicator at the bottom: driving forward (left), slowing down and taking a turn (middle), stopping for a red light (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Relative improvements achieved by the online refinement model per frame. As seen, most frames benefit from online refinement. The red horizontal line marks the mean improvement. KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>on the KITTI odometry dataset. As seen our algorithm outperforms state-Method Supervision? Motion? Cap Abs Rel Sq Rel RMSE RMSE log δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 Garg et al.</figDesc><table><row><cell>[4]*</cell><cell>-</cell><cell>-</cell><cell>50m</cell><cell>0.169</cell><cell>1.08</cell><cell>5.104</cell><cell>0.273</cell><cell>0.740</cell><cell>0.904</cell><cell>0.962</cell></row><row><cell>Mahjourian et al. [13]</cell><cell>-</cell><cell>-</cell><cell>50m</cell><cell>0.155</cell><cell>0.927</cell><cell>4.549</cell><cell>0.231</cell><cell>0.781</cell><cell>0.931</cell><cell>0.975</cell></row><row><cell>GeoNet [20]</cell><cell>-</cell><cell>-</cell><cell>50m</cell><cell>0.147</cell><cell>0.936</cell><cell>4.348</cell><cell>0.218</cell><cell>0.810</cell><cell>0.941</cell><cell>0.977</cell></row><row><cell>DDVO [16] †</cell><cell>-</cell><cell>-</cell><cell>50m</cell><cell cols="3">0.1436 0.9348 4.2338</cell><cell>0.2144</cell><cell>0.8267</cell><cell>0.9446</cell><cell>0.9774</cell></row><row><cell>Ours (Baseline)</cell><cell>-</cell><cell>-</cell><cell>50m</cell><cell cols="3">0.1343 0.8229 4.1078</cell><cell>0.2038</cell><cell>0.8365</cell><cell>0.9506</cell><cell>0.9802</cell></row><row><cell>Ours (R)</cell><cell>-</cell><cell>-</cell><cell>50m</cell><cell cols="3">0.1141 0.9284 3.8777</cell><cell>0.1897</cell><cell>0.8841</cell><cell>0.9571</cell><cell>0.9792</cell></row><row><cell>Ours (M)</cell><cell>-</cell><cell>Yes</cell><cell>50m</cell><cell cols="3">0.1350 0.7912 4.0573</cell><cell>0.2031</cell><cell>0.8311</cell><cell>0.9527</cell><cell>0.9822</cell></row><row><cell>Ours (M+R)</cell><cell>-</cell><cell>Yes</cell><cell>50m</cell><cell cols="3">0.1030 0.6217 3.5546</cell><cell>0.1749</cell><cell>0.8866</cell><cell>0.9632</cell><cell>0.9846</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Depth prediction results when training on Cityscapes. Evaluation on both KITTI (K) and Cityscapes (C) is shown here; 80m range cap. Methods marked with an asterik (*) might use a different cropping as the exact parameters were not available.</figDesc><table><row><cell cols="3">Figure 6: Online refinement model (R). Cityscapes (left</cell></row><row><cell cols="3">columns), KITTI (right columns). The model is trained on</cell></row><row><cell>KITTI.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Seq. 09</cell><cell>Seq. 10</cell></row><row><cell>Mean Odometry</cell><cell>0.032 ±0.026</cell><cell>0.028 ± 0.023</cell></row><row><cell>ORB-SLAM (short)</cell><cell>0.064 ± 0.141</cell><cell>0.064 ± 0.130</cell></row><row><cell>Vid2Depth (Mahjourian 2018)</cell><cell>0.013 ± 0.010</cell><cell>0.012 ± 0.011</cell></row><row><cell>Godard (Godard 2018) †</cell><cell>0.023 ± 0.013</cell><cell>0.018 ± 0.014</cell></row><row><cell>Zhou (Zhou 2017) †</cell><cell>0.021 ± 0.017</cell><cell>0.020 ± 0.015</cell></row><row><cell>GeoNet (Yin 2018)</cell><cell>0.012 ± 0.007</cell><cell>0.012 ± 0.009</cell></row><row><cell>ORB-SLAM (full)*</cell><cell>0.014 ± 0.008</cell><cell>0.012 ± 0.011</cell></row><row><cell>Ours</cell><cell cols="2">0.011 ± 0.006 0.011 ± 0.010</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Quantitative evaluation of odometry on the KITTI Odometry test sequences. Methods using more information than a set of rolling 3-frames are marked (*). Models that are trained on a different part of the dataset are marked ( †).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Cap Abs Rel Sq Rel RMSE RMSE log δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 Mahjourian et al.</figDesc><table><row><cell>Method</cell><cell cols="3">Supervision? Motion? [13] † --</cell><cell>50m</cell><cell cols="3">0.1563 0.9602 4.4051</cell><cell>0.2320</cell><cell>0.7940</cell><cell>0.9323</cell><cell>0.9732</cell></row><row><cell>GeoNet [20] †</cell><cell></cell><cell>-</cell><cell>-</cell><cell>50m</cell><cell cols="3">0.1512 0.9455 4.3627</cell><cell>0.2224</cell><cell>0.8023</cell><cell>0.9387</cell><cell>0.9759</cell></row><row><cell>DDVO [16] †</cell><cell></cell><cell>-</cell><cell>-</cell><cell>50m</cell><cell cols="3">0.1488 0.9498 4.2551</cell><cell>0.2195</cell><cell>0.8181</cell><cell>0.9423</cell><cell>0.9765</cell></row><row><cell>Ours (Baseline)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>50m</cell><cell cols="3">0.1388 0.8344 4.1306</cell><cell>0.2087</cell><cell>0.8291</cell><cell>0.9480</cell><cell>0.9793</cell></row><row><cell>Ours (M)</cell><cell></cell><cell>-</cell><cell>Yes</cell><cell>50m</cell><cell cols="3">0.1377 0.7918 4.0514</cell><cell>0.2066</cell><cell>0.8260</cell><cell>0.9506</cell><cell>0.9814</cell></row><row><cell>Ours (R)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>50m</cell><cell cols="3">0.1175 0.9398 3.8833</cell><cell>0.1932</cell><cell>0.8799</cell><cell>0.9560</cell><cell>0.9787</cell></row><row><cell>Ours (M+R)</cell><cell></cell><cell>-</cell><cell>Yes</cell><cell>50m</cell><cell cols="3">0.1052 0.6231 3.5591</cell><cell>0.1779</cell><cell>0.8834</cell><cell>0.9620</cell><cell>0.9841</cell></row><row><cell>Train set mean</cell><cell></cell><cell>-</cell><cell>-</cell><cell>80m</cell><cell>0.361</cell><cell>4.826</cell><cell>8.102</cell><cell>0.377</cell><cell>0.638</cell><cell>0.804</cell><cell>0.894</cell></row><row><cell cols="2">Mahjourian et al. [13] †</cell><cell>-</cell><cell>-</cell><cell>80m</cell><cell cols="3">0.1635 1.2467 5.9579</cell><cell>0.2478</cell><cell>0.7752</cell><cell>0.9197</cell><cell>0.9680</cell></row><row><cell>GeoNet [20] †</cell><cell></cell><cell>-</cell><cell>-</cell><cell>80m</cell><cell cols="3">0.1590 1.3032 5.8732</cell><cell>0.2379</cell><cell>0.7853</cell><cell>0.9286</cell><cell>0.9713</cell></row><row><cell>DDVO [16] †</cell><cell></cell><cell>-</cell><cell>-</cell><cell>80m</cell><cell cols="3">0.1562 1.2750 5.6136</cell><cell>0.2336</cell><cell>0.8015</cell><cell>0.9330</cell><cell>0.9724</cell></row><row><cell>Godard et al. [6]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>80m</cell><cell>0.137</cell><cell>1.153</cell><cell>5.353</cell><cell>0.212</cell><cell>0.836</cell><cell>0.947</cell><cell>0.978</cell></row><row><cell>Ours (Baseline)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>80m</cell><cell cols="3">0.1463 1.1506 5.5520</cell><cell>0.2236</cell><cell>0.8127</cell><cell>0.9386</cell><cell>0.9751</cell></row><row><cell>Ours (M)</cell><cell></cell><cell>-</cell><cell>Yes</cell><cell>80m</cell><cell cols="3">0.1439 1.0247 5.2914</cell><cell>0.2189</cell><cell>0.8110</cell><cell>0.9429</cell><cell>0.9782</cell></row><row><cell>Ours (R)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>80m</cell><cell cols="3">0.1265 1.4453 5.3122</cell><cell>0.2078</cell><cell>0.8663</cell><cell>0.9501</cell><cell>0.9759</cell></row><row><cell>Ours (M+R)</cell><cell></cell><cell>-</cell><cell>Yes</cell><cell>80m</cell><cell cols="3">0.1108 0.8254 4.7619</cell><cell>0.1897</cell><cell>0.8704</cell><cell>0.9563</cell><cell>0.9819</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While in theory the whole formulation can be done with two consecutive frames, for convenience, we consider three images in a sequence and impose constraints between two pairs of frames.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We would like to thank Ayzaan Wahid for helping us with data collection, members of the Brain team for discussions, and Chaoyang Wang, Zhenheng Yang, Zhichao Yin and Jianping Shi for their generous sharing of results. We also would like to thank Clément Godard for helping with reproducing some previous results.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dig-Figure 9: Example failure cases of our model can be due to unfamiliar objects, for example, the large tank truck and the bridge in the right image, or side vegetation (in the middle) and fence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>right image</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ging into self-supervised monocular depth estimation</title>
		<idno>arxiv.org/pdf/1806.01260</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sfm-net: Learning of structure and motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00373</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Geometry-based next frame prediction from monocular video. Intelligent Vehicles Symposium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised adversarial depth estimation using cycled generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Every pixel counts: Unsupervised geometry learning with holistic 3d motion understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV Workshop</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lego: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03665</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geonet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
