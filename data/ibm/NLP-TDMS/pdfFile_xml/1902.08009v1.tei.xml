<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dressing as a Whole: Outfit Compatibility Learning Based on Node-wise Graph Neural Networks CCS CONCEPTS Compatibility learning, graph neural networks, multi-modal ACM Reference Format</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>May 13-17, 2019, May 13-17, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
							<email>zeyu.cui@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Li</surname></persName>
							<email>lizekunlee@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Engineering, Chinese Academy of Sciences University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
							<email>shu.wu@nlpr.ia.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation, Chinese Academy of Sciences University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
							<email>zhangxiaoyu@iie.ac.cn</email>
							<affiliation key="aff3">
								<orgName type="department">Institute of Information Engineering, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
							<affiliation key="aff4">
								<orgName type="department">Institute of Automation, Chinese Academy of Sciences University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
							<affiliation key="aff5">
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Li</surname></persName>
							<affiliation key="aff5">
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
							<affiliation key="aff5">
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff5">
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff5">
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dressing as a Whole: Outfit Compatibility Learning Based on Node-wise Graph Neural Networks CCS CONCEPTS Compatibility learning, graph neural networks, multi-modal ACM Reference Format</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">May 13-17, 2019, May 13-17, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3308558.3313444</idno>
					<note>The first two authors Zeyu Cui and Zekun Li contribute to this work equally and are listed as joint first authors. Shu Wu and Xiaoyu Zhang are both corresponding authors. This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW &apos;19, • Applied computing → Online shopping. KEYWORDS. 2019. Dressing as a Whole: Outfit Compatibility Learning Based on Node-wise Graph Neural Networks. In Proceedings of the 2019 World Wide Web Conference (WWW &apos;19),</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rapid development of fashion market, the customers' demands of customers for fashion recommendation are rising. In this paper, we aim to investigate a practical problem of fashion recommendation by answering the question "which item should we select to match with the given fashion items and form a compatible outfit". The key to this problem is to estimate the outfit compatibility. Previous works which focus on the compatibility of two items or represent an outfit as a sequence fail to make full use of the complex relations among items in an outfit. To remedy this, we propose to represent an outfit as a graph. In particular, we construct a Fashion Graph, where each node represents a category and each edge represents interaction between two categories. Accordingly, each outfit can be represented as a subgraph by putting items into their corresponding category nodes. To infer the outfit compatibility from such a graph, we propose Node-wise Graph Neural Networks (NGNN) which can better model node interactions and learn better node representations. In NGNN, the node interaction on each edge is different, which is determined by parameters correlated to the two connected nodes. An attention mechanism is utilized to calculate the outfit compatibility score with learned node representations. NGNN can not only be used to model outfit compatibility from visual or textual modality but also from multiple modalities. We conduct experiments on two tasks: (1) Fill-in-the-blank: suggesting an item that matches with existing components of outfit; (2) Compatibility prediction: predicting the compatibility scores of given outfits. Experimental results demonstrate the great superiority of our proposed method over others.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Clothing plays an increasingly significant role in human's social life, as a proper outfit can enhance one's beauty and display the personality. However, not everyone has a strong fashion sensitivity. "How to make a suitable and beautiful outfit?" has become a daily headache for many people. Solving this problem requires modeling human notions of the compatibility. Here we aim to propose an effective method to predict the compatibility of outfits and thus help people make proper outfits.</p><p>Previous works on compatibility learning mainly focused on the compatibility of two items. These works followed the idea to map the items into a style space and estimate the distance between style vectors of items. Veit et.al used SiameseCNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">38]</ref> to transform items from the image space to the style space. Mcauley et.al <ref type="bibr" target="#b22">[23]</ref> proposed to use Low-rank Mahalanobis Transformation to map items into the style space. Lately, some works proposed to model the compatibility of items by mapping items into several style spaces <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref>. Recently, there appear a few works directly measure the compatibility of outfit. Li et.al <ref type="bibr" target="#b16">[17]</ref> proposed to represent an outfit as a sequence and adopt Recurrent Neural Network (RNN) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref> to model the compatibility. Similarly, another work <ref type="bibr" target="#b8">[9]</ref> further represented an outfit as a bidirectional sequence with a specific order (i.e., top to bottom and then to accessories), and used Bidirectional Long Short Term Memories (Bi-LSTMs) to predict the next item in an outfit. Solving the task of modeling outfit compatibility relies on an appropriate outfit representation. The previous works take advantage of two kinds of outfit representations: pair representation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref> and sequence representation <ref type="bibr" target="#b8">[9]</ref>. We show different outfit representations in <ref type="figure" target="#fig_0">Figure 1</ref>. From (a) we can see that representing the outfit as pairs separately cannot reflect the complex relations among multiple items. In term of sequence representation in (b), there doesn't exist a fixed order of items. More importantly, the relations among items in an outfit are not ordered since an item has relations with not only its prior or rear item in the sequence. To remedy this, we propose a new outfit representation: graph representation. As shown in (c), it can better reflect the dense and complex relations among multiple items in an outfit.</p><p>In particular, we construct a Fashion Graph based on Polyvore dataset, where each node represents a category and each edge represents the interaction between two nodes. If two categories have matching relation (i.e., they appear in the same outfit), there should be interactions between them. We have two directed edges between them in Fashion Graph since the interactions in two directions ought to be different. For example, one may select a pair of socks to match the shoes but not select shoes for socks in particular. A compatible outfit should have a key property: the categories of items in it are not overlapped (e.g., there won't be two pairs of shoes in an outfit). Thus, by putting each item into its corresponding node, an outfit can be represented as a subgraph of Fashion Graph. The task of modeling outfit compatibility can be accordingly converted to a graph inference problem.</p><p>A possible method is using Graph Neural Networks (GNN) to model the node interactions and infer compatibility from the graph. GNN is an approach to applying neural networks to graph-structured data, first proposed by Gori et al. <ref type="bibr" target="#b6">[7]</ref>. It can be applied on most kinds of graphs, including directed, undirected and cyclic graphs. However, it has trouble in propagating information across a long range in a graph. To address this, Li et al. <ref type="bibr" target="#b17">[18]</ref> proposed Gated Graph Neural Network (GGNN) by introducing a Gated Recurrent Units (GRU) for updating. Nevertheless, in GGNN, the nodes interact with others by communicating their state information on the edges in a fixed way, which leads to a limitation that it has difficulty in modeling flexible and complex node interactions.</p><p>To address this limitation, we propose a novel model NGNN to better model node interactions and better infer the compatibility from graph. In NGNN, the communication of state information on each edge is different. Specifically, when two nodes communicate state information on the edge, the state information will first be transformed by a transformation function determined by parameters correlated to the two nodes. In this way, NGNN can model edge-wise interactions with node-wise parameters. Since the parameters are correlated to nodes instead of numerous edges, it can greatly reduce the parameter space. During training process, NGNN only deals with the nodes in the subgraph each time, which reduces the time complexity as well. Moreover, when there are more data to train the model, we only need to update the parameters of these concerned nodes without changing the edges or other nodes, which is effective in real applications. With the learned node representations, we utilize an attention mechanism to calculate a graph-level output to serve as the outfit compatibility score.</p><p>The framework of our proposed method is shown in <ref type="figure">Figure 2</ref>. We first construct a Fashion Graph where an outfit can be represented as a subgraph. Then we use the proposed novel model NGNN to model node interactions and learn the node representations. Finally, we predict the compatibility score by calculating a graph-level output using an attention layer. This proposed method can model outfit compatibility from visual, textual or any single modality with a channel of input features. By using two channels of input to NGNN, it can also be used to jointly model the outfit compatibility from visual and textual modality.</p><p>We conduct experiments on two tasks: (1) Fill-in-the-blank: suggesting an item that matches with existing components of outfit;</p><p>(2) Compatibility prediction: predicting the compatibility scores of given outfits. Experiment results on the Polyvore dataset demonstrate the great superiority of our method over others. The code and dataset of our work have been released 1 .</p><p>Our main contributions can be summarized in threefold:</p><p>• To the best of our knowledge, this is the first attempt to represent an outfit as a graph, which can better capture the complex relations among multiple items in an outfit.  <ref type="figure">Figure 2</ref>: Overview of the proposed method. Based on the dataset, we first construct a Fashion Graph, where each node represents a category and each edge represents the interaction between two nodes. An outfit (e.g. sweater, short, sandals, shoulder bag) can be represented as a subgraph. We then design NGNN to model the node interactions and learn node representations. An attention layer is finally utilized to calculate the compatibility score.</p><p>measured the distance there <ref type="bibr" target="#b22">[23]</ref>. Following that, Veit et al. proposed to learn the distance metric with an end-to-end SiameseCNNs <ref type="bibr" target="#b37">[38]</ref>. Some other works mapped items into several latent spaces and jointly modeled the distances in these latent spaces, which can measure the compatibility of items in different aspects <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref>. However, these works only focus on the compatibility of two items instead of a whole outfit.</p><p>Recently, some studies have been trying to model the compatibility of a whole outfit directly. Li et al. extracted multi-modal information from items and evaluated the outfit compatibility with an Recurrent Neural Network (RNN) <ref type="bibr" target="#b16">[17]</ref>. Then they constructed an outfit recommendation by selecting the items which achieve the highest compatibility scores along with the given items. Han et al. represented an outfit as a sequence with a specific order. Then they used bidirectional LSTMs to predict the next item given a set of items and also the compatibility scores of outfits <ref type="bibr" target="#b8">[9]</ref>. Modeling outfit compatibility relies on a proper representation of the outfit, which can reflect the complex relations among multiple items. In this work, we represent an outfit as a graph, which is proved to be more effective than pairs and sequence in the later experiments.</p><p>Categorical information plays a dominant role in the representation of an item. In the above conventional methods modeling items in a common visual latent space, the items of same categories tends to be close, which makes it harder to reflect the style information. For example, in the common visual latent space, the similarity between suit pants and leather shoes is much smaller than the similarity between suit pants and jeans. In Sherlock <ref type="bibr" target="#b9">[10]</ref>, the embedding matrices for transferring visual features to style features vary among different categories. Liu et al. proposed that items consist of two components: style and category <ref type="bibr" target="#b19">[20]</ref>. In this work, we utilize the categorical information directly by putting items into its corresponding category nodes in the Fashion Graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>There have been various ways to deal with graph-structured data. Some works convert graph-structured data into sequence-structured data to deal with. Inspired by word2vec <ref type="bibr" target="#b25">[26]</ref>, Perozzi et al. proposed an unsupervised DeepWalk algorithm to learn node embedding in graph based on random walks <ref type="bibr" target="#b26">[27]</ref>. After that, Tang et al. proposed a network embedding algorithm LINE <ref type="bibr" target="#b35">[36]</ref>, followed by node2vec <ref type="bibr" target="#b7">[8]</ref>. Some works apply neural networks on the graph-structured data directly. Duvenaud et al. designed a special hashing function so that Convolutional Neural Network (CNN) can be used on the graphs <ref type="bibr" target="#b4">[5]</ref>. Kipf et al. proposed a semi-supervised method on graphs based on an efficient variant of CNN <ref type="bibr" target="#b13">[14]</ref>.</p><p>Gori et al. first proposed Graph Neural Networks (GNN) <ref type="bibr" target="#b6">[7]</ref>, another approach to applying neural networks directly to graphstructured data. GNN can be applied on most kinds of graphs, including directed, undirected and cyclic graphs. Feed-forward neural networks are applied to every node in the graph recurrently. The nodes interact with others by communicating their state information. Thus, the hidden state of nodes will update dynamically. Scarselli et al. utilized multi-layer perceptions (MLP) to update the hidden state of nodes <ref type="bibr" target="#b28">[29]</ref>. However, there exists problem in propagating information across a long range in a graph. To remedy this, Li et al. proposed Gated Graph Neural Network (GGNN) by introducing a Gated Recurrent Units (GRU) for updating <ref type="bibr" target="#b17">[18]</ref>. GGNN has been utilized for various tasks, such as image classification <ref type="bibr" target="#b21">[22]</ref>, situation recognition <ref type="bibr" target="#b15">[16]</ref>, recommendation <ref type="bibr" target="#b38">[39]</ref> and script event prediction <ref type="bibr" target="#b18">[19]</ref>. Nevertheless, it has difficulty in modeling complex node interactions since the communication of state information on the edges are in a fixed way. To address this limitation, we propose a novel model NGNN, which can better model node interactions so as to better infer outfit compatibility from the graph. In NGNN, the communication of state information on each edge is different. It can model edge-wise interactions with node-wise parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET AND FEATURES</head><p>In this section, we introduce the dataset we use and how we extract the visual and textual features of items, which are used as input of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Polyvore Dataset</head><p>Polyvore dataset released by <ref type="bibr" target="#b8">[9]</ref> was collected from a popular fashion website Polyvore.com 2 , where fashion stylists can share the outfits they created to the public. As shown in <ref type="figure">Figure 3</ref>, items in the outfits have rich information including clear images, category information, titles, numbers of likes, etc. We only use the images, titles and category information of items in this paper. The dataset has been utilized by several works on fashion analysis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Irregular lapel blue trench coat</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beautiful cotton lace skirt yellow</head><p>Mcq alexander mcqueen mirrored leather sandals</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Navy blue yellow statement bib necklace earrings set Necklaces</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sandals</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knee Length Skirts</head><p>Coats Category Title Image <ref type="figure">Figure 3</ref>: Examples of outfit composition on Polyvore.com. Each item has a clear image, titles and category information.</p><p>There are totally 21889 outfits covering 380 categories in the Polyvore dataset, which was split into three non-overlapping sets, 17316 for training, 1497 for validation and 3076 for testing. Following <ref type="bibr" target="#b31">[32]</ref>, there are many categories appearing too few times in the dataset such as Toys, Furniture and Cleaning, which are not actually fashion categories. To ensure the training quality, we keep categories which appear more than 100 times in the dataset and 120 categories remain. To ensure the integrity of outfits, we filtered out the outfits consisting of less than 3 items. Finally, there remain 16983 outfits in the training set, 1497 in the validation set and 2697 in the test set, with totally 126054 items covering 120 categories. The maximum size of outfits is 8 and the average size is 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features</head><p>We extract the visual features using the images of items and textual features using titles. We will model the outfit compatibility using these features of items as described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Visual</head><p>Feature. We utilize the advanced deep convolutional neural network GoogleNet InceptionV3 model <ref type="bibr" target="#b33">[34]</ref> to extract the visual features, which has been proved to be effective in image representation learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref>. In particular, we feed the images of items into the model and adopted the linear layer output as the visual feature. The visual feature of each item is a 2048dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Textual</head><p>Feature. Following <ref type="bibr" target="#b31">[32]</ref>, we first construct a vocabulary based on the words of the titles in the dataset. Due to the noise of these user-generated titles, we first filter out the words which appear in less than 5 items. Since there are many meaningless words such as 'a', 'an' and 'de', we then filter out words with less than 3 characters. Finally we obtain a vocabulary with 2757 words and therefore we represented the textual feature of each item as a 2757-dimensional Boolean vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED METHOD</head><p>We first formulate the problem. Then we introduce how to construct the Fashion Graph and model outfit compatibility from the graph with NGNN. We also introduce how to jointly model outfit  <ref type="figure">Figure 4</ref>: Illustration of the most popular categories in Polyvore dataset. Each circle denotes a category, and each link denotes the matching relation between two categories (i.e., they have appeared in the same outfit). The areas of circles and the widths of the links are proportional to the number of fashion items in the corresponding categories and the co-occurrence frequency between categories.</p><p>compatibility from multiple modalities with NGNN. Finally, we describe the training strategy and analyze our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><p>In this paper, we aim to model the compatibility of outfits. We have a set of outfits S = {s 1 , s 2 , s 3 , ...} in the training set. The total item set is V. Given an outfit s consisting of |s | items (each item has an image and textual description), we aim to predict the compatibility score x s of the outfit. The score can be used for various tasks of fashion recommendation. We introduce some notations which will be used in the following. For each item v i ∈ V, its feature is f i and the representation of f i in the latent space is r i . The category of v i is c i . The corresponding node of c i in the Fashion Graph is n i , while the state of node n i in NGNN is h i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Construction</head><p>In <ref type="figure">Figure 4</ref>, we illustrate the most popular categories in the Polyvore dataset. Each circle denotes a category, and each link denotes the matching relation between two categories (i.e., the two categories have appeared in the same outfit). The areas of circles and the widths of the links are proportional to the number of fashion items in the corresponding categories and the co-occurrence frequency between categories.</p><p>Similar to <ref type="figure">Figure 4</ref>, we construct a Fashion Graph G = (N, E) according to the training dataset. Each node in the graph n i ∈ N represents a category c i ∈ C, so that |N | = |C|. The edge represents interaction between nodes. If two categories have matching relation in the training dataset, there are two directed edges in reverse directions between the two corresponding nodes in Fashion Graph, since the interactions in two directions ought to different.</p><p>For example, one may select a pair of socks to match the shoes but less likely to select a pair of shoes to match socks particularly.</p><p>According to the training dataset, we calculate the weights of directed edges as follow,</p><formula xml:id="formula_0">w(n i , n j ) = Count c i ,c j /Count c j k Count c i ,c k /Count c k ,<label>(1)</label></formula><p>where w(n i , n j ) denotes the weight of edge from n i to n j , Count c i ,c j is the co-occurrence frequency of category c i and c j , and Count c j is the occurrence frequency of category c j . The constructed Fashion Graph has 120 nodes and 12500 directed and weighted edges. By putting each item into its corresponding node, an outfit can be represented as a subgraph of Fashion Graph. Then we use NGNN to infer the compatibility from the subgraph as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Node-wise Graph Neural Network</head><p>Our proposed method NGNN consists of three steps. The first step is to learn the initial node state. The second step is to model node interactions and update node state. The third step is to calculate the compatibility score with attention mechanism. 4.3.1 Learning Initial Node State. In NGNN, each node n i is associated with a hidden state vector h i , which will be updated dynamically. The input of NGNN is features (either visual or textual features) of items, which can be used to initialize the state of their corresponding nodes. For each item v i ∈ s, we first map its feature f i to a latent style space with the size of d. Considering the difference among the categories, we have a linear mapping matrix W i h for each category c i . Thus, we can obtain the representation of item v i in the latent space as follows,</p><formula xml:id="formula_1">r i = W i h f i .<label>(2)</label></formula><p>We then use the representation to initialize the hidden state h 0 i of its category corresponding node n i as,</p><formula xml:id="formula_2">h 0 i = tanh(r i ).<label>(3)</label></formula><p>Thus, the size of node state is also d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Modeling Node Interactions.</head><p>Each time the nodes communicate (i.e., propagate their state information to others and receive others' information), which is called a propagation step, the state of nodes will be updated.</p><p>In the traditional GGNN, at propagation step t, the nodes will receive sum of neighbors' state as,</p><formula xml:id="formula_3">a t i = n j →n i ∈ E A[n j , n i ]W p h t −1 j + b p ,<label>(4)</label></formula><p>where W p and b p are weights and biases of a shared linear transformation on all the edges. A is the adjacency matrix:</p><formula xml:id="formula_4">A[n i , n j ] = w(n i , n j ), if n i → n j ∈ E, 0, others .<label>(5)</label></formula><p>The traditional GGNN use the sameW p and b p to model interactions between different nodes. However, it fails to model the complex and flexible interactions between them. Since the node interaction on each edge ought to be different, we aim to model edge-wise interaction. There is a straight-forward way to achieve edge-wise node interactions, that is to let each edge has its own transformation weight and bias W p and b p . However, it will cost a great parameter space with numerous edges in a graph and cannot be applied on large scale graphs.</p><p>To remedy the problem above, we have an output matrix W i out and an input matrix W i in for each node n i in NGNN. The transformation function of edge n i → n j from node n i to node n j could be written as W n i →n j p</p><formula xml:id="formula_5">= W i out W j in .<label>(6)</label></formula><p>Thus, the Equation 4 could be rewritten as,</p><formula xml:id="formula_6">a t i = n j →n i ∈E A[n j , n i ]W j out W i in h t −1 j + b p .<label>(7)</label></formula><p>After receiving state information a t i , the state of node n i will be updated as following,</p><formula xml:id="formula_7">z t i = σ (W z a t i + U z h t −1 i + b z ),<label>(8)</label></formula><formula xml:id="formula_8">r t i = σ (W r a t i + U r h t −1 i + b r ),<label>(9)</label></formula><formula xml:id="formula_9">h t i = tanh(W h a t i + U h (r t i ⊙ h t −1 i ) + b h ),<label>(10)</label></formula><formula xml:id="formula_10">h t i =h t i ⊙ z t i + h t −1 i ⊙ (1 − z t i ),<label>(11)</label></formula><p>where, W z , W r , W h , b z , b r , b h are weights and biases of the updating function, which are similar to Gated Recurrent Unit (GRU) <ref type="bibr" target="#b17">[18]</ref>. z t i and r t i are update gate vector and reset gate vector, respectively. After T propagation steps, we can obtain the final state of nodes, which are also the final node representations. Then we can generate a graph-level output to serve as the compatibility score x s of outfit s. We adopt the attention mechanism described in next section to compute the graph-level output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Compatibility</head><p>Calculating with Attention Mechanism. We contend two viewpoints on how fashion item influence the outfit compatibility here. The first viewpoint is that items have different influence on an outfit. For example, an improper top and an improper necklace have different influence to the outfit influence. The second is that the same item plays different roles in different outfits. For example, a coat maybe suitable for an outfit and promote the compatibility, while it will decrease the compatibility together with summer clothes. Thus, we aim to design an attention mechanism to model the influence of items on the outfit compatibility. Since a node receives state information from other nodes, their node representations are aware of the global information (i.e., other nodes' state). Similar with <ref type="bibr" target="#b17">[18]</ref>, we utilize self-attention <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref> to calculate the graph-level output and also the outfit compatibility score as following,</p><formula xml:id="formula_11">x s = |s | i=1 σ (θ (h t i )) · α(δ (h t i )).<label>(12)</label></formula><p>θ (·) and δ (·) are two perception networks to output a real value. θ (·) is used to model the weights of items (i.e., importance of items' influence on the outfit compatibility), and δ (·) is used to model the compatibility score of items (i.e., how the item goes with the other items in the outfit). α(·) and σ (·) are leaky relu and sigmoid activate functions α(x) = max(0.01x, x), σ (x) = 1/(1 + e −x ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">NGNN with Multi-modal Input</head><p>As described above, with the input of visual features or textual features of items, NGNN can be used to model the outfit compatibility from visual or textual modality. We here introduce how to jointly model the outfit compatibility from both visual and textual modality with NGNN. We use two channels of NGNN, the input of a channel is visual features, and the input of another channel is textual features. Specifically, for each outfit s, we input the visual features and textual features to the two channels of NGNN respectively and obtain a visual compatibility score x vis s and a textual compatibility score x t x t s . We calculate the final outfit compatibility score x s as follow,</p><formula xml:id="formula_12">x s = βx vis s + (1 − β)x t x t s<label>(13)</label></formula><p>where β is a non-negative trade-off parameter.</p><p>As described in Section 4.3.1, for each item v i , its visual feature f vis i and textual feature f t x t i are mapped into a shared latent space respectively. We denote their representations in latent space as r vis i and r t x t i . Considering the coherent relation between the visual and textual information of items, we use a regularization to ensure the consistency between the visual and textual feature of the same item in the shared latent space as follow,</p><formula xml:id="formula_13">L con = |s | i=1 r vis i − r t x t i 2 (14)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training Strategy</head><p>We denote outfits in the training dataset as positive outfits. For each outfit s = v 1 , v 2 , ..., v p , ... ∈ S , we randomly select an item v p and replace it with a random item v q ∈ V to form a negative outfit s − = v 1 , v 2 , ..., v q , ... . Thus, we can obtain a dataset P t r ain consisting of these pairs (s, s − ). Similar to Bayesian Personalized Ranking <ref type="bibr" target="#b27">[28]</ref>, we assume that the positive outfit has higher compatibility than the negative one. Therefore, we have the following objective function,</p><formula xml:id="formula_14">L bpr = (s,s − )∈ P t r ain − ln σ (x s − x s − ).<label>(15)</label></formula><p>The objective function of NGNN is:</p><formula xml:id="formula_15">L 1 = L bpr + λ 2 ∥Θ∥ 2 ,<label>(16)</label></formula><p>where Θ refers to the set of parameters and λ is the hyper-parameter of L2 regularizer, which is designed to avoid overfitting. When it comes to NGNN with multi-modal input, we have an additional loss function L con as described in Section 4.4. Thus, we have the following objective function,</p><formula xml:id="formula_16">L 2 = L bpr + L con + λ 2 ∥Θ∥ 2 .<label>(17)</label></formula><p>We only train NGNN on the subgraph of outfit each time. That is to say, only the nodes in the subgraph are trained and only their parameters are updated each time, which greatly reduces time complexity. When we have more data to train the model, we only need to update the parameters of concerned nodes without changing the parameters of edges or other nodes, which is effective in real applications.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model Analysis</head><p>In this section, we analyze the parameter space and time complexity of our proposed method NGNN.</p><p>4.6.1 Parameter Space. The parameter space of our proposed method consists of two parts: the parameters correlated to nodes and the perception networks in attention mechanism. For each node n i , we have a linear mapping matrix W i h to map the input features into the latent space, an input matrix W i in and an output matrix W i out to propagate state information. Totally we have 3 |N | matrices, which are proportional to the number of nodes |N |. In addition, we have two matrices of perception networks in the attention mechanism. Overall, we have 3 |N | matrices. Therefore, we say that the parameters of NGNN is node-wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Time Complexity.</head><p>For each input outfit s, the time complexity of training process in NGNN consists of three parts. The first part is calculating the transformed state information nodes receive, which is O(|s |). The second part is updating the state of the concerned nodes, which is also O(|s |). The last part is updating the parameters of concerned nodes. Since the number of parameters correlated to the concerned nodes is 3 |s | as described above, its complexity is also O(|s |). According, the total complexity is O(|s |).</p><p>We compare NGNN with GGNN <ref type="bibr" target="#b17">[18]</ref> and Edge-wise GNN (EGNN). EGNN is a straight-forward way to achieve edge-wise interactions. In EGNN, each edge has an unique transformation matrix for propagating state information. We train the three models on a toy dataset consisting of outfits with the number of items varying from 2 to 30. In another word, the three models are trained on graphs with the number of nodes varying from 2 to 30. The parameter size and running time are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. As can be seen, the parameter size and running time of EGNN increased quadratically while those of NGNN and GGNN increased linearly.</p><p>In summary, NGNN can better model edge-wise interactions and learn better node representations with appropriate parameter space and time complexity. When there are more training data, we can only update the parameters of concerned nodes offline and upload them online without changing the edges or other nodes, which is effective in real applications.</p><p>As the main contribution of this work is to model outfit compatibility, we aim to answer the following research questions via experiments.</p><p>RQ1 How does our proposed NGNN perform compared with other pair or sequence representation models? RQ2 How do different components in NGNN effect the performance? RQ3 How do different modalities influence the inference of compatibility? RQ4 How do the hyper-parameters d and T effect the performance?</p><p>Next, we first describe the two tasks we conduct experiments on and the experimental settings. We then report the results by answering the above research questions in turn. Finally, we analyze the parameter space and time complexity of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Description</head><p>We conduct experiments on two tasks: fill-in-the-blank fashion recommendation and compatibility prediction on the polyvore dataset described in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1.1</head><p>Fill-in-the-blank Fashion Recommendation. The task of fill-inthe-blank (FITB) in fashion recommendation is first introduced in <ref type="bibr" target="#b8">[9]</ref>. In this task, Given a set of fashion items and a blank, we aim to find the most compatible item from the candidates set to fill in the blank as shown in <ref type="figure" target="#fig_5">Figure 6</ref>. This is a practical scenario in people's daily life. For example, a person wants to select clothes to match the pants, hat and shoes. Following <ref type="bibr" target="#b8">[9]</ref>, we create a fill-in-the-blank dataset based on the Polyvore test dataset. For each outfit in the test dataset, we randomly select an item from it and replace the item with a blank. By randomly select 3 negative items from I we can form a 4-length candidates set along with the positive one, assuming the negative items are less compatible than the positive one. In our work, we respectively put the four candidate items into the blank to form an outfit and calculate their compatibility scores with our model. The one which achieves the highest outfit compatibility score is chosen as the answer. The performance is evaluated by the accuracy of choosing the right answer from the four candidates (FITB accuracy), which is 25% by randomly selecting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Compatibility</head><p>Prediction. This task is to predict the compatibility score for any given outfit. To evaluate our model, we first construct a dataset P t est using the Polyvore test dataset. Specifically, for each positive outfit s in the Polyvore test dataset, we create a negative outfit s − by randomly selecting |s | items from V. Thus, we can obtain a dataset P t est consisting of evaluation pairs (s, s − ). We then adopt the widely used metric AUC (Area Under the ROC curve) to evaluate the performance, which is defined as,</p><formula xml:id="formula_17">AU C = 1 |P t est | (s,s − )∈ P t es t δ (x s &gt; x s − ) ,<label>(18)</label></formula><p>where δ (a) is an indicate function that returns one if the argument a is true and zero otherwise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>For optimization, we adopt the RMSProp optimizer. The optimal hyper-parameters (learning rate, batch size, trade-off parameter β, parameter for L2 regularization λ, hidden size d and the propagation steps T ) are determined by the grid search strategy. We experimentally find that the model achieves optimal performance with the learning rate as 0.001, batch size as 16, β as 0.2, λ as 0.001, d as 12 and T as 3. We early stop the training process when the loss stabilizes, usually at 12 epoch. All the experiments were conducted over a sever equipped with 3 NVIDIA Titan X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Comparison (RQ1)</head><p>To evaluate our proposed method, we compare it with the following state-of-the-art baselines. Note that SiameseCNNs and LMT focus on the compatibility between two items. In the task of outfit recommendation, we sum up the compatibility scores of all the pairs in the outfit as the score of the outfit. Random: Randomly assign the compatibility scores of outfits. SiameseCNNs: It's a pair-wise compatibility learning method <ref type="bibr" target="#b37">[38]</ref>, which minimizes the distance of image vectors between compatible items and maximums the distance between incompatible items. This method only focuses on the visual information.</p><p>LMT: Proposed by <ref type="bibr" target="#b22">[23]</ref>, it models compatibility between items in a style space via a single low-rank Mahalanobis embedding matrix. This method also focuses on visual modality. Bi-LSTM: It's proposed by <ref type="bibr" target="#b8">[9]</ref>. By viewing an outfit as a sequence, it uses the bidirectional LSTMs to predict the next item, taking multi-modal data as input, which can also predict the compatibility scores of outfits. This method focus on multi-modal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GGNN (visual / textual):</head><p>We use GGNN to model the outfit compatibility with input of visual or textual features of items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NGNN (visual / textual):</head><p>We use our proposed NGNN to model the outfit compatibility with input of visual or textual features of items. NGNN (multi-modal): We jointly models the outfit compatibility with NGNN from the visual and textual modalities. We conduct comparison experiments on the tasks of fill-in-theblank fashion recommendation and compatibility prediction. The results are shown in <ref type="table" target="#tab_1">Table 1</ref>. Next we analyze the results of FITB accuracy and compatibility AUC respectively.   <ref type="table" target="#tab_1">Table 1</ref>. We can obtain the following observations from the table: (1) SiameseCNNs and LMT achieve worse performance compared with the other methods, which demonstrates that the pair representation ignores the integrity of outfit. (2) GGNN outperforms Bi-LSTM, suggesting that our proposed graph representation can better reflect the complex relations among items in an outfit than sequence representation. (3) NGNN achieves a better performance than GGNN, which proves that our proposed NGNN can better infer compatibility information from the graph by better modeling node interactions. (4) Using textual features achieves better performance than visual features for both GGNN and NGNN, which suggests that the textual features can better represent fashion items than visual features in that the titles accurately summarize the key features of items. (5) NGNN with Multi-modal input achieves best performance, proving that the visual and textual information are good supplements to each other. In addition, our proposed method can jointly model the outfit compatibility from multiple modalities. We randomly select several example outfits in the FITB dataset to test on our model and Bi-LSTM. The results are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. From example 1, we can see that Bi-LSTM and our model can both correctly select the item of complementary category as the outfit lacks of a pair of shoes. In example 2, our model chooses the right item while Bi-LSTM choose the skirt wrongly, which conflicts with the pants. This may be because that the position of pants is far away from the blank so that it has little 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7</head><p>: Comparison of our model and Bi-LSTM on several example of task compatibility prediction. Green fonts represent the right evaluation and red fonts represent wrong.</p><p>influence on it. In example 3, our model selects the most compatible bottom jeans with the whole set from the four bottoms while Bi-LSTM selects the white short skirt which may be most compatible with the white shirt, but not such compatible with the other items. The wrong chosen of Bi-LSTM may be because that the nearby white shirt has strong influence on the blank. From these example, we can draw a conclusion: the items have strong influence on nearby items in the sequence while have less influence on items faraway. This makes the sequence represented method relies on a proper sequence order. Compared with the sequence-represented model Bi-LSTM, our graph-represented model can better model the complex relations among items in an outfit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Compatibility AUC.</head><p>The performance comparison of models evaluated by compatibility AUC is shown in the third column of <ref type="table" target="#tab_1">Table 1</ref>. Similar with the comparison of performance evaluated by FITB accuracy, NGNN achieves best performance, GGNN is second best, and then is the sequence represented method Bi-LSTM, the pair represented methods (SiameseCNNs, LMT) are worst. Using textual features can achieve better performance than visual features. NGNN with multi-modal data achieves the best performance. These observations support our analysis in Section 5.3.1. In addition, it can be seen that GGNN and NGNN all achieve great performance and the gap is small, which proves the superiority of graph representation on compatibility prediction. GGNN is already good enough for compatibility prediction. We randomly select several example outfits and show the compatibility scores predicted of Bi-LSTM and our model in <ref type="figure">Figure 7</ref>. In example 1, our model gives the outfit a high score correctly while Bi-LSTM gives it a relatively low score, which may be because that the sequence representation method fails to capture the high compatibility between the first top and the fifth handbag. While Bi-LSTM and our model both give the outfit a high score correctly in example 2. In example 3, the fifth shorts conflict with the first skirts. Our model gives it a low score correctly while Bi-LSTM gives it a relatively high score, which may be because Bi-LSTM fails to capture the conflict of the two bottoms across a long distance. Examples 2 and 3 suggest that Bi-LSTM relies on a proper order of sequence, which is flexible and hard to learn. The influence between two items is inversely proportional to their distance in the sequence. Our model can better reflect the relation among items compared with Bi-LSTM. In example 4, our model gives it a low score to the positive outfit. A plausible explanation is that there is little matching of coat, sweater and shorts in training dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Component Comparison (RQ2)</head><p>To investigate the effect of different components on the performance of our proposed method, we compare our method with different components. NGNN (-W): NGNN without weighted adjacency matrix. Specifically, we use 0-1 adjacency matrix instead of the weighted one as described in Section 4.2. NGNN (-A): NGNN without attention mechanism. We only use a perception network to model the compatibility scores of nodes and sum them up them. NGNN (-W-A): NGNN with neither the attention mechanism nor weighted adjacency matrix. The performances are shown in <ref type="table" target="#tab_2">Table 2</ref>. If not specifically stated, we use visual features as input of NGNN. We observe that NGNN outperforms all the ablative methods, which proves the necessity of the two components in our model. NGNN (-W) achieves worse performance than NGNN, suggesting that the weighted adjacency matrix we calculated is a supplement to the information of graph. NGNN outperforms NGNN (-A), which confirms the necessity of our attention mechanism.</p><p>To intuitively illustrate how the attention mechanism works, we show the attention weights of several example outfits in <ref type="figure" target="#fig_7">Figure  8</ref>. The depths of color are proportional to the attention weight of items. The higher attention weight item has, the deeper the color of bar below it is, the stronger influence the item has to the outfit compatibility. As can be seen in example 1, the shirt, short skirt and necklace have distinct and compatible styles and they have strong influence to the outfit compatibility. The black bag and black shoes have little influence to outfit compatibility while they have ordinary styles. Similarly, the sweater, shoes and hat in example 2 are all of distinct rustic style and they have strong influence to the outfit compatibility. The jeans and shoes in example 3 can match well with most styles while they have little influence to the outfit compatibility. The white outwear and jeans which can 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>3. match with many styles in example 4 also have little influence to outfit compatibility. From these examples, we can find that more distinct style items have, more influential they are to the outfit compatibility. On the contrary, more ordinary items are (i.e., can match with many styles), less influential they are. This is reasonable in common sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Modality Comparison (RQ3)</head><p>To investigate the influence of different modalities on the performance, we compare the NGNNs (multi-modal) with different modality combinations. As described in <ref type="bibr">Equation 13</ref>, β is the weight of visual compatibility score while that of textual compatibility score is 1 − β. Note that when β = 0 the model is actually NGNN (visual), when β = 1 it's NGNN (textual). In <ref type="figure" target="#fig_0">Figure 10</ref>, we show the performance of NGNN (multi-modal) with β varying from 0 to 1. It can be seen when β = 0.2, our model achieves the best performance, which suggests that the visual and textual information are good supplement to each other and both contribute s to the inference of compatibility. We can also see that using textual features can achieve better performance than visual features. A plausible explanation is that the textual information is more concise to capture the key features of items.</p><p>To intuitively see the difference of NGNN with visual, textual and multi-modal input, we show the comparison of them on several testing examples in <ref type="figure">Figure 9</ref>. In the example 1, Visual NGNN chose the false answer due to its similar color and material with the jacket, which the visual information can reflect. However the similarity between the textual information (titles) of the true answer and the dress, jacket directly reveals their high compatibility. In example 2, it's hard to choose the answer with textual input since the titles of the two answers are similar, while with visual input NGNN can distinct the high compatibility between the styles of the true answer and the outfit from visual information. In summary, the visual and textual information are great supplements to each other. By integrating them we can achieve better performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Hyper-parameter Discussion (RQ4)</head><p>In this section, we examine how the dimensionality of hidden vectors d and the propagation time d affect the performance. Comparison experiments are conducted on NGNN (multi-modal). <ref type="figure" target="#fig_0">Figure 11</ref> shows the performance of our model with varying d and d. From the figure in the left, we can observe that the FITB accuracy and compatibility AUC reach the highest point when d is around 12.</p><p>When d is large than 12, the performance drops sharply. This indicates that there is no need to represent the node state with too much parameters. From the figure in the right, it can be seen that the performance saturates when T is 3 and the nodes in NGNN have interacted with others sufficiently, suggesting that it's important to model the multi-hop propagation so that the nodes are better aware of other nodes' information. Moreover, when T is high, the superfluous information propagation will confuse the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we first propose to model the outfit compatibility by representing an outfit as a graph since the graph structure can better capture the complex relations among items in outfits. To infer the compatibility information from the graph, we further propose a novel model NGNN, which can better model flexible node interactions and learn better node representations. Our proposed method can model outfit compatibility not only from visual or textual modality respectively, but also from multiple modalities jointly. Experimental results on a real-world dataset (Polyvore dataset) prove the great superiority of our method over existing works. In the future, we aim to model personalized compatibility since people have different notions of fashion compatibility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Different representations of an outfit. Left is the outfit. (a), (b) and (c) in the right are its pair, sequence and graph representations respectively. Circles represent the items and links represent the relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The parameter sizes and running time of GGNN, EGNN and NGNN on graphs with the number of nodes varying from 2 to 30. EGNN is a simple variant of GGNN, in which every edge has a different dtransformation matrix for propagating state information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of our model and Bi-LSTM on several examples in fill-in-the-blank task. Green fonts represent the right chosen and red fonts represent wrong chosen. 5.3.1 FITB Accuracy. The comparison of FITB accuracy is shown in the middle column of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Illustration of attention mechanism. The depth of color are proportional to the attention weights of items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Comparison of NGNN(visual), NGNN(textual) and NGNN(multi-modal). For each outfit in the left, there is a true answer (T) and a false answer (F). Green fonts reprsent true chosen while red fonts represent false chosen. FITB accuracy and compatibility AUC of our model with different β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>AUC and FITB accuracy performance with different dimensionality d (left) and propagation times T (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>We propose a novel model NGNN which can better model node interactions in the graph and learn better node representations. Taking advantage of NGNN, we can not only model outfit compatibility from visual or textual modality, but also from multiple modalities.• Experimental results on a real-world dataset (Polyvore dataset) demonstrate the great superiority of our method over others.</figDesc><table><row><cell>...</cell><cell>Attention layer</cell><cell>score</cell><cell>Compatibility</cell></row><row><cell>2 RELATED WORK</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2.1 Fashion Compatibility Modeling</cell><cell></cell><cell></cell><cell></cell></row></table><note>There have been many studies focusing on modeling the compatibil- ity of clothing recently, since it is the key to fashion recommenda- tion. McAuley et al. assumed that there existed a latent style space where compatible items stayed close. They used Low-rank Maha- lanobis Transformation (LMT) to map items into a latent space and0 H1 HT H Representation Layer</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The performance of models evaluated by Fill-in-theblank accuracy and compatibility prediction AUC.</figDesc><table><row><cell>Method</cell><cell cols="2">Accuracy (FITB) AUC (Compatibility)</cell></row><row><cell>Random</cell><cell>24.07%</cell><cell>0.5104</cell></row><row><cell>SiameseCNNs</cell><cell>48.09%</cell><cell>0.7087</cell></row><row><cell>LMT</cell><cell>50.91%</cell><cell>0.6782</cell></row><row><cell>Bi-LSTM</cell><cell>67.01%</cell><cell>0.8427</cell></row><row><cell>GGNN (visual)</cell><cell>72.78%</cell><cell>0.9516</cell></row><row><cell>GGNN (textual)</cell><cell>73.63%</cell><cell>0.9591</cell></row><row><cell>NGNN (visual)</cell><cell>77.01%</cell><cell>0.9600</cell></row><row><cell>NGNN (textual)</cell><cell>77.78%</cell><cell>0.9716</cell></row><row><cell>NGNN (multi-modal)</cell><cell>78.13%</cell><cell>0.9722</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance of our proposed method with different components.</figDesc><table><row><cell>Method</cell><cell cols="2">FITB Accuracy Compatibility AUC</cell></row><row><cell>NGNN(-W-A)</cell><cell>75.17%</cell><cell>0.9541</cell></row><row><cell>NGNN(-W)</cell><cell>75.71%</cell><cell>0.9559</cell></row><row><cell>NGNN(-A)</cell><cell>76.12%</cell><cell>0.9578</cell></row><row><cell>NGNN</cell><cell>77.01%</cell><cell>0.9600</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/CRIPAC-DIG/NGNN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.polyvore.com/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is jointly supported by National Natural Science Foundation of China (61772528, 61871378) and National Key Research and Development Program (2016YFB1001000).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.</head><p>C. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. B. C. D.</head><p>A. B. C. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth: A</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dress Fashionably: Learn Fashion Collocation With Deep Mixed-Category Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2103" to="2110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MV-RNN: A Multi-View Recurrent Neural Network for Sequential Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning fashion compatibility with bidirectional lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1078" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sherlock: sparse hierarchical embeddings for visually-aware one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3740" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning compatibility across categories for heterogeneous item recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="937" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collaborative fashion recommendation: A functional tensor factorization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbit</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinseok</forename><surname>Seol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04014</idno>
		<title level="m">Style2Vec: Representation Learning for Fashion Items from Style Sets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4173" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining fashion outfit composition using an end-to-end deep learning approach on set data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1946" to="1955" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Constructing Narrative Event Evolutionary Graph for Script Event Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05081</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepstyle: Learning user preferences for visual recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="841" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting the next location: a recurrent model with spatial and temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="194" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The More You Know: Using Knowledge Graphs for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
		<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compatibility family learning for item recommendation and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Siang</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Yueh</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neurostylist: Neural compatibility modeling for clothing matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-03" />
			<biblScope unit="page" from="753" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01586</idno>
		<title level="m">Deep Semantic Role Labeling with Self-Attention</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 24th international conference on world wide web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning visual clothing style with heterogeneous dyadic cooccurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balazs</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4642" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sessionbased Recommendation with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
