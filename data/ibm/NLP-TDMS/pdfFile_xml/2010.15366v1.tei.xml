<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SELF-SUPERVISED PRE-TRAINING REDUCES LABEL PERMUTATION INSTABILITY OF SPEECH SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Feng</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun-Po</forename><surname>Chuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Rong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene-Ping</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
							<email>hungyilee@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SELF-SUPERVISED PRE-TRAINING REDUCES LABEL PERMUTATION INSTABILITY OF SPEECH SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speech Enhancement</term>
					<term>Self-supervised Pre-train</term>
					<term>Speech Separation</term>
					<term>Label Permutation Switch</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech separation has been well-developed while there are still problems waiting to be solved. The main problem we focus on in this paper is the frequent label permutation switching of permutation invariant training (PIT). For N -speaker separation, there would be N ! possible label permutations. How to stably select correct label permutations is a longstanding problem. In this paper, we utilize self-supervised pre-training to stabilize the label permutations. Among several types of self-supervised tasks, speech enhancement based pre-training tasks show significant effectiveness in our experiments. When using off-the-shelf pre-trained models, training duration could be shortened to one-third to two-thirds. Furthermore, even taking pre-training time into account, the entire training process could still be shorter without a performance drop when using a larger batch size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years, self-supervised learning is a hot topic in machine learning. It is well-known that data labeling costs a lot, and unlabeled data is relatively much easier to collect. By first utilizing a large amount of unlabeled data to pre-train the model, self-supervised learning has shown considerable improvements in many fields. In the field of natural language processing (NLP) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, BERT <ref type="bibr" target="#b0">[1]</ref> learns powerful representations by self-supervised pre-training to encode contextual information. The representations learned from pre-training can be beneficial in downstream tasks, especially when the data is limited. Furthermore, BERT could be finetuned as a part of the downstream model. Most of the top models in NLP are BERT-based models. In computer vision (CV) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, SimCLRv2 <ref type="bibr" target="#b4">[5]</ref> outperforms the previous stateof-the-art in semi-supervised learning on ImageNet by selfsupervised pre-training. Both NLP and CV have shown that self-supervised pre-trained models are more label-efficient than previous semi-supervised training methods, especially when only with a little labeled data.</p><p>In the speech processing domain, self-supervised learning also shows its advantages when labeled data is limited, whenever the learned model is used for feature-extraction or fine-tuning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Wav2vec 2.0 <ref type="bibr" target="#b7">[8]</ref> shows that 10 minutes of labeled data is already enough for training an ASR system with 53k hours of unlabeled data. TERA <ref type="bibr" target="#b8">[9]</ref> pretrains a Transformer model with a BERT-like objective. The learned representations are robust for a wide range of downstream tasks. The model could even outperform supervised learning when fine-tuned with 0.1% labeled data only.</p><p>Speech separation has always been a fundamental issue towards robust speech processing in the real-world acoustic environment. The considered speech signal is often disturbed by some additional signals coming from other speakers. To date, the models that directly process the time-domain samples achieve great performances of single-channel speech separation. While such models perform pretty well that the separated speech estimates are hard to be distinguished from the reference signals, their training usually takes several days to weeks. Also, the label permutation switching problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> would lead to a slower convergence rate in the early training stage, and a longer training duration would be required to get sufficiently good results.</p><p>In this paper, self-supervised learning is used to pretrain the model to deal with the label permutation switching problem. With the input signal disturbed by noise, our selfsupervised pre-training method forces the model to learn how to fetch the information of the clean source from the input mixture, which is beneficial in learning speech separation. Experiments show that the pre-training strategy could obviously decrease label permutation switches, and increase the speed of convergence significantly. Only one-third to two-thirds of training time is needed to achieve the same performance as the baseline model. Even taking pre-training time into account, pre-training then fine-tuning with a larger batch size could still shorten the training process without any performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">LABEL PERMUTATION SWITCHING PROBLEM OF PERMUTATION INVARIANT TRAINING</head><p>In general, deep learning techniques for single-channel speech separation can be divided into two categories: timefrequency (T-F) domain methods and end-to-end time-domain approaches. Based on T-F features created by calculating the short-time Fourier transform (STFT), T-F domain methods separate the T-F features for each source and then reconstruct the source waveforms by inverse STFT <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref>. On the other hand, time-domain approaches directly model the mixture waveform using an encode-decoder framework, and this line of research has made significant progress in recent years <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16]</ref>. However, both the T-F domain and time-domain approaches suffer from label ambiguity problem when it comes to matching the ground truths and the estimated signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Label Ambiguity Problem</head><p>In single-channel speech separation, the speech signals are assumed to have been linearly mixed: y = N n=1 x n , where N is the number of sources; the goal is to extract all speech signals (i.e., {x n } N n=1 ) from the mixed-signal (i.e., y). To simplify the settings, let's consider mixing only two sources, y = x 1 + x 2 . In this case, we could employ a model with two outputs, o 1 and o 2 , to solve this problem. However, there exists two possible label assignments: (1) o 1 regresses to x 1 and o 2 regresses to x 2 , or (2) o 1 regresses to x 2 and o 2 regresses to x 1 . When N is not limited to two, there would be N ! possible label permutation assignments. Incorrect permutation assignment would force the separation model to be updated to wrong direction, and even destroy what it had already learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">PIT and Label Permutation Switching Problem</head><p>Permutation invariant training (PIT) <ref type="bibr" target="#b17">[17]</ref> was then proposed to solve this problem. All of the permutations are used to calculate the regression loss, and the permutation with minimum loss is chosen. While PIT gives a relatively dynamic permutation selection, different selected permutations between epochs still handicap the model's performance. In <ref type="bibr" target="#b9">[10]</ref>, a soft version of PIT was proposed to relax the permutation changing problem between epochs, but it is restricted to be used for L2-based objective functions only. <ref type="bibr" target="#b10">[11]</ref> proposed the interrupted and cascaded strategy, aiming to fix the label permutation switching problem. In <ref type="bibr" target="#b10">[11]</ref>, fixed permutation was firstly obtained from a trained separation model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TRAINING STRATEGIES</head><p>When labeled data is limited, semi-supervised training is usually unstable, which heavily depends on the selected model architecture, optimizer, and random seed. Self-supervised learning is then used to pre-learn structural information from large-scale unpaired data, which significantly benefits in stabilizing and boosting the continual training procedures. Although speech separation does not have the problem of lack of labeled data, its unstable label arrangement still seriously affects the training process. So in this section, we propose a self-supervised pre-training framework for speech separation. Multi-task learning is also proposed to be compared. More details are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-train</head><p>In this paper, we've tried three different self-supervised tasks as our pre-training tasks: speech enhancement (SE), Masked Acoustic Model with Alteration (MAMA) proposed by TERA <ref type="bibr" target="#b8">[9]</ref>, and continuous contrastive task (CC) proposed by wav2vec 2.0 <ref type="bibr" target="#b7">[8]</ref>. CC is a masked reconstruction task using contrastive loss, while MAMA is a masked reconstruction task with a probability of noise disturbance. Thus MAMA could be regarded as a task with both SE and CC characteristics. <ref type="figure" target="#fig_2">Fig. 1(a)</ref> shows the flowchart of pre-training with speech enhancement, where the input signal is mixed with random noise, and the model would need to reconstruct the original clean source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fine-tune</head><p>After pre-training, the model is then fine-tuned with the normal separation training objective. To be noticed, while most of the model parameters would be loaded from the pre-trained model, we re-initialize the parameters used to generate specific output masks. The fine-tuning procedure is shown in <ref type="figure" target="#fig_2">Fig. 1(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-task Learning</head><p>To make sure that whether the model benefits from the "pretrain then fine-tune" training procedure, jointly training enhancement and separation as multi-task learning is proposed as a comparing baseline. The flowchart of multi-task learning is illustrated in <ref type="figure" target="#fig_2">Fig. 1(c)</ref>. Either separation or enhancement would train each batch, so the model should learn both tasks simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>Speech separation is trained and evaluated on WSJ0-2mix <ref type="bibr" target="#b12">[12]</ref> and Libri2Mix <ref type="bibr" target="#b18">[18]</ref>, and speech enhancement is trained using Libri1Mix train-360 set <ref type="bibr" target="#b18">[18]</ref>. The WSJ0-2mix dataset is derived from the WSJ0 data corpus <ref type="bibr" target="#b19">[19]</ref>. The training and validation data contain two-speaker mixtures generated by randomly selecting utterances from different speakers in the WSJ0 si tr s set, and test set is similarly generated using utterances from unseen speakers in WSJ0 si dt 05 and si et 05 set. Libri2Mix is created based on the Librispeech dataset <ref type="bibr" target="#b20">[20]</ref> with the similar generating procedure as WSJ0-2mix. Libri2Mix train-100 set uses speakers randomly selected from the train-clean-100 set of Librispeech, while the dev and test set use the utterances from unseen speakers in the dev and test sets of Librispeech respectively. Libri1Mix train-360 dataset is created with the same settings as Libri2Mix, while only one speaker is randomly selected from the train-clean-360 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Self-supervised pre-training could be used for any separation model, so the study mainly focuses on clarifying the effectiveness of pre-training. In this paper, we choose Conv-TasNet <ref type="bibr" target="#b14">[14]</ref> as our main baseline model, DPRNN <ref type="bibr" target="#b15">[15]</ref> and DPT-Net <ref type="bibr" target="#b16">[16]</ref> are also used in our experiments. All of our experiments were implemented with Asteroid <ref type="bibr" target="#b22">[22]</ref>, including all of the configurations except for the number of epochs. For pre-training tasks, we trained speech enhancement for 200 epochs, MAMA for 200 epochs, and contrastive loss for 100 epochs. For speech separation, Conv-TasNet was trained with 100 epochs, DPRNN and DPTNet were trained with 200 epochs. It should be noted that in all experiments, our batch size was set to 24 to speed up the training process, which resulted in poorer final performance compared to the original papers because of fewer update steps. In our experiments, the model would be trained with three different strategies: from scratch, pre-trained then fine-tune (pre+FT), and multi-task training. To be noticed, all of the three strategies have the same number of update steps in training separation. Separation performance would be evaluated in scale-invariant signal-to-noise ratio improvement (SI-SNRi) <ref type="bibr" target="#b23">[23]</ref> and signalto-distortion ratio improvement (SDRi) <ref type="bibr" target="#b24">[24]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the results of three different training strategies. As shown, both pre-training and multi-task learning improve the model on both WSJ0-2mix and Libri2Mix, while pre-training improves more significantly. Validation SI-SNR results during training are reported in <ref type="figure" target="#fig_3">Fig. 2(a)</ref> and <ref type="figure" target="#fig_3">Fig. 2(b)</ref>. As shown in the figures, multi-task learning improvements gradually decrease while training on Libri2Mix and nearly hard to see on WSJ0-2mix. But the pre-trained model leads  the baselines all the way and achieves the final result of the baseline with 66 epochs only for WSJ0-2mix and 37 epochs for Libri2Mix, which is about one-third to two-thirds of the baseline training time. Notably, although pre-training also takes time, off-the-shelf pre-trained models are usually easy to get on the Internet, so directly using them won't take any additional training time. From <ref type="figure" target="#fig_3">Fig. 2</ref>(c) and <ref type="figure" target="#fig_3">Fig. 2(d)</ref>, we could see that only pre-training with speech enhancement significantly reduces label permutation switches.  More experiments with different model architectures are shown in <ref type="table" target="#tab_3">Table 3</ref>. As mentioned in section 4.2, a larger batch size helps train faster (6x-12x faster compared to batch size 2-4 in original papers) but causes lower performance (1.8 SI-SNRi drop in DPRNN, and 1.1 SI-SNRi drop in DPTNet). However, pre-training with speech enhancement improves the models of all kinds of architectures and achieves comparable or even better performance than those reported in the original papers. Overall, even taking pre-training time into account, pre-training then fine-tuning with a larger batch size still takes shorter training duration, which indicates that pre+FT would be a great option to accelerate the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on WSJ0-2mix and Libri2Mix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of Self-supervised Pre-training Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>This paper shows that speech enhancement pre-training could stabilize the separation training process without any additional paired data and training time, and could be generalized to any corpus or model architecture. Investigating other possible tasks, the correlation between tasks and more model architectures would be the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A new model was then pre-trained with this fixed permutation and fine-tuned with flexible PIT, giving the trained model the flexibility to find a better permutation assignment. Although interrupted and cascaded training decreases the permutation switches, the iterative training causes the training time several times longer compared to the original PIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Pre-training (b) Fine-tuning (c) Multi-task training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Three training flowcharts under our frameworks: (a) pre-training with speech enhancement, (b) fine-tuning, and (c) multi-task training. Gray blocks mean the corresponding parts are not used during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Validation SI-SNR (dB) and the percentage of label permutation switches in total training data (%) at each epoch on two datasets. (a) Validation SI-SNR on WSJ0-2mix. (b) Validation SI-SNR on Libri2Mix. (c) The percentage of label permutation switches on WSJ0-2mix. (d) The percentage of label permutation switches on Libri2Mix. In (c), training from scratch switches 17% of training data at around epoch 20, and multi-task learning switches 89% at around epoch 40, which indicate the instability of the baseline models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with different training strategies for Conv-TasNet trained on WSJ0-2mix and Libri2Mix in SI-SNRi (dB) and SDRi (dB). Strategy "pre+FT (SE)" indicates the model is pre-trained with speech enhancement then finetuned.</figDesc><table><row><cell>Corpus</cell><cell>Strategy</cell><cell cols="2">SI-SNRi SDRi</cell></row><row><cell></cell><cell>from scratch</cell><cell>13.2</cell><cell>13.6</cell></row><row><cell>Libri2Mix</cell><cell>pre+FT (SE)</cell><cell>14.1</cell><cell>14.6</cell></row><row><cell></cell><cell>multi-task</cell><cell>13.7</cell><cell>14.1</cell></row><row><cell></cell><cell>from scratch</cell><cell>15.6</cell><cell>15.8</cell></row><row><cell>WSJ0-2mix</cell><cell>pre+FT (SE)</cell><cell>16.3</cell><cell>16.5</cell></row><row><cell></cell><cell>multi-task</cell><cell>15.7</cell><cell>16.0</cell></row><row><cell cols="4">set of Librispeech, and mixed with a random noise sam-</cell></row><row><cell cols="4">pled from WHAM! [21]. The speaker sets of Libri1Mix</cell></row><row><cell cols="4">train-360 and Libri2Mix train-100 set are disjoint.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison</figDesc><table><row><cell cols="3">with different self-supervised pre-</cell></row><row><cell cols="3">training tasks for Conv-TasNet trained on WSJ0-2mix in SI-</cell></row><row><cell cols="3">SNRi and SDRi. The first row indicates training from scratch.</cell></row><row><cell cols="3">Pre-training task SI-SNRi SDRi</cell></row><row><cell>-</cell><cell>15.6</cell><cell>15.8</cell></row><row><cell>SE</cell><cell>16.3</cell><cell>16.5</cell></row><row><cell>MAMA [9]</cell><cell>16.2</cell><cell>16.5</cell></row><row><cell>CC [8]</cell><cell>15.5</cell><cell>15.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>shows that speech enhancement based tasks (SE and MAMA) lead to significant improvement, but CC doesn't help. It indicates that speech enhancement is more effective for pre-training speech separation than masked reconstruction. A possible explanation is, while speech enhancement, the model learns to extract information about the single speaker from the noisy input signal. Thus during separation fine-tuning, the model would already be good at fetching individual target speakers from the input mixture signal, while only a small part of the model needs to learn from scratch, as mentioned in section 3.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with different model architectures on WSJ0-2mix in SI-SNRi (dB) and SDRi (dB).</figDesc><table><row><cell>Model</cell><cell cols="3">Pre-training task SI-SNRi SDRi</cell></row><row><cell>Conv-TasNet</cell><cell>-SE</cell><cell>15.6 16.3</cell><cell>15.8 16.5</cell></row><row><cell>DPRNN</cell><cell>-SE</cell><cell>17.0 18.6</cell><cell>17.3 18.9</cell></row><row><cell>DPTNet</cell><cell>-SE</cell><cell>19.1 20.7</cell><cell>19.3 20.9</cell></row><row><cell cols="2">5.3. More Results on WSJ0-2mix</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Big selfsupervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03240</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for selfsupervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Tera: Self-supervised learning of transformer encoder representation for speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Andy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06028</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic permutation invariant training for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Midia</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Khorram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Hl</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4604" to="4608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interrupted and cascaded permutation invariant training for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene-Ping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Wen</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hungyi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Shan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="6369" to="6373" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dualpath rnn: efficient long sequence modeling for timedomain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dualpath transformer network: Direct context-aware modeling for end-to-end monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13975</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Librimix: An open-source dataset for generalizable speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11262</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Csr-i (wsj0) complete ldc93s6a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Download. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Wham!: Extending speech separation to noisy environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><forename type="middle">Richard</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmett</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dwight</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01160</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Asteroid: the pytorchbased audio source separation toolkit for researchers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunit</forename><surname>Sivasankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Olvera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Juan M Martín-Doñas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04132</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speakerindependent speech separation with deep attractor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
