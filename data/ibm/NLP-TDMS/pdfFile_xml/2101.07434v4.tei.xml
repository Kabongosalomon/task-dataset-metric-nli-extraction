<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CHANNELIZED AXIAL ATTENTION FOR SEMANTIC SEGMENTATION Channelized Axial Attention for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Ye</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Wenjing</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xiangjian</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Liu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
						</author>
						<title level="a" type="main">CHANNELIZED AXIAL ATTENTION FOR SEMANTIC SEGMENTATION Channelized Axial Attention for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic Segmentation</term>
					<term>Axial Attention</term>
					<term>Chan- nelization</term>
					<term>Grouped Vectorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-attention and channel attention, modelling the semantic interdependencies in spatial and channel dimensions respectively, have recently been widely used for semantic segmentation. However, computing spatial-attention and channel attention separately and then fusing them directly can cause conflicting feature representations. In this paper, we propose the Channelized Axial Attention (CAA) to seamlessly integrate channel attention and axial attention with reduced computational complexity. After computing axial attention maps, we propose to channelize the intermediate results obtained from the transposed dot-product so that the channel importance of each axial representation is optimized across the whole receptive field. We further develop grouped vectorization, which allows our model to be run with very little memory consumption at a speed comparable to the full vectorization. Comparative experiments conducted on multiple benchmark datasets, including Cityscapes, PASCAL Context and COCO-Stuff, demonstrate that our CAA not only requires much less computation resources compared with other dual attention models such as DANet [1], but also outperforms the state-of-the-art ResNet-101-based segmentation models on all tested datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S EMANTIC segmentation is a fundamental task in many computer vision applications, which assigns a class label to each pixel in the image. Most of the existing approaches for semantic segmentation (e.g., <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>) have adopted a pipeline similar to the one that is defined by Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b6">[7]</ref> using fully convolutional layers to output pixel-level segmentation results of the input image, and have achieved state-of-the-art performance. After the FCN approach, there have been many approaches dedicated to extracting enhanced pixel representations from the backbone. Earlier approaches, including PSPNet <ref type="bibr" target="#b8">[8]</ref> and DeepLab <ref type="bibr" target="#b9">[9]</ref>, used a Pyramid Pooling Module (PPM) or an Atrous Spatial Pyramid Pooling (ASPP) module to expand the receptive field and capture multiple-range information to enhance the representation capabilities.</p><p>The latest research works on segmentation head in the past few years have mainly focused on using the attention mechanisms to improve the performance. During the early days of the attention mechanisms, the Squeeze and Excitation Networks (SENets) <ref type="bibr" target="#b10">[10]</ref> introduced a simple and yet efficient channel attention module to explicitly model the interdependencies between channels. Meanwhile, the Non-Local Networks in <ref type="bibr" target="#b11">[11]</ref> proposed self-attention to model long-range dependencies in spatial domain, so as to produce more correct  Thus, for each pixel in the feature maps, spatial self-attention makes its representation more similar to the representations of the pixels that are closer, whereas channel attention finds important channels in the entire feature maps and applies different weights to the extracted features.</p><p>To enjoy the advantages of both spatial attention and channel attention, some approaches (e.g., <ref type="bibr" target="#b0">[1]</ref>) proposed to directly fuse their results with an element-wise addition (see <ref type="figure" target="#fig_1">Fig. 1(a)</ref>). Although they have achieved improved performance, the relationship between the contributions of spatial self-attention and channel attention to the final result is unclear.</p><p>Moreover, calculating the two attentions separately not only increases the computational complexity, but may also result in conflicting importance of feature representations. For example, some channels may appear to be important in channel attention for a pixel that belongs to a partial region in the feature maps, But spatial attention may have its own perspective, which is calculated by summing up the similarities over the entire feature maps, and weakens the impact of the channel attention.</p><p>The existing designs (e.g. <ref type="bibr" target="#b12">[12]</ref>) combining channel attention and spatial attention in a cascaded, sequential manner ( <ref type="figure" target="#fig_1">Fig. 1(a)</ref>) have similar issues. Channel attention can ignore the partial region representation obtained from the overall perspective, which may be required by spatial attention. Thus, directly fusing the spatial attention results with channel atten-arXiv:2101.07434v4 [cs.CV] 20 Apr 2021 tion results may yield incorrect importance weights for pixel representations. In the Experiments section of this paper, we develop an approach to visualize the impact of the conflicting feature representation on the final segmentation results.</p><p>Attempting to combine the advantages of spatial-attention and channel attention seamlessly and efficiently in a complementary way, we propose a Channelized Axial Attention (CAA), which is based on a redefined axial attention to reduce the computation cost of self-attention. Specifically, when applying the redefined axial attention maps to the input signal <ref type="bibr" target="#b11">[11]</ref>, we capture the intermediate results of the dot product before they are summed up along the corresponding axes. Capturing these intermediate results allows channel attention to be integrated for each column and each row, instead of computing on the mean or sum of the features in the entire feature maps. More importantly, when applying the attention maps, we propose a novel transposed approach, which allows the channel attention to be conducted in the whole receptive field. Last but not the least, we develop a novel grouped vectorization approach to maximize the computation speed under limited GPU memory.</p><p>In summary, our contributions of this paper include:</p><p>• We propose a novel Channelized Axial Attention, which integrates spatial self-attention with channel attention seamlessly and efficiently and significantly boosts the segmentation performance with only minor computation overhead of the original axial attention. <ref type="bibr">•</ref> We develop a novel approach to visualize the impact of the conflicting pixel representation of the existing dual attention designs on segmentation. • To balance the computation speed and GPU memory usage, we propose a novel grouped vectorization approach to compute the channelized attentions, which is particularly advantageous when processing large images with a limited GPU memory. • Extensive experiments on three challenging benchmark datasets, i.e., PASCAL Context <ref type="bibr" target="#b13">[13]</ref>, COCO-Stuff <ref type="bibr" target="#b14">[14]</ref> and Cityscapes <ref type="bibr" target="#b15">[15]</ref>, demonstrate the superiority of our approach over the state-of-the-art approaches. Next, Sect. II briefly summarizes the related work. Then, we illustrate the details of our proposed approach in Sect. III. Sect. IV presents the experiments and ablation studies. The paper concludes in Sect. V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Towards using the attention mechanisms to improve the performance of semantic segmentation, many research works have been reported. In this section, we introduce these approaches in the way of their evolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Capturing Information from Fixed Ranges</head><p>The PSPNet <ref type="bibr" target="#b8">[8]</ref> proposed a PPM, which used multiple average pooling layers with different sizes together to get average pixel representations in multiple receptive fields, and then upsampled and concatenated them together. Similarly, the ASPP in DeepLab <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[9]</ref> used parallel atrous convolutions with different rates to capture information from multiple ranges. The core ideas of both models are to utilize the surrounding information of each pixel in multiple ranges to achieve better pixel representations. Both methods have achieved highest scores in some popular public datasets <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b15">[15]</ref>. However, as claimed in <ref type="bibr" target="#b4">[5]</ref>, fixed receptive fields may lose important information, to which stacking more receptive fields can be a solution, at the cost of dramatically increased computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanisms</head><p>Spatial Self-Attention. Non-Local networks <ref type="bibr" target="#b11">[11]</ref> introduced the self-attention mechanism to examine the pixel relationship in spatial domain. It usually calculates dot-product similarity or cosine similarity to obtain the similarity measurement between every two pixels in feature maps, and recalculate the feature representation of each pixel according to its similarity with others. Spatial self-attention has successfully addressed the feature map coverage issue of multiple fixedrange approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b4">[5]</ref>, but it has also introduced huge computation cost for computing the full feature map. This means, for each pixel in the feature maps, its attention similarity concerns all other pixels. Recently, many approaches <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref> have been developed to optimize the spatial self-attention. They have not only reduced computation and GPU memory costs but also improved the performance.</p><p>Channel Attention. Channel attention <ref type="bibr" target="#b10">[10]</ref> examined the relationships between channels, and enhanced the important channels so as to improve the performance. SENets <ref type="bibr" target="#b10">[10]</ref> conducted a global average pooling to get mean feature representations, and then went through two fully connected layers, where the first one had reduced channels and the second one recovered the original channels, resulting in channelwise weights according to the importance of channels. In DANet <ref type="bibr" target="#b0">[1]</ref>, channel-wise relationships were modelled by a 2D attention matrix, similar to the spatial self-attention mechanism except that it computed the attention with a dimension of C × C rather than H × W × H × W (C denotes the number of channels, and H and W denote the height and width of the feature maps, respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial Attention + Channel Attention</head><p>Combining spatial attention and channel attention can provide fully optimized pixel representations in a feature map. However, it is not easy to enjoy both advantages seamlessly. In the DANet <ref type="bibr" target="#b0">[1]</ref>, the results of the channel attention and spatial attention are directly added together. Supposing that there is a pixel belonging to a semantic class that has a tiny region in the feature maps, spatial-attention can find its similar pixels. However, channel representation of the semantic class with a partial region of the feature maps may not be important in the perspective of entire feature maps, so it may be ignored when conducting channel attention computation. Computing self-attention and channel attention separately (as illustrated in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>) can cause conflicting results, and thus weaken their performance when both results are summarized together. In the cascaded model (see <ref type="figure" target="#fig_1">Fig. 1(b)</ref>), the spatial attention module after the channel attention module may pick up the incorrect pixel representation enhanced by channel attention, as channel attention computes the channel importance according to the entire feature maps.</p><p>In our work, we propose a Channelized Axial Attention approach, which first computes the spatial attention row-byrow and column-by-column, and then inserts the channel attention module to integrate both approaches seamlessly, as detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Formulation of the Spatial Self-Attention</head><p>Following <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b20">[20]</ref>, a 2D self-attention operation in spatial domain of neural networks can be defined by:</p><formula xml:id="formula_0">y i,j = ∀m,n f (x i,j , xm,n)g(xm,n).<label>(1)</label></formula><p>Here, a pairwise function f computes the similarity between the pixel representations x i,j , x ∈ R H×W ×C , at the position (i, j) and the pixel representation x m,n at all other possible positions (m, n), producing a spatial attention map over the whole feature maps. The unary function g maps the original representation at position (m, n) to a new domain. In our work, we also take the softmax function as f , i.e.,</p><formula xml:id="formula_1">f (x i,j , xm,n) = softmaxm,n(θ(x i,j ) T θ(xm,n)).<label>(2)</label></formula><p>Thus, given a feature map output from a backbone network such as ResNet <ref type="bibr" target="#b21">[21]</ref>, the self-attention operation firstly uses a 1 × 1 convolution θ to map the feature maps x to a new domain, and then applies dot-product similarity <ref type="bibr" target="#b11">[11]</ref> between every two pixels. Then, using this similarity as the weight, Eq. (1) calculates a weighted global sum over all pixels in the feature maps and outputs a new pixel representation y i,j at the position (i, j).</p><p>It can be seen from Eq. (2) that, the original similarity map is of H × W × H × W dimensions, and is computed as the dot product over the whole feature maps for each pixel.</p><p>Axial Attention, proposed in <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b22">[22]</ref> for NLP and Panoptic Segmentation, has a computation complexity of O(HW 2 + H 2 W ), smaller than the self attention's O(H 2 W 2 ) because its attention is computed within the same column or row only for each pixel. However, it has not yet had a baseline in semantic segmentation. In this work, in order to take the computation complexity advantage of the axial attention, we redefine the axial attention in <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b22">[22]</ref> and convert it to a specialized semantic segmentation model. In the next section, we first formulate the axial attention and then introduce our proposed channelized axial attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Formulation of the Spatial Axial Attention</head><p>In axial attention, the spatial attention map is calculated along the column axis and row axis, respectively. For the convenience of reference, we call the partial attention map calculated along the Y axis as 'column attention' and 'row attention' for the partial attention map calculated along the X axis. For the j-th column attention, the attention similarity tensor is calculated by the similarity between the current position (i, j) and each of the other positions (m, j) in the j-th column (instead of all other positions, as in the self-attention), i.e.,</p><formula xml:id="formula_2">A col (x i,j , x m,j ) = softmaxm θ(x i,j ) T θ(x m,j ) , j ∈ [W ]. 1<label>(3)</label></formula><p>Here, θ represents the learned feature extraction process for the Y axis. Each A col (x i,j , x m,j ) represents the similarity between</p><formula xml:id="formula_3">x i,j and x m,j for i, m ∈ [H], so each x i,j corresponds to H column-attention maps A col (x i,j , x m,j ). Thus, the resultant column attention map A col is a tensor of W × H × H dimensions.</formula><p>Similarly, for the i-th row attention, the similarity attention tensor calculates the similarity between the current position (i, j) and other positions (i, n) in the i-th row, i.e.,</p><formula xml:id="formula_4">Arow(x i,j , x i,n ) = softmaxn φ(x i,j ) T φ(x i,n ) , i ∈ [H],<label>(4)</label></formula><p>where φ represents the learned feature extraction process for the X axis. Similarly, each</p><formula xml:id="formula_5">x i,j corresponds to W row- attention maps A row (x i,j , x i,n ). Thus, the resultant row atten- tion map A row is a tensor of H × W × W dimensions.</formula><p>It is also worth of pointing out that, in Eqs. <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_4">(4)</ref>, the calculations of column and row attention maps both use the same feature x i,j extracted from the backbone module as the input, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. This is different from <ref type="bibr" target="#b22">[22]</ref>, where the row attention map was computed based on the result of the column attention. By using the same feature as the input, the dependency of the final output y i,j on the feature x i,j has been enhanced effectively, instead of using skip connections, as in <ref type="bibr" target="#b22">[22]</ref>.</p><p>With the column and row attention maps A col and A row , the final value weighted by the column and row attention maps can be represented as:</p><formula xml:id="formula_6">y i,j = ∀n Arow(x i,j , x i,n )( ∀m A col (x i,j , x m,j )g(xm,n))<label>(5)</label></formula><p>For the convenience of illustration, we introduce two variables α i,j,m and β i,j,n to capture the intermediate, weighted features, respectively, where</p><formula xml:id="formula_7">α i,j,m = A col (x i,j , x m,j )g(x m,j )<label>(6)</label></formula><p>and</p><formula xml:id="formula_8">β i,j,n = Arow(x i,j , x i,n ) ∀m α i,j,m .<label>(7)</label></formula><p>As illustrated later in Sect. III-C, capturing the intermediate attention results brings opportunity to conduct independent channel attentions for each partial attention result.</p><p>Thus, Eq. (5) can be simplified as:</p><formula xml:id="formula_9">y i,j = ∀n β i,j,n = ∀n Arow(x i,j , x i,n ) ∀m α i,j,m .<label>(8)</label></formula><p>The above Eqs. (6), <ref type="bibr" target="#b6">(7)</ref> and <ref type="bibr" target="#b8">(8)</ref> show that, the computation of the dot product is composed of two steps: 1) The elementwise multiplication for applying the column attention as shown in Eq. (6) and for applying the row attention as shown in Eq. <ref type="bibr" target="#b6">(7)</ref> for column and row attentions, respectively; 2) The summarization of the elements along each row and column according to Eq. <ref type="bibr" target="#b8">(8)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Proposed Channelized Axial Attention</head><p>In order to address the feature conflicting issue of the dual attention and seamlessly combine the advantages of spatial attention and channel attention, we propose a novel Channelized Axial Attention, which takes the intermediate results α i,j,m and β i,j,n in Eqs. <ref type="bibr" target="#b5">(6)</ref> and <ref type="formula" target="#formula_8">(7)</ref> as input.</p><p>Note that, in Eqs. <ref type="formula" target="#formula_7">(6)</ref> and <ref type="formula" target="#formula_8">(7)</ref>, we apply the column and row attention maps in a transposed order. That is to say, the column and row attention results are decomposed along the transposed axis (i.e., decomposing α i,j,m along the row direction and β i,j,n along the column direction), instead of along the column and row, into multiple 3-dimension column or row attention results for different i or j. This is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>This transpositional way of applying the axial attentions not only produces partial column and row attention results with consistent dimensions, but also enables them to capture the dependencies inherent in the other axis so as to conduct channelization in the whole receptive field. Now, we introduce our channelized attentions C col and C row , corresponding to the column attention and row attention, respectively, as: <ref type="bibr" target="#b9">(9)</ref> and Crow(βi,j,n) = Sigmod ReLU( ∀i,n (βi,j,n)</p><formula xml:id="formula_10">C col (αi,j,m) = Sigmod ReLU( ∀m,j (αi,j,m) H × W ωc1)ωc2 αi,j,m</formula><formula xml:id="formula_11">H × W ωr1)ωr2 βi,j,n<label>(10)</label></formula><p>where ω c1 , ω c2 , ω r1 and ω r2 represent the learned relationships between different channels in α i,j,m and β i,j,n , which will be discussed later in Sect. III-E. Thus, instead of directly using α i,j,m and β i,j,n as in Eq. <ref type="formula" target="#formula_9">(8)</ref>, for each column and row, we obtain the seamlessly mixed attention results for spatial channels, where the interme-diate results α i,j,m and β i,j,n are weighted by the channelized axial attention defined in Eqs. <ref type="bibr" target="#b9">(9)</ref> and <ref type="formula" target="#formula_0">(10)</ref> as:</p><formula xml:id="formula_12">y i,j = ∀n Crow Arow(x i,j , x i,n )( ∀m C col (α i,j,m )) .<label>(11)</label></formula><p>The bottom section in <ref type="figure" target="#fig_2">Fig. 2</ref> illustrates the channelization of the column attention at i = H. Later in Sect. IV-B <ref type="table" target="#tab_1">(TABLE II  and</ref>  <ref type="figure">Fig. 5</ref>), we will show with ablation experiments and visualized feature maps the impact of the channelization on improving the performance of the segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Grouped Vectorization</head><p>Computing spatial attention row by row and column by column can save computation but it is still too slow even with parallelization. Vectorization can achieve a very high speed but it has a high requirement on GPU memory for storing the intermediate partial axial attention results α (which has a dimension of H × H × W × C) and β (which has a dimension of W × H × W × C) in Eqs. <ref type="formula" target="#formula_7">(6)</ref> and <ref type="bibr" target="#b6">(7)</ref>. To enjoy the high speed benefit of the vectorized computation with reduced GPU memory usage, in our implementation we propose grouped vectorization to dynamically batch rows and columns into multiple groups, and then perform vectorization for each group respectively. Algorithm 1 shows the pseudo code of implementing the grouped vectorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Going Deeper in Channel Attention</head><p>The channel attention in our method firstly uses a fully connected layer with a smaller rate to compress channels, and then uses another fully connected layer with the same rate as the original channels, followed by a sigmoid function to generate the final channel attention weights. To further boost the performance, we explore the design of more powerful channel attention modules in channelization. </p><formula xml:id="formula_13">Yg ← Channelization (X, Ag), Yg ∈ [H + // G, N, C, W ] 8: end for 9: Y ← Concat(Y0,1,...G), Y ∈ [G, H + // G, N, C, W ] 10: Y ← Reshape Y into [H + , N, C, W ] 11: Y ← Remove padding from Y into [H, N, C, W ] 12: Y ← Transpose Y into [N, C, H, W ] return Y</formula><p>The simplest way of gaining performance is enhancing the representation ability of the neural networks, and it is usually achieved by increasing the depth and width of the networks. Here, we simply add more hidden layers before the last layer. This design allows channel attention to find better relationship between channels and find more important channels for each axial attention's intermediate results. We also find that it is not effective to increase the width (i.e., adding more hidden units to each layer except for the last layer), so we keep the original settings.</p><p>Furthermore, in the spatial domain, each channel of a pixel contains unique information that can lead to unique semantic representation. In our channel attention module, we find that using Leaky ReLU <ref type="bibr" target="#b23">[23]</ref>, instead of ReLU, is more effective in preventing the loss of information along deeper activations <ref type="bibr" target="#b24">[24]</ref>. Apparently, this replacement only works in our channel attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To demonstrate the performance of our proposed CAA, comprehensive experiments are conducted with results compared with the state-of-the-art results on three benchmark datasets, i.e., PASCAL Context <ref type="bibr" target="#b13">[13]</ref>, COCO-Stuff <ref type="bibr" target="#b14">[14]</ref> and Cityscapes <ref type="bibr" target="#b15">[15]</ref>.</p><p>The same as the other existing works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b3">[4]</ref>, we measure the segmentation accuracy using mIOU (Mean Intersection Over Union). Moreover, to demonstrate the efficiency of our CAA, we also report and compare the FLOPS (Floating Point Operations per Second) of different approaches. Note that, a higher mIOU value means more accurate segmentation, whereas a lower FLOPS value indicates less computation operations. Experimental results show that our CAA outperforms the state-of-the-art performance on all tested datasets.</p><p>Moreover, as we have analysed earlier in Sect. II-C, computing spatial attention and channel attention separately and then fusing them together directly can cause conflicting feature representations. This means, channels important for a class in spatial domain may not dominate and therefore can be ignored in the computation of the global channel attention. In our experiments, to illustrate the feature conflicting issue caused by existing dual attention approaches, we design a simple way to visualize the effects of spatial attention and channel attention on pixel representation.</p><p>For the parallel dual attention design such as DANet <ref type="bibr" target="#b0">[1]</ref>, since it has two auxiliary losses for spatial attention and channel attention respectively, we directly use their logits during inference and generate their segmentation results to compare with the result generated by the main logits. For the sequential dual attention design, we add an extra branch that directly uses the pixel representation obtained from channel attention to perform the segmentation logits. Note that, since the original sequential design does not have independent logits of channel attention, we stop the gradient from the main branch to make sure our newly added branch has no effect on the main branch.</p><p>Next, we first present the implementation details. This is followed by a series of ablation experiments conducted on the PASCAL Context dataset showing the effectiveness of each of our proposed ideas. Then, we report the comparative results obtained on PASCAL Context <ref type="bibr" target="#b13">[13]</ref>, COCO-Stuff <ref type="bibr" target="#b14">[14]</ref> and Cityscapes <ref type="bibr" target="#b15">[15]</ref> datasets, respectively. For fair comparison, we only compare with the methods that use ResNet-101 and naive 8× bilinear upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Backbone: Our network is built on ResNet-101 <ref type="bibr" target="#b21">[21]</ref> pretrained on ImageNet. The original ResNet results in a feature map of 1/32 of the input size. Following other similar works <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b5">[6]</ref>, we apply dilated convolution at the output stride = 16 during training for most of the ablation experiments. We conduct experiments with the output stride = 8 during training to compare with the state of the arts.</p><p>Segmentation Head: We use a 3×3 convolution to reduce the number of feature map channels from 2,048 to 512 <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, which is then followed by our proposed Channelized Axial Attention module. Note that, our Axial Attention generates the column attention map and the row attention map from the same feature maps, instead of generating one based on the computation results of the other, as in <ref type="bibr" target="#b22">[22]</ref>. Also, after the computation of the attention maps, we do not add the original pixel representations to the resultant feature maps. In the end, we directly upsample our logits to the input size by applying bilinear interpolation.</p><p>Training Settings: We employ SGD (Stochastic Gradient Descent) for optimization, where the poly decay learning rate policy (1 − iter maxiter ) 0.9 is applied with an initial learning rate = 0.007. We use synchronized batch normalization during training. Our experiments are conducted on TPUv3 and V100. For data argumentation, we only apply the most basic data argumentation strategies in <ref type="bibr" target="#b9">[9]</ref> including random flip, random scale and random crop, same as in the other works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on PASCAL Context Dataset</head><p>PASCAL Context <ref type="bibr" target="#b25">[25]</ref> dataset has 59 classes with 4,998 images for training and 5,105 images for testing. We train the network model on PASCAL Context Training set with the batch size = 16 with 70k iterations. During training , we set the output stride = 16 and use an output stride = 8 for  <ref type="bibr" target="#b6">[7]</ref> 48.12 +0G ASPP <ref type="bibr" target="#b9">[9]</ref> 50.47 +16.7G Non-Local <ref type="bibr" target="#b11">[11]</ref> 50.42 +11.18G Redefined Axial Attention 50.27 (±0. <ref type="bibr" target="#b1">2)</ref> +8.85G</p><p>inference. Later in TABLE VI, we present our CAA results with an output stride = 16 and 8, where it can be seen clearly a 1.4% increase can be observed with the output stride = 8. Next, we first present a series of ablation experiments conducted on the PASCAL Context dataset to show the effectiveness of our proposed channelized axial attention. Then, quantitative and qualitative comparisons with the state of the arts are presented.</p><p>1) Axial Attention for Semantic Segmentation: To verify the effectiveness of Axial Attention for semantic segmentation (see Sect. III-B), we compare the mIOU and FLOPS achieved with our channelized Axial Attention with other segmentation heads implemented by us, as shown in TABLE I. Note that our redefined Axial Attention used for semantic segmentation is different from <ref type="bibr" target="#b22">[22]</ref>, as mentioned in Sect. III-A, and in this table we only compare with the methods that are independent with backbone <ref type="bibr" target="#b21">[21]</ref>. Also, all results in this table are obtained with an output stride = 16.</p><p>From TABLE I, we can easily see that our redefined Axial Attention improves mIOU a lot compared to the Dilation-FCN (50.27 vs 48.12), which has a naive segmentation head. The mIOU obtained with our redefined axial attention is also comparable with other approaches, such as ASPP <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[9]</ref> and Non-Local <ref type="bibr" target="#b11">[11]</ref>. However, the redefined axial attention has much lower FLOPS than the original self-attention <ref type="bibr" target="#b11">[11]</ref> (an increase of 8.85G vs 11.18G over the baseline), which demonstrates that the redefined axial attention for semantic segmentation can achieve comparable performance with the original self-attention at much lower computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Examples of Conflicting Feature Representations:</head><p>To visualize the impact of the feature conflicting issue of the existing dual attention designs (see Sect. II-C), <ref type="figure" target="#fig_4">Fig. 3</ref> shows two groups of examples of the segmentation results obtained with the conflicting features in the parallel dual attention design (see <ref type="figure" target="#fig_3">Figs. 3a and 3b</ref>) and the sequential dual attention design (see <ref type="figure" target="#fig_4">Fig. 3c</ref>).</p><p>As it can be observed from <ref type="figure" target="#fig_3">Figs. 3a and 3b</ref>, the parallel design of dual attentions directly sums up the pixel representations obtained from spatial attention and channel attention. With this approach, the advantages of the pixel representations obtained from one can be weakened by the other.</p><p>The sequential way of combining the dual attentions avoids taking their average but still has its issue. As shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, the pixel representation obtained from the spatial attention abandons the correct pixel representation obtained from the channel attention, and worsens the prediction result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground truth Spatial Attention Channel Attention Prediction Image</p><p>Ground truth Spatial Attention Channel Attention Prediction   3) Effectiveness of the Proposed Channelization: We then use our proposed channelized dot product to replace the naive dot product in Axial Attention (see Sect. III-C). We report the impact of adding channelized dot product and with different depth and width in TABLE II, where '-' for the baseline result indicates no channelization is performed.</p><p>As it can be seen from this table, our proposed channel-  ization improves the mIOU performance over the baseline regardless of the layer counts and the number of channels used. In particular, a best performance is achieved when the Layer Counts = 5 and the number of Channels = 128.</p><p>We also compare our model with the sequential design of Axial Attention + SE, as shown in TABLE III. We repeated the experiments many times but found the sequential design only brings slightly contributions on the performance, indicating that our purposed channelization method can combine the advantages of both spatial attention and channel attention effectively. Also note that, <ref type="figure" target="#fig_1">Fig. 1(b)</ref> shows a failure result of the sequential design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Channelized Self-Attention:</head><p>In this section, we conduct additional experiments on the PASCAL Context testing set by applying channelization to the original self-attention. We report its single-scale performance in TABLE IV with ResNet-101 <ref type="bibr" target="#b21">[21]</ref>.</p><p>We can see from the table that our proposed channelized method can further improve the performance of self-attention slightly by 0.67%. It also shows the current channelized design is more effective for our Axial Attention (0.79% vs 0.67%). 5) Impact of the Testing Strategies: We report and compare the performance and computation cost of our proposed model against the baseline and the DANet with different testing strategies. This is shown in TABLE V. Same as the settings in other works <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b0">[1]</ref>, we add multi-scale, left-right flip and aux loss <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b0">[1]</ref> during inference. Note that, in this table, we report the mean mIOU figures with a dynamic range to show the stability of our algorithm. As it shows in this table, We found our proposed CAA can be further boosted with OS = 8 since the channel attention can learn and optimize three times more pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6)</head><p>Comparison with the State of the Arts: Finally, we compare our proposed approach with the state-of-the-art approaches. The results on the PASCAL Context dataset is shown in TABLE VI. Like other similar works, we apply multi-scale and left-right flip during inference. For fair comparison, we only compare with the methods that use ResNet-101 and naive decoder (directly upsampling logits). Also note that, in this and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth FCN Dual Attention Ours Image</p><p>Ground Truth FCN Dual Attention Ours <ref type="figure">Fig. 4</ref>: Examples of the segmentation results obtained on the PASCAL Context dataset <ref type="bibr" target="#b25">[25]</ref> with our proposed CAA approach in comparison to the results obtained with FCN <ref type="bibr" target="#b6">[7]</ref>, DANet <ref type="bibr" target="#b0">[1]</ref> and the ground truth. All results are inferenced with an output stride of 8.</p><p>the following tables, we report the best results of our approach obtained in experiments. As shown in this table, our proposed CAA method achieves the highest score in the methods trained with an output stride = 16 with ResNet-101 and naive decoder, and even outperforms some methods trained with an output stride = 8. Moreover, after we train our model with an output stride = 8, the performance of our model has been further improved and outperforms all of the state-of-the-art models, including the ones recently published in CVPR2019 and CVPR2020.</p><p>In <ref type="figure">Fig. 4</ref>, we provide the visualizations of the prediction results obtained with our CAA model in comparison with the state-of-the-art approaches. As shown in the figure, our model is able to segment objects very well without requiring any post-processing.</p><p>To further demonstrate the effectiveness of our proposed channelization, in <ref type="figure">Fig. 5</ref> we visualize the feature maps obtained after applying the column attention and row attention maps and the difference between the corresponding feature maps with and without applying the channel attentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7) Alternative Backbones:</head><p>In previous sections, we have reported our CAA's performance using ResNet-101 <ref type="bibr" target="#b21">[21]</ref> as backbone, which is widely used in semantic segmentation <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b18">[18]</ref>. In this section, we conduct additional experiments on Pascal Context by attaching our CAA module with some other backbones. We report our results obtained with single scale without flipping in TABLE VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8) Result with EfficientNet:</head><p>As mentioned in Sect. IV-B6, our CAA outperforms the SOTA methods <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b5">[6]</ref> with the same settings (ResNet-101 w/o decoder). Furthermore, TABLE VII shows the universality of our proposed CAA with different backbones. In this section, we report our CAA's performance with EfficientNet-B7 <ref type="bibr" target="#b33">[33]</ref> in TABLEVIII. Note that, this is not a fair comparison, since the listed methods  <ref type="figure">Fig. 5</ref>: Visualization of the feature maps (α i,j,m , C col (α i,j,m ), β i,j,n and C row (β i,j,n )) on PASCAL Context <ref type="bibr" target="#b25">[25]</ref> (top two rows) and Cityscapes <ref type="bibr" target="#b15">[15]</ref> (bottom two rows). For each input image, we list the feature maps obtained after applying the column attention map and row attention map, the difference between the corresponding feature maps with and without applying the channel attentions, as well as our prediction and the ground truth segmentation, respectively For more details, please refer to Sect. 2.  <ref type="bibr" target="#b26">[26]</ref> 51.7 CVPR2018 Deeplab <ref type="bibr" target="#b9">[9]</ref> 52.7 ECCV2018 ANNet <ref type="bibr" target="#b18">[18]</ref> 52.8 ICCV2019 EMANet <ref type="bibr" target="#b5">[6]</ref> 53.1 ICCV2019 SVCNet <ref type="bibr" target="#b27">[27]</ref> 53.2 CVPR2019 SPYGR <ref type="bibr" target="#b28">[28]</ref> 52.8 CVPR2020 CPN <ref type="bibr" target="#b29">[29]</ref> 53.9 CVPR2020 CFNet <ref type="bibr" target="#b30">[30]</ref> 54.0 CVPR2019</p><p>DANet <ref type="bibr" target="#b0">[1]</ref> 52.6 CVPR2019</p><p>Our CAA (OS = 16) 53.7 -Our CAA (OS = 8) 55.0 were not trained under the same settings, or using the same backbone. The results show that our method can still improve the performance even with a strong CNN backbone Efficientnet-B7, and outperform the latest Transformer <ref type="bibr" target="#b34">[34]</ref> based hybrid models such as SETR <ref type="bibr" target="#b35">[35]</ref> and DPT <ref type="bibr" target="#b36">[36]</ref>.</p><p>C. Results on the COCO-Stuff 10K Dataset 1) Comparison with the State of the Arts: Following the other works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b0">[1]</ref>, we demonstrate that our model can handle complex images with a large number of classes. We further evaluate our model on the COCO-Stuff 10K dataset <ref type="bibr" target="#b14">[14]</ref>, which contains 9,000 training images and 1,000 testing images, as shown in TABLE IX. As it can been from the table, our proposed CAA outperforms all other state-ofthe-art approaches by a large margin of 1.3%.</p><p>We also report results obtained with our CAA with Efficientnet-b7 <ref type="bibr" target="#b33">[33]</ref> in TABLE X.</p><p>2) Visualization of the Segmentation Results: <ref type="figure">Fig. 6</ref> show some examples of the segmentation results obtained on the COCO-Stuff 10K dataset <ref type="bibr" target="#b14">[14]</ref> with our proposed CAA in comparison to the results of FCN <ref type="bibr" target="#b6">[7]</ref>, DANet <ref type="bibr" target="#b0">[1]</ref> and the   <ref type="bibr" target="#b27">[27]</ref> 56.2 ResNeSt-269 [37] + DeepLab V3+ <ref type="bibr" target="#b9">[9]</ref> 58.9 HRNetV2 + OCR + RMI <ref type="bibr" target="#b3">[4]</ref> 59.6 DPT-Hybrid <ref type="bibr" target="#b36">[36]</ref> 60.46</p><p>Our CAA (EfficientNet-B7, w/o decoder) 60.12 Our CAA (EfficientNet-B7 + simple decoder <ref type="bibr" target="#b9">[9]</ref>) 60.50 ground truth. All results are inferenced with an output stride of 8. As it can be seen, our CAA can segment common objects such as building, human, or sea very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on the Cityscapes Dataset</head><p>The Cityscapes dataset <ref type="bibr" target="#b15">[15]</ref> has 19 classes. Its fine set contains high quality pixel-level annotations of 5,000 images, where there are 2,975, 500 and 1,525 images in the Training, Validation, and Test sets, respectively. Like other works <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b0">[1]</ref>, We only use fine set with a crop size 769×769 during training, and our training iteration is set to 90k. We report our results on test set in TABLE XI and also visualize our feature maps and results in <ref type="figure">Fig. 5</ref> (the bottom two rows).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effectiveness of Our Grouped Vectorization</head><p>In Sect. III-D, we developed the grouped vectorization to split tensors into multiple groups so as to reduce the GPU memory usage when preforming channel attention in Eqs. <ref type="bibr" target="#b9">(9)</ref> and <ref type="bibr" target="#b10">(10)</ref>. The more groups used in group vectorization, the proportionally less GPU memory is needed for the computation, yet with longer inference time. In this section, we conduct experiments to show the variation of the inference  <ref type="bibr" target="#b27">[27]</ref> 39.6 CVPR2019 EMANet <ref type="bibr" target="#b5">[6]</ref> 39.9 ICCV2019 SPYGR <ref type="bibr" target="#b28">[28]</ref> 39.9 CVPR2020 OCR <ref type="bibr" target="#b3">[4]</ref> 39.5 ECCV2020</p><p>DANet <ref type="bibr" target="#b0">[1]</ref> 39.7 CVPR2019</p><p>Our CAA 41.2 - time (seconds/image) when different numbers of groups are used in group vectorization. <ref type="figure">Fig. 7</ref> shows the results where three different input resolutions are tested. As shown in this graph, when splitting the vectorization into smaller numbers of groups, e.g., 2 or 4, our grouped vectorization can achieve comparable inference speed with one half or one quarter of the original spacial complexity.</p><p>V. CONCLUSION In this paper, aiming to combine the advantages of the popular spatial-attention and channel attention, we have proposed a novel and effective Channelized Axial Attention approach for semantic segmentation. After computing column and row attentions, we proposed to capture the intermediate results and perform the corresponding channel attention on each of them. Our proposed approach of applying the column and row attentions transpositionally has allowed the channelization to be conducted in the whole respective field. Experiments on the three popular benchmark datasets have demonstrated the superiority and effectiveness of our proposed channelized axial attention in terms of both segmentation performance and computational complexity.</p><p>TABLE VII shows that both our Axial Attention and Channelization approaches have improved the mIOU of the baseline in multiple well-know backbones. We also find that our Channelization approach is more effective with ResNet and EfficientNet, whereas the improvement on Xception65 is relatively small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground truth FCN Dual Attention Ours Image</p><p>Ground truth FCN Dual Attention Ours <ref type="figure">Fig. 6</ref>: Examples of the segmentation results obtained on the COCO-Stuff 10K dataset <ref type="bibr" target="#b14">[14]</ref> with our proposed CAA approach in comparison to the results obtained with FCN <ref type="bibr" target="#b6">[7]</ref>, DANet <ref type="bibr" target="#b0">[1]</ref> and the ground truth. All results are inferenced with an output stride of 8. For fair comparison, we only compare with the methods that use ResNet-101 and naive decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods mIOU% Ref</head><p>PSPNet <ref type="bibr" target="#b8">[8]</ref> 78.4 CVPR2017 CFNet <ref type="bibr" target="#b30">[30]</ref> 79.6 CVPR2019 ANNet <ref type="bibr" target="#b18">[18]</ref> 81.3 ICCV2019 CCNet <ref type="bibr" target="#b17">[17]</ref> 81.4 ICCV2019 CPN <ref type="bibr" target="#b29">[29]</ref> 81.3 CVPR2020 SPYGR <ref type="bibr" target="#b28">[28]</ref> 81.6 CVPR2020 OCR <ref type="bibr" target="#b3">[4]</ref> 81.8 ECCV2020</p><p>DANet <ref type="bibr" target="#b0">[1]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="81.5">CVPR2019</head><p>Our CAA 82.6 -</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Different dual attention designs pixel representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>The detailed architecture of our proposed Channelized Axial Attention model. To obtain H × W × C inputs for the channel attention, we apply the resultant column and row attentions in a transposed way. The bottom section illustrates the channelization of the column attention for i = H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>The bad channel attention representation negatively influences the good spatial attention representation. See the highlighted areas. The bad spatial attention representation negatively influences the good channel attention representation. See the highlighted areas. The spatial attention representation abandons the correct channel attention representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Examples of conflicting feature representation in the parallel (a and b) and sequential dual attention (c) designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>Our proposed grouped vectorization algorithm Require: G: Group Number, A: Attention Map [N, H, H, W ], X: Feature Map [N, C, H, W ] 1: padding ← H % G 2: A ← Transpose A into [H, N, H, W ] 3: H + ← H + padding 4: A ← padding zero to A into [H + , N, H, W ] 5: A ← Reshape A into [G, H + // G, N, H, W ] 6: for g ∈ G do</figDesc><table><row><cell>7:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Comparison results with different segmentation heads in the PASCAL Context dataset<ref type="bibr" target="#b25">[25]</ref> </figDesc><table><row><cell>Methods</cell><cell>mIOU%</cell><cell>FLOPS</cell></row><row><cell>ResNet-101 [21]</cell><cell>-</cell><cell>59.85G</cell></row><row><cell>FCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="7">: Result comparison without using channelization</cell></row><row><cell cols="9">(Row 1) and using channelization with different layer counts</cell></row><row><cell cols="4">and channel numbers.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="2">Layer Counts 3 5</cell><cell>7</cell><cell cols="3"># of Channels 64 128 256</cell><cell>mIOU%</cell><cell>FLOPS</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.27(±0.2)</cell><cell>68.7G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.75(±0.2)</cell><cell>+0.00024G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.85(±0.2)</cell><cell>+0.00027G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51.06(±0.2)</cell><cell>+0.00030G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.40(±0.3)</cell><cell>+0.00043G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.12(±0.2)</cell><cell>+0.00015G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.35(±0.4)</cell><cell>+0.00098G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Result comparison between axial attention, channelized axial attention and axial attention + SE<ref type="bibr" target="#b10">[10]</ref> </figDesc><table><row><cell cols="2">Axial Attention + Channelization</cell><cell>+ SE</cell></row><row><cell>50.27(±0.2)</cell><cell>51.06(±0.2)</cell><cell>50.37(±0.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation study of applying our Channelized Attention on self-attention with ResNet-101<ref type="bibr" target="#b21">[21]</ref>. Eval OS: Output strides<ref type="bibr" target="#b9">[9]</ref> during evaluation.</figDesc><table><row><cell>Attention Base</cell><cell>Eval OS</cell><cell>Channelized mIOU%</cell></row><row><cell>Axial Attention</cell><cell>16 16</cell><cell>50.27 51.06</cell></row><row><cell>Self Attention</cell><cell>16 16</cell><cell>50.42 51.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Comparison results with different testing strategies. Train OS: Output stride in training. Eval OS: Output stride in inference. MS: Apply multi-scale during inference. Aux loss: Add auxiliary loss during training. "+" refers to the FLOPS over the baseline FLOPS of ResNet-101.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Methods</cell><cell cols="2">Train OS 16 8</cell><cell cols="2">Eval OS 16 8</cell><cell cols="2">Strategies MS flip</cell><cell>Aux Loss</cell><cell cols="2">mIOU%</cell><cell cols="2">FLOPs</cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet-101 [21]</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">59.85G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">190.70G</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DANet [1]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">+101.25G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.60</cell><cell></cell><cell>-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Our CAA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">51.06(±0.2)</cell><cell cols="2">+8.85G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">53.09(±0.3)</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Our CAA + Aux loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">51.80(±0.2)</cell><cell cols="2">+8.85G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">53.52(±0.2)</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">53.48(±0.3)</cell><cell cols="2">+34.33G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">54.65(±0.4)</cell><cell>-</cell><cell></cell></row><row><cell>Image</cell><cell>α</cell><cell>, , i j m</cell><cell>Difference</cell><cell></cell><cell>col C α (</cell><cell>, , i j m</cell><cell>)</cell><cell></cell><cell>, , i j n</cell><cell>Difference</cell><cell>row C  (</cell><cell>i j n , ,</cell><cell>)</cell><cell>Prediction</cell><cell>Ground Truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Result comparison with the state-of-the-art approaches on the PASCAL Context testing set for multi-scale prediction. For fair comparison, we only compare with the methods that use ResNet-101 and naive decoder.</figDesc><table><row><cell>Methods</cell><cell>mIOU%</cell><cell>Ref</cell></row><row><cell>FCN [7]</cell><cell>50.8</cell><cell>CVPR2015</cell></row><row><cell>ENCNet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Ablation study of applying our Channelized Axial Attention to other backbones. All results are obtained in single scale without flipping. Axial Attention: Using our Axial Attention after backbone. Channelized: Applying our Channelized approach. Eval OS: Output strides<ref type="bibr" target="#b9">[9]</ref> during evaluation.</figDesc><table><row><cell>Backbone</cell><cell>Eval OS</cell><cell>Axial Attention</cell><cell>Channelized</cell><cell>mIOU%</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>46.92</cell></row><row><cell>ResNet-50 [21]</cell><cell>16</cell><cell></cell><cell></cell><cell>49.73</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>50.23</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>48.12</cell></row><row><cell>ResNet-101 [21]</cell><cell>16</cell><cell></cell><cell></cell><cell>50.27</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>51.06</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>49.40</cell></row><row><cell>Xception65 [32], [9]</cell><cell>16</cell><cell></cell><cell></cell><cell>52.42</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>52.65</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>56.80</cell></row><row><cell>EfficientNetB7 [33]</cell><cell>16 16</cell><cell></cell><cell></cell><cell>57.24 57.93</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell></cell><cell>58.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII :</head><label>VIII</label><figDesc>Result comparison with the state-of-the-art approaches on the PASCAL Context testing set for multi-scale prediction. Note that, this is not a fair comparison, since all listed methods were not trained under same settings, or using same backbone.</figDesc><table><row><cell>Methods</cell><cell>mIOU%</cell></row><row><cell>SETR-MLA [35]</cell><cell>55.83</cell></row><row><cell>HRNetV2 + OCR</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX :</head><label>IX</label><figDesc>Comparison results with the state-of-the-art approaches on the COCO-Stuff 10K testing set for multi-scale prediction. For fair comparison, we only compare with the methods that use ResNet-101 and naive decoder.</figDesc><table><row><cell>Methods</cell><cell>mIOU%</cell><cell>Ref</cell></row><row><cell>DSSPN [38]</cell><cell>38.9</cell><cell>CVPR2018</cell></row><row><cell>SVCNet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE X :</head><label>X</label><figDesc>Result comparison with the state-of-the-art approaches on the COCO-Stuff-10K testing set for multi-scale prediction. Note that, this is not a fair comparison, since all listed methods were not trained under same settings, or using same backbone.</figDesc><table><row><cell>Methods</cell><cell>mIOU%</cell></row><row><cell>HRNetV2 + OCR [27]</cell><cell>40.5</cell></row><row><cell>DRAN</cell><cell>41.2</cell></row><row><cell>HRNetV2 + OCR + RMI [4]</cell><cell>45.2</cell></row><row><cell>Our CAA (EfficientNet-B7)</cell><cell>45.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XI :</head><label>XI</label><figDesc>Comparison results with other state-of-the-art approaches on the Cityscapes Test set for multi-scale prediction.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use i ∈ [n] to denote that i is generated from [n] = {1, 2, ..., n}.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Expectationmaximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">7: Inference time (seconds/image) when applying different numbers of groups in grouped vectorization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Wiliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Mass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic correlation promoted shape-variant context for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial pyramid based graph reasoning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic correlation promoted shape-variant context for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Squeezeand-attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bidart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Daya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mingxing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sixiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiachen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hengshuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiatian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zekun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yabiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yanwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P H S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic-structured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
