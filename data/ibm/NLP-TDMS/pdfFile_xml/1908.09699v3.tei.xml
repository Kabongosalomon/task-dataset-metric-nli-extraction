<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Convolutional Networks with Hybrid Connectivity for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanguang</forename><surname>Yang</surname></persName>
							<email>yangchuanguang@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhulin</forename><surname>An</surname></persName>
							<email>anzhulin@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhu</surname></persName>
							<email>zhuhui@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Hu</surname></persName>
							<email>huxiaolong18g@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
							<email>zhangkun17g@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Xu</surname></persName>
							<email>xukaiqiang@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
							<email>lichao@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gated Convolutional Networks with Hybrid Connectivity for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a simple yet effective method to reduce the redundancy of DenseNet by substantially decreasing the number of stacked modules by replacing the original bottleneck by our SMG module, which is augmented by local residual. Furthermore, SMG module is equipped with an efficient twostage pipeline, which aims to DenseNet-like architectures that need to integrate all previous outputs, i.e., squeezing the incoming informative but redundant features gradually by hierarchical convolutions as a hourglass shape and then exciting it by multi-kernel depthwise convolutions, the output of which would be compact and hold more informative multi-scale features. We further develop a forget and an update gate by introducing the popular attention modules to implement the effective fusion instead of a simple addition between reused and new features. Due to the Hybrid Connectivity (nested combination of global dense and local residual) and Gated mechanisms, we called our network as the HCGNet. Experimental results on CIFAR and ImageNet datasets show that HCGNet is more prominently efficient than DenseNet, and can also significantly outperform state-of-the-art networks with less complexity. Moreover, HCGNet also shows the remarkable interpretability and robustness by network dissection and adversarial defense, respectively. On MS-COCO, HCGNet can consistently learn better features than popular backbones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep convolutional neural networks (CNNs) are becoming more and more efficient in parameter and computation without sacrificing the performance owing to novel architectures design. ResNet <ref type="bibr" target="#b5">(He et al. 2016)</ref> introduces the residual connectivity to implement the addition of the input and output features for each micro-block. DenseNet <ref type="bibr" target="#b9">(Huang et al. 2017)</ref> holds the dense connectivity by changing skip connections from addition to concatenation. Both of their feature aggregation connectivities can not only encourage feature reuse, but also ease the training problems. For a detailed comparison, dense connectivity is more effect for feature exploitation and exploration but exists a certain redundancy, while residual connectivity contributes to efficient feature reuse by <ref type="figure">Figure 1</ref>: The diagram of a hybrid block including n = 2 modules, where n 2. The symbol "+" and " " denote element-wise addition and channel-wise concatenation among multiple feature maps, respectively. parameter sharing mechanism and thus leads to low redundancy, but lacks the capability of feature preservation and exploration. To enjoy their advantages and avoid inherent limitations, many networks combine them to build a more effective aggregation topology, such as DPN <ref type="bibr" target="#b2">(Chen et al. 2017)</ref>, MixNet <ref type="bibr" target="#b20">(Wang et al. 2018a</ref>) and AOGNet <ref type="bibr" target="#b12">(Li, Song, and Wu 2019)</ref>. Different from them, we develop a hybrid connectivity ( <ref type="figure">Figure 1</ref>) with nested aggregation that facilitates feature flow by dense connectivity for global channelwise concatenation of outputs produced by all precedent modules (blue links in <ref type="figure">Figure 1</ref>) and residual connectivity for local element-wise addition within the module (red links in <ref type="figure">Figure 1</ref>).</p><p>Our main motivation for this pattern design originates from reducing the redundancy of dense connectivity. As the depth of network linearly increases, the number of skip connections and required parameters grow by a rate of O(n 2 ), where n denotes the number of stacked modules under dense connectivity. Meanwhile, early superfluous features which have few contributions are transferred quadratically to subsequent modules. So one simple method to reduce redundancy is to decrease the number of modules directly, but it can attenuate the representational power of features and then deteriorate the performance. Thus we develop a novel module by embedding the residual connectivity to assist feature learning within the local module. Experimentally, the number of our proposed modules under dense connectivity can be quite fewer than that of classical modules in the dense block but without sacrificing the performance.</p><p>For further adaptation with hybrid connectivity, we instantiate the basic module that includes a squeeze cell (cell 1 in <ref type="figure">Figure 1</ref>) for transforming the input to a compact feature map, and a multi-scale excitation cell (cell 2 in <ref type="figure">Figure 1</ref>) to further extract multi-scale features by multi-kernel convolutions. It is widely known that convolution builds pixel relationship in a local neighborhood, which leads to ineffective modeling of long-range dependency. To fully address this issue, we develop an update gate to model the global context features from more informative multi-scale features. Moreover, we locate a forget gate on the residual connection to capture channel-wise dependency for decaying the reused features produced by cell 1. Finally, global context features are added to the reused feature map of each spatial position to form the output, which can not only promote effective feature exploration but also retain the capability of feature re-exploitation to some extent. Moreover, both forget gate and update gate are lightweight and general plug-ins, which can be integrated into any CNNs with negligible overheads.</p><p>We perform extensive experiments across the three highly competitive image classification datasets: CIFAR-10/100 <ref type="bibr" target="#b11">(Krizhevsky and Hinton 2009)</ref>, and ImageNet (ILSVRC 2012) <ref type="bibr" target="#b3">(Deng et al. 2009</ref>). On CIFAR datasets, HCGNets outperform state-of-the-art both human-designed and autosearched networks but only requiring extremely fewer parameters, e.g., HCGNet-A3 obtains the better result than the most competitive NASNet-A ) with 4.5× fewer parameters. On ImageNet datasets, it also consistently obtains the best accuracy, interpretability, robustness based on classification and transferability to object detection as well as segmentation among the widely used networks with least complexity, e.g., HCGNet-B outperforms previous SOTA AOGNet across a broad range of tasks with similar complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Improvements of ResNet and DenseNet. ResNeXt <ref type="bibr" target="#b23">(Xie et al. 2017)</ref> outperforms ResNet with less overheads since it adopts 3×3 group convolutions in residual blocks. Afterwards, group convolutions become popular in efficient CNNs design due to the properties of lower parameter and computational cost, including our HCGNets. Wide ResNet <ref type="bibr" target="#b25">(Zagoruyko and Komodakis 2016)</ref> show that increasing width while decreasing depth of residual networks can surpass very deep counterparts, meanwhile tackling the problems of slow training and weakened feature reuse. By representing the multi-scale features and widening the receptive fields (RF) within the residual block, Res2Net <ref type="bibr" target="#b4">(Gao et al. 2019</ref>) outperforms the other backbones across a broad range of tasks. Multi-scale information has been widely demonstrated a effective way to improve the performance, our HCGNet also constructs the multi-scale features by multibranch convolutions.</p><p>It is widely known that DenseNet has a certain redundancy, thus a typical practice is sparsification. Log-DenseNet <ref type="bibr" target="#b7">(Hu et al. 2017)</ref> and SparseNet <ref type="bibr" target="#b27">(Zhu et al. 2018)</ref> regularly conduct a sparse rather than full aggregation of all previous outputs, which change the number of connections from linear to be logarithmic in the overall topology.</p><p>Learned group convolutions are adopted in CondenseNet  to automatically prune unimportant channels for the incoming feature map based on channelwise L1-norm. However, excessive sparsification affects the superiority of collective learning. Thus we only decrease the number of modules under dense connectivity to reduce redundancy, which is empirically more effective than sparsification.</p><p>Combinations of ResNet and DenseNet. To enjoy the advantages and avoid drawbacks of both two connectivities, many combinations have proposed. DPN adopts dual path architectures, which can facilitate effective feature reuse by residual path and feature exploration by dense path in parallel. MixNet blends two connectivities to implement feature aggregation with more flexible positions and sizes, further ResNet, DenseNet and DPN can be treated as particular cases of MixNet. Recently proposed AOGNet utilizes AND-OR Grammar to generate CNNs by parsing feature map as a sentence, where AND-node denotes channel-wise concatenation and OR-node denotes element-wise addition. It demonstrates that the compositional and hierarchical aggregation in AOGNet is more effective than cascade-based way in DPN. Moreover, addition and concatenation as the meta-operations are also widely applied in the field of neural architecture search, such as NASNet, PNASNet  and AmoebaNet <ref type="bibr" target="#b15">(Real et al. 2019)</ref>. Extensive experiments indicate that the nested way for feature aggregation in our HCGNets perform the best.</p><p>Attention Mechanisms Attention has been widely applied in computer vision, e.g., image classification <ref type="bibr" target="#b19">(Wang et al. 2017)</ref>. SENet <ref type="bibr" target="#b8">(Hu, Shen, and Sun 2018)</ref> introduces a lightweight gate to capture channel-wise dependencies for rescaling channel features. SKNet ) further employs a dynamic kernel selection attention for weighted multi-scale features fusion, which is inspired by Inception-Nets <ref type="bibr" target="#b18">(Szegedy et al. 2017)</ref>. Beyond channel, CBAM <ref type="bibr" target="#b22">(Woo et al. 2018</ref>) also constructs a spatial attention map to recalibrate spatial features. To capture long-range dependency, GCNet <ref type="bibr" target="#b1">(Cao et al. 2019</ref>) simplifies non-local block <ref type="bibr" target="#b21">(Wang et al. 2018b</ref>) to implement query-independent context modeling based on single branch information. Different from them in roles or mechanisms, we build a forget gate to capture channel-wise dependency for decaying the reused features, while an update gate fully models the global context features from multi-scale information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Revisiting ResNet and DenseNet</head><p>We revisit the classical ResNet and DenseNet with their individual residual connectivity and dense connectivity, and further investigate their mechanisms of parameter sharing and feature learning. Finally, we analyse the overall efficiency of ResNet and DenseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Sharing</head><p>Intuitively, residual connectivity implicitly accompanies a parameter sharing mechanism between the reused and newly extracted features by processing their mixed features. We now formally describe why the parameter sharing mechanism can take place in residual connectivity but not in dense connectivity. Concretely, we use F to denote the bottleneck unit. Consider the input feature map x l−1 ∈ R H×W ×C to the l-th residual block, corresponding formula is as follows:</p><formula xml:id="formula_0">x l = x l−1 + F l (x l−1 ; W l ) = x l−1 +x l (1)</formula><p>Where x l−1 can be considered as the reused feature map, W l and F l (x l−1 ; W l ) refer to convolutional weights and newly extracted feature map, respectively.x l ∈ R H×W ×C represents F l (x l−1 ; W l ) for simplicity. Afterwards, x l becomes a new input for the next residual block to proceed the transformation:</p><formula xml:id="formula_1">x l+1 = x l + F l+1 (x l ; W l+1 ) = x l + F l+1 (x l−1 +x l ; W l+1 )<label>(2)</label></formula><p>In the l + 1-th residual block, x l−1 andx l are shared with the same W l+1 and operations. Similar analysis about dense connectivity is exhibited as follows. Output of the l-th module under dense connectivity can be regarded as the concatenation of input x l−1 ∈ R H×W ×C and newly extracted feature mapx l ∈ R H×W ×C along the channels:</p><formula xml:id="formula_2">x l = x l−1 F l (x l−1 ; W l ) = x l−1 x l<label>(3)</label></formula><p>Then, the next module receives x l ∈ R H×W ×(C+C) and conducts the following transformation:</p><formula xml:id="formula_3">x l+1 = x l F l+1 (x l ; W l+1 ) = x l F l+1 (x l−1 x l ; W l+1 )<label>(4)</label></formula><p>In the l+1-th dense block, x l−1 andx l are not shared with the same W l+1 because of the different locations of feature space between reused and newly extracted feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Learning</head><p>The final output of residual block is the element-wise addition of input and newly extracted feature maps. This addition pattern facilitates efficient feature reuse without increasing the size of feature map thus reducing parameter redundancy. But one potential fact is that too many aggregations by addition may collapse the feature representation and thus impede the information flow, hence some early informative features may be lost inevitably. Moreover, parameter sharing mechanism may damage the capability of exploring new features.</p><p>Subsequently proposed DenseNet develops a global dense connectivity, where the output feature map of each preceding module flows to the all subsequent modules directly. Different from the element-wise addition, input and newly extracted feature maps are combined by concatenation along the channels. Thus dense connectivity can transfer the early feature maps to later modules, which preserves the all preceding feature information and facilitate the full exploitation of existing features. Moreover, various modules with different weights conduct a collective learning for the same features, which can promote effective feature exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Efficiency</head><p>It is widely known that DenseNet-100 with 0.8M parameters slightly outperforms ResNet-1001 with 10.2M parameters on CIFAR10 dataset. The explicit parameter gap is that DenseNet-100 is quite shallower than ResNet-1001 due to the more effective feature exploitation and exploration capabilities produced by collective learning, while ResNet mainly depends on increasing depth to improve the representational power of features. Empirically, DenseNet can also have extremely few number of filters in each convolutional layer due to the collective learning mechanism that further improve the efficiency. However, one potential weakness of dense connectivity is the redundancy of repeated extraction with the same features. In this case, early features flow to all subsequent layers, even if they have few contributions. By contrast, residual connectivity has a relatively low redundancy due to the parameter sharing mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Networks Architecture Hybrid Connectivity Pattern</head><p>We develop a hybrid connectivity pattern, which can enjoy the effective feature learning and few filters of each module from global dense connectivity as well as efficient feature reuse by parameter sharing from local residual connectivity. <ref type="figure">Figure 1</ref> illustrates this pattern within the hybrid block schematically. Note that hybrid connectivity pattern exists in the hybrid block which consists of n (n 2) modules. To match the definition of growth rate in DenseNet, each module produces one feature map with k channels. The basic module consists of successive two cells, which we call them as cell 1 and cell 2, respectively. Globally, input of each module is a concatenation of all feature-maps produced by preceding modules and transferred by dense connectivity. Locally, residual connectivity provides a shortcut that allows the output of cell 1 bypassing cell 2 and then being added to the new features generated by cell 2 to form the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instantiation of Basic Module</head><p>To orchestrate our hybrid connectivity, we design a basic SMG module which includes a Squeeze cell (cell 1), a Multi-scale excitation cell (cell 2) and Gate mechanisms. Unless specified otherwise, each convolution is bound a preactivation, which refers to the three consecutive operations: batch normalization (BN)-rectified linear unit (ReLU)-Conv.</p><p>Squeeze Cell. This cell which locates at the beginning of SMG module is responsible for generating the compact feature map from input to improve parameter and computational efficiency for subsequent processing. 1×1 convolution is firstly adopted for changing the number of input channelsC to α · C , where α &gt; 0 can be reckoned as a width multiplier which is mostly used to reduce the number of channels, i.e.C &gt; α · C , and C denotes the number of final output channels of squeeze cell. Then, 3×3 group convolution (GConv) with g groups proceeds to squeeze the features by reducing the number of channels from α · C to C, where C needs to be divisible by g. Moreover, it can also play a down-sampling by 3×3 kernel with stride S = 2.  <ref type="figure">Figure 2</ref>: Illustrations of SMG module, update gate and forget gate. In all figures, and denote broadcast element-wise addition and multiplication, respectively. We employ feature dimensions to describe the flow of feature maps for better understanding. Note that spatial sizeH ×W = H × W when default stride S = 1 of 3×3 GConv in <ref type="figure">Figure 2a</ref>.</p><formula xml:id="formula_4">onv × × × × 1 × 1 × ෩ × ෩ × ෩ × ෩ × ሚ Conv te gate 5×5 DWConv ale excitation cell : 5×5 : 1×1 Conv Forget gate × × × × × × 1 × 1 × × × × × 1 × 1 × ෩ × ෩ × ෩ × ෩ × ሚ<label>3×3</label></formula><p>Multi-scale Excitation Cell. Squeezed feature map enters this cell for multi-scale excitation by multi-branch convolutions with different kernel sizes. Note that the costs of parameter and computation are extremely cheap because of the few input channels, and the size of feature map throughout this cell is unchanged. To further improve efficiency, we adopt 3×3 and 5×5 depthwise convolutions (DWConv) with 1 and 2 paddings, respectively. Moreover, dilation convolution <ref type="bibr" target="#b24">(Yu and Koltun 2016)</ref> with a kernel size of 3×3 and a dilation size of 2 is used to approximate 5×5 kernel for better trade-off between efficiency and performance. The output of this cell is two-branch feature maps produced by 3×3 and 5×5 DWConvs, respectively.</p><p>Update Gate. To capture long-range dependency , we utilize update gate to model the global context features from multi-scale information. <ref type="figure">Figure 2b</ref> shows the overall details about the update gate, which can be sequentially summarized for 3 stages: spatial attention, pooling and channel attention. spatial attention and pooling: We perform a global context modeling for calculating spatial-wise weights of each position. For the given feature map X 3×3 ∈ R H×W ×C , a 1×1 convolutional filter shrinks it along channel dimensions to a spatial attention mapS 3×3 ∈ R H×W ×1 , an then a softmax function normalizes it to obtain the final spatial attention map S 3×3 ∈ R H×W ×1 , each element of which is as follows:</p><formula xml:id="formula_5">S 3×3 i,j,1 = eS 3×3 i,j,1 H x=1 W y=1 eS 3×3 x,y,1<label>(5)</label></formula><p>We employ global attention pooling via weighted averaging with S 3×3 to shrink the global spatial information and generate the global context feature map z 3×3 ∈ R 1×1×C . The c-th channel of z 3×3 is as follows:</p><formula xml:id="formula_6">z 3×3 c = H x=1 W y=1 X 3×3 x,y,c * S 3×3 x,y,c<label>(6)</label></formula><p>Here, * denotes element multiplication. Based on the above framework, z 5×5 ∈ R 1×1×C can also be obtained by input feature map X 5×5 ∈ R H×W ×C . channel attention: To maintain the integrity of information, we concatenate z 3×3 and z 5×5 as the input. Then it is transformed to a hidden representation h ∈ R 1×1×2 * C/r u , which is always a compact feature map by setting a reduction ratio r u for better efficiency. This is achieved by a fully connected (FC) layer with non-linearity:</p><formula xml:id="formula_7">h = tanh(BN (W[z 3×3 z 5×5 ]) + b)<label>(7)</label></formula><p>Where BN is the batch normalization, W ∈ R 2 * C×2 * C/r u and b ∈ R 2 * C/r denotes the weights and biases of FC layer. It is noteworthy that we adopt tanh rather than ReLU as our non-linearity function. For the one side, ReLU inevitably destroys feature representational power especially in lowdimensional space to a great extent, while tanh preserves information by a smoother way. For the other side, although it is widely known that tanh is more prone to cause gradient vanish as the increasing depth of CNN, this problem could not occur in our HCGNets because of the hybrid connectivity that can significantly strength the gradient backpropagation. Experimental evidence also proves that tanh is more effective than ReLU in our HCGNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-branch FC layers act on fusion representation h to generate two intermediate channel attention mapsũ</head><formula xml:id="formula_8">3×3 ∈ R 1×1×C andũ 5×5 ∈ R 1×1×C : u 3×3 = W 3×3 h + b 3×3 ,ũ 5×5 = W 5×5 h + b 3×3 (8)</formula><p>Where W 3×3 ,W 5×5 ∈ R 2 * C/r u ×C and b 3×3 ,b 5×5 ∈ R C denotes the weights and biases of two FC layers. Then a simple softmax function conducts a normalization betweeñ u 3×3 andũ 5×5 to produce the two final channel attention maps u 3×3 ∈ R 1×1×C and u 5×5 ∈ R 1×1×C :  </p><formula xml:id="formula_9">u 3×3 = eũ 3×3 eũ 3×3 + eũ 5×5 , u 5×5 = eũ 5×5 eũ 3×3 + eũ 5×5<label>(9)</label></formula><formula xml:id="formula_10">v c = u 3×3 c · z 3×3 c + u 5×5 c · z 5×5 c , u 3×3 c + u 5×5 c = 1 (10)</formula><p>Where v c is the c-th channel of the output v ∈ R 1×1×C .</p><p>Forget Gate. To decay the reused feature map by channelwise weights, we locate a forget gate (see <ref type="figure">Figure 2c</ref>) on the residual connection before information fusion. It can also be sequentially summarized for 3 stages: spatial attention, pooling and channel attention. spatial attention and pooling: For the given feature map X ∈ R H×W ×C , we perform the global attention pooling as same as update gate, thus a channel descriptor z f ∈ R 1×1×C can be obtained.</p><p>channel attention: To meet the requirement of weighted decay for each channel, the final output of each channel weight should be within (0, 1), thus we refer SE block, which stacks two continuous FC layers as a bottleneck and is ended by sigmoid function. Different from SE block, we insert a batch normalization layer for easing optimization and replace ReLU with tanh as our non-linearity. In short, the sequent transformations are as follows for the input z f :</p><formula xml:id="formula_11">f = σ(W f 2 (tanh(BN (W f 1 z f + b f 1 ))) + b f 2 )<label>(11)</label></formula><p>Where σ is the sigmoid function,</p><formula xml:id="formula_12">W f 1 ∈ R C×C/r f , b f 1 ∈ R C/r f , W f 2 ∈ R C/r f ×C and b f 2 ∈ R C .</formula><p>r f is the bottleneck ratio and f ∈ R 1×1×C is the final channel attention map.</p><p>Information Fusion. For any given feature map entering SMG module, squeeze cell firstly condenses it to a compact feature map denoted by X . Then X enters multi-scale excitation cell and generate two-branch outputs X 3×3 and X 5×5 by 3×3 and 5×5 DWConvs, respectively. Since then, X can be regarded as the reused features, while X 3×3 and X 5×5 are the newly extracted features. An update gate integrates X 3×3 and X 5×5 to model a global context feature map v and we aggregate it to the decayed X of each spatial position by addition to build the final output O ∈ R H×W ×C . It can be observed that we maintain the magnitude of new features unchanged while decaying reused features, which can facilitate the effective feature exploration and retain the capability of feature re-exploitation to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Macro-architecture.</head><p>As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, at the beginning of HCGNet is a stem, which is a composite function to process the initial input images. Then multiple hybrid blocks are stacked with various spatial stage. Between two adjacent hybrid blocks, we adopt Both hybrid block and transition layer adopt SMG modules but with different hyperparameter settings. We only stack one SMG module to build each transition layer and a compression factor θ = 0.5 is utilized to reduce the number of channels, i.e, C = θC. For each SMG module, we set g = 4, α = 4 and r u = r f = 2 in hybrid blocks, as well as set g = 1, α = 1.5, S = 2 and r u = r f = 4 in transition layers. Note that we apply the standard convolutions in transition layers for best capability of feature extraction and group convolutions in hybrid blocks for better trade-off between efficiency and performance. Compared with the hybrid block, we set less multiplier α and larger reduction ratios r u and r f for better efficiency due to the more channels of feature maps in transition layers.</p><p>Specifically, we construct several networks to act on the image classification across the CIFAR and Ima-geNet datasets. For CIFAR, we adopt a 3×3 standard convolution with stride 1 as the stem that the number of output channels is twice the growth rate of the first hybrid block. And we build three networks with various model specifications: <ref type="figure">HCGNet-(8,8,8</ref>)-(k=12,24,36)(A1), HCGNet-(8,8,8)-(k=24,36,64)(A2) and HCGNet-(12,12,12)-(k=36,48,80)(A3). Formally, the first m-tuple indicates that there are m hybrid blocks, where each figure denotes the number of SMG modules in the corresponding hybrid block. The second m-tuple denotes m growth rates of m hybrid blocks, respectively. For Im-ageNet, the stem consists of three contiguous 3×3 Conv-BN-ReLU layers (stride 2 for the first layer) with 32, 32, 64 output channels, and attached by a 3×3 max pooling with stride 2. We construct two networks: HCGNet-(3,6,12,8)-(k=32,48,64,96)(B, as <ref type="table" target="#tab_2">Table 1</ref>) and HCGNet-(6,12,18,14)-(k=48,56,72,112)(C). test images corresponding to 10 and 100 classes, respectively. We apply a standard data augmentation following <ref type="bibr" target="#b9">Huang et al. (2017)</ref>. We employ a stochastic gradient descent (SGD) optimizer with momentum 0.9 and batch size 128. Training is regularized by weight decay 1 × 10 −4 and mixup with α = 1 . For HCGNet-A1, we train it for 1270 epochs by SGDR <ref type="bibr" target="#b14">(Loshchilov and Hutter 2016)</ref> learning rate curve with initial learning rate 0.1, T 0 = 10, T mul = 2. For HCGNet-A2 and A3, we train them for 1260 epochs including two continuous 630 epochs, each of them is a SGDR learning rate curve with initial learning rate 0.1, T 0 = 10, T mul = 2.</p><p>Comparisons with Human-designed Networks. Quantitatively in <ref type="table" target="#tab_3">Table 2</ref>, DenseNet-190 has 31 modules in each dense block, while HCGNet-A2 only has 8 modules in each hybrid block thus reduces 93% redundancy but with substantial accuracy gains. Moreover, HCGNet-A2 significantly outperforms other sparsification variants, such as LogDenseNet, SparseNet and CondenseNet, which indicate that our optimization of DenseNet is more effective than sparsification method. HCGNet-A2 using 16× fewer parameters surpasses MixNet-190, which represents the most gen-   images corresponding to 1000 classes. We employ the data augmentation following <ref type="bibr" target="#b9">Huang et al. (2017)</ref> Model Interpretability We quantify the interpretability by network dissection, which compares the number of unique detectors in the final convolutional layer. <ref type="figure" target="#fig_1">Figure 4</ref> shows that HCGNet-B obtains the overall highest score with least complexity, which shows that the designs of hybrid connectivity and SMG module can not only achieve the best accuracy, but also generate the best latent representations.</p><p>Adversarial Robustness We attack HCGNet-B by popular FGSM across various perturbation energies to test the adversarial robustness against widely applied models, results of which are shown in <ref type="figure" target="#fig_2">Figure 5</ref>. HCGNet-B has a more remarkable robustness than other models in adversarial defense, especially the perturbation is relatively high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Detection and Instance Segmentation</head><p>To show the transferability, we experiment HCGNet-B pretrained on ImageNet as a backbone on the Mask-RCNN system to implement object detection and instance segmentation tasks. We use COCO train2017 set to finetune the HCGNet-B by the 1x training schedule, and evaluate the performance on COCO val2017 set. We report the results by standard COCO metrics of Average Precision (AP), i.e, AP 50:95 , AP 50 , and AP 75 for bounding box detection (AP bb ) and instance segmentation (AP m ) in <ref type="table" target="#tab_5">Table 4</ref>. The results show that HCGNet-B can learn better features than SOTA ResNet and AOGNet backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>This paper develops an efficient architecture with the innovative designs of hybrid connectivity, micro-module and attention-based forget and update gates. On CIFAR and Im-ageNet datasets, HCGNets outperform state-of-the-art networks with less or comparable complexity. Extensive experiments based on the ImageNet pretrained model further show the remarkable interpretability, robustness for recognition and transferability for detection. We hope our HCGNets may inspire the future study of architectural design and search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>A HCGNet with three hybrid blocks, where each green box denotes SMG module.u 3×3 and u 5×5 can be regarded as the proportions of aggregating multi-scale global context features. Weighted fusion of z 3×3 and z 5×5 is the output of update gate:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Comparisons of interpretability by network dissection (Bau et al. 2017) among popular models based on ImageNet pretrained models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Comparisons of adversarial robustness by FGSM (Goodfellow, Shlens, and Szegedy 2015) attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>HCGNet-B network architecture for ImageNet classification. Each row describes the stage, modules information and input resolution (IR).</figDesc><table><row><cell>Stage</cell><cell>Module</cell><cell>IR</cell></row><row><cell>Stem</cell><cell cols="2">[3×3 Conv-BN-ReLU]×3 224×224 3×3 max pool 112×112</cell></row><row><cell>Hybrid Block</cell><cell>SMG×3 (k = 32)</cell><cell>56×56</cell></row><row><cell>Transition</cell><cell>SMG×1</cell><cell>56×56</cell></row><row><cell>Hybrid Block</cell><cell>SMG×6 (k = 48)</cell><cell>28×28</cell></row><row><cell>Transition</cell><cell>SMG×1</cell><cell>28×28</cell></row><row><cell>Hybrid Block</cell><cell>SMG×12 (k = 64)</cell><cell>14×14</cell></row><row><cell>Transition</cell><cell>SMG×1</cell><cell>14×14</cell></row><row><cell>Hybrid Block</cell><cell>SMG×8 (k = 96)</cell><cell>7×7</cell></row><row><cell>Classification</cell><cell>global average pool 1000D FC, softmax</cell><cell>1×1 -</cell></row><row><cell cols="3">a transition layer to perform down-sampling and connectiv-</cell></row><row><cell cols="3">ity truncation. After the final hybrid block, a global aver-</cell></row><row><cell cols="3">age pooling attached with a softmax classifier calculates the</cell></row><row><cell cols="2">probabilities of various categories.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of our HCGNets against state-ofthe-art networks about test error rates (%) across CIFAR-10 and CIFAR-100 datasets. Note that the first and second blocks contain human-designed and auto-searched architectures, respectively.</figDesc><table><row><cell>Model</cell><cell cols="4">Params FLOPs C-10 C-100</cell></row><row><cell cols="2">CondenseNet-182 4.2M</cell><cell>0.5G</cell><cell cols="2">3.76 18.47</cell></row><row><cell>SparseNet-BC</cell><cell>16.7M</cell><cell>-</cell><cell cols="2">4.10 18.22</cell></row><row><cell>AOGNet</cell><cell>24.8M</cell><cell>3.7G</cell><cell cols="2">3.27 16.63</cell></row><row><cell>LogDenseNetV2</cell><cell>19.0M</cell><cell>11.1G</cell><cell cols="2">3.75 18.80</cell></row><row><cell>Wide ResNet-28</cell><cell>36.5M</cell><cell>5.2G</cell><cell cols="2">4.17 20.50</cell></row><row><cell>ResNeXt-29+SK</cell><cell>27.7M</cell><cell>-</cell><cell cols="2">3.47 17.33</cell></row><row><cell>Res2NeXt-29</cell><cell>36.9M</cell><cell>-</cell><cell>-</cell><cell>16.56</cell></row><row><cell cols="2">DenseNet-BC-190 25.6M</cell><cell>9.4G</cell><cell cols="2">3.46 17.18</cell></row><row><cell>DPN-28-10</cell><cell>47.8M</cell><cell>-</cell><cell cols="2">3.65 20.23</cell></row><row><cell>MixNet-190</cell><cell>48.5M</cell><cell>17.3G</cell><cell cols="2">3.13 16.96</cell></row><row><cell>PNASNet</cell><cell>3.2M</cell><cell>-</cell><cell cols="2">3.41 19.53</cell></row><row><cell>NASNet-A</cell><cell>3.3M</cell><cell>-</cell><cell cols="2">3.41 19.70</cell></row><row><cell>ENASNet</cell><cell>4.6M</cell><cell>-</cell><cell cols="2">3.54 19.43</cell></row><row><cell>AmoebaNet-A</cell><cell>4.6M</cell><cell>-</cell><cell>3.34</cell><cell>-</cell></row><row><cell>AmoebaNet-B</cell><cell>34.9M</cell><cell>-</cell><cell cols="2">2.98 17.66</cell></row><row><cell>NASNet-A</cell><cell>50.9M</cell><cell>-</cell><cell>-</cell><cell>16.03</cell></row><row><cell>ENASNet</cell><cell>52.7M</cell><cell>-</cell><cell>-</cell><cell>16.44</cell></row><row><cell>PNASNet</cell><cell>53.0M</cell><cell>-</cell><cell>-</cell><cell>16.70</cell></row><row><cell>HCGNet-A1</cell><cell>1.1M</cell><cell>0.2G</cell><cell cols="2">3.15 18.13</cell></row><row><cell>HCGNet-A2</cell><cell>3.1M</cell><cell>0.5G</cell><cell cols="2">2.29 16.54</cell></row><row><cell>HCGNet-A3</cell><cell>11.4M</cell><cell>2.0G</cell><cell cols="2">2.14 15.96</cell></row><row><cell></cell><cell cols="2">Experiments</cell><cell></cell><cell></cell></row><row><cell cols="2">Experiments on CIFAR</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Dataset and training details. Both CIFAR-10 and</cell></row><row><cell cols="5">CIFAR-100 datasets comprise 50k training images and 10k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of our HCGNets against SOTA networks about Top-1 and Top-5 error rates (%) on ImageNet.</figDesc><table><row><cell>Model</cell><cell cols="4">Params FLOPs T-1 T-5</cell></row><row><cell>MixNet-105</cell><cell>11.2M</cell><cell>5.0G</cell><cell cols="2">23.3 6.7</cell></row><row><cell>MixNet-121</cell><cell>21.9M</cell><cell>8.3G</cell><cell cols="2">21.9 5.9</cell></row><row><cell>MixNet-141</cell><cell>41.1M</cell><cell cols="3">13.1G 20.4 5.3</cell></row><row><cell>DPN-68</cell><cell>12.8M</cell><cell>2.5G</cell><cell cols="2">23.6 6.9</cell></row><row><cell>DPN-92</cell><cell>38.0M</cell><cell>6.5G</cell><cell cols="2">20.7 5.4</cell></row><row><cell>DPN-98</cell><cell>61.6M</cell><cell cols="3">11.7G 20.2 5.2</cell></row><row><cell>DenseNet-169</cell><cell>14.2M</cell><cell>3.5G</cell><cell cols="2">23.8 6.9</cell></row><row><cell>DenseNet-201</cell><cell>20.0M</cell><cell>4.4G</cell><cell cols="2">22.6 6.3</cell></row><row><cell>DenseNet-264</cell><cell>33.4M</cell><cell>6.0G</cell><cell cols="2">22.2 6.1</cell></row><row><cell>SparseNet-201</cell><cell>14.9M</cell><cell>9.2G</cell><cell>22.7</cell><cell>-</cell></row><row><cell>ResNet-50</cell><cell>25.6M</cell><cell>3.9G</cell><cell cols="2">24.6 7.5</cell></row><row><cell>ResNet-50+SE</cell><cell>28.1M</cell><cell>3.9G</cell><cell cols="2">23.1 6.7</cell></row><row><cell>ResNet-50+CBAM</cell><cell>28.1M</cell><cell>3.9G</cell><cell cols="2">22.7 6.3</cell></row><row><cell>ResNet-101</cell><cell>44.6M</cell><cell>7.6G</cell><cell cols="2">23.4 6.9</cell></row><row><cell>ResNet-101+SE</cell><cell>49.3M</cell><cell>7.6G</cell><cell cols="2">22.4 6.2</cell></row><row><cell cols="2">ResNet-101+CBAM 49.3M</cell><cell>7.6G</cell><cell cols="2">21.5 5.7</cell></row><row><cell>ResNeXt-50</cell><cell>25.0M</cell><cell>3.8G</cell><cell cols="2">22.9 6.5</cell></row><row><cell>ResNeXt-50+SE</cell><cell>27.6M</cell><cell>3.8G</cell><cell cols="2">21.9 6.0</cell></row><row><cell>ResNeXt-101</cell><cell>44.2M</cell><cell>7.5G</cell><cell cols="2">21.5 5.8</cell></row><row><cell>ResNeXt-101+SE</cell><cell>49.0M</cell><cell>7.5G</cell><cell cols="2">21.2 5.7</cell></row><row><cell>ResNeXt-101+SK</cell><cell>48.9M</cell><cell>8.5G</cell><cell>20.2</cell><cell>-</cell></row><row><cell>WideResNet-18</cell><cell>45.6M</cell><cell>6.7G</cell><cell cols="2">25.6 8.2</cell></row><row><cell cols="2">WideResNet-18+SE 46.0M</cell><cell>6.7G</cell><cell cols="2">24.9 7.7</cell></row><row><cell>AOGNet-12M</cell><cell>11.9M</cell><cell>2.4G</cell><cell cols="2">22.3 6.1</cell></row><row><cell>AOGNet-40M</cell><cell>40.3M</cell><cell>8.9G</cell><cell cols="2">19.8 4.9</cell></row><row><cell>HCGNet-B</cell><cell>12.9M</cell><cell>2.0G</cell><cell cols="2">21.5 5.8</cell></row><row><cell>HCGNet-C</cell><cell>42.2M</cell><cell>7.1G</cell><cell cols="2">19.5 4.8</cell></row><row><cell cols="5">eral form of ResNet and DenseNet. It also uses 8× fewer pa-</cell></row><row><cell cols="5">rameters but obtains better results than concurrent AOGNet,</cell></row><row><cell cols="5">which is the state-of-the-art human network by hierarchical</cell></row><row><cell cols="5">and compositional feature aggregation. Consequently, our</cell></row><row><cell cols="5">nested aggregation is the best method among other combi-</cell></row><row><cell cols="3">nations and variants of ResNet and DenseNet.</cell><cell></cell><cell></cell></row><row><cell cols="5">Comparisons with auto-searched Networks. Notably,</cell></row><row><cell cols="5">Our HCGNets are also more efficient than auto-searched</cell></row><row><cell cols="5">networks. Compared with other networks with small set-</cell></row><row><cell cols="5">ting, HCGNet-A2 achieves around 1% and 3% reductions on</cell></row><row><cell cols="5">CIFAR-10 and CIFAR-100 error rates, respectively. More-</cell></row><row><cell cols="5">over, it is noteworthy that HCGNet-A1 can also obtain su-</cell></row><row><cell cols="5">perior performance with unprecedent efficiency. For large</cell></row><row><cell cols="5">setting, HCGNet-A3 achieves the best results with least</cell></row><row><cell cols="5">complexity. Somewhat surprisingly, HCGNet-A3 can out-</cell></row><row><cell cols="5">perform the most competitive NASNet-A with only 22% pa-</cell></row><row><cell>rameters.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Experiments on ImageNet 2012</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Dataset and Training Details. ImageNet 2012 dataset</cell></row><row><cell cols="5">comprises 1.2 million training images and 50k validation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of HCGNet-B against other backbones on the Mask-RCNN system.</figDesc><table><row><cell>Backbone</cell><cell cols="3">Params FLOPs AP bb 50:95</cell><cell>AP bb 50</cell><cell>AP bb 75</cell><cell>AP m 50:95</cell><cell>AP m 50</cell><cell>AP m 75</cell></row><row><cell>ResNet-50-FPN</cell><cell cols="2">44.2M 275.6G</cell><cell>37.3</cell><cell>59.9</cell><cell>40.2</cell><cell>34.2</cell><cell>55.9</cell><cell>36.2</cell></row><row><cell cols="2">AOGNet-12M-FPN 31.2M</cell><cell>-</cell><cell>38.0</cell><cell>59.8</cell><cell>41.3</cell><cell>34.6</cell><cell>56.6</cell><cell>36.4</cell></row><row><cell>HCGNet-B-FPN</cell><cell cols="2">32.1M 230.4G</cell><cell>38.3</cell><cell>60.6</cell><cell>41.3</cell><cell>35.2</cell><cell>57.5</cell><cell>37.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. Final error rates are reported by single-crop with size 224 × 224 at test time on the validation set. We employ synchronous SGD with momentum 0.9 and batch size 256. Training is regularized by weight decay 4×10 −5 , label smoothing with = 0.1<ref type="bibr" target="#b17">(Szegedy et al. 2016)</ref>, mixup with α = 0.4 and dropout (Srivastava et al. 2014) with rate 0.1 before the final FC layer. All networks are trained for 630 epochs by SGDR learning rate curve with initial learning rate 0.1, T 0 = 10, T mul = 2. 3% gain of performance. Furthermore, HCGNets yield significantly better results than the families of DenseNet, MixNet and DPN under comparable complexity. Remarkably, using considerable 4.6× fewer FLOPs, HCGNet-B can also surpass SparseNet-201, which is the state-of-the-art variant of DenseNet. The family of HCGNet can consistently obtain better performance than the families of ResNet, ResNeXt, WideResNet and their attention-based variants, which represent the widely applied models in practice. Predominately, HCGNets outperform previous SOTA AOGNets across various model specifications, which show the superiority of our design.</figDesc><table><row><cell>Comparisons with popular networks. As shown in Ta-</cell></row><row><cell>ble 3, our HCGNets perform the best among all other mod-</cell></row><row><cell>els with less or comparable complexity in terms of top-1 and</cell></row><row><cell>top-5 error rates. It is noteworthy that DenseNet-169 stacks</cell></row><row><cell>4 dense blocks with 6,12,32,32 modules, while HCGNet-</cell></row><row><cell>B utilizes shallower design with 3,6,12,8 modules for 4</cell></row><row><cell>hybrid blocks, thus reducing 88% redundancy but obtain-</cell></row><row><cell>ing above absolute 2.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6541" to="6549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11492</idno>
		<title level="m">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4467" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01169</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Res2net: A new multi-scale backbone architecture</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00002</idno>
		<title level="m">Log-densenet: How to sparsify a densenet</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Condensenet: An efficient densenet using learned group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2752" to="2761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Citeseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Selective kernel networks</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aognets: Compositional grammatical architectures for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01808</idno>
		<title level="m">Mixed link networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">So</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparsely aggregated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
