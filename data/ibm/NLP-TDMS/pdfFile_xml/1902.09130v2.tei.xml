<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Chen</surname></persName>
							<email>wentao.chen@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China (USTC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China (USTC)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeleton-based action recognition is an important task that requires the adequate understanding of movement characteristics of a human action from the given skeleton sequence. Recent studies have shown that exploring spatial and temporal features of the skeleton sequence is vital for this task. Nevertheless, how to effectively extract discriminative spatial and temporal features is still a challenging problem. In this paper, we propose a novel Attention Enhanced Graph Convolutional LSTM Network (AGC-LSTM) for human action recognition from skeleton data. The proposed AGC-LSTM can not only capture discriminative features in spatial configuration and temporal dynamics but also explore the co-occurrence relationship between spatial and temporal domains. We also present a temporal hierarchical architecture to increase temporal receptive fields of the top AGC-LSTM layer, which boosts the ability to learn the high-level semantic representation and significantly reduces the computation cost. Furthermore, to select discriminative spatial information, the attention mechanism is employed to enhance information of key joints in each AGC-LSTM layer. Experimental results on two datasets are provided: NTU RGB+D dataset and Northwestern-UCLA dataset. The comparison results demonstrate the effectiveness of our approach and show that our approach outperforms the state-of-the-art methods on both datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the computer vision field, human action recognition plays a fundamental and important role, with the purpose of predicting the action classes from videos. It has been stud- <ref type="bibr">Figure 1</ref>. The structure of one AGC-LSTM layer. Different from traditional LSTM, the graph convolutional operator within AGC-LSTM causes the input, hidden state, and cell memory of AGC-LSTM to be graph-structured data.</p><p>ied for decades and is still very popular due to its extensive potential applications, e.g., video surveillance, humancomputer interaction, sports analysis and so on <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Action recognition is a challenging task in the computer vision community. There are various attempts on human action recognition based on RGB video and 3D skeleton data. The RGB video based action recognition methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref> mainly focus on modeling spatial and temporal representations from RGB frames and temporal optical flow. Despite RGB video based methods have achieved promising results, there still exist some limitations, e.g., background clutter, illumination changes, appearance variation, and so on. 3D skeleton data represents the body structure with a set of 3D coordinate positions of key joints. Since skeleton sequence does not contain color information, it is not affected by the limitations of RGB video. Such robust representation allows to model more discriminative temporal characteristics about human actions. Moreover, Johansson et al. <ref type="bibr" target="#b8">[9]</ref> have given an empirical and theoretical basis that key joints can provide highly effective information about human motion. Besides, the Microsoft Kinect <ref type="figure">Figure 2</ref>. The architecture of the proposed attention enhanced graph convolutional LSTM network (AGC-LSTM). Feature augmentation (FA) computes feature differences with position features and concatenates both position features and feature differences. LSTM is used to dispel scale variance between feature differences and position features. Three AGC-LSTM layers can model discriminative spatialtemporal features. Temporal average pooling is the implementation of average pooling in the temporal domain. We use the global feature of all joints and the local feature of focused joints from the last AGC-LSTM layer to predict the class of human action. <ref type="bibr" target="#b41">[42]</ref> and advanced human pose estimation algorithms <ref type="bibr" target="#b2">[3]</ref> make it easier to gain skeleton data.</p><p>For skeleton based action recognition, the existing methods explore different models to learn spatial and temporal features. Song et al. <ref type="bibr" target="#b24">[25]</ref> employ a spatial-temporal attention model based on LSTM to select discriminative spatial and temporal features. The Convolutional Neural Networks (CNNs) are used to learn spatial-temporal features from skeletons in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10]</ref>. <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b25">26]</ref> employ graph convolutional networks (GCN) for action recognition. Compared with <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b25">26]</ref>, Si et al. <ref type="bibr" target="#b21">[22]</ref> propose to utilize the graph neural network and LSTM to represent spatial and temporal information, respectively. In short, all these methods are trying to design an effective model that can identify spatial and temporal features of skeleton sequence. Nevertheless, how to effectively extract discriminative spatial and temporal features is still a challenging problem.</p><p>Generally, there are three notable characteristics for human skeleton sequences: 1) There are strong correlations between each node and its adjacent nodes so that the skeleton frames contain abundant body structural information. 2) Temporal continuity exists not only in the same joints (e.g., hand, wrist and elbow), but also in the body structure. 3) There is a co-occurrence relationship between spatial and temporal domains. In this paper, we propose a novel and general framework called attention enhanced graph convolutional LSTM network (AGC-LSTM) for skeleton-based action recognition, which improves the skeleton representation by synchronously learning spatiotemporal characteristics mentioned above.</p><p>The architecture of the proposed AGC-LSTM network is shown in <ref type="figure">Fig.2</ref>. Firstly, the coordinate of each joint is transformed into a spatial feature with a linear layer. Then we concatenate spatial feature and feature difference be-tween two consecutive frames to compose an augmented feature. In order to dispel scale variance between both features, a shared LSTM is adopted to process each joint sequence. Next, we apply three AGC-LSTM layers to model spatial-temporal features. As shown in <ref type="figure">Fig.1</ref>, due to the graph convolutional operator within AGC-LSTM, it can not only effectively capture discriminative features in spatial configuration and temporal dynamics but also explore the co-occurrence relationship between spatial and temporal domains. More specially, the attention mechanism is employed to enhance the features of key nodes at each time step, which can promote AGC-LSTM to learn more discriminative features. For example, the features of "elbow", "wrist" and "hand" are very important for action "handshaking" and should be enhanced in the process of identifying the behavior. Inspired by spatial pooling in CNNs, we present a temporal hierarchical architecture with temporal average pooling to increase temporal receptive fields of the top AGC-LSTM layers, which boosts the ability to learn high-level spatiotemporal semantic features and significantly reduces the computational cost. Finally, we use the global feature of all joints and the local feature of focused joints from the last AGC-LSTM layer to predict the class of human actions. Although the joint-based model achieves the state-of-the-art results, we also explore the performance of the proposed model on the part level. For the part-based model, the concatenation of joints of each part serves as a node to construct the graph. Furthermore, the two-stream model based on joint and part can lead to further performance improvement.</p><p>The main contributions of this work are summarized as follows:</p><p>• We propose a novel and general AGC-LSTM network for skeleton-based action recognition, which is the first attempt of graph convolutional LSTM for this task.</p><p>• The proposed AGC-LSTM is able to effectively capture discriminative spatiotemporal features. More specially, the attention mechanism is employed to enhance the features of key nodes, which assists in improving spatiotemporal expressions.</p><p>• A temporal hierarchical architecture is proposed to boost the ability to learn high-level spatiotemporal semantic features and significantly reduce the computational cost.</p><p>• The proposed model achieves the state-of-the-art results on both NTU RGB+D dataset and Northwestern-UCLA dataset. We perform extensive experiments to demonstrate the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Neural networks with graph Recently, graph-based models have attracted a lot of attention due to the effective representation for the graph structure data <ref type="bibr" target="#b37">[38]</ref>. Existing graph models mainly fall into two architectures. One framework called graph neural network (GNN) is the combination of graph and recurrent neural network. Through multiple iterations of message passing and states updating of nodes, each node captures the semantic relation and structural information within its neighbor nodes. Qi et al. <ref type="bibr" target="#b18">[19]</ref> apply GNN to address the task of detecting and recognizing human-object interactions in images and videos. Li et al. <ref type="bibr" target="#b14">[15]</ref> exploit the GNNs to model dependencies between roles and predict a consistent structured output for situation recognition. The other framework is graph convolutional network (GCN) that generalizes convolutional neural networks to graph. There are two types of GCNs: spectral GCNs and spatial GCNs. Spectral GCNs transform graph signals on graph spectral domains and then apply spectral filters on spectral domains. For example, the CNNs are utilized in the spectral domain relying on the graph Laplacian <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Kipf et al. <ref type="bibr" target="#b11">[12]</ref> introduce Spectral GCNs for semi-supervised classification on graph-structured data. For spatial GCNs, the convolution operation is applied to compute a new feature vector for each node using its neighborhood information. Simonovsky et al. <ref type="bibr" target="#b22">[23]</ref> formulate a convolution-like operation on graph signals performed in the spatial domain and are the first to apply graph convolutions to point cloud classification. In order to capture the spatial-temporal features of graph sequences, a graph convolutional LSTM is firstly proposed in <ref type="bibr" target="#b19">[20]</ref>, which is an extension of GCNs to have the recurrent architecture. Inspired by <ref type="bibr" target="#b19">[20]</ref>, we exploit a novel AGC-LSTM network to learn inherent spatiotemporal representations from skeleton sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton-based action recognition</head><p>Human action recognition based on skeleton data has received a lot of attention, due to its effective representation of motion dynamics. Traditional skeleton-based action recognition methods mainly focus on designing hand-crafted features <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8]</ref>. Vemulapalli et al. <ref type="bibr" target="#b28">[29]</ref> represent each skeleton using the relative 3D rotations between various body parts. The relative 3D geometry between all pairs of body parts is applied to represent the 3D human skeleton in <ref type="bibr" target="#b27">[28]</ref>.</p><p>Recent works mainly learn human action representations with deep learning networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b1">2]</ref>. Du et al. <ref type="bibr" target="#b4">[5]</ref> divide human skeleton into five parts according to the human physical structure, and then separately feed them into a hierarchical recurrent neural network to recognize actions. A spatial-temporal attention network learns to selectively focus on discriminative spatial and temporal features in <ref type="bibr" target="#b24">[25]</ref>. Zhang et al. <ref type="bibr" target="#b40">[41]</ref> present a view adaptive model for skeleton sequence, which is capable of regulating the observation viewpoints to the suitable ones by itself. The works in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref> further show that learning discriminative spatial and temporal features is the key element for human action recognition. A hierarchical CNN model is presented in <ref type="bibr" target="#b13">[14]</ref> to learn representations for joint co-occurrences and temporal evolutions. A spatial-temporal graph convolutional network (ST-GCN) is proposed for action recognition in <ref type="bibr" target="#b38">[39]</ref>. Each spatial-temporal graph convolutional layer constructs spatial characteristics with a graph convolutional operator, and models temporal dynamic with a convolutional operator. In addition, a part-based graph convolutional network (PB-GCN) is proposed to learn the relations between parts in <ref type="bibr" target="#b25">[26]</ref>. Compared with ST-GCN <ref type="bibr" target="#b38">[39]</ref> and PB-GCN <ref type="bibr" target="#b25">[26]</ref>, Si et al. <ref type="bibr" target="#b21">[22]</ref> apply graph neural networks to capture spatial structural information and then use LSTM to model temporal dynamics. Despite the significant performance improvement in <ref type="bibr" target="#b21">[22]</ref>, it ignores the co-occurrence relationship between spatial and temporal features. In this paper, we propose a novel attention enhanced graph convolutional LSTM network that can not only effectively extract discriminative spatial and temporal features but also explore the co-occurrence relationship between spatial and temporal domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Convolutional Neural Network</head><p>Graph convolutional neural network (GCN) is a general and effective framework for learning representation of graph structured data. Various GCN variants have achieved the state-of-the-art results on many tasks. For skeletonbased action recognition, let G t = {V t , E t } denotes a graph of human skeleton on a single frame at time t, where V t is the set of N joint nodes and E t is the set of skeleton edges. The neighbor set of a node v ti is defined as</p><formula xml:id="formula_0">N(v ti ) = {v tj |d(v ti , v tj ) ≤ D}, where d(v ti , v tj ) is the minimum path length from v tj to v ti . A graph labeling function : V t → {1, 2, .</formula><p>.., K} is designed to assign the labels {1, 2, ..., K} to each graph node v ti ∈ V t , which can partition the neighbor set N(v ti ) of node v ti into a fixed number of K subsets. The graph convolution is generally computed as:</p><formula xml:id="formula_1">Y out (v ti ) = vtj ∈N(vti) 1 Z ti (v tj ) X(v tj )W( (v tj )) (1) where X(v tj ) is the feature of node v tj . W(·) is a weight function that allocates a weight indexed by the label (v tj ) from K weights. Z ti (v tj )</formula><p>is the number of the corresponding subset, which normalizes feature representations. Y out (v ti ) denotes the output of graph convolution at node v ti . More specifically, with the adjacency matrix, the Eqn. 1 can be represented as:</p><formula xml:id="formula_2">Y out = K k=1 Λ − 1 2 k A k Λ − 1 2 k XW k (2) where A k is the adjacency matrix in spatial configuration of the label k ∈ {1, 2, ..., K}. Λ ii k = j A ij k is a degree matrix.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Enhanced Graph Convolutional LSTM</head><p>For sequence modeling, a lot of studies have demonstrated that LSTM, as a variant of RNN, has an amazing ability to model long-term temporal dependencies. Various LSTM-based models are employed to learn temporal dynamics of skeleton sequences. However, due to the fully connected operator within LSTM, there is a limitation of ignoring spatial correlation for skeleton-based action recognition. Compared with LSTM, AGC-LSTM can not only capture discriminative features in spatial configuration and temporal dynamics, but also explore the co-occurrence relationship between spatial and temporal domains.</p><p>Like LSTM, AGC-LSTM also contains three gates: an input gate i t , a forget gate f t , an output gate o t . However, these gates are obtained with the graph convolution operator. The input X t , hidden state H t , and cell memory C t of AGC-LSTM are graph-structured data. <ref type="figure">Fig.3</ref> shows the strcture of AGC-LSTM unit. Due to the graph convolutional operator within AGC-LSTM, the cell memory C t and hidden state H t are able to exhibit temporal dynamics, as well as contain spatial structural information. The functions of AGC-LSTM unit are defined as follows: <ref type="figure">Figure 3</ref>. The structures of AGC-LSTM unit. Compared with LSTM, the inner operator of AGC-LSTM is graph convolutional calculation. To highlight more discriminative information, the attention mechanism is employed to enhance the features of key nodes.</p><formula xml:id="formula_3">i t = σ(W xi * G X t + W hi * G H t−1 + b i ) f t = σ(W xf * G X t + W hf * G H t−1 + b f ) o t = σ(W xo * G X t + W ho * G H t−1 + b o ) u t = tanh(W xc * G X t + W hc * G H t−1 + b c ) (3) C t = f t C t−1 + i t u t H t = o t tanh(C t ) H t = f att H t + H t</formula><p>where * G denotes the graph convolution operator and denotes the Hadamard product. σ (·) is the sigmoid activation function. u t is the modulated input. H t is an intermediate hidden state. W xi * G X t denotes a graph convolution of X t with W xi , which can be written as Eqn.1. f att (·) is an attention network that can select discriminative information of key nodes. The sum of f att H t and H t as the output aims to strengthen information of key nodes without weakening information of non-focused nodes, which can maintain the integrity of spatial information.</p><p>The attention network is employed to adaptively focus on key joints with a soft attention mechanism that can automatically measure the importance of joints. The illustration of the spatial attention network is shown in <ref type="figure" target="#fig_0">Fig.4</ref>. The intermediate hidden state H t of AGC-LSTM contains rich spatial structural information and temporal dynamics that are beneficial in guiding the selection of key joints. So we first aggregate the information of all nodes as a query feature:</p><formula xml:id="formula_4">q t = ReLU N i=1 W H ti<label>(4)</label></formula><p>where W is the learnable parameter matrix. Then the atten- tion scores of all nodes can be calculated as:</p><formula xml:id="formula_5">α t = Sigmoid U s tanh W h H t + W q q t + b s + b u<label>(5)</label></formula><p>where α t = (α t1 , α t2 , ..., α tN ), and U s , W h , W q are the learnable parameter matrixes. b s , b u are the bias. We use the non-linear function of Sigmoid due to the possibility of existing multiple key joints. The hidden state H ti of node v ti can also be represented as (1 + α ti ) · H ti . The attention enhanced hidden state H t will be fed into the next AGC-LSTM layer. Note that, at the last AGC-LSTM layer, the aggregation of all node features will serve as a global feature F g t , and the weighted sum of focused nodes will serve as a local feature F l t :</p><formula xml:id="formula_6">F g t = N i=1 H ti (6) F l t = N i=1 α ti · H ti<label>(7)</label></formula><p>The global feature F g t and local feature F l t are used to predict the class of human action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">AGC-LSTM Network</head><p>We propose an end-to-end attention enhanced graph convolutional LSTM network (AGC-LSTM) for skeletonbased human action recognition. The overall pipeline of our model is shown in <ref type="figure">Fig.2</ref>. In the following, we discuss the rationale behind the proposed framework in detail.</p><p>Joints Feature Representation. For the skeleton sequence, we first map the 3D coordinate of each joint into a high-dimensional feature space using a linear layer and an LSTM layer. The first linear layer encodes the coordinates of joints into a 256-dim vector as position features P t ∈ R N ×256 , and P ti ∈ R 1×256 denotes the position representation of joint i. Due to only containing position information, the position feature P ti is beneficial for learning spatially structured characteristic in the graph model. Frame difference features V ti between two consecutive frames can facilitate the acquisition of dynamic information for AGC-LSTM. In order to take into account both advantages, the concatenation of both features serve as an augmented feature to enrich feature information. However, the concatenation of position feature P ti and frame difference feature V ti exists the scale variance of the features vectors. Therefore, we adopt an LSTM layer to dispel scale variance between both features:</p><formula xml:id="formula_7">E ti = f lstm (concat (P ti , V ti )) = f lstm concat P ti , P ti − P (t−1)i<label>(8)</label></formula><p>where E ti is the augmented feature of joint i at time t. Note that the linear layer and LSTM are shared among different joints.</p><p>Temporal Hierarchical Architecture. After the LSTM layer, the sequence {E 1 , E 2 , ..., E T } of augmented features will be fed into the following GC-LSTM layers as the node features, where E t ∈ R N ×de . The proposed model stacks three AGC-LSTM layers to learn the spatial configuration and temporal dynamics. Inspired by spatial pooling in CNNs, we present a temporal hierarchical architecture of AGC-LSTM with average pooling in temporal domain to increase the temporal receptive field of the top AGC-LSTM layers. Through the temporal hierarchical architecture, the temporal receptive field of each time input at the top AGC-LSTM layer becomes a short-term clip from a frame, which can be more sensitive to the perception of the temporal dynamics. In addition, it can significantly reduce computational cost on the premise of improving performance.</p><p>Learning AGC-LSTM. Finally, the global feature F g t and local feature F l t of each time step are transformed into the scores o g t and o l t for C classes, where o t = (o t1 , o t2 , ..., o tC ). And the predicted probability being the i th class is then obtained as:</p><formula xml:id="formula_8">y ti = e oti C j=1 e otj , i = 1, ..., C<label>(9)</label></formula><p>During training, considering that the hidden state of each time step on the top AGC-LSTM contains a short-term dynamics, we supervise our model with the following loss:</p><formula xml:id="formula_9">L = − T 3 t=1 C i=1 yilogŷ g ti − T 3 t=1 C i=1 yilogŷ l ti (10) + λ 3 j=1 N n=1 1 − T j t=1 αtnj Tj 2 + β 3 j=1 1 Tj T j t=1 N n=1 αtnj 2</formula><p>where y = (y 1 , ..., y C ) is the groundtruth label. T j denotes the number of time step on j th AGC-LSTM layer. The third term aims to pay equal attention to different joints. The last term is to limit the number of interested nodes. λ and β are weight decaying coefficients. Note that only the sum probability ofŷ g T3 andŷ l T3 at the last time step is used to predict the class of the human action.</p><p>Although the joint-based AGC-LSTM network has achieved the state-of-the-art results, we also explore the performance of the proposed model on the part level. According to human physical structure, the body can be divided into several parts. Similar to joint-based AGC-LSTM network, we first capture part features with a linear layer and a shared LSTM layer. Then the part features as node representations are fed into three AGC-LSTM layers to model spatial-temporal characteristics. The results illustrate that our model can also achieve superior performance on the part level. Furthermore, the hybrid model (shown in <ref type="figure" target="#fig_1">Fig.5</ref>) based on joints and parts can lead to further performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>NTU RGB+D dataset <ref type="bibr" target="#b20">[21]</ref>. This dataset contains 60 different human action classes that are divided into three major groups: daily actions, mutual actions, and health-related actions. There are 56,880 action samples in total which are performed by 40 distinct subjects. Each action sample contains RGB video, depth map sequence, 3D skeleton data, and infrared video captured by three Microsoft Kinect v2 cameras concurrently. The 3D skeleton data that we focus on consists of 3D positions of 25 body joints per frame. There are two evaluation protocols for this dataset: Cross-Subject (CS) and Cross-View (CV) <ref type="bibr" target="#b20">[21]</ref>. Under the Cross-Subject protocol, actions performed by 20 subjects constitute the training set and the rest of actions performed by the other 20 subjects are used for testing. For Cross-View evaluation, samples captured by the first two cameras are used for training and the rest are for testing.</p><p>Northwestern-UCLA dataset <ref type="bibr" target="#b32">[33]</ref>. This dataset contains 1494 video clips covering 10 categories. It is captured by three Kinect cameras simultaneously from a variety of viewpoints. Each action sample contains RGBD and human skeleton data performed by 10 different subjects. The evaluation protocol is the same as in <ref type="bibr" target="#b32">[33]</ref>. Samples from the first two cameras constitute the training set and samples from the other camera constitute the testing dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>In our experiments, we sample a fixed length T from each skeleton sequence as the input. We set the length T = 100 and 50 for NTU dataset and Northwestern-UCLA dataset, respectively. In the proposed AGC-LSTM, the neighbor set of each node contains only nodes directly connected with itself, so D = 1. In order to compare fairly with ST-GCN <ref type="bibr" target="#b38">[39]</ref>, the graph labeling function in AGC-LSTM will partition the neighbor set into K = 3 subsets according to <ref type="bibr" target="#b38">[39]</ref>: the root node itself, centripetal group, and centrifugal group. The channels of three AGC-LSTM layers are set to 512. During training, we use the Adam optimizer <ref type="bibr" target="#b10">[11]</ref> to optimize the network. Dropout with a probability of 0.5 is adopted to avoid over-fitting on these two datasets. We set λ and β to 0.01 and 0.001, respectively. The initial learning rate is set to 0.0005 and reduced by multiplying it by 0.1 every 20 epochs. The batch sizes for the NTU dataset and Northwestern-UCLA dataset are 64 and 30, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and Comparisons</head><p>In this section, we compare our proposed attention enhanced graph convolutional LSTM network (AGC-LSTM) with several state-of-the-art methods on the used two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">NTU RGB+D Dataset</head><p>From <ref type="table">Table 1</ref>, we can see that our proposed method achieves the best performance of 95.0% and 89.2% in terms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Year CV CS HBRNN-L <ref type="bibr" target="#b4">[5]</ref> 2015 64.0 59.1 Part-aware LSTM <ref type="bibr" target="#b20">[21]</ref> 2016 70.3 62.9 Trust Gate ST-LSTM <ref type="bibr" target="#b15">[16]</ref> 2016 77.7 69.2 Two-stream RNN <ref type="bibr" target="#b29">[30]</ref> 2017 79.5 71.3 STA-LSTM <ref type="bibr" target="#b24">[25]</ref> 2017 81.2 73.4 Ensemble TS-LSTM <ref type="bibr" target="#b12">[13]</ref> 2017 81.3 74.6 Visualization CNN <ref type="bibr" target="#b16">[17]</ref> 2017 82.6 76.0 VA-LSTM <ref type="bibr" target="#b40">[41]</ref> 2017 87.6 79.4 ST-GCN <ref type="bibr" target="#b38">[39]</ref> 2018 88.3 81.5 SR-TSL <ref type="bibr" target="#b21">[22]</ref> 2018 92.4 84.8 HCN <ref type="bibr" target="#b13">[14]</ref> 2018 91.1 86.5 PB-GCN <ref type="bibr" target="#b25">[26]</ref> 2018 93.  <ref type="table">Table 1</ref>. Comparison with the state-of-the-art methods on the NTU RGB+D dataset for Cross-View (CS) and Cross-Subject (CV) evaluation in accuracy.</p><p>of two protocols on the NTU dataset. To demonstrate the effectiveness of our method, we choose the following related methods to compare and analyze the results: AGC-LSTM vs HCN. HCN <ref type="bibr" target="#b13">[14]</ref> employs the CNN model for learning global co-occurrences from skeleton data. It treats each joint of a skeleton as a channel, then uses the convolution layer to learn the glob co-occurrence features from all joints. We can see that our performances significantly outperform the HCN <ref type="bibr" target="#b13">[14]</ref> by about 3.9% and 2.7% for cross-view evaluation and cross-subject evaluation, respectively.</p><p>AGC-LSTM vs GCN models. In order to compare fairly with <ref type="bibr" target="#b38">[39]</ref>, we use the same GCN operator in the proposed AGC-LSTM layer as in ST-GCN. On the joint-level evaluation, the results of AGC-LSTM are 93.5% and 87.5% that outperform 5.2% and 6.0% than ST-GCN. Moreover, Our model outperforms the PB-GCN [26] by 1.8% and 1.7% for tow evaluations. The comparison results prove that the AGC-LSTM is optimal for skeleton-based action recognition than ST-GCN.</p><p>Co-occurrence relationship between spatial and temporal domains. Although Si et al. <ref type="bibr" target="#b21">[22]</ref> propose a spatial reasoning and temporal stack learning network with graph neural network (GNN) and LSTM, they ignore the cooccurrence relationship between spatial and temporal domains. Due to the ability to explore the co-occurrence relationship between spatial and temporal domains, Our AGC-LSTM outperforms <ref type="bibr" target="#b21">[22]</ref> by 2.6% and 4.4%.</p><p>The performances on joint level and part level. Recent methods can be grouped into two categories: joint-based <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14]</ref> and part-based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5]</ref>. Our method achieves the state-of-the-art results on joint-level and part-level, which illustrates the better generalization of our model for joint-level and part-level inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Northwestern-UCLA Dataset</head><p>As shown in <ref type="table">Table 2</ref>, the proposed AGC-LSTM again achieves the best accuracy of 93.3% on the Northwestern-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Year Accuracy (%)  formation of key joints, which can promote the ability of feature representation. In addition, the fusion of part-based and joint-based AGC-LSTM can further improve the performance.</p><p>We also visualize the attention weights of three AGC-LSTM layers in <ref type="figure" target="#fig_3">Fig.6</ref>. For the "handshaking" action, the results show our method can gradually enhance the attention of "right elbow", "right wrist", and "right hand". Meanwhile, "tip of the right hand" and "right thumb" have some degree of attention. Furthermore, we analyze the experimental results with a confusion matrix on the Northwestern-UCLA dataset. As show in <ref type="figure" target="#fig_4">Fig.7(a)</ref>, it is very confusing for LSTM to recognize similar actions. For example, the actions "pick up with one hand" and "pick up with two hands" have very similar skeleton sequences. Nevertheless, we can see that the proposed AGC-LSTM can significantly improve the ability to classify these similar actions (shown in <ref type="figure" target="#fig_4">Fig.7(b)</ref>). The above results illustrate that the proposed AGC-LSTM is an effective method for skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Failure Case</head><p>Finally, we analyze misclassification results with a confusion matrix on the NTU dataset. <ref type="figure" target="#fig_5">Fig.8</ref> shows the part con-  It shows the part of confusion matrix comparison of the actions ("eat meal/snack", "reading", "writing", "playing with phone/tablet", "typing on a keyboard", "pointing to something with finger", "sneeze/cough", "pat on back of other person") with accuracies less than 80% on NTU dataset. fusion matrix comparison of the actions ("eat meal/snack", "reading", "writing", "playing with phone/tablet", "typing on a keyboard", "pointing to something with finger", "sneeze/cough", "pat on back of other person") with accuracies less than 80% for the cross-subject setting on the NTU dataset. We can see that misclassified actions are mainly very similar movements. For example, 20% samples of "reading" are misclassified as "writing", and there are 19% sequences of "writing" misclassified as "typing on as keyboard". For the NTU dataset, only two joints are marked on fingers ("tip of the hand" and "thumb"), so that it is very challenging to capture such subtle movements of the hands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we propose an attention enhanced graph convolutional LSTM network (AGC-LSTM) for skeletonbased action recognition, which is the first attempt of graph convolutional LSTM for this task. The proposed AGC-LSTM can not only capture discriminative features in spatial configuration and temporal dynamics, but also explore the co-occurrence relationship between spatial and temporal domains. Furthermore, the attention network is employed to enhance information of key joints in each AGC-LSTM layer. In addition, we also propose a temporal hierarchical architecture to capture high-level spatiotemporal semantic features. On two challenging benchmarks, the proposed AGC-LSTM achieves the state-of-the-art results. Learning the pose-object relation is helpful to overcome the limitations mentioned in the failure case. In the future, we will try the combination of skeleton sequence and object appearance to promote the performance of human action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the spatial attention network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of the hybrid model based on joins and parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Visualizations of the attention weights of three AGC-LSTM layers on one actor of the action "handshaking". Vertical axis denotes the joints. Horizontal axis denotes the frames. (a), (b), (c) are the attention results of the first, second and third AGC-LSTM layer, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>(a) LSTM (b) AGC-LSTM Confusion matrix comparison on the Northwestern-UCLA dataset. (a) LSTM. (b) AGC-LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Confusion matrix comparison on the NTU dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4 .</head><label>4</label><figDesc>The comparison results between several baselines and our AGC-LSTM on the Northwestern-UCLA dataset.</figDesc><table><row><cell>Lie group [28]</cell><cell>2014</cell><cell>74.2</cell></row><row><cell>Actionlet ensemble [32]</cell><cell>2014</cell><cell>76.0</cell></row><row><cell>HBRNN-L [5]</cell><cell>2015</cell><cell>78.5</cell></row><row><cell>Visualization CNN [17]</cell><cell>2017</cell><cell>86.1</cell></row><row><cell cols="2">Ensemble TS-LSTM [13] 2017</cell><cell>89.2</cell></row><row><cell>AGC-LSTM (Joint)</cell><cell>-</cell><cell>92.2</cell></row><row><cell>AGC-LSTM (Part)</cell><cell>-</cell><cell>90.1</cell></row><row><cell>AGC-LSTM (Joint&amp;Part)</cell><cell>-</cell><cell>93.3</cell></row><row><cell cols="3">Table 2. Comparison with the state-of-the-art methods on the</cell></row><row><cell cols="2">Northwestern-UCLA dataset in accuracy.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This work is jointly supported by National Key Research and Development Program of China (2016YFB1001000), National Natural Science Foundation of China (61525306, 61633021, 61721004, 61420106015, 61572504), Capital Science and Technology Leading Talent Training Project (Z181100006318030), and Beijing Science and Technology Project (Z181100008918010).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACM Computing Surveys</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human action recognition: Pose-based attention draws focus to hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motaz</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Senjian An, Ferdous Sohel, and Farid Boussaid. A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seoungyoon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cooccurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A survey on vision-based human action recognition. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Part-based graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpit</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P J</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rolling rotations for recognizing human actions from 3d skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling temporal dynamics and spatial configurations of actions using twostream recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rgb-d-based human motion recognition with deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A survey of vision-based methods for action representation, segmentation and recognition. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Memory attention networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? In arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition with spatio-temporal visual attention on skeleton image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
