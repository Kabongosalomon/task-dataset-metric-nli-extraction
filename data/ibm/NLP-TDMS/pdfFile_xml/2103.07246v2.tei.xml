<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomyoung</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangeun</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
							<email>junmo.kim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised semantic segmentation (WSSS) using image-level labels has recently attracted much attention for reducing annotation costs. Existing WSSS methods utilize localization maps from the classification network to generate pseudo segmentation labels. However, since localization maps obtained from the classifier focus only on sparse discriminative object regions, it is difficult to generate highquality segmentation labels. To address this issue, we introduce discriminative region suppression (DRS) module that is a simple yet effective method to expand object activation regions. DRS suppresses the attention on discriminative regions and spreads it to adjacent non-discriminative regions, generating dense localization maps. DRS requires few or no additional parameters and can be plugged into any network. Furthermore, we introduce an additional learning strategy to give a self-enhancement of localization maps, named localization map refinement learning. Benefiting from this refinement learning, localization maps are refined and enhanced by recovering some missing parts or removing noise itself. Due to its simplicity and effectiveness, our approach achieves mIoU 71.4% on the PASCAL VOC 2012 segmentation benchmark using only image-level labels. Extensive experiments demonstrate the effectiveness of our approach. The code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recent developments in deep learning have achieved great success on semantic segmentation tasks with the help of deep convolutional neural networks (CNNs) and rich pixel-level annotations. However, collecting a large-scale pixel-level annotated dataset requires intensive human labor, which is both expensive and time-consuming. To end this limitation, weakly-supervised semantic segmentation (WSSS) using only image-level labels has recently attracted much attention.</p><p>One problem with using image-level annotations is that we have no information about the location of the target object; we only know whether the object is present in the image or not. This makes semantic segmentation learning challenging. To learn pixel-level semantic knowledge from imagelevel labels, it is common practice to use localization maps Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>CAM DRS DRS † <ref type="figure">Figure 1</ref>: Visual comparisons of localization maps produced by original CAM, DRS, and DRS †. DRS † denotes that the refinement learning is applied.</p><p>obtained from the classification network using class activation maps (CAMs) <ref type="bibr" target="#b26">(Zhou et al. 2016)</ref>. Specifically, the discriminative region for each target class provided by CAMs is used as pixel-level supervision for segmentation network training. However, this discriminative region is usually very sparse and only covers a small part of the object, which is not enough for semantic segmentation learning as shown in the second column in <ref type="figure">Figure 1</ref>. Therefore, most studies in the weakly-supervised semantic segmentation field focus on expanding the object region to produce dense localization maps. One of the recent approaches is image-level and feature-level erasure of discriminative parts <ref type="bibr" target="#b21">(Wei et al. 2017;</ref><ref type="bibr" target="#b15">Li et al. 2018;</ref><ref type="bibr" target="#b9">Hou et al. 2018</ref>). This approach strictly erases discriminative parts, letting the network focus on other nondiscriminative parts. However, they not only tend to produce undesired true negative regions when most of the discriminative parts are erased but also require a lot of additional parameters for multiple classifiers or multiple branches. In this paper, we propose discriminative region suppression (DRS) module, which is a simple and efficient yet effective and novel approach for generating dense localization maps. The goal of DRS is to suppress discriminative regions, not to erase them, so that attention spreads to adjacent nondiscriminative regions; this mild approach helps the classifier effectively expand discriminative object regions. DRS These K maximum elements are the maximum points of each discriminative region and are considered as starting points to be suppressed. For convenience, K maximum elements are illustrated in 5 purple points. The controller predicts control values, which determine how much to suppress feature maps from these K maximum elements. These K control values are illustrated in 5 blue arrows and the length of the arrow means how much suppress feature maps from the corresponding maximum element. Using these K maximum elements and K control values, the suppressor suppresses discriminative regions and spreads the attention into adjacent non-discriminative parts.</p><p>module consists of three components as depicted in <ref type="figure">Figure  2</ref>: max-element extractor, suppression controller, and suppressor. These components work together to produce dense localization maps by reducing the attention gap between discriminative regions and adjacent non-discriminative regions. DRS not only effectively expands the object regions without generating much noise, but also can be plugged into any network with few or no additional parameters. Although we can obtain dense segmentation labels from the classification network equipped with DRS, it does not recover missing parts or weak attention by itself because the objective of the classification network is classification, not localization. To address this issue, we introduce an additional training strategy, named localization map refinement learning, inspired by ) Localization map refinement learning induces self-enhancement of localization maps by recovering missing or weak attention region. In <ref type="figure">Figure 1</ref>, we compare some results of DRS and DRS †, where DRS † denotes that refinement learning is applied.</p><p>Following the convention, we generate pseudo segmentation labels from our dense localization maps and evaluate on weakly-supervised semantic segmentation task. On the PAS-CAL VOC 2012 segmentation benchmark, we achieve mIoU 71.4% on the testset using only image-level labels. In addition, extensive experiments demonstrate the effectiveness of our approach.</p><p>In summary, the contributions of our work are as follows:</p><p>• We introduce a simple, effective, and novel approach for weakly-supervised semantic segmentation named discriminative region suppression (DRS) module, which requires few or no additional parameters and can be easily plugged into any network.</p><p>• DRS effectively and efficiently suppresses discriminative regions to generate dense localization maps, bridging the gap between discriminative regions and adjacent nondiscriminative regions.</p><p>• For self-enhancement of localization maps, we introduce an additional training strategy, named localization map refinement learning.</p><p>• Extensive experiments and analyses demonstrate the effectiveness of our DRS module and we achieve competitive performance on Pascal VOC 2012 segmentation benchmark using only image-level labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Most recent studies on semantic segmentation using imagelevel labels as weak supervision utilize CAMs <ref type="bibr" target="#b26">(Zhou et al. 2016)</ref> to localize object regions and focus on expanding them to non-discriminative parts of the objects. To this end, AE-PSL <ref type="bibr" target="#b21">(Wei et al. 2017)</ref>, GAIN , and SeeNet <ref type="bibr" target="#b9">(Hou et al. 2018)</ref> propose erasing techniques to generate dense localization maps. However, these erasure-based approaches usually require multiple classifiers and complicated training procedures. Moreover, erasing most of the discriminative regions may introduce true negative regions and confuse the classifier.</p><p>To avoid the repetitive training procedures of AE-PSL <ref type="bibr" target="#b21">(Wei et al. 2017)</ref>, MDC ) propose a multidilated convolution block in which the receptive fields of various sizes capture different patterns. As a more generalized approach, FickleNet <ref type="bibr" target="#b14">(Lee et al. 2019</ref>) aggregate diverse localization maps produced by stochastic feature selection. Although they effectively expand the activated regions, some falsely labeled regions outside the object tend to be identified because the receptive fields of these methods are not adaptive to object size. The recently proposed OAA  accumulates attention maps at different training epochs and introduces integral attention learning to enhance attention maps. However, it may produce undesired attention regions due to training instability in the early stage.</p><p>Some other works <ref type="bibr" target="#b0">(Ahn and Kwak 2018;</ref><ref type="bibr" target="#b10">Huang et al. 2018;</ref><ref type="bibr" target="#b18">Shimoda and Yanai 2019)</ref> adopt a regiongrowing technique to expand initial regions. More recently, RRM ) proposed a fully end-to-end network for joint training of classification and segmentation, and SGAN (Yao and Gong 2020) proposed a self-attention network guided by saliency priors that can produce dense and accurate localization maps from rich contextual information. BES <ref type="bibr" target="#b2">(Chen et al. 2020</ref>) explores object boundaries to refine the semantic segmentation output. ICD <ref type="bibr" target="#b7">(Fan et al. 2020)</ref> proposes an intra-class discriminator approach to separate foreground objects and the background within the same image-level class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The overview of our method is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. We sequentially train three different networks for classification, refinement, and segmentation. After training the classification network with the discriminative region suppression (DRS) module, we produce dense localization maps. Using these dense localization maps as ground truth labels for refinement learning, we train the refinement network to produce refined localization maps. Then, pseudo segmentation labels are generated from the refined localization maps and used for training the semantic segmentation network. We measure the segmentation performance to evaluate the quality of our localization maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation</head><p>We produce localization maps from the class-specific feature maps of the last convolutional layer, which have been proven by <ref type="bibr" target="#b25">(Zhang et al. 2018</ref>) to be mathematically equivalent to CAMs. We employ the VGG-16 (Simonyan and Zisserman 2014) as our classification network to produce localization maps. To be specific, we employ modified VGG-16, where all fully connected layers are removed. On top of it, three convolutional layers with 512 channels and kernel size 3, and a convolutional layer with C channels and kernel size 1 are added. Here C is the number of categories. This network produces output feature maps F ∈ R Hout×Wout×C and classification score P = σ(GAP (F )) from input image. H out and W out are the height and width of output feature maps, respectively; GAP (·) is the global average pooling; and σ(·) is the sigmoid function. For each target category c, c-th localization map M c is defined as the normalized c-th feature map F c :</p><formula xml:id="formula_0">M c = ReLU (F c ) max(F c ) .<label>(1)</label></formula><p>From the definition of M , we observe that discriminative object regions are identified with relatively high values on the feature maps F . Based on this observation, we regard the high-value areas on feature maps as discriminative regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Discriminative Region Suppression</head><p>Input: Intermediate feature maps X ∈ R H×W ×K Output: Suppressed feature maps :X ∈ R H×W ×K</p><formula xml:id="formula_1">X max ← extractor(X) //X max ∈ R 1×1×K G ← controller(X) // G ∈ [0, 1] 1×1×K τ ← X max · G // upper bound, τ ∈ R 1×1×K τ ← expand to the same shape of X X ← min(X, τ ) // suppressor</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminative Region Suppression</head><p>To produce dense localization maps, we propose discriminative region suppression (DRS) module. The main problem of segmentation label generation using CAMs is that discriminative regions only appear partially and sparsely, as shown in the second column of <ref type="figure">Figure 1</ref>. To address this issue, DRS aims to spread the attention on discriminative regions to adjacent non-discriminative regions. Specifically, DRS suppresses the attention on discriminative regions, allowing the network to focus on non-discriminative regions. Let X ∈ R H×W ×K be an intermediate feature map, where H, W , and K are the height, width, and the number of channels of X. DRS module consists of three parts: max-element extractor, suppression controller, and suppressor. The max-element extractor extracts K maximum elements from the intermediate feature map X using global max pooling. The output of the extractor is denoted as X max ∈ R 1×1×K . Based on the observation, these K maximum elements are regarded as the criteria of discriminative regions and considered as starting points to be suppressed.</p><p>The suppression controller determines how much to suppress discriminative regions. In detail, it generates G ∈ [0, 1] 1×1×K and each k-th control value in G determines the amount of suppression in X with respect to the corresponding k-th maximum element.</p><p>Using the K maximum elements and K control values, the suppressor suppresses discriminative regions. Specifically, element-wise multiplication of X max and G is regarded as the upper bound of X, denoted as τ = X max · G, τ ∈ R 1×1×K . The regions in X above this upper bound are regarded as discriminative regions to be suppressed. After the upper bound τ is expanded to the same shape of X, the element-wise minimum operation is applied on X and τ to suppress discriminative regions. For example, if the k-th control value is 0.7, X k is suppressed until no element exceeds 70% of the k-th maximum value. In this way, the suppressor bridges the gap between discriminative regions and adjacent non-discriminative regions. The whole process of DRS is described in Algorithm 1 and illustrated in <ref type="figure">Figure 2</ref>.</p><p>For the suppression controller, there are two types of controller: learnable controller and non-learnable controller. If the suppression power is too strong, the discriminative feature extraction power is weakened. The learnable controller adaptively balances between discriminative feature extraction power and suppression power of the classification net- work. Formally, the output of the learnable controller is</p><formula xml:id="formula_2">G = σ(f (GAP (X); θ)),<label>(2)</label></formula><p>where f is a fully connected layer, θ is a learnable parameter of the controller, and G ∈ [0, 1] 1×1×K . Since θ is trained with the classification objective, DRS with a learnable controller adaptively suppresses discriminative regions so as not to damage the discriminative feature extraction power much.</p><p>To produce even more dense localization maps at the expense of discriminative feature extraction power, we forcibly suppress discriminative regions; this is the goal of a nonlearnable controller. For the non-learnable controller, each element of G is set to a constant value δ. We set the hyperparameter δ to a value between 0 and 1, and a lower δ means more intense suppression resulting in more dense localization maps. Compared to the learnable controller, the non-learnable controller does not require additional training parameters but requires a hyperparameter δ. In the experiment section, we analyze both learnable and non-learnable controller with quantitative and qualitative results. <ref type="figure" target="#fig_0">Figure 3</ref> (a) illustrates the process of obtaining dense localization maps from the classification network with DRS. As shown in the third column of <ref type="figure">Figure 1</ref>, DRS reduces the gap between the activation of discriminative regions and adjacent non-discriminative regions to obtain dense localization maps. Note that DRS can be plugged into any layer of a network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Localization Map Refinement Learning</head><p>Although DRS helps produce dense localization maps, the DRS itself lacks the ability to recover missing parts of the target objects or enhance weak attention in adjacent nondiscriminative regions because the goal of the classification network is essentially classification ability, not localization map generation. Motivated by , we introduce an additional learning strategy for localization map refinement to solve the above limitations. This learning strategy for self-enhancement of localization maps is called localization map refinement learning, denoted as DRS †. After training the classification network with DRS, we exploit the output localization maps M ∈ [0, 1] Hout×Wout×C as the ground truth localization maps for refinement learning.</p><p>The network for refinement learning, called refinement network, is based on the VGG-16; all fully-connected layers are removed and three convolutional layers with 512 channels and kernel size 3, and a convolutional layer with C channels and kernel size 1 are appended. The refinement network directly produces refined localization maps N ∈ R Hout×Wout×C , which have the same shape as M . We adopt the mean squared error (MSE) loss function as the refinement loss for the refinement network. Refinement learning is depicted in <ref type="figure" target="#fig_0">Figure 3 (b)</ref>.</p><p>Benefiting from refinement learning, we can obtain more dense and high-quality localization maps through selfenhancement, as shown in <ref type="figure">Figure 1</ref> </p><formula xml:id="formula_3">(DRS † v.s. DRS)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weakly-Supervised Semantic Segmentation</head><p>Using our dense localization maps obtained from the refinement network, we generate pseudo segmentation labels and use them as weak-supervision for the semantic segmentation network. We generate pseudo segmentation labels using object cues and background cues. We extract object cues from the localization maps by taking the pixels whose values are higher than α and extract background cues using salient object detection method <ref type="bibr" target="#b16">(Liu et al. 2019)</ref>, motivated by <ref type="bibr" target="#b21">(Wei et al. 2017</ref>; the pixels with saliency values lower than β are taken as background. Those who belong to neither of the cues are ignored. Following the convention, we train the segmentation network such as <ref type="bibr" target="#b3">(Chen et al. 2014</ref><ref type="bibr" target="#b4">(Chen et al. , 2017</ref> using the generated pseudo segmentation labels, as illustrated in <ref type="figure" target="#fig_0">Figure 3</ref> (c). The segmentation performance is compared with other methods using the same segmentation network to evaluate the quality of pseudo segmentation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset and Evaluation Metrics</head><p>We demonstrate the effectiveness of the proposed approach on the PASCAL VOC 2012 segmentation benchmark dataset <ref type="bibr" target="#b6">(Everingham et al. 2014)</ref>, which contains 20 object categories and one background category. Following the common practice in previous works, the training set is augmented to 10,582 images. We evaluate the performance of our model using the mean intersection-over-union (mIoU) metric and compare it with other state-of-the-art methods on the validation (1,449 images) and test set (1,456 images). For the test results, we submit the prediction outputs to the official PASCAL VOC evaluation server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>For the classification network, we adopt the modified VGG-16 with DRS plugged into every layer, as mentioned in the method section. Its parameters are initialized by the VGG-16 (Simonyan and Zisserman 2014) pre-trained on Ima-geNet <ref type="bibr" target="#b5">(Deng et al. 2009</ref>) except for the additional convolutional layers. We train the classification network with binary cross-entropy loss using the SGD optimizer with a weight decay of 5e-4 and a momentum of 0.9. The initial learning rate is set to 1e-3 and is decreased by a factor of 10 at epoch 5 and 10. For data augmentation, we apply a random crop with 321×321 size, random horizontal flipping, and random color jittering. We use a batch size of 5 and train the classification network for 15 epochs.</p><p>We optimize the refinement network for the refinement learning with MSE loss using Adam <ref type="bibr" target="#b12">(Kingma and Ba 2014)</ref> optimizer with a learning rate of 1e-4. The batch size is 5, the total training epoch is 15, and the learning rate is dropped by a factor of 10 at epoch 5 and 10. We apply the data same augmentation strategy as in the classification network.</p><p>For the segmentation network, we experiment with three architectures: DeepLab-Large-FOV <ref type="bibr" target="#b3">(Chen et al. 2014</ref>  <ref type="bibr" target="#b4">(Chen et al. 2017)</ref> with ResNet-101 backbone. When generating pseudo segmentation labels, we empirically choose α = 0.2 for object cues and β = 0.06 for background cues. Our method is implemented on Pytorch <ref type="bibr" target="#b17">(Paszke et al. 2017)</ref>. We use the DeepLab-Large-FOV code 1 and DeepLab-ASPP code 2 implemented based on the Pytorch framework, following the same hyperparameter settings for training and the conditional random field (CRF) <ref type="bibr" target="#b13">(Krähenbühl and Koltun 2011)</ref> as the original publications. All experiments are performed on NVIDIA TITAN XP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>To analyze the effectiveness of the proposed method, we conduct several experiments. Following the convention of weakly-supervised semantic segmentation, we measure the mIoU score of the segmentation network outputs to evaluate the quality of our localization maps. For all experiments in this section, we adopt the DeepLab-Large-FOV with VGG-16 as the segmentation network and measure the mIoU score on the VOC 2012 validation set. Suppression controller. In the method section, we introduced two types of suppression controller: learnable and non-learnable controller. We investigate both controllers and the effect of the parameter δ for the non-learnable controller using visualization and quantitative analysis. For this anal-1 https://github.com/wangleihitcs/DeepLab-V1-PyTorch 2 https://github.com/kazuto1011/deeplab-pytorch CAM DRS Input Layer1 Layer2 Layer3 Layer4 Layer5 Layer6 <ref type="figure">Figure 5</ref>: Visualization of feature maps on each layer. Note that element-wise averaging and normalization are applied to feature maps of each layer for visualization.</p><p>ysis, we plugin the DRS to all layers of the classification network and skip the refinement learning procedure for precise effect analysis. In the case of a non-learnable controller, we set the same δ for all layers. Firstly, we analyze the effect of δ for the non-learnable controller and compare the output localization maps of each δ in <ref type="figure" target="#fig_1">Figure 4</ref>. When δ is 0.90, the localization map is mostly activated in the head of the cat. Consequently, the gap between discriminative regions and adjacent nondiscriminative regions is large, resulting in sparse localization maps. As the δ gets smaller, activation at the body of the cat becomes higher, and the activation gap between the head and the body of the cat is smaller. This indicates as the δ value decreases, the discriminative regions are further suppressed and the gap between discriminative regions and nondiscriminative regions becomes smaller, resulting in dense localization maps. However, if the δ value is too low (i.e., too much suppression), the gap between the background and the foreground becomes very small, resulting in a noisy localization map as shown in the rightmost result of <ref type="figure" target="#fig_1">Figure 4</ref>. Therefore, it is important to set an appropriate δ value for the non-learnable controller. The quantitative results in <ref type="table" target="#tab_1">Table 1</ref> support our arguments. The non-learnable controller with δ = 0.55 achieves better performance than that of δ = 0.90 (62.8% v.s. 51.9%), but in the case of over-suppression, e.g. the non-learnable controller with δ = 0.40, the performance is rather worse than that of δ = 0.55 (59.6% v.s. 62.8%). Through this experiment, we found that δ = 0.55 yields the best mIoU performance.</p><p>In the case of a learnable controller, it suppresses without generating much noise, creating moderately dense localization maps as in the leftmost of <ref type="figure" target="#fig_1">Figure 4</ref>. Compared to the non-learnable controller with δ = 0.55, the learnable controller produces similar mIoU performance (62.9% v.s. 62.8%) and localization maps. However, the classification accuracy of the learnable controller is much higher (72.6% v.s. 68.7%). From these results, we can notice that the learnable controller adaptively balances between the discriminative feature extraction power and the suppression power, whereas the non-learnable controller forcibly increases the suppression power at the expense of the feature extraction power. Note that the learnable controller is free from hyperparameter (i.e., δ) tuning, but requires additional training parameters (from 21.8M to 24.4M training parameters). layer1 layer2 layer3 layer4 layer5 layer6 mIoU -  Effect of DRS on each layer. To observe the effect of DRS on each layer, we employ two analytical methods: visualization and quantitative analysis. For this analysis, we use the DRS module with the learnable controller and skip refinement learning for precise effect analysis. For visualization, we apply element-wise averaging and normalization from 0 to 1 on feature maps of every layer. <ref type="figure">Figure 5</ref> shows the visualization results of the original CAM and our DRS-plugged classification network. In lower-level layers (i.e., from layer1 to layer3), we notice that the effect of DRS is minor because a network mainly focuses on the local features (e.g., edge) where the gap between the discriminative and adjacent non-discriminative regions tends to be extremely large. Meanwhile, in higher-level layers (i.e., from layer4 to layer6), a network mostly focuses on the global features (e.g., head of a bird) where the gap between the discriminative and adjacent non-discriminative regions is relatively small. In this case, the effect of DRS becomes significant because it suppresses the activation of discriminative regions and expands the attention to non-discriminative regions.</p><formula xml:id="formula_4">- - - - - 50.1% - - - - - 60.8% - - - - 62.2% - - - 62.8% - - 62.9% - 62.7% 62.9% - 58.2% - - 53.6%</formula><p>For quantitative analysis, we plug in and out DRS at each layer and evaluate the performance of each case. The results in <ref type="table" target="#tab_2">Table 2</ref> show that the more we plugin the DRS at higherlevel layers, the higher performance (from 50.1% to 62.9%). On the other hand, applying DRS in lower-level layers has little effect (62.9% v.s. 62.7%). In addition, when we plugout the DRS at higher-level layers, the performance significantly decreases (from 62.9% to 58.2% and 53.6%). From these results, we can conclude that DRS is more effective to produce dense localization maps when applied in higherlevel layers. Improvement through refinement learning. As mentioned in the method section, localization map refinement learning gives a self-enhancement effect to produce high-quality dense localization maps as in <ref type="figure">Figure 1</ref>. The improved mIoU performance is reported in  isfactory results, but DRS † leads to better segmentation results. Note that the learnable controller is used for <ref type="figure">Figure 1</ref> and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-of-the-arts Comparison</head><p>We compare our approach (DRS) with other state-of-the-art weakly-supervised semantic segmentation methods that use only image-level labels as supervision. For comparison, we apply the DRS module to all layers of the classification network and perform refinement learning. We report the performances of both learnable controller and non-learnable controller with δ = 0.55. <ref type="table" target="#tab_5">Table 4</ref> shows the mIoU performance comparison on the PASCAL VOC 2012 validation set and test set. We fairly compare the performance of each of the three architectures of the semantic segmentation network with other works using the same network. Note that S in <ref type="table" target="#tab_5">Table 4</ref> indicates whether the saliency map is used as extra guidance. As shown in <ref type="table" target="#tab_5">Table 4</ref>, DRS outperforms erasing-based methods (e.g., AE-PSL <ref type="bibr" target="#b21">(Wei et al. 2017)</ref>, GAIN , SeeNet <ref type="bibr" target="#b9">(Hou et al. 2018))</ref>, showing that suppression is more effective than erasing. Compared to the recent state-of-the-art methods, we achieve competitive performance despite our simplicity. In contrast to some works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth DRS DRS † <ref type="figure">Figure 6</ref>: Qualitative results on the PASCAL VOC 2012 validation set.</p><p>(e.g., DSRG , FickleNet <ref type="bibr" target="#b14">(Lee et al. 2019)</ref>, AffinityNet (Ahn and Kwak 2018), BES <ref type="bibr" target="#b2">(Chen et al. 2020)</ref>) where CRF in the training stage slows down the training process, our method does not apply CRF during learning, thereby achieving high performance with short training time. Although ICD <ref type="bibr" target="#b7">(Fan et al. 2020</ref>) achieves higher mIoU scores using an intra-class discriminator approach for separating foreground and background within the same imagelevel class, it requires a careful training strategy for stable optimization. Unlike these methods, our approach enables fast and stable training procedure and is the simplest and the most effective way to achieve high segmentation performance. The highlighted rows in <ref type="table" target="#tab_5">Table 4</ref> show that the learnable and non-learnable controllers are both effective, with only a marginal difference in performance. As mentioned in the analysis section, there is a trade-off between the two controller types, so we can choose depending on the situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel approach called DRS for enlarging the object regions highlighted by localization maps. DRS propagates the initial attention to nondiscriminative regions, generating dense localization maps. The main advantage of our approach is that it is intuitive, efficient, and easily applicable to any classification network. Together with refinement learning, our proposed method successfully generates dense segmentation labels that cover the entire target objects. When applied to a weakly-supervised segmentation task, it achieves 71.4% mIoU on pascal VOC segmentation benchmark using only image-level labels as weak supervision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the proposed method. (a) Classification network with DRS for obtaining localization maps, (b) localization map refinement learning, and (c) weakly-supervised semantic segmentation. (a), (b), and (c) are executed sequentially, not simultaneously. Note that GAP in (a) means the global average pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of localization maps of two types of controllers and the non-learnable controller with different δ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Effect of the two types of controllers and the hyperparameter δ for the non-learnable controller. VGG-16 and DeepLab-Large-FOV with ResNet-101 (He et al. 2016) backbones, and DeepLab-ASPP</figDesc><table><row><cell>) with</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effect of DRS in each layer. means DRS is applied.</figDesc><table><row><cell cols="3">suppression controller without refine with refine</cell></row><row><cell>learnable</cell><cell>62.9%</cell><cell>63.5%</cell></row><row><cell>non-learnable (δ=0.55)</cell><cell>62.8%</cell><cell>63.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effect of localization map refinement learning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 (</head><label>3</label><figDesc>+0.6% and +0.8%). In addition,Figure 6shows some segmentation results on the PASCAL VOC 2012, where both DRS and DRS † show sat-</figDesc><table><row><cell>Method</cell><cell>S</cell><cell>val</cell><cell>test</cell></row><row><cell cols="4">Segmentation Network : DeepLab-Large-FOV (VGG-16)</cell></row><row><cell>AE-PSL (Wei et al. 2017)</cell><cell></cell><cell>55.0%</cell><cell>55.7%</cell></row><row><cell>GAIN (Li et al. 2018)</cell><cell></cell><cell>55.3%</cell><cell>56.8%</cell></row><row><cell>MCOF (Wang et al. 2018)</cell><cell></cell><cell>56.2%</cell><cell>57.6%</cell></row><row><cell cols="2">AffinityNet (Ahn and Kwak 2018) -</cell><cell>58.4%</cell><cell>60.5%</cell></row><row><cell>SeeNet (Hou et al. 2018)</cell><cell></cell><cell>61.1%</cell><cell>60.7%</cell></row><row><cell>MDC (Wei et al. 2018)</cell><cell></cell><cell>60.4%</cell><cell>60.8%</cell></row><row><cell>RRM (Zhang et al. 2020)</cell><cell>-</cell><cell>60.7%</cell><cell>61.0%</cell></row><row><cell>FickleNet (Lee et al. 2019)</cell><cell></cell><cell>61.2%</cell><cell>61.8%</cell></row><row><cell>OAA (Jiang et al. 2019)</cell><cell></cell><cell>63.1%</cell><cell>62.8%</cell></row><row><cell>ICD (Fan et al. 2020)</cell><cell></cell><cell>64.0%</cell><cell>63.9%</cell></row><row><cell>BES (Chen et al. 2020)</cell><cell>-</cell><cell>60.1%</cell><cell>61.1%</cell></row><row><cell>Ours (learnable)</cell><cell></cell><cell>63.5%</cell><cell>64.5%</cell></row><row><cell>Ours (non-learnable)</cell><cell></cell><cell>63.6%</cell><cell>64.4%</cell></row><row><cell cols="4">Segmentation Network : DeepLab-Large-FOV (ResNet-101)</cell></row><row><cell>MCOF (Wang et al. 2018)</cell><cell></cell><cell>60.3%</cell><cell>61.2%</cell></row><row><cell>SeeNet (Hou et al. 2018)</cell><cell></cell><cell>63.1%</cell><cell>62.8%</cell></row><row><cell cols="2">AffinityNet (Ahn and Kwak 2018) -</cell><cell>61.7%</cell><cell>63.7%</cell></row><row><cell>FickleNet (Lee et al. 2019)</cell><cell></cell><cell>64.9%</cell><cell>65.3%</cell></row><row><cell>RRM (Zhang et al. 2020)</cell><cell>-</cell><cell>66.3%</cell><cell>65.5%</cell></row><row><cell>OAA (Jiang et al. 2019)</cell><cell></cell><cell>65.2%</cell><cell>66.4%</cell></row><row><cell>ICD (Fan et al. 2020)</cell><cell></cell><cell>67.8%</cell><cell>68.0%</cell></row><row><cell>Ours (learnable)</cell><cell></cell><cell>66.5%</cell><cell>67.5%</cell></row><row><cell>Ours (non-learnable)</cell><cell></cell><cell>66.8%</cell><cell>67.4%</cell></row><row><cell cols="4">Segmentation Network : DeepLab-ASPP (ResNet-101)</cell></row><row><cell>DSRG (Huang et al. 2018)</cell><cell></cell><cell>61.4%</cell><cell>63.2%</cell></row><row><cell>BES (Chen et al. 2020)</cell><cell>-</cell><cell>65.7%</cell><cell>66.6%</cell></row><row><cell>SGAN (Yao and Gong 2020)</cell><cell></cell><cell>67.1%</cell><cell>67.2%</cell></row><row><cell>Ours (learnable)</cell><cell></cell><cell>70.4%</cell><cell>70.7%</cell></row><row><cell>Ours (non-learnable)</cell><cell></cell><cell>71.2%</cell><cell>71.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of state-of-the-art weakly-supervised semantic segmentation methods on the Pascal VOC 2012 dataset. S means the saliency map is used for extra guidance.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Discovering class-specific pixels for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05821</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly Supervised Semantic Segmentation with Boundary Exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062.5</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes Challenge: A Retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Integral Objects With Intra-Class Discriminator for Weakly-Supervised Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>770-778. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selferasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Integral Object Mining via Online Attention Accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5208" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Saliency Guided Self-Attention Network for Weakly and Semi-Supervised Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reliability does matter: An end-to-end weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
