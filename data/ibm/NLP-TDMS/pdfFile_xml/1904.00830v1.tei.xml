<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Depth-Aware Video Frame Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Communication and Network Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Communication and Network Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Communication and Network Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Depth-Aware Video Frame Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>4 Google Overlayed inputs Estimated optical flow Estimated depth map Interpolated frame Ground-truth frame <ref type="figure">Figure 1</ref>. Example of video frame interpolation. We propose a depth-aware video frame interpolation approach to exploit the depth cue for detecting occlusion. Our method estimates optical flow with clear motion boundaries and thus generates high-quality frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Video frame interpolation aims to synthesize nonexistent frames in-between the original frames. While significant advances have been made from the recent deep convolutional neural networks, the quality of interpolation is often reduced due to large object motion or occlusion. In this work, we propose a video frame interpolation method which explicitly detects the occlusion by exploring the depth information. Specifically, we develop a depth-aware flow projection layer to synthesize intermediate flows that preferably sample closer objects than farther ones. In addition, we learn hierarchical features to gather contextual information from neighboring pixels. The proposed model then warps the input frames, depth maps, and contextual features based on the optical flow and local interpolation kernels for synthesizing the output frame. Our model is compact, efficient, and fully differentiable. Quantitative and qualitative results demonstrate that the proposed model performs favorably against state-of-the-art frame interpolation methods on a wide variety of datasets. The source code and pre-trained model are available at https://github.com/baowenbo/DAIN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video frame interpolation has attracted considerable attention in the computer vision community as it can be applied to numerous applications such as slow motion gen- * Corresponding author eration <ref type="bibr" target="#b13">[14]</ref>, novel view synthesis <ref type="bibr" target="#b9">[10]</ref>, frame rate upconversion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, and frame recovery in video streaming <ref type="bibr" target="#b37">[38]</ref>. The videos with a high frame rate can avoid common artifacts, such as temporal jittering and motion blurriness, and therefore are visually more appealing to the viewers. However, with the advances of recent deep convolutional neural networks (CNNs) on video frame interpolation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>, it is still challenging to generate high-quality frames due to large motion and occlusions.</p><p>To handle large motion, several approaches use a coarseto-fine strategy <ref type="bibr" target="#b20">[21]</ref> or adopt advanced flow estimation architecture <ref type="bibr" target="#b22">[23]</ref>, e.g., PWC-Net <ref type="bibr" target="#b33">[34]</ref>, to estimate more accurate optical flow. On the other hand, a straightforward approach to handle occlusion is to estimate an occlusion mask for adaptively blending the pixels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39]</ref>. Some recent methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> learn spatially-varying interpolation kernels to adaptively synthesize pixels from a large neighborhood. Recently, the contextual features from a pre-trained classification network have been shown effective for frame synthesis <ref type="bibr" target="#b22">[23]</ref> as the contextual features are extracted from a large receptive field. However, all the existing methods rely on a large amount of training data and the model capacity to implicitly infer the occlusion, which may not be effective to handle a wide variety of scenes in the wild.</p><p>In this work, we propose to explicitly detect the occlusion by exploiting the depth information for video frame interpolation. The proposed algorithm is based on a simple observation that closer objects should be preferably synthesized in the intermediate frame. Specifically, we first estimate the bi-directional optical flow and depth maps from the two input frames. To warp the input frames, we adopt a flow projection layer <ref type="bibr" target="#b1">[2]</ref> to generate intermediate flows. As multiple flow vectors may encounter at the same position, we calculate the contribution of each flow vector based on the depth value for aggregation. In contrast to a simple average of flows, the proposed depth-aware flow projection layer generates flows with clearer motion boundaries due to the effect of depth.</p><p>Based on our depth-aware flow projection layer, we propose a Depth-Aware video frame INterpolation (DAIN) model that effectively exploits the optical flow, local interpolation kernels, depth maps, and contextual features to synthesize high-quality video frames. Instead of relying on a pre-trained recognition network, e.g., ResNet <ref type="bibr" target="#b12">[13]</ref>, we learn hierarchical features to extract effective context information from a large neighborhood. We use the adaptive warping layer <ref type="bibr" target="#b1">[2]</ref> to warp the input frames, contextual features, and depth maps based on the estimated flows and local interpolation kernels. Finally, we generate the output frame with residual learning. As shown in Figure 1, our model is able to generate frames with clear object shapes and sharp edges. Furthermore, the proposed method can generate arbitrary in-between frames for creating slow-motion videos. Extensive experiments on multiple benchmarks, including the Middlebury <ref type="bibr" target="#b0">[1]</ref>, UCF101 <ref type="bibr" target="#b32">[33]</ref>, Vimeo90K <ref type="bibr" target="#b38">[39]</ref>, and HD <ref type="bibr" target="#b1">[2]</ref> datasets, demonstrate that the proposed DAIN performs favorably against existing video frame interpolation methods.</p><p>We make the following contributions in this work:</p><p>• We explicitly detect the occlusion within a depthaware flow projection layer to preferably synthesize closer objects than farther ones. • We propose a depth-aware video frame interpolation method that tightly integrates optical flow, local interpolation kernels, depth maps, and learnable hierarchical features for high-quality frame synthesis. • We demonstrate that the proposed model is more effective, efficient, and compact than the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video frame interpolation is a long-standing topic and has been extensively studied in the literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>. In this section, we focus our discussions on recent learning-based algorithms. In addition, we discuss the related topic on depth estimation. Video frame interpolation. As a pioneer of CNN-based methods, Long et al. <ref type="bibr" target="#b21">[22]</ref> train a generic CNN to directly synthesize the in-between frame. Their results, however, suffer from severe blurriness as a generic CNN is not able to capture the multi-modal distribution of natural images and videos. Then, Liu et al. <ref type="bibr" target="#b20">[21]</ref> propose the deep voxel flow, a 3D optical flow across space and time, to warp input frames based on a trilinear sampling. While the frames synthesized from flow suffer less blurriness, the flow estimation is still challenging for scenes with large motion. Inaccurate flow may result in severe distortion and visual artifacts.</p><p>Instead of relying on optical flow, the AdaConv <ref type="bibr" target="#b23">[24]</ref> and SepConv <ref type="bibr" target="#b24">[25]</ref> methods estimate spatially-adaptive interpolation kernels to synthesize pixels from a large neighborhood. However, these kernel-based approaches typically require high memory footprint and entail heavy computational load. Recently, Bao et al. <ref type="bibr" target="#b1">[2]</ref> integrate the flow-based and kernel-based approaches into an end-to-end network to inherit the benefit from both sides. The input frames are first warped by the optical flow and then sampled via the learned interpolation kernels within an adaptive warping layer.</p><p>Existing methods implicitly handle the occlusion by estimating occlusion masks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39]</ref>, extracting contextual features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>, or learning large local interpolation kernels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. In contrast, we explicitly detect the occlusion by utilizing the depth information in the flow projection layer. Moreover, we incorporate the depth map with the learned hierarchical features as the contextual information to synthesize the output frame.</p><p>Depth estimation. Depth is one of the key visual information to understand the 3D geometry of a scene and has been exploited in several recognition tasks, e.g., image segmentation <ref type="bibr" target="#b40">[41]</ref> and object detection <ref type="bibr" target="#b34">[35]</ref>. Conventional methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref> require stereo images as input to estimate the disparity. Recently, several learning-based approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref> aim to estimate the depth from a single image. In this work, we use the model of Chen et al. <ref type="bibr" target="#b5">[6]</ref>, which is an hourglass network trained on the MegaDepth dataset <ref type="bibr" target="#b18">[19]</ref>, for predicting the depth maps from the input frames. We show that the initialization of depth network is crucial to infer the occlusion. We then jointly fine-tune the depth network with other sub-modules for frame interpolation. Therefore, our model learns a relative depth for warping and interpolation.</p><p>We note that several approaches jointly estimate optical flow and depth by exploiting the cross-task constraints and consistency <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. While the proposed model also jointly estimates optical flow and depth, our flow and depth are optimized for frame interpolation, which may not resemble the real values of the pixel motion and scene depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Depth-Aware Video Frame Interpolation</head><p>In this section, we first provide an overview of our frame interpolation algorithm. We then introduce the proposed depth-aware flow projection layer, which is the key component to handle occlusion for flow aggregation. Finally, we describe the design of all the sub-modules and provide the implementation details of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Algorithm Overview</head><p>Given two input frames I 0 (x) and I 1 (x), where x ∈ [1, H] × [1, W ] indicates the 2D spatial coordinate of the image plane, and H and W are the height and width of the image, our goal is to synthesize an intermediate frameÎ t at time t ∈ [0, 1]. The proposed method requires optical flows to warp the input frames for synthesizing the intermediate frame. We first estimate the bi-directional optical flows, denoted by F 0→1 and F 1→0 , respectively. To synthesize the intermediate frameÎ t , there are two common strategies. First, one could apply the forward warping <ref type="bibr" target="#b22">[23]</ref> to warp I 0 based on F 0→1 and warp I 1 based on F 1→0 . However, the forward warping may lead to holes on the warped image. The second strategy is to approximate the intermediate flows, i.e., F t→0 and F t→1 , and then apply the backward warping to sample the input frames. To approximate the intermediate flows, one can borrow the flow vectors from the same grid coordinate in F 0→1 and F 1→0 <ref type="bibr" target="#b13">[14]</ref>, or aggregate the flow vectors that pass through the same position <ref type="bibr" target="#b1">[2]</ref>. In this work, we adopt the flow projection layer in Bao et al. <ref type="bibr" target="#b1">[2]</ref> to aggregate the flow vectors while considering the depth order to detect the occlusion.</p><p>After obtaining the intermediate flows, we warp the input frames, contextual features, and depth maps within an adaptive warping layer <ref type="bibr" target="#b1">[2]</ref> based on the optical flows and interpolation kernels. Finally, we adopt a frame synthesis network to generate the interpolated frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Depth-Aware Flow Projection</head><p>The flow projection layer approximates the intermediate flow at a given position x by "reversing" the flow vectors passing through x at time t. If the flow F 0→1 (y) passes through x at time t, one can approximate F t→0 (x) by −t F 0→1 (y). Similarly, we approximate F t→1 (x) by −(1 − t) F 1→0 (y). However, as illustrated in the 1D spacetime example of <ref type="figure" target="#fig_0">Figure 2</ref>, multiple flow vectors could be projected to the same position at time t. Instead of aggregating the flows by a simple average <ref type="bibr" target="#b1">[2]</ref>, we propose to consider the depth ordering for aggregation. Specifically, we assume that D 0 is the depth map of I 0 and S(x) = y :</p><formula xml:id="formula_0">round(y + t F 0→1 (y)) = x, ∀ y ∈ [1, H] × [1, W ] in-</formula><p>dicates the set of pixels that pass through the position x at time t. The projected flow F t→0 is defined by:</p><formula xml:id="formula_1">F t→0 (x) = −t · y∈S(x) w 0 (y) · F 0→1 (y) y∈S(x) w 0 (y) ,<label>(1)</label></formula><p>where the weight w 0 is the reciprocal of depth:</p><formula xml:id="formula_2">w 0 (y) = 1 D 0 (y)</formula><p>.</p><p>(2) Similarly, the projected flow F t→1 can be obtained from the flow F 1→0 and depth map D 1 . By this way, the projected flows tend to sample the closer objects and reduce the contribution of occluded pixels which have larger depth values. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the flow projection used in <ref type="bibr" target="#b1">[2]</ref> generates an average flow vector (the green arrow), which may not point to the correct pixel for sampling. In contrast, the projected flow from our depth-aware flow projection layer (the red arrow) points to the pixel with a smaller depth value.</p><p>On the other hand, there might exist positions where none of the flow vectors pass through, leading to holes in the intermediate flow. To fill in the holes, we use the outside-in strategy <ref type="bibr" target="#b0">[1]</ref>: the flow in the hole position is computed by averaging the available flows from its neighbors:</p><formula xml:id="formula_3">F t→0 (x) = 1 |N (x)| x ∈N (x) F t→0 (x ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">N (x) = {x : |S(x )| &gt; 0} is the 4-neighbors of x.</formula><p>From <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula" target="#formula_3">(3)</ref>, we obtain dense intermediate flow fields F t→0 and F t→1 for warping the input frames. The proposed depth-aware flow projection layer is fully differentiable so that both the flow and depth estimation networks can be jointly optimized during the training. We provide the details of back-propagation in depth-aware flow projection in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Video Frame Interpolation</head><p>The proposed model consists of the following submodules: the flow estimation, depth estimation, context extraction, kernel estimation, and frame synthesis networks. We use the proposed depth-aware flow projection layer to obtain intermediate flows and then warp the input frames, depth maps, and contextual features within the adaptive warping layer. Finally, the frame synthesis network generates the output frame with residual learning. We show the  <ref type="figure">Figure 3</ref>. Architecture of the proposed depth-aware video frame interpolation model. Given two input frames, we first estimate the optical flows and depth maps and use the proposed depth-aware flow projection layer to generate intermediate flows. We then adopt the adaptive warping layer to warp the input frames, depth maps, and contextual features based on the flows and spatially varying interpolation kernels. Finally, we apply a frame synthesis network to generate the output frame.  overall network architecture in <ref type="figure">Figure 3</ref>. Below we describe the details of each sub-network.</p><p>Flow estimation. We adopt the state-of-the-art flow model, PWC-Net <ref type="bibr" target="#b33">[34]</ref>, as our flow estimation network. As learning optical flow without ground-truth supervision is extremely difficult, we initialize our flow estimation network from the pre-trained PWC-Net.</p><p>Depth estimation. We use the hourglass architecture <ref type="bibr" target="#b5">[6]</ref> as our depth estimation network. To obtain meaningful depth information for the flow projection, we initialize the depth estimation network from the pre-trained model of Li et al. <ref type="bibr" target="#b18">[19]</ref>.</p><p>Context extraction. In <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b22">[23]</ref>, the contextual information is extracted by a pre-trained ResNet <ref type="bibr" target="#b12">[13]</ref>, i.e., the feature maps of the first convolutional layer. However, the features from the ResNet are for the image classification task, which may not be effective for video frame interpolation. Therefore, we propose to learn the contextual features. Specifically, we construct a context extraction network with one 7 × 7 convolutional layer and two residual blocks, as shown in <ref type="figure" target="#fig_1">Figure 4</ref>(a). The residual block consists of two 3 × 3 convolutional and two ReLU activation layers <ref type="figure" target="#fig_1">(Figure 4(b)</ref>). We do not use any normalization layer, e.g., batch normalization. We then concatenate the features from the first convolutional layer and the two residual blocks, resulting in a hierarchical feature. Our context extraction network is trained from scratch and, therefore, learns effective contextual features for video frame interpolation.</p><p>Kernel estimation and adaptive warping layer. The local interpolation kernels have been shown to be effective for synthesizing a pixel from a large local neighborhood <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Bao et al. <ref type="bibr" target="#b1">[2]</ref> further integrate the interpolation kernels and optical flow within an adaptive warping layer. The adaptive warping layer synthesizes a new pixel by sampling the input image within a local window, where the center of the window is specified by optical flow. Here we use a U-Net architecture <ref type="bibr" target="#b29">[30]</ref> to estimate 4 × 4 local kernels for each pixel. With the interpolation kernels and intermediate flows generated from the depth-aware flow projection layer, we adopt the adaptive warping layer <ref type="bibr" target="#b1">[2]</ref> to warp the input frames, depth maps, and contextual features. More details of the adaptive warping layer and the configuration of the kernel estimation network are provided in the supplementary materials.</p><p>Frame synthesis. To generate the final output frame, we construct a frame synthesis network, which consists of 3 residual blocks. We concatenate the warped input frames, warped depth maps, warped contextual features, projected flows, and interpolation kernels as the input to the frame synthesis network. In addition, we linearly blend the two warped frames and enforce the network to predict the residuals between the ground-truth frame and the blended frame. We note that the warped frames are already aligned by the optical flow. Therefore, the frame synthesis network focuses on enhancing the details to make the output frame look sharper. We provide the detailed configurations of the frame synthesis network in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>Loss Function. We denote the synthesized frame byÎ t and the ground-truth frame by I GT t . We train the proposed model by optimizing the following loss function:</p><formula xml:id="formula_5">L = x ρ Î t (x) − I GT t (x) ,<label>(4)</label></formula><p>where ρ(x) = √ x 2 + 2 is the Charbonnier penalty function <ref type="bibr" target="#b4">[5]</ref>. We set the constant to 1e − 6.</p><p>Training Dataset. We use the Vimeo90K dataset <ref type="bibr" target="#b38">[39]</ref> to train our model. The Vimeo90K dataset has 51,312 triplets for training, where each triplet contains 3 consecutive video frames with a resolution of 256 × 448 pixels. We train our network to predict the middle frame (i.e., t = 0.5) of each triplet. At the test time, our model is able to generate arbitrary intermediate frames for any t ∈ [0, 1]. We augment the training data by horizontal and vertical flipping as well as reversing the temporal order of the triplet.</p><p>Training Strategy. We use the AdaMax <ref type="bibr" target="#b16">[17]</ref> to optimize the proposed network. We set the β 1 and β 2 to 0.9 and 0.999 and use a batch size of 2. The initial learning rates of the kernel estimation, context extraction, and frame synthesis networks are set to 1e − 4. As both the flow estimation and depth estimation networks are initialized from pre-trained models, we use smaller learning rates of 1e − 6 and 1e − 7, respectively. We jointly train the entire model for 30 epochs and then reduce the learning rate of each network by a factor of 0.2 and fine-tune the entire model for another 10 epochs. We train our model on an NVIDIA Titan X (Pascal) GPU card, which takes about 5 days to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we first introduce the datasets for evaluation. We then conduct ablation study to analyze the contribution of the proposed depth-aware flow projection and hierarchical contextual features. Then, we compare the proposed model with state-of-the-art frame interpolation algorithms. Finally, we discuss the limitation and future work of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Datasets and Metrics</head><p>We evaluate the proposed algorithm on multiple video datasets with different image resolutions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Analysis</head><p>We analyze the contribution of the two key components in the proposed model: the depth-aware flow projection layer and learned hierarchical contextual features. Depth-aware flow projection. To analyze the effectiveness of our depth-aware flow projection layer, we train the following variations (DA is short for Depth-Aware):</p><p>• DA-None: We remove the depth estimation network and use a simple average <ref type="bibr" target="#b1">[2]</ref> to aggregate the flows in the flow projection layer. • DA-Scra: We initialize the depth estimation network from scratch and optimize it with the whole model. • DA-Pret: We initialize the depth estimation network from the pre-trained model of <ref type="bibr" target="#b18">[19]</ref> but freeze the parameters. • DA-Opti: We initialize the depth estimation network from the pre-trained model of <ref type="bibr" target="#b18">[19]</ref> and jointly optimize it with the entire model. We show the quantitative results of the above models in <ref type="table" target="#tab_2">Table 1</ref> and provide a visualization of the depth, flow, and interpolated frames in <ref type="figure" target="#fig_2">Figure 5</ref>. First, the DA-Scra model performs worse than the DA-None model. As shown in the second row of <ref type="figure" target="#fig_2">Figure 5</ref>, the DA-Scra model cannot learn any meaningful depth information from the random initialization. When initializing from the pre-trained depth model, the DA-Pret model shows a substantial performance improvement and generates flow with clear motion boundaries. After jointly optimizing the whole network, the DA-Opti model further improves the depth maps, e.g., the man's legs, and generates sharper edges for the shoes and skateboard in the interpolated frame. The analysis demonstrates that the proposed model effectively utilizes the depth information to generate high-quality results.</p><p>Learned hierarchical context. In the proposed model, we use contextual features as one of the inputs to the frame synthesis network. We analyze the contribution of the different contextual features, including the pre-trained conv1 features (PCF), the learned conv1 features (LCF), and the learned hierarchical features (LHF). In addition, we also consider the depth maps (D) as the additional contextual features.</p><p>We show the quantitative results in <ref type="table">Table 2</ref> and compare the interpolated images in <ref type="figure" target="#fig_3">Figure 6</ref>. Without using any contextual information, the model does not perform well and generates blurred results. By introducing the contextual features, e.g., the pre-trained conv1 features or depth maps, the performance is greatly improved. We further demonstrate that the learned contextual features, especially the learned hierarchical features, lead to a substantial improvement on <ref type="table">Table 2</ref>. Analysis on contextual features. We compare the contextual features from different sources: the pre-trained conv1 features (PCF), learned conv1 features (LCF), learned hierarchical features (LHF), and the depth maps (D).</p><p>Context UCF101 <ref type="bibr" target="#b32">[33]</ref> Vimeo <ref type="bibr" target="#b38">[39]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with State-of-the-arts</head><p>We evaluate the proposed DAIN against the following CNN-based frame interpolation algorithms: MIND <ref type="bibr" target="#b21">[22]</ref>, DVF <ref type="bibr" target="#b20">[21]</ref>, SepConv <ref type="bibr" target="#b24">[25]</ref>, CtxSyn <ref type="bibr" target="#b22">[23]</ref>, ToFlow <ref type="bibr" target="#b38">[39]</ref>, Super SloMo <ref type="bibr" target="#b13">[14]</ref> and MEMC-Net <ref type="bibr" target="#b1">[2]</ref>. In addition, we use the algorithm of Baker et al. <ref type="bibr" target="#b0">[1]</ref> to generate interpolation results for two optical flow estimation algorithms, EpicFlow <ref type="bibr" target="#b28">[29]</ref> and SPyNet <ref type="bibr" target="#b27">[28]</ref>, for comparisons.</p><p>In <ref type="table">Table 3</ref>, we show the comparisons on the EVALUA-TION set of the Middlebury benchmark <ref type="bibr" target="#b0">[1]</ref>, which are also available on the Middlebury website. The proposed model performs favorably against all the compared methods. At the time of submission, our method ranks 1 st in terms of NIE and 3 rd in terms of IE among all published algorithms on the Middlebury website. We show a visual comparison in <ref type="figure">Figure 7</ref>, where the EpicFlow <ref type="bibr" target="#b28">[29]</ref>, ToFlow <ref type="bibr" target="#b38">[39]</ref>, SepConv <ref type="bibr" target="#b24">[25]</ref> and MEMC-Net <ref type="bibr" target="#b1">[2]</ref> methods produce ghosting artifacts on the balls or foot. In contrast, the proposed method reconstructs a clear shape of the ball. Compared to the CtxSyn <ref type="bibr" target="#b22">[23]</ref> and Super SloMo <ref type="bibr" target="#b13">[14]</ref> methods, our approach generates more details on the slippers and foot.</p><p>In <ref type="table" target="#tab_5">Table 4</ref>, we provide quantitative performances on the UCF101 <ref type="bibr" target="#b32">[33]</ref>, Vimeo90K <ref type="bibr" target="#b38">[39]</ref>, HD <ref type="bibr" target="#b1">[2]</ref>, and Middle- <ref type="table">Table 3</ref>. Quantitative comparisons on the Middlebury EVALUATION set. The numbers in red and blue represent the best and second best performance. The proposed DAIN method performs favorably against other approaches in terms of IE and NIE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mequon</head><p>Schefflera <ref type="table">Urban  Teddy  Backyard  Basketball  Dumptruck  Evergreen  Average   IE  NIE  IE  NIE  IE  NIE  IE  NIE  IE  NIE  IE  NIE  IE  NIE  IE  NIE  IE</ref>   <ref type="figure">Figure 7</ref>. Visual comparisons on the Middlebury EVALUATION set. The proposed method reconstructs a clear shape of the ball and restores more details on the slippers and foot.  <ref type="figure">Figure 8</ref>. Visual comparisons on the UCF101 dataset <ref type="bibr" target="#b32">[33]</ref>. The proposed method aligns the content (e.g., the pole) well and restores more details on the man's leg.</p><p>bury <ref type="bibr" target="#b0">[1]</ref> OTHER set. Our approach performs favorably against existing methods for all the datasets, especially on the Vimeo90K <ref type="bibr" target="#b38">[39]</ref> dataset with a 0.42dB gain over MEMC-Net <ref type="bibr" target="#b1">[2]</ref> in terms of PSNR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MEMC-Net DAIN (Ours)</head><p>Ground-truth <ref type="figure">Figure 9</ref>. Visual comparisons on the HD dataset <ref type="bibr" target="#b1">[2]</ref>. The Sep-Conv <ref type="bibr" target="#b24">[25]</ref> method cannot align the content as the motion is larger than the size of interpolation kernels, e.g., 51 × 51. The proposed DAIN reveals more details on the hair and eyes than the state-ofthe-art MEMC-Net <ref type="bibr" target="#b1">[2]</ref>.</p><p>In <ref type="figure">Figure 8</ref>, the SPyNet <ref type="bibr" target="#b27">[28]</ref>, EpicFlow <ref type="bibr" target="#b28">[29]</ref> and Sep-Conv <ref type="bibr" target="#b24">[25]</ref> methods cannot align the pole well and thus produce ghosting or broken results. The MIND <ref type="bibr" target="#b21">[22]</ref>, DVF <ref type="bibr" target="#b20">[21]</ref>, ToFlow <ref type="bibr" target="#b38">[39]</ref> and MEMC-Net <ref type="bibr" target="#b1">[2]</ref> methods generate blurred results on the man's leg. In contrast, the proposed method aligns the pole well and generates clearer results. In <ref type="figure">Figure 9</ref>, we show an example from the HD dataset. The Sep-Conv <ref type="bibr" target="#b24">[25]</ref> method cannot align the content at all as the motion is larger than the size of the interpolation kernels (e.g., 51 × 51). Compared to the MEMC-Net <ref type="bibr" target="#b1">[2]</ref>, our method restores clearer details on the hair and face (e.g., eyes and mouth). Overall, the proposed DAIN generates more visually pleasing results with fewer artifacts than existing frame interpolation methods. In our supplementary materials, we demonstrate that our method can generate arbitrary intermediate frames to create 10× slow-motion videos. More image and video results are available in our project website.</p><p>We also list the number of model parameters and execution time (test on a 640 × 480 image) of each method in <ref type="table" target="#tab_5">Table 4</ref>. The proposed model uses a similar amount of parameters as the SepConv <ref type="bibr" target="#b24">[25]</ref> but runs faster. Compared to the MEMC-Net <ref type="bibr" target="#b1">[2]</ref>, we use 69% fewer parameters (see the detailed comparison of the sub-modules in <ref type="table" target="#tab_6">Table 5</ref>) and achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussions and limitations</head><p>The proposed method relies on the depth maps to detect the occlusion for flow aggregation. However, in some challenging cases, the depth maps are not estimated well and lead to ambiguous object boundaries, as shown in the highlight region of <ref type="figure" target="#fig_4">Figure 10</ref>. Our method generates blurred results with unclear boundaries (e.g., between the shoe and skateboard). However, compared to the ToFlow <ref type="bibr" target="#b38">[39]</ref>, our method still reconstructs the skateboard well. While our current model estimates depth from a single image, it would be beneficial to obtain more accurate depth maps by jointly estimating the depth from the two input frames or modeling the consistency between optical flow and depth <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a novel depth-aware video frame interpolation algorithm, which explicitly detects the occlusion using the depth information. We propose a depthaware flow projection layer that encourages sampling of closer objects than farther ones. Furthermore, we exploit the learned hierarchical features and depth maps as the contextual information to synthesize the intermediate frame. The proposed model is compact and efficient. Extensive quantitative and qualitative evaluations demonstrate that the proposed method performs favorably against existing frame interpolation algorithms on diverse datasets. The state-of-theart achievement from the proposed method sheds light for future research on exploiting the depth cue for video frame interpolation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Proposed depth-aware flow projection. The existing flow projection method<ref type="bibr" target="#b1">[2]</ref> obtains an average flow vector which may not point to the correct object or pixel. In contrast, we rewrite the flows according to the depth values and generate the flow vector pointing to the closer pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Structure of the context extraction network. Instead of using the weights of a pre-trained classification network<ref type="bibr" target="#b22">[23]</ref>, we train our context extraction network from scratch and learn hierarchical features for video frame interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Effect of the depth-aware flow projection. The DA-Scra model cannot learn any meaningful depth information. The DA-Pret model initializes the depth estimation network from a pre-trained model and generates clear motion boundaries for frame interpolation. The DA-Opti model further optimizes the depth maps and generates sharper edges and shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Effect of contextual features. The proposed model uses the learned hierarchical features (LHF) and depth maps (D) for frame synthesis, which generates clearer and sharper content. the Vimeo90K and the Middlebury datasets. The model using both the depth maps and learned hierarchical features also generates sharper and clearer content.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 .</head><label>10</label><figDesc>Limitations of the proposed method. When the depth maps are not estimated well, our method tends to generate blurred results and less clear boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Analysis on Depth-Aware (DA) flow projection. M.B. is short for the OTHER set of the Middlebury dataset. The proposed model (DA-Opti) shows a substantial improvement against the other variations.</figDesc><table><row><cell cols="2">UCF101 [33] Vimeo90K [39] M.B. [1]</cell><cell>HD [2]</cell></row><row><cell>Method</cell><cell></cell><cell></cell></row><row><cell>PSNR SSIM PSNR SSIM</cell><cell>IE</cell><cell>PSNR SSIM</cell></row><row><cell>DA-None 34.91 0.9679 34.47 0.9746</cell><cell>2.10</cell><cell>31.46 0.9174</cell></row><row><cell>DA-Scra 34.85 0.9677 34.30 0.9735</cell><cell>2.13</cell><cell>31.42 0.9164</cell></row><row><cell>DA-Pret 34.91 0.9680 34.52 0.9747</cell><cell>2.07</cell><cell>31.52 0.9178</cell></row><row><cell>DA-Opti 34.99 0.9683 34.71 0.9756</cell><cell>2.04</cell><cell>31.70 0.9193</cell></row><row><cell cols="3">Middlebury. The Middlebury benchmark [1] is widely</cell></row><row><cell cols="3">used to evaluate video frame interpolation methods. There</cell></row><row><cell cols="3">are two subsets. The OTHER set provides the ground-</cell></row><row><cell cols="3">truth middle frames, while the EVALUATION set hides the</cell></row><row><cell cols="3">ground-truth and can be evaluated by uploading the results</cell></row><row><cell cols="3">to the benchmark website. The image resolution in this</cell></row><row><cell>dataset is around 640 × 480 pixels.</cell><cell></cell><cell></cell></row><row><cell cols="3">Vimeo90K. There are 3,782 triplets in the test set of the</cell></row><row><cell cols="3">Vimeo90K dataset [39]. The image resolution in this dataset</cell></row><row><cell>is 448 × 256 pixels.</cell><cell></cell><cell></cell></row></table><note>UCF101. The UCF101 dataset [33] contains videos with a large variety of human actions. There are 379 triplets with a resolution of 256 × 256 pixels. HD. Bao et al. [2] collect 11 high-resolution videos for eval- uation. The HD dataset consists of four 1920×1080p, three 1280 × 720p and four 1280 × 544p videos. The motion in this dataset is typically larger than other datasets. Metrics. We compute the average Interpolation Error (IE) and Normalized Interpolation Error (NIE) on the Middle- bury dataset. Lower IEs or NIEs indicate better perfor- mance. We evaluate the PSNR and SSIM on the UCF101, Vimeo90K, and the HD datasets for comparisons.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>NIEEpicFlow<ref type="bibr" target="#b28">[29]</ref> 3.17 0.62 3.79 0.70 4.28 1.06 6.37 1.09 11.2 1.18 6.23 1.10 8.11 1.00 8.76 1.04 6.49 0.97</figDesc><table><row><cell cols="6">SepConv-L 1 [25] 2.52 0.54 3.56 0.67 4.17 1.07 5.41 1.03 10.2 0.99 5.47 0.96 6.88 0.68 6.63 0.70 5.61 0.83</cell></row><row><cell cols="6">ToFlow [39] 2.54 0.55 3.70 0.72 3.43 0.92 5.05 0.96 9.84 0.97 5.34 0.98 6.88 0.72 7.14 0.90 5.49 0.84</cell></row><row><cell cols="6">Super SloMo [14] 2.51 0.59 3.66 0.72 2.91 0.74 5.05 0.98 9.56 0.94 5.37 0.96 6.69 0.60 6.73 0.69 5.31 0.78</cell></row><row><cell cols="6">CtxSyn [23] 2.24 0.50 2.96 0.55 4.32 1.42 4.21 0.87 9.59 0.95 5.22 0.94 7.02 0.68 6.66 0.67 5.28 0.82</cell></row><row><cell cols="6">MEMC-Net [2] 2.47 0.60 3.49 0.65 4.63 1.42 4.94 0.88 8.91 0.93 4.70 0.86 6.46 0.66 6.35 0.64 5.24 0.83</cell></row><row><cell cols="6">DAIN (Ours) 2.38 0.58 3.28 0.60 3.32 0.69 4.65 0.86 7.88 0.87 4.73 0.85 6.36 0.59 6.25 0.66 4.86 0.71</cell></row><row><cell>Inputs</cell><cell>ToFlow</cell><cell>EpicFlow</cell><cell>SepConv-L 1 Super SloMo</cell><cell>CtxSyn</cell><cell>MEMC-Net DAIN (Ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Quantitative comparisons on the UCF101, Vimeo90K, HD, and Middlebury OTHER datasets. The numbers in red and blue indicate the best and second best performance. We also compare the model parameters and runtime of each method.</figDesc><table><row><cell></cell><cell cols="2">Method #Parameters (million)</cell><cell>Runtime (seconds)</cell><cell cols="2">UCF101 [33] PSNR SSIM</cell><cell cols="2">Vimeo90K [39] PSNR SSIM</cell><cell>Middlebury [1] IE</cell><cell cols="2">HD [2] PSNR SSIM</cell></row><row><cell></cell><cell>SPyNet [28]</cell><cell>1.20</cell><cell>0.11</cell><cell>33.67</cell><cell>0.9633</cell><cell>31.95</cell><cell>0.9601</cell><cell>2.49</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>EpicFlow [29]</cell><cell>-</cell><cell>8.80</cell><cell>33.71</cell><cell>0.9635</cell><cell>32.02</cell><cell>0.9622</cell><cell>2.47</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MIND [22]</cell><cell>7.60</cell><cell>0.01</cell><cell>33.93</cell><cell>0.9661</cell><cell>33.50</cell><cell>0.9429</cell><cell>3.35</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DVF [21]</cell><cell>1.60</cell><cell>0.47</cell><cell>34.12</cell><cell>0.9631</cell><cell>31.54</cell><cell>0.9462</cell><cell>7.75</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ToFlow [39]</cell><cell>1.07</cell><cell>0.43</cell><cell>34.58</cell><cell>0.9667</cell><cell>33.73</cell><cell>0.9682</cell><cell>2.51</cell><cell>29.37</cell><cell>0.8772</cell></row><row><cell></cell><cell>SepConv-L f [25]</cell><cell>21.6</cell><cell>0.20</cell><cell>34.69</cell><cell>0.9655</cell><cell>33.45</cell><cell>0.9674</cell><cell>2.44</cell><cell>30.61</cell><cell>0.8978</cell></row><row><cell></cell><cell>SepConv-L 1 [25]</cell><cell>21.6</cell><cell>0.20</cell><cell>34.78</cell><cell>0.9669</cell><cell>33.79</cell><cell>0.9702</cell><cell>2.27</cell><cell>30.87</cell><cell>0.9077</cell></row><row><cell></cell><cell>MEMC-Net [2]</cell><cell>70.3</cell><cell>0.12</cell><cell>34.96</cell><cell>0.9682</cell><cell>34.29</cell><cell>0.9739</cell><cell>2.12</cell><cell>31.39</cell><cell>0.9163</cell></row><row><cell></cell><cell>DAIN (Ours)</cell><cell>24.0</cell><cell>0.13</cell><cell>34.99</cell><cell>0.9683</cell><cell>34.71</cell><cell>0.9756</cell><cell>2.04</cell><cell>31.64</cell><cell>0.9205</cell></row><row><cell>Inputs</cell><cell>SPyNet</cell><cell>EpicFlow</cell><cell>MIND</cell><cell>DVF</cell><cell cols="2">ToFlow</cell><cell cols="4">SepConv-L 1 MEMC-Net DAIN (Ours) Ground-truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparisons with MEMC-Net [2] on parameter and runtime. We list the parameters (million) and runtime (seconds) of each sub-module in the MEMC-Net and the proposed model.</figDesc><table><row><cell></cell><cell cols="2">MEMC-Net [2]</cell><cell cols="2">DAIN (Ours)</cell></row><row><cell>Sub-module</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">#Parameters Runtime #Parameters Runtime</cell></row><row><cell>Depth</cell><cell>-</cell><cell>-</cell><cell>5.35</cell><cell>0.043</cell></row><row><cell>Flow</cell><cell>38.6</cell><cell>0.024</cell><cell>9.37</cell><cell>0.074</cell></row><row><cell>Context</cell><cell>0.01</cell><cell>0.002</cell><cell>0.16</cell><cell>0.002</cell></row><row><cell>Kernel</cell><cell>14.2</cell><cell>0.008</cell><cell>5.51</cell><cell>0.004</cell></row><row><cell>Mask</cell><cell>14.2</cell><cell>0.008</cell><cell>-</cell><cell>-</cell></row><row><cell>Synthesis</cell><cell>3.30</cell><cell>0.080</cell><cell>3.63</cell><cell>0.002</cell></row><row><cell>Total</cell><cell>70.3</cell><cell>0.122</cell><cell>24.0</cell><cell>0.125</cell></row><row><cell>Overlayed inputs</cell><cell></cell><cell>SepConv-L f</cell><cell cols="2">SepConv-L 1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment.</head><p>This work was supported in part by National Key Research and Development Program of China (2016YFB1001003), NSFC (61771306), Natural Science Foundation of Shanghai (18ZR1418100), Chinese National Key S&amp;T Special Program (2013ZX01033001-002-002), Shanghai Key Laboratory of Digital Media Processing and Transmissions (STCSM 18DZ2270700 and 18DZ1112300). It was also supported in part by NSF Career Grant (1149783) and gifts from Adobe, Verisk, and NEC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-Order Model and Dynamic Filtering for Frame Rate Up-Conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A method for motion adaptive frame rate up-conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Castagno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haavisto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramponi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>TCSVT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two deterministic half-quadratic regularization algorithms for computed imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Blanc-Féraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Map-based motion refinement algorithm for block-based motion-compensated frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Highquality depth from uncalibrated small motion clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">New frame rate up-conversion algorithms with low computational complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Sunwoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overlapped block motion compensation: An estimation-theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depth estimation and image restoration using defocused stereo pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mudenagudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Depthencoded hough voting for joint object detection and shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Frame rate upconversion using trilateral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling and optimization of high frame rate video transmission over wireless networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TWC</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Video enhancement with task-oriented flow. arXiv</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic segmentation of urban scenes using dense depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
