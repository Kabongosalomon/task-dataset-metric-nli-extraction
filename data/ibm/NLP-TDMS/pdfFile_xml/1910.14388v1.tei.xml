<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image-Conditioned Graph Generation for Road Network Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Belli</surname></persName>
							<email>davidebelli95@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Informatics Institute University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
							<email>t.n.kipf@uva.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">Informatics Institute University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image-Conditioned Graph Generation for Road Network Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep generative models for graphs have shown great promise in the area of drug design, but have so far found little application beyond generating graph-structured molecules. In this work, we demonstrate a proof of concept for the challenging task of road network extraction from image data. This task can be framed as image-conditioned graph generation, for which we develop the Generative Graph Transformer (GGT), a deep autoregressive model that makes use of attention mechanisms for image conditioning and the recurrent generation of graphs. We benchmark GGT on the application of road network extraction from semantic segmentation data. For this, we introduce the Toulouse Road Network dataset, based on real-world publicly-available data. We further propose the StreetMover distance: a metric based on the Sinkhorn distance for effectively evaluating the quality of road network generation. The code and dataset are publicly available 1 . <ref type="bibr" target="#b0">1</ref> The code and dataset are available in https://github.com/davide-belli/generative-graph-transformer and https://github.com/davide-belli/toulouse-road-network-dataset</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hundreds of thousands kilometres of roads around the world have not been mapped yet <ref type="bibr" target="#b0">[1]</ref>. Collecting and regularly updating world maps is key to improving autonomous driving systems, optimizing industrial transportation and helping first-aid operations in case of natural disasters. Current research on automated road network extraction tries to find efficient and scalable solutions to this task by using state-of-the-art deep learning models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. In particular, existing approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> generate a semantic segmentation of road networks and then apply manually-engineered heuristics and post-processing steps to extract graph representations. Post-processing is a critical component in those methods, used for example to join disconnected road sections or to remove isolated subgraphs resulting from noisy segmentations. In this work, we present a graph generative model to automatically extract road networks from semantic segmentation data, removing the necessity for post-processing heuristics and allowing for a complete end-to-end solution to the problem.</p><p>The contribution of this paper is threefold:</p><p>• We release the Toulouse Road Network dataset for the task of road network extraction from semantic segmentation of satellite images. • We introduce the Generative Graph Transformer, a deep autoregressive model for conditional graph generation which scales well to large graphs thanks to a recurrent formulation and self-attention mechanisms. • We propose the StreetMover distance, an efficient metric for the comparison of road networks.</p><p>This metric is based on the Sinkhorn distance <ref type="bibr" target="#b9">[10]</ref> between point clouds, and it is invariant with respect to node permutation, graph translation and rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generative Graph Transformer</head><p>There are two main approaches to neural network-based graph generation explored in recent literature: one-shot and recurrent generation. One-shot approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> emit graphs at once in the form of complete adjacency and feature matrices. In the case of recurrent generation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, a deep-autoregressive model sequentially expands a graph by iteratively adding new nodes and edges. Conditional generation of graphs has been applied for scene graph generation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, drug discovery <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref> and for modeling chemical reactions <ref type="bibr" target="#b20">[21]</ref>. Concurrently and independently of our work Chu et al. <ref type="bibr" target="#b21">[22]</ref> introduced a generative model for street network extraction from satellite images. The proposed GGT model is designed for the recurrent, conditional generation of graphs and consists of an encoder-decoder architecture as outlined in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Encoder</head><p>Self-Attentive Decoder . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditioning Vector</head><p>Input Image G 1 <ref type="figure">Figure 1</ref>: Outline of GGT. An image I is passed through the encoder which produces a conditioning vector c t using a context attention mechanisms on the previously generated node. The self-attentive decoder uses the conditioning vector and the previously generated nodes to predict the next node in the graph. This sequential process incrementally generates the graph G T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I c t G 2 G 3 G T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-Attentive Graph Generation</head><p>The proposed model takes as input a grayscale image I ∈ R 64×64 and incrementally generates a graph G = Ã ∈ R N ×N ,X ∈ R N ×2 representing the road network. Here, N is the number of nodes in the graph,Ã is a soft adjacency matrix containing probability values in range [0,1] andX is a node feature matrix containing normalized coordinates in range [-1,+1].</p><p>The decoder network is inspired by the self-attentive decoder in the Transformer <ref type="bibr" target="#b22">[23]</ref>, which has proven effective in a variety of tasks and domains <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. At each time-step t in the generation, the inputs to the decoder are the previously generated node featuresx &lt;t , adjacency vectorsã &lt;t , and a conditioning vector c t obtained from the image encoder. The concatenated inputs are positionally encoded using a sinusoidal vector p t <ref type="bibr" target="#b22">[23]</ref>, and then fed to a linear layer to obtain the initial hidden representation h</p><formula xml:id="formula_0">(0) t = W in ã t−1 ,x t−1 , c t + p t ∈ R d , where d = 256.</formula><p>Afterwards, a series of L decoding blocks with 1 ≤ l ≤ L, defined as follows, are applied:</p><formula xml:id="formula_1">h (l) t = LN h (l) t + MultiHead(h (l) t , h (l) &lt;t ) , h (l+1) t = LN h (l) t + W (l) n ReLU(W (l) mh (l) t ) (1)</formula><p>where the MultiHead operator refers to the self-attention as in Vaswani et al. <ref type="bibr" target="#b22">[23]</ref>, LN is layer normalization <ref type="bibr" target="#b29">[30]</ref>, and ∀l, W</p><formula xml:id="formula_2">(l) m ∈ R 2048×d , W (l) n ∈ R d×2048 , h (l) t ,h (l) t ∈ R d .</formula><p>The representation after the last layer is then fed to two distinct MLP heads which emit the predicted node coordinates and (soft) adjacency vector as follows:</p><formula xml:id="formula_3">a t = σ W a2 ReLU(W a1 h (L) t ) ,x t = tanh W x2 ReLU(W x1 h (L) t ) (2) where W a1 , W x1 ∈ R 128×d , W a2 ∈ R M ×128 , W x2 ∈ R 2×128</formula><p>, and M is the maximum size of the frontier in the BFS-ordering (see Appendix A.2 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Conditioning</head><p>Image encoder To condition the generative process, we use a simple CNN encoder which takes as input a semantic segmentation I ∈ R 64×64 and emits a low-dimensional representation as c = CNN(I) ∈ R 900 . To speed up the training and improve the convergence of the end-to-end model, we pre-train the encoder as part of an autoencoder trained for a reconstruction task.</p><p>Image attention In the basic implementation of the CNN architecture, the image features are kept the same for every time-step in the graph generation, i.e., c t = c ∀t. However, the decoder model may benefit from focusing on different parts of the image depending on what components are currently being generated. For this reason, we introduce an image attention mechanism on the CNN encoder based on the context attention introduced by Xu et al. <ref type="bibr" target="#b30">[31]</ref>. This mechanism is implemented as an MLP which takes as input the flattened visual features c = CNN(I) ∈ R 900 and the previously generated node featuresx &lt;t andã &lt;t , and outputs a mask vector:</p><formula xml:id="formula_4">s t = W c2 ReLU(W c1 [ã &lt;t ,x &lt;t , c]), m t = exp(s t ) |st| i=1 exp(s ti ) , c t = c m t<label>(3)</label></formula><p>where W c1 , W c2 ∈ R 1800×900 , s t is the vector of attention scores of length |s t | = 900, and m t is the mask vector applied on the visual features through the element-wise product operation . To benchmark the Generative Graph Transformer, we introduce the Toulouse Road Network dataset, based on publicly available data from OpenStreetMap 2 . The dataset contains patches of road maps from the city of Toulouse, represented both as graphs G = (A, X) and as grayscale segmentation images I. The generation of the dataset includes a sequence of preprocessing and filtering steps to clean the data, followed by data augmentation and the representation of graphs under a canonical ordering, as in You et al. <ref type="bibr" target="#b15">[16]</ref>. In Appendix A.1, we present additional dataset details and discuss the procedure used to generate the dataset. This work introduces the StreetMover distance to evaluate generative models for road networks. Road networks are first represented as point clouds by sampling a fixed number of equidistant points over the edges of the graphs. Then, the StreetMover distance is computed as the optimal cost of moving the predicted proposal point cloud to the ground-truth target point cloud. Sinkhorn iterations <ref type="bibr" target="#b9">[10]</ref> are used for an efficient approximation of the Wasserstein distance. The StreetMover distance can be interpreted as describing the cost of moving road segments in the predicted graph to match the shape of the ground-truth target graph, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The main benefits of the StreetMover distance are its interpretability, scalability, and invariance with respect to node permutation graph translations, and rotations. In Appendix A.3, we further motivate the introduction of the StreetMover distance by discussing other related evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Toulouse Road Network Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">StreetMover Distance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Setup</head><p>We evaluate the proposed Generative Graph Transformer in the road extraction task on the Toulouse Road Network dataset. We compare the performance of the model with simple MLP and RNN baselines and with an extension of GraphRNN <ref type="bibr" target="#b15">[16]</ref> for labeled graph generation. We also conduct an ablation study comparing the GGT with and without context attention in the encoder. We choose the StreetMover distance as the main metric, supported with additional metrics such as the average error in the number of nodes and edges (∆|V |, ∆|E|). We report additional details on baseline implementations and hyper-parameter setup in Appendix A.2.</p><p>The models are trained optimizing the following loss:</p><formula xml:id="formula_5">L = λL A + (1 − λ)L X = λ BCE(Ã, A) + (1 − λ) MSE(X, X)<label>(4)</label></formula><p>which combines the reconstruction errors in the predicted adjacency and node feature matricesÃ andX. The hyper-parameter λ regulates the trade-off between the two components. We use teacher forcing in the self-attentive decoder during training. <ref type="table" target="#tab_0">Table 1</ref> reports the evaluations on the Toulouse Road Network dataset. The proposed GGT model achieves the best performance according to all metrics. The GGT decreases the average StreetMover distance by 45% with respect to the simple RNN baseline, compared to only 15% decrease when choosing GraphRNN. The one-shot generation using the MLP does not seem to be effective for this task, as proven by the sharp decline in all the scores. The ablation study on the context attention confirms that introducing attention in the encoder contributes to improvements in the overall results. To better understand the performance of GGT, we present in <ref type="figure" target="#fig_3">Fig. 4</ref> a set of qualitative studies to analyze the reconstructions and attention mechanisms. Overall, the reconstructed graphs have high fidelity, even in more complicated cases with loops, cluttered edges or large graphs (see <ref type="figure" target="#fig_3">Fig. 4a</ref>). Moreover, we show in <ref type="figure" target="#fig_3">Fig. 4b</ref> how graphs from adjacent patches can be easily merged to reconstruct road networks at larger scales. Finally, by inspecting the self-attention layers in the GGT, we see in <ref type="figure" target="#fig_3">Fig. 4c</ref> how some heads are responsible for learning the structure in the graphs, emitting attention weights that highly correlate with corresponding lower triangular adjacency matrices.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In this work we presented the real-world Toulosue Road Network dataset to benchmark methods for road network extraction from images. We propose the Generative Graph Transformer, a deep autoregressive model based on self-attention for the recurrent, conditional generation of graphs. Moreover, we introduced the StreetMover distance, a scalable, efficient and permutation-invariant metric for graph comparison.</p><p>A challenge that remains open in this field is the development of a complete end-to-end solution combining semantic segmentation and graph extraction. Applying the proposed GGT model to other graph generation tasks, such as drug design, is another interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Dataset details <ref type="figure">Figure 5</ref>: Criteria for defining the different splits in the dataset. Gray areas are discarded due to the content overlap resulting from the augmentation by translation.</p><p>The Toulouse Road Network dataset contains 111,034 data points (map tiles), of which 80,357 are in the training set (around 72.4%), 11,679 in the validation set (around 10.5%), and 18,998 in the test set (around 17.1%). Each tile represents a squared region of side 0.001 degrees of latitude and longitude on the map, which corresponds to a square of around 110 meters. The semantic segmentation of each patch is represented as a 64 × 64 grayscale image. The three splits are obtained using the criteria in <ref type="figure">Fig. 5</ref>. This criterion is chosen to optimize the diversity inside each split while keeping similar the data distributions between different splits. Also, this criterion minimizes the amount of data that has to be discarded in boundary regions between splits due to the use of horizontal and vertical translation in the augmentation procedure.</p><p>Dataset generation To generate the Toulouse Road Network dataset we start from publicly available data from OpenStreetMap, where the road network is represented as a set of segments defined by the coordinates of extreme points. In the first step of dataset generation, we extract squared tiles from the map, each with side 0.001 degrees. The following preprocessing steps include: i) detection of edge intersections, ii) merging nodes that are distant less than 0.00005 degrees apart, and iii) merging consecutive edges resulting in an almost straight road, where the incidence angle is between 75 • and 90 • . Furthermore, we filter the proposed data points in order to remove trivial graphs (|V | ≤ 3), and extremely cluttered graphs, removing the right tail of the population after the 95th percentile (|V | ≥ 10, |E| ≥ 16). Finally, we include the possibility to augment the dataset with translation, flip and rotation, resulting in an augmentation factor of up to 128 times the number of original data points. Samples of networks in our dataset are shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, where every graph is plotted as a 64 × 64 image.</p><p>In order to train auto-regressive models, we also define a canonical ordering for each graph in the dataset based on a BFS-ordering over the nodes, breaking ties in the ordering consistently. When choosing the initial node in the sequence, or when continuing the BFS after completing the traversing of a connected component, we select the top-left node among the unvisited ones. In the case of multiple edges branching out from the current node, we order the edges clockwise with regards to the incoming edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Details on Experimental Setup</head><p>Training settings For all the baselines and Generative Graph Transformer variations, we run the same hyper-parameter search on: learning rate, batch size, weight decay, and λ coefficient. In all the experiments we use the Adam optimizer <ref type="bibr" target="#b31">[32]</ref> with parameters η ∈ [3 · 10 −4 ; 5 · 10 −4 ], β 1 = 0.9, β 2 = 0.999 and = 10 −8 . For all the models, the batch size is set to 64, except for the RNN where we find 16 to be better. Weight decay parameters in the range [10 −5 , 5 · 10 −5 ] are found to be optimal for regularization. For the λ hyper-parameter, we notice best performance in the range [0.30, 0.70], and we set λ = 0.5 in our experiments. We also try different sizes for the output of the node-wise GRU in GraphRNN, and for the number of decoder blocks and heads in the GGT.</p><p>We train all the models for 200 epochs (around 10 hours) on an Nvidia GeForce 1080Ti GPU. We use early stopping according to the StreetMover distance in the validation set to select the best model.</p><p>At test time, we greedily generate binary vectors from the predicted adjacency vectorsã by thresholding the values at 0.5. We expect that more advanced techniques like beam search or better sampling strategies <ref type="bibr" target="#b32">[33]</ref> could significantly improve the results. We leave these experiments as future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details and baselines</head><p>The baselines for our experiments are implemented as follows. The MLP decoder consists of two MLP heads each with a single hidden layer of 1600 neurons, followed by a ReLU non-linearity. The two heads emit the node featuresX and a symmetric adjacency matrixÃ (the simmetricity is enforced by modeling only the upper-triangular portion). The RNN decoder introduces a single-layer GRU <ref type="bibr" target="#b33">[34]</ref> with 256-dimensional output before the two MLP heads. In this case, the MLP heads only emit a feature vector and adjacency vector describing the current node in the BFS-ordering. For the GraphRNN, we extend the original architecture with an MLP head on the node-level GRU in order to emit node features. We find a 16-dimensional node representation to work best for the modified GraphRNN. Similarly to the GGT decoder, both the simple RNN and the GraphRNN decoders are conditioned on the image I by concatenating the visual features c to the inputs of the node-level GRU. Finally, the Generative Graph Transformer has L = 12 self-attention + MLP decoding blocks, with input and output dimensionalities fixed to: d = 256.</p><p>Each multi-head self-attention has 8 heads, meaning that each head attends over 32-dimensional vectors. On top of the decoding blocks, two MLP heads emit node features and adjacency vectors as in the RNN decoder.</p><p>The CNN encoder is composed of two 3 × 3 convolutional layers followed by a 1 × 1 convolution, with a 2 × 2 max pooling after the first convolutional layer. Batch normalization and Leaky-ReLU are used after every hidden layer.</p><p>Loss function The loss presented in Eq. 4 can be expanded as:</p><formula xml:id="formula_6">L = λL A valid + (1 − λ)L X valid = λ BCE(Ã, A) + (1 − λ) MSE(X, X) = λ N · M N i=1 M j=1 − a i,j log(ã i,j ) + (1 − a i,j ) log(1 −ã i,j ) + 1 − λ 2N N i=1 ||x i −x i || 2 2 ,<label>(5)</label></formula><p>A andX are the predicted adjacency and feature matrices, respectively. N is the lenght of the graph sequence under BFS-ordering (including termination tokens). M is the maximum size of the frontier of the BFS-ordering, set to the 99th percentile in the dataset population as in You et al. <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Evaluation Metrics</head><p>An accurate choice of the evaluation metrics is necessary for a meaningful evaluation of the methods we compare. In particular, an optimal metric should jointly capture the accuracy ofX andÃ while being invariant to changes in graph representation, graph transformations, and graph dimensionality (|V | and |E|).</p><p>Pixel-based metrics like IoU, PSNR and SSI <ref type="bibr" target="#b34">[35]</ref> computed on the image representation of the ground-truth and predicted graphs are not good candidates. Indeed, these metrics measure the local similarity between corresponding pixels in images but are not able to capture the magnitude of errors in the reconstructed graphs. Etten et al. <ref type="bibr" target="#b7">[8]</ref> introduce Average Path Length Similarity (APLS), a metric to compare pairs of graphs based on simulated routing tasks between nodes in the graph. A relevant issue with APLS are the several post-processing steps used to clean and convert segmentation in graphs. Moreover, APLS is designed to evaluate problems related to semantic segmentation, like the presence of gaps between segments of roads. Metrics based on the sequential representation of the graphs, like the ones used in the loss function, are not useful at inference time because susceptible to mismatches between the ground-truth and reconstructed sequence.</p><p>The proposed StreetMover distance overcomes the problems presented for other metrics. Besides, the StreetMover distance is easily interpretable by plotting the alignment weights and efficient to compute thanks to the use of Sinkhorn iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Robustness to noisy segmentations</head><p>To investigate the effectiveness of the GGT in a setting closer to real-world applications, we simulate the quality seen in neural network trained for semantic segmentations by manually injecting random noise in the ground-truth segmentations. As shown for a few random samples in <ref type="figure">Fig. 6</ref>, we see good graph reconstructions in case of simple road networks. Low and medium levels of noise result in significant inaccuracies for graphs with more cluttered edges. The results are obtained without pre-training of the CNN encoder. We expect that pre-training the encoder as part of a denoising auto-encoder would significantly improve the robustness to input noise. <ref type="figure">Figure 6</ref>: Experiment on the robustness of GGT to noisy input segmentations. In each column, the three left images show the input segmentations with zero, low and medium noise. On the right side of each column, the corresponding reconstructed graphs are compared with the ground-truths in the top row. Results are randomly sampled from the test set. Best seen zoomed-in on a screen. (b) Comparing road network reconstructions generated by GGT with respect to other baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Additional Qualitative Results</head><p>(c) Distribution of StreetMover distances between pairs of reconstructed and ground-truth networks in the test set, using GGT. <ref type="figure">Figure 7</ref>: Additional qualitative studies on the GGT. In a) we report more reconstructions (bottom rows) and ground-truths (top rows) sampled from the test set. Nodes are added in red for better visualization. In b) we compare reconstructions generated by different models, confirming the relative increase in performance observed in table 1. In c) we visualize the histogram of StreetMover distances between graphs in the test set and reconstructions using GGT. We notice how in half of the cases the reconstructions are very accurate, with a StreetMover distance lower than 0.010. In the right tail of the population, few failure cases contribute to a mean StreetMover distance much higher than the median. The failure cases most frequently happen for complex graphs with very cluttered edges, as shown in the seventh column of examples in a).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Samples from the Toulouse Road Network dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Examples of StreetMover distances between several graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Ground truth networks (top row) vs. generated networks (bottom row) using GGT.(b) Large patch reconstruction.(c) Adjacency matrix vs. learned self-attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative studies on the GGT. In a) we compare ground-truth road networks (top row), with generated ones (bottom row). Nodes are added in red for visualization purposes. In b) we show the reconstruction of a larger 4×4 patch of the map (ground-truth on the left, reconstruction on the right). In c) we explore with two examples the correlation between ground-truth adjacency matrices (left) and attention weights emitted by self-attention heads in intermediate GGT layers (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )</head><label>a</label><figDesc>Additional qualitative results showing reconstructions (bottom rows) and ground-truths (top rows) sampled from the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the different baselines, and ablation study removing the context attention from the encoder (GGT without CA). Standard deviation is computed over 3 runs with each model.</figDesc><table><row><cell></cell><cell>StreetMover</cell><cell>L valid</cell><cell>∆|E|</cell><cell>∆|V |</cell></row><row><cell>MLP</cell><cell cols="4">0.1380 ± 0.0050 0.1090 ± 0.0020 3.38 ± 0.07 3.02 ± 0.05</cell></row><row><cell>RNN</cell><cell cols="4">0.0289 ± 0.0003 0.0330 ± 0.0002 1.01 ± 0.05 0.96 ± 0.02</cell></row><row><cell>GraphRNN</cell><cell>0.0245</cell><cell></cell><cell></cell></row></table><note>± 0.0004 0.0311 ± 0.0001 0.87 ± 0.06 0.85 ± 0.06 GGT without CA 0.0192 ± 0.0007 0.0213 ± 0.0001 0.75 ± 0.04 0.79 ± 0.03 GGT 0.0158 ± 0.0006 0.0205 ± 0.0001 0.65 ± 0.05 0.71 ± 0.06</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.openstreetmap.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>T.K. acknowledges funding by SAP SE. We would like to thank Sindy Löwe, Gabriele Cesa and Gabriele Bani for their feedback on the first draft of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The world&apos;s user-generated road map is more than 80% complete</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Barrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Millard-Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">180698</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Road extraction from high-resolution remote sensing imagery using refined deep residual convolutional neural network. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiguang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">552</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Road segmentation in sar satellite images with deep fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><surname>Majid Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Merkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1867" to="1871" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic segmentation of satellite images using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivaprakash</forename><surname>Muruganandham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Van Etten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><forename type="middle">M</forename><surname>Bacastow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01232</idno>
		<title level="m">Spacenet: A remote sensing dataset and challenge series</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ai is supercharging the creation of maps around the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Klaiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drishtie</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Underwood</surname></persName>
		</author>
		<ptr target="https://tech.fb.com" />
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
	<note>blog post. july 23, 2019</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="412" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep generative models for generating labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7795" to="7804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08773</idno>
		<title level="m">Graphrnn: A deep generative model for graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient graph generation with graph recurrent attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth workshop on vision and language</title>
		<meeting>the fourth workshop on vision and language</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-objective de novo drug design with conditional graph generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangren</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A generative model for electron paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooks</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Marwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Miguel Hernández-Lobato</forename><surname>Segler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10970</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlan</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Shugrina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinkai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02055</idno>
		<title level="m">Neural turtle graphics for modeling city road layouts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benson</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12712</idno>
		<title level="m">Path-augmented graph transformer network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">One model to learn them all</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05137</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural particle smoothing for sampling from conditional sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10747</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
