<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyiming</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
							<email>xiujun@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
							<email>ybisk@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhgan@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the Frontier Aware Search with backTracking (FAST) Navigator, a general framework for action decoding, that achieves state-of-the-art results on the Roomto-Room (R2R) Vision-and-Language navigation challenge of . Given a natural language instruction and photo-realistic image views of a previously unseen environment, the agent was tasked with navigating from source to target location as quickly as possible. While all current approaches make local action decisions or score entire trajectories using beam search, ours balances local and global signals when exploring an unobserved environment. Importantly, this lets us act greedily but use global signals to backtrack when necessary. Applying FAST framework to existing state-of-the-art models achieved a 17% relative gain, an absolute 6% gain on Success rate weighted by Path Length (SPL). 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When reading an instruction (e.g. "Exit the bathroom, take the second door on your right, pass the sofa and stop at the top of the stairs ."), a person builds a mental map of how to arrive at a specific location. This map can include landmarks, such as the second door, and markers such as reaching the top of the stairs. Training an embodied agent to accomplish such a task with access to only ego-centric vision and individually supervised actions requires building rich multi-modal representations from limited data <ref type="bibr" target="#b1">[2]</ref>.</p><p>Most current approaches to Vision-and-Language Navigation (VLN) formulate the task to use the seq2seq (or encoder-decoder) framework <ref type="bibr" target="#b20">[21]</ref>, where language and vision are encoded as input and an optimal action sequence is * Work done partially as an intern at MSR <ref type="bibr" target="#b0">1</ref> The code is available at https://github.com/Kelym/FAST. decoded as output. Several subsequent architectures also use this framing; however, they augment it with important advances in attention mechanisms, global scoring, and beam search <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref>. Inherent to the seq2seq formulation is the problem of exposure bias <ref type="bibr" target="#b18">[19]</ref>: a model that has been trained to predict one-step into the future given the ground-truth sequence cannot perform accurately given its self-generated sequence. Previous work with seq2seq models attempted to address this using student forcing and beam search.</p><p>Student forcing exposes a model to its own generated sequence during training, teaching the agent how to recover. However, once the agent has deviated from the correct path, the original instruction no longer applies. The Supplementary Materials ( §A.1) show that student forcing cannot solve the exposure bias problem, causing the confused agent to fall into loops.</p><p>Beam search, at the other extreme, collects multiple global trajectories to score and incurs a cost proportional to the number of trajectories, which can be prohibitively high. This approach runs counter to the goal of building an agent that can efficiently navigate an environment: No one would likely deploy a household robot that re-navigates an entire house 100 times 2 before executing each command, even if it ultimately arrives at the correct location. The top performing systems on the VLN leaderboard 3 all require broad exploration that yields long trajectories, causing poor SPL performance (Success weighted by Path Length <ref type="bibr" target="#b0">[1]</ref>).</p><p>To alleviate the issues of exposure bias and expensive, inefficient beam-search decoding, we propose the Frontier Aware Search with backTracking(FAST NAVIGA-TOR). This framework lets agents compare partial paths of different lengths based on local and global information and then backtrack if it discerns a mistake. <ref type="figure" target="#fig_0">Figure 1</ref> shows trajectory graphs created by the current published state-ofthe-art (SoTA) agent using beam search versus our own.</p><p>Our method is a form of asynchronous search, which combines global and local knowledge to score and compare partial trajectories of different lengths. We evaluate our progress to the goal by modeling how closely our previous actions align with the given text instructions. To achieve this, we use a fusion function, which converts local action knowledge and history into an estimated score of progress. This score determines which local action to take and whether the agent should backtrack. This insight yields significant gains on evaluation metrics relative to existing models. The primary contributions of our work are: • A method to alleviate the exposure bias of action decoding and expensiveness of beam search. • An algorithm that makes use of asynchronous search with neural decoding. • An extensible framework that can be applied to existing models to achieve significant gains on SPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>The VLN challenge requires an agent to carry out a natural language instruction in photo-realistic environments. The agent takes an input instruction X , which contains several sentences describing a desired trajectory. At each step t, the agent observes its surroundings V t . Because the agent can look around for 360 degrees, V t is in fact a set of K = 36 different views. We denote each view as V k t . Using this multimodal input, the agent is trained to execute a sequence of actions a 1 , a 2 , ...., a T ∈ A to reach a desired location. Consistent with recent work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10]</ref>, we use a panoramic action space, where each action corresponds to moving towards one of the K views, instead of R2R's original primitive action space (i.e, left, right, etc.) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>. In addition, this formulation includes a stop action to indicate that the agent has reached its goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Greedy</head><p>FAST Beam Search <ref type="figure">Figure 2</ref>. All VLN agents are performing a search. The orange areas highlight the frontier for different navigation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Learning Signals</head><p>Key to progress in visual navigation is that all VLN approaches performs a search ( <ref type="figure">Figure 2</ref>). Current work often goes toward two extremes: using only local information, e.g. greedy decoding, or fully sweeping multiple paths simultaneously, e.g. beam search. To build an agent that can navigate an environment successfully and efficiently, we leverage both local and global information, letting the agent make a local decision while remaining aware of its global progress and efficiently backtracking when the agent discerns a mistake. Inspired by previous work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref>, our work uses three learning signals:</p><p>LOGIT l t : local distribution over action. The logit of the action chosen at time t is denoted l t . Specifically, the original language instruction is encoded via LSTM. Another LSTM acts as a decoder, using attention mechanism to generate logits over actions. At each time step t of decoding, logits are calculated by taking the dot product of the decoder's hidden state and each candidate action a i t . PM p pm t : global progress monitor. It tracks how much of an instruction has been completed <ref type="bibr" target="#b12">[13]</ref>. Formally, the model takes as input the (decoder) LSTM's current cell state, c t , previous hidden state, h t−1 , visual inputs, V t , and attention over language embeddings, α t to compute a score p pm t . The score ranges between [-1,1], indicating the agent's normalized progress. Training this indicator regularizes attention alignments, helping the model learn language-to-vision correspondences that it can use to compare multiple trajectories.</p><p>SPEAKER S: global scoring. Given a sequence of visual observations and actions, we train a seq2seq captioning model as a "speaker" <ref type="bibr" target="#b9">[10]</ref> to produce a textual description. Doing so provides two benefits: (1) the new speaker can automatically annotate new trajectories in the environment with the synthetic instructions, and (2) the speaker can score the likelihood that a given trajectory will correspond to the original instruction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Framework</head><p>We now introduce an extendible framework 4 that integrates the preceding three signals (l t , p pm t , S) 5 and to train new indicators, equipping an agent to answer:</p><p>1. Should we backtrack? 2. Where should we backtrack to? 3. Which visited node is most likely to be the goal? 4. When does it terminate this search?</p><p>These questions pertain to all existing approaches in navigation task. In particular, greedy approaches never backtrack and do not compare partial trajectories. Global beam search techniques always backtrack but can waste efforts. By taking a more principled approach to modeling navigation as graph traversal, our framework permits nuanced and adaptive answers to each of these questions.</p><p>For navigation, the graph is defined by a series of locations in the environment, called nodes. For each task, the agent is placed at a starting node, and the agent's movement in the house creates a trajectory comprised of a sequence of &lt;node u, action a &gt; pairs. We denote a partial trajectory up to time t as τ t , or the set of physical locations visited and the action taken at each point:</p><formula xml:id="formula_0">τ t = {(u i , a i )} t i=1 (1)</formula><p>For any partial trajectory, the last action is proposed and evaluated, but not executed. Instead, the model chooses whether to expand a partial trajectory or execute a stop action to complete the trajectory. Importantly, this means that every node the agent visited can serve as a possible final destination. The agent moves in the environment by choosing to extend a partial trajectory: it does this by moving to the last node of the partial trajectory and executing its last action to arrive at a new node. The agent then realizes the actions available at the new node and collects them to build a set of new partial trajectories.</p><p>At each time step, the agent must (1) access the set of partial trajectories it has not expanded, (2) access the completed trajectories that might constitute the candidate path, (3) calculate the accumulated cost of partial trajectories and the expected gain of its proposed action, and (4) compares all partial trajectories.</p><p>To do so, we maintain two priority queues: a frontier queue, Q F , for partial trajectories, and a global candidate queue, Q C , for completed trajectories. These queues are sorted by local L and global G scores, respectively. L scores the quality of all partial trajectories with their proposed actions and maintains their order in Q F ; G scores the quality of completed trajectories and maintains the order in Q C .</p><p>In §4.3, we explore alternative formulas for L and G. For example, we define L and G using the signals described in §2.1 and a function, f , that is implemented as a neural network.</p><formula xml:id="formula_1">L ← Σ 0→t l i (2) G ← f (S, p pm t , Σ 0→t l i , ...)<label>(3)</label></formula><p>To allow the agent to efficiently navigate and follow the instruction, we use an approximation of the D* search. FAST expands its optimal partial trajectory until it decides to backtrack (Q1). It decides on where to backtrack (Q2) by ranking all partial trajectories. To propose the final goal location (Q3 &amp; Q4), the agent ranks the completed global trajectories in candidate queue Q C . We explore these questions in more detail below.</p><p>(a) Both local L and global G scores can be trained to condition on arbitrary information. Here, we show the fusion of historical logits and progress monitor information into a single score.  Q1: Should we backtrack? When an agent makes a mistake or gets lost, backtracking lets it move to a more promising partial trajectory; however, retracing steps increases the length of the final path. To determine when it is worth incurring this cost, we proposed two simple strategies: explore and exploit. 1. Explore always backtracks to the most promising partial trajectory. This approach resembles beam search, but, rather than simply moving to the next partial trajectory in the beam, the agent computes the most promising node to backtrack to (Q2). 2. Exploit, in contrast, commits to the current partial trajectory, always executing the best action available at the agent's current location. This approach resembles greedy decoding, except that the agent backtracks when it is confused (i.e, when the best local action causes the agent to revisit a node, creating a loop; see the SMNA examples in Supplementary Materials §A.1).</p><p>Q2: Where should we backtrack to? Making this decision involves using L to score all partial trajectories. Intuitively, the better a partial trajectory aligns with a given description, the higher the value of L. Thus, if we can assume the veracity of L, the agent simply returns to the highest scoring node when backtracking. Throughout this paper, we explore several functions for computing L, but we present two simple techniques here, each acting over the sequence of actions that comprise a trajectory: 1. Sum-of-log 0→t log p i sums the log-probabilities of every previous action, thereby computing the probability of a partial trajectory. 2. Sum-of-logits 0→t l i sums the unnormalized logits of previous actions, which outperforms summing probabilities. These values are computed using an attention mechanism over the hidden state, observations, and language. In this way, their magnitude captures how well the action was aligned with the target description (this information is lost during normalization). <ref type="bibr" target="#b5">6</ref> Finally, during exploration, the agent implicitly constructs a "mental map" of the visited space. This lets it search more efficient by refusing to revisit nodes, unless they lead to a high-value unexplored path.</p><p>Q3: Which visited node is most likely to be the goal? Unlike existing approaches, FAST considers every point that the agent has visited as a candidate for the final destination, <ref type="bibr" target="#b6">7</ref> meaning we must rerank all candidates. We achieve this using G, a trainable neural network function that incorporates all global information for each candidate and ranks them accordingly. <ref type="figure" target="#fig_4">Figure 4</ref>(a) shows a simple visualization.</p><p>We experimented with several approaches to compute G, e.g., by integrating L, the progress monitor, speaker score, and a trainable ensemble in ( §4.3).</p><p>Q4: When do we terminate the search? The flexibility of FAST allows it to recover both the greedy decoding and beam search framework. In addition, we define two alternative stopping criteria:</p><p>1. When a partial trajectory decides to terminate. 2. When we have expanded M nodes. In §3 we ablate the effect of choosing a different M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Algorithm</head><p>We present the algorithm flow of our FAST framework. When an agent is initialized and placed on the starting node, both the candidate and frontier queues are empty. The agent if need backtrack orτ == ∅ then 8:τ ← Q F .pop <ref type="bibr">9:</ref> end if 10:û t−1 ,â t−1 ←τ .last <ref type="bibr">11:</ref> if (û t−1 ,â t−1 ) ∈ M then 12:</p><formula xml:id="formula_2">u t ← M (û t−1 ,â t−1 ) 13:</formula><p>else <ref type="bibr">14:</ref> u t ← move to u t−1 and execute a t−1 15:</p><formula xml:id="formula_3">M (û t−1 ,â t−1 ) ← u t 16:</formula><p>end if <ref type="bibr">17:</ref> for a k in best K next actions do 18:</p><formula xml:id="formula_4">Q F ← Q F ∪ {τ + (u t , a k )} 19:</formula><p>end for 20:</p><formula xml:id="formula_5">Q C ← Q C ∪τ 21:τ ←τ + (u t , a * )</formula><p>where a * is the best action <ref type="bibr">22:</ref> end while <ref type="bibr">23:</ref> return Q C .pop <ref type="bibr">24:</ref> end procedure then adds all possible next actions to the frontier queue and adds its current location to the candidate queue:</p><formula xml:id="formula_6">Q F ← Q F + ∀ i∈K {τ 0 ∪ (u 0 , a i )} (4) Q C ← Q C + τ 0<label>(5)</label></formula><p>Now that the Q F is not empty and the stop criterion is not met, FAST can choose the best partial trajectory from the frontier queue under the local scoring function:</p><formula xml:id="formula_7">τ ← arg max τi L(Q F )<label>(6)</label></formula><p>Followingτ , we perform the final action proposal, a t , to move to a new node (location in the house). FAST can now update the candidate queue with this location and the frontier queue with all possible new actions. We then either continue, by exploiting the available actions at the new location, or backtrack, depending on the choice of backtrack criteria. We repeat this process until the model chooses to stop and returns the best candidate trajectory.</p><formula xml:id="formula_8">τ * ← arg max τ G(Q C )<label>(7)</label></formula><p>Algorithm 1 more precisely outlines the full procedure for our approach. §4.3 details the different approaches to scoring partial and complete trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We evaluate our approach using the Room-to-Room (R2R) dataset <ref type="bibr" target="#b1">[2]</ref>. At the beginning of the task, the agent receives a natural language instruction and a specific start location in the environment; the agent must navigate to the target location specified in the instruction as quickly as possible. R2R is built upon the Matterport3D dataset <ref type="bibr" target="#b4">[5]</ref>, which consists of &gt;194K images, yielding 10,800 panoramic views ("nodes") and 7,189 paths. Each path is matched with three natural language instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation Criteria</head><p>We evaluate our approach on the following metrics in the R2R dataset:</p><p>TL Trajectory Length measures the average length of the navigation trajectory. NE Navigation Error is the mean of the shortest path distance in meters between the agent's final location and the goal location. SR Success Rate is the percentage of the agent's final location that is less than 3 meters away from the goal location. SPL Success weighted by Path Length <ref type="bibr" target="#b0">[1]</ref> trades-off SR against TL. Higher score represents more efficiency in navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Baselines</head><p>We compare our results to four published baselines for this task. <ref type="bibr" target="#b7">8</ref> • RANDOM: an agent that randomly selects a direction and moves five step in that direction <ref type="bibr" target="#b1">[2]</ref>. • SEQ2SEQ: the best performing model in the R2R dataset paper <ref type="bibr" target="#b1">[2]</ref>. • SPEAKER-FOLLOWER <ref type="bibr" target="#b9">[10]</ref>: an agent trained with data augmentation from a speaker model on the panoramic action space. • SMNA <ref type="bibr" target="#b12">[13]</ref>: an agent trained with a visual-textual co-grounding module and a progress monitor on the panoramic action space. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Our Model</head><p>As our framework provides a flexible design space, we report performance for two versions:</p><p>• FAST(short) uses the exploit strategy. We use the sum of logits fusion method to compute L and terminate when the best local action is stop.  <ref type="table">Table 1</ref>. Our results and SMNA re-implementation are shown in gray highlighted rows. Bolding indicates the best value per section and blue indicates best values overall. We include both a short and long version of our approach to compare to existing models greedy and beam search approaches.</p><p>• FAST(long) uses the explore strategy. We again use the sum of logits for fusion, terminating the search after fixed number of nodes and using a trained neural network reranker to select the goal state G. <ref type="table">Table 1</ref> compares the performance of our model against published numbers of existing models. Our approach significantly outperforms the existing model in terms of efficiency, matching the best overall success rate despite taking 150 -1,000 fewer steps. This efficiency gain can be seen in the SPL metric, where our models outperform previous approaches in every setting. Note that our short trajectory model appreciably outperforms current approaches in both SR and SPL. If our agent could continue exploring, it matches existing peak success rates in half of the steps (196 vs 373). Another key advantage of our technique is how simple it is to integrate with current approaches to achieve dramatic performance gains. <ref type="table">Table 2</ref> shows how the sum-of-logits fusion method enhances the two previously best performing models. Simply changing their greedy decoders to FAST with no added global information and therefore no reranking yields immediate gains of 6 and 9 points in success rate for SPEAKER-FOLLOWER and SMNA, respectively. Due to those models' new ability to backtrack, the trajectory lengths increase slightly. However, the success rate increases so much that SPL increases, as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis</head><p>Here, we isolate the effects of local and global knowledge, the importance of backtracking, and various stopping criteria. In addition, we include three qualitative intuitive examples to illustrate the model's behavior in the Supplementary Materials ( §A.1). We can perform this analysis because our approach has access to the same information as previous architectures, but it is more efficient. Our claims and results are general, and our FAST approach should benefit future VLN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fixing Your Mistakes</head><p>To investigate the degree to which models benefit from backtracking, <ref type="figure" target="#fig_6">Figure 5</ref> plots a model's likelihood of successfully completing the task after making its first mistake at each step. We use SMNA as our greedy baseline. Our analysis finds that the previous SoTA model makes a mistake at the very first action 40% of the time. <ref type="figure" target="#fig_6">Figure  5</ref> shows the effect of this error: the greedy approach, if made a mistake at its first step, has a &lt;30% chance of successfully completing the task. In contrast, because FAST detects its mistake, it returns to the starting position and tries again. This simple one-step backtracking increases its likelihood of success by over 10%. In fact, the greedy approach is equally successful only if it progresses over halfway through the instruction without making a mistake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Knowing When To Stop Exploring</head><p>The stopping criterion balances exploration and exploitation. Unlike previous approaches, our framework lets us compare different criteria and offers the flexibility to determine which is optimal for a given domain. The best available stopping criterion for VLN is not necessarily the best in general. We investigated the number of nodes to expand before terminating the algorithm, and we plot the resulting success rate and SPL in <ref type="figure">Figure 6</ref>. One important finding is that the model's success rate, though increasing with more nodes expanded, does not match the oracle's rate, i.e., as the agent expands 40 nodes, it has visited the true target node over 90% of the time but cannot recognize it as the final destination. This motivates an analysis of the utility of our global information and whether it is truly predictive <ref type="table">(Table  4)</ref>, which we investigate further in §4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Local and Global Scoring</head><p>As noted in §2.3, core to our approach are two queues, frontier queue for expansion and the candidate queue for proposing the final candidate. Each queue can use arbitrary information for scoring (partial) trajectories. We now compare the effects of combining different set of signals for scoring each queue.</p><p>Fusion methods for scoring partial trajectories An ideal model would include as much global information as possible when scoring partial trajectories in the frontier expansion queue. Thus, we investigated several sources of pseudo-global information and ten different ways to combine them. The first four use only local information, while the others attempts to fuse local and global information.</p><p>The top half of <ref type="table">Table 3</ref> shows the performance when considering only local information providers. For example, the third row of the table shows that summing the logit scores of nodes along the partial trajectory as the L score for that <ref type="figure">Figure 6</ref>. The SR increases with the number of nodes explored before plateauing, while SPL (which is extremely sensitive to length) continually decreases with added exploration.  <ref type="table">Table 3</ref>. Performance of different fusion methods for scoring partial trajectories. Tested on the validation unseen set.</p><p>trajectory achieves an SR score of 56.66. Note although all information originates with the same hidden vectors, the values computed and how they are aggregated substantially affect performance. Overall, we find that summing unnormalized logits (the 3rd row) performs the best considering its outstanding SR. This suggests that important activation information in the network outputs is being thrown away by normalization and therefore discarded by other techniques. The bottom part of <ref type="table">Table 3</ref> explores ways of combining local and global information providers. These are motivated by beam-rescoring techniques in previous work (e.g., multiplying by the normalized progress monitor score). Correctly integrating signals is challenging, in part due to differences in scale. For example, the logit is unbounded (+/-), log probabilities are unbounded in the negative, and the progress monitor is normalized to a score between 0 and 1. Unfortunately, direct integration of the progress monitor did not yield promising results, but future signals may prove more powerful.</p><p>Fusion methods for ranking complete trajectories . Previous work <ref type="bibr" target="#b9">[10]</ref> used state-factored beam search to generate M candidates and rank the complete trajectories using probability of speaker and follower scores argmax r∈R(d) P S (d|r) λ * P F (d|r) <ref type="bibr">(1−λ)</ref> . In addition to the speaker and progress monitor scores used by previous models, we also experiment with using L to compute G. To inspect the performance of using different fusion methods, we ran FAST NAVIGATOR to expand 40 nodes on the frontier and collect candidate trajectories. <ref type="table">Table 4</ref> shows the performance of different fusion scores that rank complete trajectories. We see that most techniques have a limited understanding of the global task's goal and formulation. We do, however, find a significant improvement on unseen trajectories when all signals are combined. For this we train a multi-layer perceptron to aggregate and weight our predictors. Note that any improvements to the underlying models or new features introduced by future work will directly correlate to gains in this component of the pipeline.</p><p>The top line of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Intuitive Behavior</head><p>The Supplementary Materials ( §A.1) provide three real examples to show how our model performs when compared to greedy decoding (SMNA model). It highlights how the same observations can lead to drastically different behaviors during an agent's rollout. Specifically, in <ref type="figure" target="#fig_0">Figures A1 and  A2</ref>, the greedy decoder is forced into a behavioral loop because only local improvements are considered. Using FAST clearly shows that even a single backtracking step can free the agent of poor behavioral choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Our work focuses on and complements recent advances in Vision-and-Language Navigation (VLN) as introduced by <ref type="bibr" target="#b1">[2]</ref>, but many aspects of the task and core technologies date back much further. The natural language com-munity has explored instruction following using 2D maps <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14]</ref> and computer-rendered 3D environments <ref type="bibr" target="#b15">[16]</ref>. Due to the enormous visual complexity of real-world scenes, the VLN literature usually builds on computer vision work from referring expressions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>, visual question answering <ref type="bibr" target="#b2">[3]</ref>, and ego-centric QA that requires navigation to answer questions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Finally, core to the our work is the field of search algorithm, dating back to the earliest days of AI <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, but largely absent from recent VLN literature that tends to focuses more on neural architecture design.</p><p>During publishing the Room-to-Room dataset (VLN), <ref type="bibr" target="#b1">[2]</ref> introduced the "student forcing" method for seq2seq model. Later work integrated a planning module to combined model-based and model-free reinforcement learning to better generalize to unseen environments <ref type="bibr" target="#b22">[23]</ref>, and a Cross-Modal Matching method that enforces cross-modal grounding both locally and globally via reinforcement learning <ref type="bibr" target="#b21">[22]</ref>. Two substantial improvements came from panoramic action spaces and a "speaker" model trained to enable data augmentation and trajectory reranking for beam search <ref type="bibr" target="#b9">[10]</ref>. Most recently, <ref type="bibr" target="#b12">[13]</ref> leverages a visualtextual co-grounding attention mechanism to better align the instruction and visual scenes and incorporates a progress monitor to estimate the agent's current progress towards a goal. These approaches require beam search for peak SR. Beam search techniques can unfortunately lead to long trajectories when exploring unknown environments. This limitation motivates the work we present here. Existing approaches trade off a high success rate and long trajectories: greedy decoding provides short, often incorrect paths, the beam search yields high success rates but long trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present FAST NAVIGATOR, a framework for using asynchronous search to boost any VLN navigator by enabling explicit backtrack when an agent detects if it is lost. This framework can be easily plugged into the most advanced agents to immediately improve their efficiency. Further, empirical results on the Room-to-Room dataset show that our agent achieves state-of-the-art Success Rates and SPLs. Our search-based method is easily extendible to more challenging settings, e.g., when an agent is given a goal without any route instruction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>, or a complicated real visual environment <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head><p>Our appendix is structured to provide both corresponding qualitative examples for the quantitative results in the paper and additional implementation details for replication. <ref type="figure" target="#fig_0">Figures A1 through A3</ref> show three examples comparing our approach to the previous state-of-the-art. In addition, the following URL includes a 90 second video (https:// youtu.be/AD9TNohXoPA) showing a first-person view of several agents navigating the environment with corresponding birds-eye-view maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Qualitative comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Candidate Reranker</head><p>Given a collection of candidate trajectories, our reranker module assigns a score to each of the trajectories. The highest scoring trajectory is selected for the FAST agent's next step. In our implementation, we use a 2-layer MLP as the reranker. We train the neural network using pairwise crossentropy loss <ref type="bibr" target="#b3">[4]</ref>.</p><p>As input to the reranker, we concatenate the following features to obtain a 6-dimensional vector:</p><p>• Sum of score logits for actions on the trajectory. • Mean of score logits for actions on the trajectory. • Sum of log probabilities for actions on the trajectory. • Mean of log probability for actions on the trajectory. • Progress monitor score for the completed trajector.</p><p>• Speaker score for the completed trajectory.</p><p>We feed the 6-dimensional vector through an MLP: BN → FC → BN → Tanh → FC, where BN is a layer of Batch Normalization, FC is a Fully Connected layer, and Tanh is the nonlinearity used. The first FC layer transforms the 6-dimensional input vector to a 6-dimensional hidden vector. The second FC layer project the 6-dimensional vector to a single floating-point value, which is used as the score for the given partial trajectory.</p><p>To train the MLP, we cache the candidate queue after running FAST for 40 steps. Each candidate trajectory in the queue has a corresponding score s i . To calculate the loss, we minimize the pairwise cross-entropy loss:</p><formula xml:id="formula_9">−(s 1 − s 2 ) + log(1 + exp(s 1 − s 2 ))</formula><p>where s 1 is the score for a qualified candidate and s 2 is the score for an unqualified candidate. We define qualified candidate trajectories as those that end within 3 meters of ground truth destination. In our cached training set, we have 4, 378, 729 pairs of training data. We train using a batch size of 3600, SGD optimizer with a learning rate of 5e −5 , and momentum 0.6; We train for 30 epochs. <ref type="figure" target="#fig_0">Figure A1</ref>. Comparison of the previously state-of-the-art SMNA model <ref type="bibr" target="#b12">[13]</ref> to our FAST NAVIGATOR method, with the ground truth as reference. Note how SMNA retraces its steps multiple times due to the lack of global information. This example is taken from Room-to-Room, path 2617, instruction set 3. You can view a video of this trajectory here: https://youtu.be/AD9TNohXoPA. <ref type="figure">Figure A2</ref>. Identical to previous figure A1, except that this example is taken from Room-to-Room, path 15, instruction set 1. <ref type="figure" target="#fig_2">Figure A3</ref>. Identical to previous figure A1, except that this example is taken from Room-to-Room, path 1759, instruction set 1. The typo 'direclty' comes from the dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Top-down view of the trajectory graphs for beam search and FAST. Blue Star is the start and Red Stop is the target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Instructions and visual observations are encoded as hidden vectors defining multiple paths through the world. These vectors can then be accumulated to score a sequence of actions.(b) At each time step, the predicted action sequence and visual observation are fed into an attention module with the encoded instruction, to produce both the logits for the next actions and a progress monitor score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a). How the three signals are extracted from the partial trajectory in a seq2seq VLN framework; (b). How to compute the three signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b) An expansion queue maintains all possible next actions from all partial trajectories. The options are sorted by their scores(Figure 4(a)) in order to select the next action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Arbitrary signals can be computed from partial trajectories to learn a scoring function (left) that ranks all possible actions in our expansion queue (right). This provides a flexible and extendible framework for optimal action decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>while Q F = ∅ and stop criterion do 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Circle sizes represent the what percentage of agents diverge on step N. Most divergences occur in the early steps. FAST recovers from early divergences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 Table 4 .</head><label>44</label><figDesc>Success rate using seven different fusion scores as G to rerank the destination node from the candidate pool.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">, shows oracle's performance.</cell></row><row><cell cols="4">This indicates how far current global information providers</cell></row><row><cell cols="4">have yet to achieve. Closing this gap is an important direc-</cell></row><row><cell cols="2">tion for future work.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Train Val Seen Val Unseen</cell></row><row><cell cols="2">Oracle 99.13</cell><cell>92.85</cell><cell>90.20</cell></row><row><cell>Σ l i</cell><cell>78.78</cell><cell>62.49</cell><cell>56.49</cell></row><row><cell>µ l i</cell><cell>85.78</cell><cell>66.99</cell><cell>54.41</cell></row><row><cell>Σ p i</cell><cell>91.25</cell><cell>68.56</cell><cell>56.15</cell></row><row><cell>µ p i p pm t</cell><cell>91.60 66.71</cell><cell>69.34 53.67</cell><cell>58.75 50.15</cell></row><row><cell>S</cell><cell>69.99</cell><cell>53.77</cell><cell>43.68</cell></row><row><cell>All</cell><cell>90.16</cell><cell>71.00</cell><cell>64.03</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is calculated based on the length of SPEAKER-FOLLOWER agent paths and human paths on the R2R dataset.<ref type="bibr" target="#b2">3</ref> https://evalai.cloudcv.org/web/challenges/challengepage/97/leaderboard/270</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Figure 3(a) shows an example of integrating the three signals in a seq2seq framework.<ref type="bibr" target="#b4">5</ref> Figure 3(b) shows how to compute the three signals.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">This is particularly problematic when an agent is lost. Normalizing many low-value logits can yield a comparatively high probability (e.g. uniform or random). We also experiment with variations of this approach (e.g. means instead of sums) in §4.<ref type="bibr" target="#b6">7</ref> There can be more than one trajectory connecting the starting node to each visited node.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Some baselines on the leader-board are not yet public when submitted; therefore, we cannot compare with them directly on the training and validation sets.<ref type="bibr" target="#b8">9</ref> Our SMNA implementation matches published validation numbers. All our experiments are based on full re-implementations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On evaluation of embodied navigation agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06757</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">van den Hengel. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gated-attention architectures for task-oriented language grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Sathyendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Pasumarthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Touchdown: Natural language navigation and spatial reasoning in visual street environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Embodied question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03367</idno>
		<title level="m">Talk the walk: Navigating new york city through grounded dialogue</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speaker-follower models for vision-andlanguage navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">IQA: Visual question answering in interactive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teplyashin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06551</idno>
		<title level="m">Grounded language learning in a simulated 3d world</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-monitoring navigation agent via auxiliary progress estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Listen, attend, and walk: Neural mapping of navigational instructions to action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mapping instructions to actions in 3d environments with visual goal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shatkhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mapping instructions and visual observations to actions with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Heuristics: intelligent search strategies for computer problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno>2016. 1</idno>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Artificial intelligence: a modern approach. Malaysia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Pearson Education Limited</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reinforced cross-modal matching and self-supervised imitation learning for visionlanguage navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Target-driven visual navigation in indoor scenes using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3357" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
