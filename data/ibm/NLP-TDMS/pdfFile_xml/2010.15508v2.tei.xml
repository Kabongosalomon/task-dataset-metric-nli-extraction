<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FULLSUBNET: A FULL-BAND AND SUB-BAND FUSION MODEL FOR REAL-TIME SINGLE-CHANNEL SPEECH ENHANCEMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Hao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Westlake University &amp; Westlake Institute for Advanced Study</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Inner Mongolia University</orgName>
								<address>
									<settlement>Hohhot</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Inner Mongolia University</orgName>
								<address>
									<settlement>Hohhot</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Inria Grenoble Rhône-Alpes</orgName>
								<address>
									<settlement>Montbonnot Saint-Martin</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Westlake University &amp; Westlake Institute for Advanced Study</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FULLSUBNET: A FULL-BAND AND SUB-BAND FUSION MODEL FOR REAL-TIME SINGLE-CHANNEL SPEECH ENHANCEMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-FullSubNet</term>
					<term>Full-band and Sub-band Fusion</term>
					<term>Sub-band</term>
					<term>Speech Enhancement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a full-band and sub-band fusion model, named as FullSubNet, for single-channel real-time speech enhancement. Full-band and sub-band refer to the models that input full-band and sub-band noisy spectral feature, output full-band and sub-band speech target, respectively. The sub-band model processes each frequency independently. Its input consists of one frequency and several context frequencies. The output is the prediction of the clean speech target for the corresponding frequency. These two types of models have distinct characteristics. The full-band model can capture the global spectral context and the long-distance crossband dependencies. However, it lacks the ability to modeling signal stationarity and attending the local spectral pattern. The sub-band model is just the opposite. In our proposed FullSubNet, we connect a pure full-band model and a pure sub-band model sequentially and use practical joint training to integrate these two types of models' advantages. We conducted experiments on the DNS challenge (INTERSPEECH 2020) dataset to evaluate the proposed method. Experimental results show that full-band and sub-band information are complementary, and the FullSubNet can effectively integrate them. Besides, the performance of the FullSubNet also exceeds that of the top-ranked methods in the DNS Challenge (INTERSPEECH 2020).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years, deep learning-based single-channel speech enhancement methods have greatly improved speech enhancement systems' speech quality and intelligibility. These methods are often trained in a supervised setting and can be divided into time-domain and frequency-domain methods. The time-domain methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> use the neural network to map noisy speech waveform to clean speech waveform directly. The frequency-domain methods <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> typically use the noisy spectral feature (e.g., complex spectrum, magnitude spectrum) as the input of a neural model. Learning target is the spectral feature of clean speech or a certain mask (e.g., Ideal Binary Mask <ref type="bibr" target="#b7">[8]</ref>, Ideal Ratio Mask <ref type="bibr" target="#b8">[9]</ref>, complex Ideal Ratio Mask (cIRM) <ref type="bibr" target="#b9">[10]</ref>). In general, due to the high dimension and the lack of apparent geometric structure for the time domain signal, the frequency domain methods still dominate the vast majority of speech enhancement methods. In this paper, we focus on real-time singlechannel speech enhancement in the frequency domain. * Equal contribution † Corresponding author</p><p>In our previous work <ref type="bibr" target="#b10">[11]</ref>, a sub-band-based method was proposed for single-channel speech enhancement. Unlike the traditional full-band-based methods, the method performed in a sub-band style:</p><p>The input of the model consists of one frequency, together with several context frequencies. The output is a prediction of the clean speech target for the corresponding frequency. All the frequencies are processed independently. This method is designed on the following grounds. (1) It learns the frequency-wise signal stationarity to discriminate between speech and stationary noise. It is known that speech is non-stationary, while many types of noise are relatively stationary. The temporal evolution of frequency-wise STFT magnitude reflects the stationarity, which is the foundation for the conventional noise power estimators <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> and speech enhancement methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. (2) It focuses on the local spectral pattern presented in the current and context frequencies. The local spectral pattern has been proved to be informative for discriminating between speech and other signals. This method was submitted to the DNS challenge <ref type="bibr" target="#b15">[16]</ref> in INTERSPEECH 2020 and ranked the fourth place out of the 16 real-time track submissions.</p><p>The sub-band model meets the DNS challenge's real-time requirement, and the performance is also very competitive. However, since it cannot model the global spectral pattern and exploit the longdistance cross-band dependencies. Especially for the sub-band with an extremely low signal-to-noise ratio (SNR), the sub-band model can hardly recover the clean speech, while it will be possible with the help of full-band dependency. On the other hand, the full-band models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> are trained to learn the regression between the highdimensional input and output, lacking a mechanism dedicated to the sub-band information, such as the signal stationarity. This paper proposes a full-band and sub-band fusion model named FullSubNet to address the above problems. Based on plenty of preliminary experiments, the FullSubNet is designed as a series connection of the full-band model and sub-band model. In short, the full-band model's output is input to the sub-band model. Through effective joint training, these two models are jointly optimized. The FullSubNet can capture the global (full-band) context while retaining the ability to model signal stationarity and attend the local spectral pattern. Like the sub-band model, the FullSubNet still meets the real-time requirement and can exploit future information within a reasonable latency. We evaluate the FullSubNet on the DNS challenge (INTERSPEECH 2020) dataset. Experimental results show that the FullSubNet prominently outperforms both the sub-band model <ref type="bibr" target="#b16">[17]</ref> and a pure full-band model with a larger amount of parameters with the FullSubNet, which indicates that the sub-band information and the full-band information are complementary. The proposed fusion model is effective for integrating them. Besides, we also compare the performance with the top-ranked methods in the DNS challenge, and the results show that our objective performance measures are better than them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head><p>We use the representation of speech signal in the short-time fourier transform (STFT) domain:</p><formula xml:id="formula_0">X(t, f ) = S(t, f ) + N (t, f ).<label>(1)</label></formula><p>where X(t, f ), S(t, f ) and N (t, f ) respectively represent the complex-valued time-frequency (T-F) bin of noisy speech, noisefree speech (the reverberant image signal received at the microphone) and interference noise at time frame t and frequency bin f with t = 1, · · · , T and f = 0, · · · , F − 1. T and F denote the total number of frames and frequency bins, respectively. This paper focuses only on the denoising task, and the target is to suppress noise N (t, f ) and recover the reverberant speech signal S(t, f ). We propose a full-band and sub-band fusion model to accomplish this task, including a pure full-band model Gfull and a pure sub-band model Gsub. The basic workflow is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Next, we will introduce each part in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Input</head><p>Previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref> have proved that magnitude spectral feature can provide crucial clues about the global spectral pattern at full-band, while the local spectral pattern and signal stationarity at sub-band. Therefore, we use the noisy full-band magnitude spectral features</p><formula xml:id="formula_1">X(t) = [|X(t, 0)|, · · · , |X(t, f )|, · · · , |X(t, F − 1)|] T ∈ R F . (2)</formula><p>We use its sequence</p><formula xml:id="formula_2">X = (X(1), · · · , X(t), · · · , X(T ))<label>(3)</label></formula><p>as the input of the full-band model Gfull. Then, Gfull can capture the global contextual information and outputs a spectral embedding with the size being the same as X, which is expected to provide complementary information to the following sub-band model Gsub.</p><p>The sub-band model Gsub predicts the frequency-wise cleanspeech target according to the signal stationarity and local spectral mode encoded in the noisy sub-band signal, as well as the full-band model's output. In detail, we take a time-frequency point |X(t, f )| and its adjacent 2 × N time-frequency points as a sub-band unit. N is the number of neighbor frequencies considered on each side. For boundary frequencies, with f − N &lt; 0 or f + N &gt; F − 1, circular Fourier frequencies are used. We concatenate the sub-band unit and the output of the full-band model, denoted as Gfull(|X(t, f )|), as the input of the sub-band model Gsub,</p><formula xml:id="formula_3">x(t, f ) =[|X(t, f − N )|, · · · , |X(t, f − 1)|, |X(t, f )| (4) |X(t, f + 1)|, · · · , |X(t, f + N )|, Gfull(|X(t, f )|)] T ∈ R 2N +2 .</formula><p>For the frequency f , the input sequence of Gsub is</p><formula xml:id="formula_4">x(f ) = (x(1, f ), · · · , x(t, f ), · · · , x(T, f )). (5)</formula><p>In this sequence, the temporal evolution along with time axis reflect the signal stationarity, which is an efficient cues to discriminate between speech and relatively stationary noise. The noisy sub-band spectra (composed of 2N + 1 frequencies) and its temporal dynamics provides the local spectral pattern, which can be learned by the dedicated sub-band model. While the signal stationarity cues and the local pattern are actually present in the input of the full-band model Gfull as well, however, they are not especially learned by the full-band model Gfull. Consequently, the sub-band model Gsub still learns some extra/different information relative to the full-band model Gfull. Meanwhile, the output of the full-band model Gfull provide some complementary information not seen by the sub-band model Gsub.</p><p>Since the full-band spectral feature X(t) contains F frequencies, we eventually generate F independent input sequences for Gsub with a dimension of 2N + 2 for each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning target</head><p>There is no doubt that the precise estimation of phase can provide more hearing perception quality improvement, especially in low signal-to-noise ratio (SNR) conditions. However, the phase is wrapped in −π ∼ π and has chaotic data distribution, which makes it not easy to estimate. Instead of estimating the phase directly, like the previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>, we adopt the complex Ideal Ratio Mask (cIRM) as our model's learning target. Follow <ref type="bibr" target="#b9">[10]</ref>, we use hyperbolic tangent to compress cIRM in training and use inverse function to uncompressed mask in inference (K = 10, C = 0.1). We denote cIRM as y(t, f ) ∈ R 2 for one T-F bin. The sub-band model takes as input sequence x(f ) for the frequency f and then predicts the cIRM sequence y(f ) = (y(1, f ), · · · , y(t, f ), · · · , y(T, f )). . According to our previous experiments, the sub-band model is not necessary to be as large as the full-band model, and thus 384 hidden units are used in each layer of LSTM. According to the settings in <ref type="bibr" target="#b9">[10]</ref>, the output layer of the sub-band model does not use activation functions. It is important to note that all the frequencies share one unique sub-band network (and its parameters). During training, considering the limited LSTM memory capacity, the input-target sequence pairs are generated with a constant-length sequence. To make the model easier to optimize, the input sequence must be normalized to equalize the input levels. For the full-band model, we empirically calculate the mean value µfull of the magnitude spectral features on the full-band sequence X and normalize the input sequence as X µ full . The sub-band model process the frequencies independently. For frequency f , we caculate the mean value µsub(f ) on the input sequence x(f ) and normalize the input sequence as x(f ) µ sub (f ) . In the real-time inference stage, we usually use the cumulative normalization method <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, i.e., at each time, the mean value used for normalization is computed using all the available frames. However, in the practical real-time speech enhancement system, the speech signal is usually silent initially, which means that the speech signal's beginning part is mostly invalid. In this work, to better show the FullSubNet's performance regardless of the normalization problem, we directly use µfull and µsub(f ) computed on the entire test clip to perform normalization during inference.</p><p>Same as the method mentioned in <ref type="bibr" target="#b16">[17]</ref>, our proposed method supports output delay, which enables the model to explore future information within a reasonably small delay. As shown in the <ref type="figure" target="#fig_0">Fig. 1</ref>, to infer y(t − τ ), the future time steps, i.e. x(t − τ + 1), · · · , x(t), are provided in the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We evaluated the FullSubNet on the DNS Challenge (INTER-SPEECH 2020) dataset <ref type="bibr" target="#b15">[16]</ref>. The clean speech set includes over 500 hours of clips from 2150 speakers. The noise dataset includes over 180 hours of clips from 150 classes. To make full use of the dataset, we simulate the speech-noise mixture with dynamic mixing during model training. In detail, before the start of each training epoch, 75% of the clean speeches are mixed with randomly selected room impulse responses (RIR) from (1) the Multichannel Impulse Response Database <ref type="bibr" target="#b19">[20]</ref> with three reverberation times (T60) 0.16 s, 0.36 s, and 0.61 s. (2) the Reverb Challenge dataset <ref type="bibr" target="#b20">[21]</ref> with three reverberation times 0.3 s, 0.6 s and 0.7 s. After that, the speech-noise mixtures are dynamically generated by mixing the clean speech (75% of them are reverberant) and noise with a random SNR in between -5 and 20 dB. The total data "seen" by the model is over 5000 hours after ten epochs of training. The DNS Challenge provides a publicly available test dataset, including two categories of synthetic clips, i.e., without and with reverberations. Each category has 150 noisy clips with SNR levels distributed in between 0 dB to 20 dB. We use this test dataset for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation</head><p>The signals are transformed to the STFT domain using a 512-sample (32 ms) Hanning window with a frame step of 256 samples. We use PyTorch to implement the FullSubNet. Adam optimizer is used with a learning rate of 0.001. The sequence length for training is set to T = 192 frames (about 3 s). According to the real-time requirement of the DNS Challenge (INTERSPEECH 2020), we set τ = 2, which exploits two future frames to enhance the current frame, and uses a 16 × 2 = 32ms look ahead. As in <ref type="bibr" target="#b16">[17]</ref>, we set 15 neighbor frequencies for each side of the input frequency of the sub-band model in the FullSubNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Baselines</head><p>To testify the full-band and sub-band fusion method's effectiveness, we compare with the following two models, which use the same experimental settings and learning target (cIRM) as the FullSubNet. · Sub-band model <ref type="bibr" target="#b16">[17]</ref>: The sub-band model has achieved very competitive performance in the DNS-Challenge (the fourth place of the real-time track). To compare performance fairly, like to train the FullSubNet, we use dynamic mixing during training. · Full-band model: We construct a pure full-band model, which contains three LSTM layers with 512 hidden units for each layer. The full-band model's architecture, i.e., a stack of LSTM layers, is actually widely used for speech enhancement, such as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>. This model is slightly larger than the proposed fusion model, and thence the comparison would be fair enough.</p><p>In addition to these two models, we also compared with the topranked methods in the DNS challenge (INTERSPEECH 2020), including NSNet <ref type="bibr" target="#b21">[22]</ref>, DTLN <ref type="bibr" target="#b22">[23]</ref>, Conv-TasNet <ref type="bibr" target="#b23">[24]</ref>, DCCRN <ref type="bibr" target="#b18">[19]</ref> and PoCoNet <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with the baselines</head><p>In the last three rows of <ref type="table" target="#tab_0">Table 1</ref>, we compare the performance of the sub-band model, the full-band model, and the FullSubNet. "# Para" and "Look Ahead" in the table respectively represent the parameter amount of the model and the length of used future information. "With Reverb" means that the noisy speeches in the test dataset have not only noise but also a certain degree of reverberation, which significantly increases the difficulty for speech enhancement. "Without Reverb" means that the noisy speeches in the test dataset have only noise. For a fair comparison, these three models use the same training target (cIRM), experimental settings, and look ahead.</p><p>From the table, we can find that most of the full-band model's evaluation scores are better than the ones of the sub-band model, as the full-band model exploits the wide-band information using a larger network. It is interesting to find that, relative to the full-band model, the sub-band model seems more effective for the "With Reverb" data, as the superiority of the full-band model for "With Reverb" is smaller than the one for "Without Reverb." This indicates that the sub-band model effectively models the reverberation effect by focusing on the temporal evolution of the narrow-band spectrum. The possible reason is that the cross-band dependency of the reverberation effect is actually much lower than the one of signal spectra. Regarding the FullSubNet: (1) Although the sub-band model's performance is already very competitive, after integrating the fullband model (stacked by two LSTM layers and one linear layer), the model performance has been dramatically improved. This improvement shows that the global spectral pattern and the long-distance cross-band dependencies are essential for speech enhancement. <ref type="bibr" target="#b1">(2)</ref> The performance of the FullSubNet also significantly exceeds the full-band model. We must first point out that this improvement does not come from using more parameters. In fact, the FullSubNet (two layers of full-band LSTM plus two layers of sub-band LSTM) has even fewer parameters than the full-band model (three layers of fullband LSTM). After integrating the sub-band model, the FullSub-Net inherits the sub-band model's unique ability, namely exploiting signal stationarity and local spectral patterns, and the capability of modeling the reverberation effect. The apparent superiority of the FullSubNet over the full-band model demonstrates that the information exploited by the sub-band model is indeed not learned by the full-band model, which is complementary to the full-band model. Overall, these results testify that the proposed fusion model successfully integrates the virtues of full-band and sub-band techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the state-of-the-art methods</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, in addition to showing that the FullSubNet can effectively integrate two complementary models, we also compare its performance with the top-ranked methods in DNS Challenge (INTER-SPEECH 2020). The "Rank" column in the table indicates whether to support real-time processing and the challenge's ranking. e.g., "RT-8" means the eighth place of the real-time (RT) track. "NRT-1" means the first place of the non-real-time (NRT) track.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, NSNet is the official baseline method of the DNS challenge, which uses a compact RNN to enhance the noisy shorttime speech spectra in a single-frame-in, single-frame-out manner. We use the DNS challenge recipe provided in the asteroid toolkit 1 to implement and train NSNet. The training data are generated using the method mentioned in <ref type="bibr" target="#b16">[17]</ref>. In the table, no matter which metric, our proposed method greatly surpasses NSNet with all metrics. DTLN, Conv-TasNet, DCCRN, and PoCoNet are the top-ranked methods in the DNS challenge's subjective listening test. To ensure the fairness of comparison, we directly quote performance scores from their original papers. The vacant place in the table means that the corresponding score was not reported in the original paper. DTLN <ref type="bibr" target="#b22">[23]</ref> is capable of real-time processing. It combines the STFT operation and a learned analysis and synthesis basis into a stacked-network with less than one million parameters. <ref type="bibr" target="#b23">[24]</ref> proposed a low-latency Conv-TasNet. Conv-TasNet <ref type="bibr" target="#b17">[18]</ref> is a widelyused time-domain audio separation network, which has a large computational complexity. Consequently, the low-latency Conv-TasNet does not satisfy the real-time requirement. DCCRN <ref type="bibr" target="#b18">[19]</ref> simulates the complex-valued operation inside the convolution recurrent network. It won the first place of the real-time track. PoCoNet <ref type="bibr" target="#b24">[25]</ref> is a convolutional neural network with frequency-positional embeddings employed. Besides, a semi-supervised method is adopted to increase conversational training data by pre-enhancing the noisy datasets. It won the first place of the non-real-time track. These methods cover a large range of advanced deep learning-based speech enhancement techniques and represent the state-of-the-arts to an extent. The original paper of these methods provided the evaluation results on the same test set used in this work but with not all the metrics used in this work. It can be seen that the proposed fusion model achieves considerably better objective scores than all of them on this limited dataset. The performance of PoCoNet is close to ours, but it is a non-real-time model with a much larger network (about 50 M parameters). The proposed FullSubNet provides a new model dedicated to the full-band/sub-band fusion, which is likely not conflicting with the advanced techniques employed in these state-of-the-art models. Therefore, it worth expecting that speech enhancement capability can be further improved by properly combining them.</p><p>Regarding the computational complexity, the one STFT frame (32 ms) processing time of the proposed model (PyTorch implementation) is 10.32 ms tested on a virtual quad-core CPU (2.4 GHz) based on Intel Xeon E5-2680 v4, which obviously meets the realtime requirement. Later, we will open-source the code and pretrained models, and show some enhanced audio clips at https: //github.com/haoxiangsnr/FullSubNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we propose a full-band and sub-band fusion model, named as FullSubNet, for real-time single-channel speech enhancement. This model is designed to integrate the advantages of the full-band and the sub-band models, that is, it can capture the global (full-band) spectral information and the long-distance cross-band dependencies, meanwhile retaining the ability to modeling signal stationarity and attending the local spectral pattern. On the DNS challenge (INTERSPEECH 2020) test dataset, we demonstrated that the sub-band information and the full-band information are complementary, and the FullSubNet can effectively integrate them. We also compared the performance with some top-ranked methods for the DNS challenge, and the results show that the FullSubNet outperforms these methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Diagram of the proposed FullSubNet. The second line in the rectangle describes the dimensions of the data at the current stage, e.g., "1 (F )" represents one F -dimensional vector. "F (2N + 1)" represents F independent (2N + 1)-dimensional vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>shows the architecture of the FullSubNet. The full-band and sub-band models in the FullSuNet have the same model structure, including two stacked unidirectional LSTM layers and one linear (fully connected) layer. The LSTM of the full-band model contains 512 hidden units in each layer and uses ReLU as the output layer's activation function. The full-band model outputs a F -dimensional vector at each time step, with one element for each frequency. The sub-band units are then concatenated with this vector frequency by frequency to form F independent input samples for the sub-band model following Equation 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The performance in terms of WB-PESQ [MOS], NB-PESQ [MOS], STOI [%], and SI-SDR [dB] on the DNS challege test dataset.</figDesc><table><row><cell>Method</cell><cell># Para (M)</cell><cell>Look Ahead (ms)</cell><cell>Rank</cell><cell cols="6">With Reverb WB-PESQ NB-PESQ STOI SI-SDR WB-PESQ NB-PESQ STOI SI-SDR Without Reverb</cell></row><row><cell>Noisy</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.822</cell><cell>2.753</cell><cell>86.62 9.033</cell><cell>1.582</cell><cell>2.454</cell><cell>91.52 9.071</cell></row><row><cell>NSNet [22]</cell><cell>5.1</cell><cell>0</cell><cell>-</cell><cell>2.365</cell><cell>3.076</cell><cell>90.43 14.721</cell><cell>2.145</cell><cell>2.873</cell><cell>94.47 15.613</cell></row><row><cell>DTLN [23]</cell><cell>1.0</cell><cell></cell><cell>RT-8</cell><cell></cell><cell>2.70</cell><cell>84.68 10.53</cell><cell></cell><cell>3.04</cell><cell>94.76 16.34</cell></row><row><cell>Conv-TasNet [24]</cell><cell>5.08</cell><cell>33</cell><cell>NRT-5</cell><cell>2.750</cell><cell></cell><cell></cell><cell>2.73</cell><cell></cell><cell></cell></row><row><cell>DCCRN-E [19]</cell><cell>3.7</cell><cell>37.5</cell><cell>RT-1</cell><cell></cell><cell>3.077</cell><cell></cell><cell></cell><cell>3.266</cell><cell></cell></row><row><cell>PoCoNet [25]</cell><cell>50</cell><cell></cell><cell>NRT-1</cell><cell>2.832</cell><cell></cell><cell></cell><cell>2.748</cell><cell></cell><cell></cell></row><row><cell cols="2">Sub-band Model [17] 1.3</cell><cell>32</cell><cell>RT-4</cell><cell>2.650</cell><cell>3.274</cell><cell>90.53 14.673</cell><cell>2.369</cell><cell>3.052</cell><cell>94.24 16.153</cell></row><row><cell>Full-band Model</cell><cell>6.0</cell><cell>32</cell><cell>-</cell><cell>2.681</cell><cell>3.344</cell><cell>90.64 13.580</cell><cell>2.731</cell><cell>3.256</cell><cell>95.71 16.190</cell></row><row><cell>FullSubNet</cell><cell>5.6</cell><cell>32</cell><cell>-</cell><cell>2.969</cell><cell>3.473</cell><cell>92.62 15.750</cell><cell>2.777</cell><cell>3.305</cell><cell>96.11 17.290</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/mpariente/asteroid/tree/mas ter/egs/dns_challenge</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5069" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tcnn: Temporal convolutional neural network for real-time speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6875" to="6879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">UNetGAN: A Robust Speech Enhancement Approach in Time Domain for Extremely Low Signal-to-Noise Ratio Condition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Batushiren</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1786" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory for speaker generalization in supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4705" to="4714" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On Ideal Binary Mask As the Computational Goal of Auditory Scene Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Separation by Humans and Machines</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="181" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multipletarget deep learning for lstm-rnn based speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Rong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Hands-free Speech Communications and Microphone Arrays (HSCMA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Narrow-band Deep Filtering for Multichannel Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unbiased mmsebased noise power estimation with low complexity and low tracking delay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Gerkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendriks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1383" to="1393" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-stationary noise power spectral density estimation based on regional statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="181" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yariv</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1109" to="1121" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech enhancement for non-stationary noise environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baruch</forename><surname>Berdugo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2403" to="2418" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The interspeech 2020 deep noise suppression challenge: Datasets, subjective speech quality and testing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahim</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harishchandra</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishak</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiy</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashkan</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Braun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08662</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Online monaural speech enhancement using delayed subband lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05037</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dccrn: Deep complex convolution recurrent network for phase-aware speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubo</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengtao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00264</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multichannel audio database in various acoustic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elior</forename><surname>Hadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Heese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 14th International Workshop on Acoustic Signal Enhancement (IWAENC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="313" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A summary of the reverb challenge: state-of-the-art and remaining challenges in reverberant speech processing research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Emanuël</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Habets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Kellermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Leutnant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weighted speech distortion losses for neuralnetwork-based real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="871" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dual-signal transformation lstm network for real-time noise suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><forename type="middle">T</forename><surname>Westhausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07551</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploring the Best Loss Function for DNN-Based Lowlatency Speech Enhancement with Temporal Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiro</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11611</idno>
		<idno>arXiv: 2005.11611</idno>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Poconet: Better speech enhancement with frequency-positional embeddings, semi-supervised conversational data, and biased loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neerad</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04470</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
