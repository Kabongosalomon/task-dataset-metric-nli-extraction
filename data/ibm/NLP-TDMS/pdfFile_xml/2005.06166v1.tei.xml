<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallel Corpus Filtering via Pre-trained Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
							<email>boliangzhang@didiglobal.com</email>
							<affiliation key="aff0">
								<orgName type="department">DiDi Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Nagesh</surname></persName>
							<email>ajaynagesh@didiglobal.com</email>
							<affiliation key="aff0">
								<orgName type="department">DiDi Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
							<email>kevinknight@didiglobal.com</email>
							<affiliation key="aff0">
								<orgName type="department">DiDi Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Parallel Corpus Filtering via Pre-trained Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Web-crawled data provides a good source of parallel corpora for training machine translation models. It is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods. In this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models. We measure sentence parallelism by leveraging the multilingual capability of BERT and use the Generative Pre-training (GPT) language model as a domain filter to balance data domains. We evaluate the proposed method on the WMT 2018 Parallel Corpus Filtering shared task, and on our own web-crawled Japanese-Chinese parallel corpus. Our method significantly outperforms baselines and achieves a new stateof-the-art. In an unsupervised setting, our method achieves comparable performance to the top-1 supervised method. We also evaluate on a web-crawled Japanese-Chinese parallel corpus that we make publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Training modern neural machine translation (NMT) systems requires large parallel-text resources. Publicly-available parallel corpora are mostly paired with English, such as German-English, French-English, Chinese-English, etc., and their domains are limited. For building machine translation systems between non-English language pairs, such as Chinese and Japanese, existing parallel corpora are insufficient and often low quality. To address this problem, system builders have trained NMT systems on web-crawled data and achieved promising results <ref type="bibr" target="#b32">(Xu and Koehn, 2017;</ref><ref type="bibr" target="#b15">Junczys-Dowmunt, 2018;</ref><ref type="bibr" target="#b28">Schwenk, 2018;</ref>. However, data automatically crawled from the web is extremely noisy.  and <ref type="bibr" target="#b3">Belinkov and Bisk (2018)</ref> show that neural translation models are far more sensitive to noisy parallel training data than statistical machine translation. Data selection methods that can filter noisy parallel sentences from large-scale web crawled resources are in demand.</p><p>In this paper, we study the problem in a realworld scenario where we crawl a large Japanese-Chinese parallel corpus from various websites and build open-domain machine translation systems between Japanese and Chinese, by filtering the web crawled parallel corpus. In addition, a small amount of clean parallel data is available, in the software domain. In order to confirm our results on a public data, we also apply our filter to the WMT 2018 German-English Parallel Corpus Filtering shared task.</p><p>Previous work on parallel corpus filtering performs poorly in our scenario as it either requires large clean parallel corpora or dictionaries <ref type="bibr" target="#b32">(Xu and Koehn, 2017;</ref><ref type="bibr" target="#b0">Artetxe and Schwenk, 2019;</ref><ref type="bibr" target="#b15">Junczys-Dowmunt, 2018;</ref>, or relies on multilingual word embeddings and neglects context when measuring translation parallelism <ref type="bibr" target="#b11">(Hangya and Fraser, 2018)</ref>.</p><p>In this paper, we propose a simple but effective parallel corpus filtering method. Multilingual BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> projects multilingual sentences into a shared space and has shown a great potential for cross-lingual model transfer <ref type="bibr" target="#b26">(Pires et al., 2019)</ref>. We use pre-trained multilingual BERT as prior knowledge and fine-tune it on a synthetic dataset. This multilingual BERT-based classifier forms an acceptability filter that determines whether or not a sentence pair consists of a bona-fide translation.</p><p>As the domain of training data largely affects machine translation model performance, we also introduce a domain filter. It uses the pre-trained Generative Pre-training (GPT) as in-domain language model and is an extension of the existing crossentropy difference based domain filter (Moore and <ref type="bibr" target="#b24">Lewis, 2010;</ref><ref type="bibr" target="#b15">Junczys-Dowmunt, 2018)</ref>.</p><p>We evaluate our proposed method on the WMT 2018 German-English Parallel Corpus Filtering shared task and achieve a new state-of-the-art. Our unsupervised method achieves comparable performance to the top system that is trained on millions of clean parallel sentence pairs. Our proposed methods also significantly outperform baselines in our own Japanese-Chinese parallel corpus filtering task.</p><p>We make the following contributions:</p><p>• We propose a novel approach to filter noisy parallel corpora by using pre-trained language models. Our approach outperforms strong baselines and achieves a new state-of-the-art.</p><p>• We devise an unsupervised filtering approach that does not require an identifiable clean subset of parallel segments. Our unsupervised method matches the results of previous supervised methods.</p><p>• We release a large web-crawled Japanese-Chinese parallel corpus which can be a useful resource for machine translation research on non-English language pairs. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several recent works address parallel corpus filtering. <ref type="bibr" target="#b7">Denkowski et al. (2012)</ref>, <ref type="bibr" target="#b10">Dyer et al. (2010)</ref> and <ref type="bibr" target="#b12">Heafield (2011)</ref> use language models and word alignments to determine how likely sentences are to be a good translation of another. <ref type="bibr" target="#b32">Xu and Koehn (2017)</ref> introduce a noise filtering tool, Zipporah, that discriminates parallel and non-parallel sentences based on word-frequency vectors and a dictionary. Junczys-Dowmunt (2018) proposes a dual conditional cross-entropy filtering method, which achieved first place in the WMT 2018 German-English Parallel Corpus Filtering shared task. They train two translation models in inverse directions on millions of parallel sentences and score sentence pairs based on the word-normalized conditional cross-entropy from the translation models. <ref type="bibr" target="#b0">Artetxe and Schwenk (2019)</ref> and <ref type="bibr" target="#b28">Schwenk (2018)</ref> propose a margin-based scoring method that compares the similarity of the source and target sentence representations. The sentence representations are produced by a sentence encoder trained on clean parallel data via a neural encoder-decoder architecture.</p><p>Other works based on sentence embeddings include <ref type="bibr" target="#b11">Hangya and Fraser (2018)</ref> and , as well as , which mines millions of parallel sentences in 1620 language pairs from Wikipedia. These encoder-decoder based methods require large amounts of clean parallel training data and are not applicable in our scenario where available data is noisy. Ondrej Bojar (2020) organize an open domain translation challenge where participants are provided a large, noisy set of Japanese-Chinese segment pairs built from web data, and the task is to clean the noisy data and build an end-to-end machine translation system. Work on data selection is also related. Moore and Lewis (2010); Junczys-Dowmunt (2018) select domain-related data by computing the crossentropy difference between in-domain and outdomain language models. <ref type="bibr" target="#b9">Duh et al. (2013)</ref> use neural language models for data selection. <ref type="bibr" target="#b1">Axelrod et al. (2011)</ref> and <ref type="bibr" target="#b2">Axelrod et al. (2015)</ref> expand cross-entropy difference filtering to both sides of the parallel corpus. Since we aim to build a general machine translation system, instead of selecting data that are relevant to a specific domain, we select data whose domains are as general as possible, by using Generative Pre-training (GPT) models trained on large and diverse corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we introduce a language detection filter, a translation-acceptability filter, and a domain filter. Each filter produces a score for every candidate source/target sentence pair. The partial score produced by each filter ranges from 0 to 1. Values beyond this range are normalized by minmax normalization:ŷ = (y − min)/(max − min). The final score is the product of the partial scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Detection Filter</head><p>Targeting a web-crawler at a given language pair still results in many pages written in the wrong language. For example, while a URL pair may clearly indicate translation (e.g., ".jp" and ".zh"), it may happen that the text content is simply copied rather than translated. We observe this in both our Japanese-Chinese data and the German-English Paracrawl data set. It is necessary to filter out sen-tence pairs with undesired languages.</p><p>We adopt the fastText <ref type="bibr" target="#b14">(Joulin et al., 2017</ref><ref type="bibr" target="#b13">(Joulin et al., , 2016</ref>) language identification toolkit in our language detection filter. For each sentence, the toolkit produces a list of language candidates and their corresponding confidence scores. We select the language that has the highest confidence score from fastText as the language of the sentence. Sentence pairs that have both of the elements detected as the desired language are assigned score 1 and otherwise 0. By discarding sentence pairs with undesired language IDs, we filter out 27% of our Chinese-Japanese parallel sentences and nearly 70% of the German-English parallel sentences from Paracrawl data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Acceptability Filter</head><p>In this section, we introduce our translation acceptability filter, one of the main contributions in the paper. It aims to measure the parallelism of sentence pairs and filter out sentence pairs that are not mutual translations.</p><p>The pre-trained language model BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> has been shown to be effective in many NLP tasks as it produces better and meaningful contextualized word representations. Multilingual BERT, a transformer Masked Language Model pre-trained on Wikipedia dumps of 104 languages, shows remarkable multilingual capability, given that it is not exposed to any multilingual signals, such as parallel data or dictionaries. A thorough study by <ref type="bibr" target="#b26">Pires et al. (2019)</ref> shows the promising zero-shot cross-lingual model transfer ability of multilingual BERT on named entity recognition and part-of-speech tagging tasks. They hypothesize that having language-universal word pieces, such as numbers and URLs, mapped to a shared space forces the co-occurring pieces to also be mapped to a shared space, thus spreading the effect to other word pieces, until different languages are close in the shared space.</p><p>We use pre-trained multilingual BERT to encode a sentence pair (s, t) and create the sentence embeddings v s and v t by using the representations of the [CLS] token of s and t. We find that the cosine similarity between v s and v t does not necessarily reflect the parallelism of sentence s and t. We suspect that the word representations from multilingual BERT are loosely aligned across languages as there is no parallel data or dictionary used during the pre-training. A similar observation was made in <ref type="bibr" target="#b19">Lample et al. (2018)</ref>, where the cross-lingual word embeddings learned in an unsupervised manner are loosely aligned. However, after fine-tuning on a few anchor pairs (word translations), they become more aligned.</p><p>Similarly, we use an unsupervised synthetic training set as anchors to fine-tune multilingual BERT with a binary classification objective. <ref type="bibr" target="#b32">Xu and Koehn (2017)</ref> did similar work to train a filtering classifier on synthetic data, but via bag-ofwords translation features.</p><p>Synthetic Training Set. In cases where a small number of clean parallel sentence pairs are available, we use them as positive training samples for our classifier. In Japanese-Chinese filtering, we use around 300k sentence pairs, mostly from open-source software documentation, 2 as our positive samples. In extreme cases where no identifiable, clean parallel data is available, we sub-select high quality parallel sentences, which are used as positive samples, from the noisy parallel corpus based on the Hunalign <ref type="bibr" target="#b31">(Varga et al., 2007)</ref> sentencealignment score. We sample negative instances by simulating the noise produced by web crawling and alignment. Given a positive pair (s, t), we create a negative sample by randomly choosing one of the following options:</p><p>• Randomly select a target sentence from its adjacent sentences within a window size of k (where k = 2 in our experiments).</p><p>• Randomly truncate 30%-70% of the source or target sentence.</p><p>• Swap the order of 30%-70% words of the source or target sentence.</p><p>To balance the training set, we create the same number of positive instances and sampled negative instances. Binary Classification Objective. We feed the sentence pair (s, t) into multilingual BERT, which accepts two-sentence input due to its next-sentence prediction objective <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. Instead of using the [CLS] token representation, we use a Convolutional Network (CNN) layer that takes the BERT output and generates the final representation of the pair. Our experiments show that using CNN layer pooling achieves marginal gains over <ref type="bibr">[CLS]</ref> pooling. The final layer is a feed-forward network with a softmax activation function to produce label probabilities. We use the softmax probability as the degree of parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Domain Filter</head><p>Web-crawled data contains noise of various types, due to the complicated structure of web pages. By inspecting the training data generated by the above methods, we notice much of the content is not well-formed, e.g., concatenated lists of months and dates, randomly mixed content from tables, series of emojis and punctuation marks, etc. These are certainly written in the desired language, thus not filtered out by language detection. The translation acceptability filter also accepts them. However, such malformatted data is not helpful to machine translation models, and we prefer a training corpus to contain meaningful content.</p><p>For our domain filter, we adopt the cross-entropy difference scoring method proposed by Moore and Lewis <ref type="formula">(2010)</ref> and <ref type="bibr" target="#b15">Junczys-Dowmunt (2018)</ref>. More specifically, we treat a general domain monolingual corpus as our in-domain data set I, and the noisy parallel corpus without any filtering as our nondomain data set N. We train two language models L I and L N and measure how the target sentence t is domain-related to I and less domain-related to N by a perplexity ratio, which is a transformation of cross-entropy difference:</p><formula xml:id="formula_0">f dom (s, t) = PPL N (t) PPL I (t)</formula><p>where PPL M (x) is the word-normalized perplexity of the sentence x defined by the language model L M :</p><formula xml:id="formula_1">PPL M (x) = exp( 1 |x| |x| i=1 log P M (x i |x &lt;i ))</formula><p>The intuition is fairly straightforward: the higher the perplexity of the sentence to the non-domain corpus and the lower the perplexity of the sentence to the in-domain corpus, the more likely the sentence is meaningful.</p><p>Our contribution is to use GPT <ref type="bibr" target="#b27">(Radford et al., 2019)</ref> as our in-domain language model, instead of news domain text (Junczys-Dowmunt, 2018).</p><p>This minor yet crucial change yields non-trivial performance gains in our experiments for German-English parallel corpus filtering. As GPT is trained on data from various sources, such as Wikipedia, Reddit, news websites, etc., it covers a wide range of domains, so our filtered data is more diverse and performs better on multi-domain test sets, as well as in the real world application.</p><p>For our in-domain language model, we use pre-trained Chinese GPT 3 for Japanese-Chinese and pre-trained GPT-2 4 for German-English. We randomly sample 4 million sentences from the unfiltered noisy parallel corpus and use KenLM <ref type="bibr" target="#b12">(Heafield, 2011)</ref> to train the non-domain language model. Perplexity scores from different language models are compatible.</p><p>Following Junczys-Dowmunt (2018), we introduce two operations, clip and cutoff, to postprocess the domain filter scoref dom (s, t). The clip operation clips the maximum value of the domain score to a threshold τ clip : f clip (x, τ clip ) = min(x, τ clip ) and the cutoff operation modifies scores below a threshold τ cutoff and changes them to 0:</p><formula xml:id="formula_2">f cutoff (x, τ cutoff ) =</formula><p>x, if x &gt; τ cutoff 0, otherwise τ clip prevents a high monolingual in-domain score from overwriting scores from other filters. τ cutoff eliminates out-domain sentence pairs and ensures that highly parallel sentence pairs are at least somewhat in-domain. We tune τ clip and τ cutoff on the development set. The scoring method of our final domain filter becomes:</p><formula xml:id="formula_3">f dom (s, t) = f clip (f cutoff (f dom (s, t), τ cutoff ), τ clip )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">WMT 2018 Parallel Corpus Filtering</head><p>We use the WMT 2018 Parallel Corpus Filtering shared task  as a benchmark to evaluate our methods. Participants in the shared task are provided a very noisy 1 billion word (English token count) German-English corpus crawled from the web by the Paracrawl project. <ref type="bibr">5</ref> The task is to sub-select clean sentence pairs amounting to (a) 10 million words, and (b) 100 million words, counted on the English side. The quality of the resulting subsets is determined by training a neural machine translation system (Marian) 6 (Junczys-Dowmunt et al., 2018) on this data. The quality of the machine translation system is measured by BLEU score on six test sets from various domains. As the task is to address the challenge of the data quality and not domain-relatedness of the data for a particular use, sub-sampling the corpus for relevance to the news domain is not encouraged by the shared task organizers. All parameters used for training Marian machine translation models are the same as described in . We use CLIP = 5 and CUTOFF = 1.5 in the experiments. We use 4 GPUs for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Web-Crawled Japanese-Chinese Parallel Corpus Filtering</head><p>Due to the lack of publicly available Japanese-Chinese parallel corpus, we build a data harvesting pipeline to fetch Japanese-Chinese parallel text from the Internet. The crawled bi-text are extremely noisy, but we rely on the proposed parallel corpus filtering method to clean up the data and eventually train a satisfactory machine translation system. In this paper, we use these crawled data as another test bed to evaluate our proposed method. A single run of the of the data harvesting pipeline is the following. We first identify Japanese-Chinese parallel webpages by programmatically analyzing the URL structure of the 5 billion URLs from CommonCrawl, 7 for example, https://www.gotokyo.org/jp/ and https: //www.gotokyo.org/cn/ only differ by jp and cn. Then we download the webpages and conduct a series of cascaded data cleaning methods, including removing HTML markups, sentence segmentation, etc. Finally we perform segment alignment and filtering. Our workflow consists of several runs of the data harvesting pipeline with entry points at different modules (for instance, a more targeted crawling of higher quality material from a previous run).</p><p>We also integrate existing Japanese-Chinese parallel datasets from other publicly available sources for a final parallel data size of 527m characters in 20.9M parallel segments.</p><p>We include all details of our data harvesting 6 https://github.com/marian-nmt/marian (We do not evaluate our method using Moses, the statistical machine translation system provided by WMT, as neural machine translation better fits our real world scenario.) 7 https://commoncrawl.org/ pipeline, as well as the statistics of the obtained dataset, in Appendix A.</p><p>Test and Development Dataset. We curate two parallel test sets by manually processing web data involving daily expressions (337 parallel segments) and news (437 parallel segments). For our development set, we use 5304 Japanese-Chinese basic expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head><p>WMT 2018 Parallel Corpus Filtering. <ref type="table" target="#tab_1">Table 1</ref> presents the BLEU scores of neural machine translation systems trained on 10 million and 100 million words of training data, selected by different filtering methods. In the table, we list the top three performers from the shared task, as well as another two work that are similar to ours. Junczys-Dowmunt (2018) has a dual conditional crossentropy adequacy filter and a domain filter trained on news corpora. <ref type="bibr" target="#b11">Hangya and Fraser (2018)</ref> generate sentence embeddings by using unsupervised word embedding alignment and measure parallelism via multilingual sentence embedding similarity.  leverage massive publicly available English-German parallel corpora to train multilingual sentence embeddings via bidirectional Long Short Term Memory (LSTM) encoderdecoder network.</p><p>We replicate the adequacy and domain-news filters from Junczys-Dowmunt (2018) and obtain similar results. By replacing the domain-news filter with our domain-GPT filter, we achieve new stateof-the-art scores on 10M and 100M word data sets (bold scores in the table). Given the very compact score range in the shared task , we consider this gain very successful. It is stated in the shared task that the test sets are from multiple domains. Domain-news filter in Junczys-Dowmunt (2018) tends to select sentence pairs from news domain as the filter is trained on news domain data, and this leads to a biased parallel corpus for training machine translation system. Our proposed domain-GPT filter is trained from various sources and thus covers a wide range of domains, so our filtered data is more diverse and performs better on multidomain test sets.</p><p>For our supervised acceptability filter, we train a mulitlingual BERT classifier on clean parallel sentences as positive examples and randomly sampling negative instances, using the method described in Section 3.2. For our unsupervised acceptabil-   <ref type="table">Table 2</ref>: BLEU scores of Japanese-Chinese and Chinese-Japanese MT systems trained on data sets generated by various filtering methods. We rank sentence pairs by filtering scores and train an MT system on N percent of the top ranked data. N is selected based on the development set and we report the best BLEU score. domain-GPT is the domain filter whose in-domain language model is the pre-trained GPT language model; note that for ZH-JA, we do not have access to pre-trained Japanese GPT.</p><p>ity filter, we rank noisy parallel sentences by (a) the alignment score from Hunalign, and <ref type="formula">(</ref> Japanese-Chinese Parallel Corpus Filtering. In <ref type="table">Table 2</ref>, we evaluate machine translation systems trained on data generated by different filtering methods. Unfiltered refers to data generated by Hunalign without any filtering.  refer to LASER, the top performing filtering system in WMT 2019 Parallel Corpus Filtering shared task. We use the pretrained 93-language LASER model to generate sentence pair scores. The model is trained on a large parallel corpus that contains 3.2M English-Japanese and 8.2M English-Chinese sentence pairs (English is used as pivot to connect Japanese and Chinese during their training). Adequacy refers to the dual conditional cross-entropy filtering method that we replicate from Junczys-Dowmunt (2018). It is trained on around 300k high quality softwaredomain parallel sentences from Microsoft Developer Network (MSDN) and Ubuntu. The GPT domain filter uses a pre-trained Chinese GPT 8 as the in-domain language model and trains a four-gram KenLM <ref type="bibr" target="#b12">(Heafield, 2011)</ref> language model on the Chinese side of our 4 million unfiltered noisy parallel sentences as a non-domain language model. Acceptability is our proposed multilingual BERT based filtering method, which is trained on a synthetic dataset, where we use 300k high-quality software domain parallel sentences as positive examples and sample equal-sized negative sentence pairs, using the sampling methods described in Section 3.2.  train a multilingual sentence encoder on various English-Foreign Language parallel corpus and prove the zero-shot cross-lingual transfer capability between non-English pairs, such as Japanese and Chinese. However, when English is used as the pivot, the distance between Japanese and Chinese become larger, resulting in not effectively capturing the correlation between them. The conditional cross-entropy metric in adequacy relies on the quality of machine translation system. Due to the difficulty of training high-quality machine translation systems on 300k sentence pairs, the adequacy filter cannot produce accurate conditional cross-entropy. The GPT domain filter assigns higher score to sentences that are more like human natural language and downgrades malformatted sentence pairs. It is effective in the German-English filtering task, where a fixed-size subset is selected and we want to fill the subset with as much domain relevant data as possible. However, to best fit the real world scenario where the goal is to have the best machine translation system, we do not limit the amount of data to select for training machine translation system and let the system decide the amount of the data to select, according to each filtering method. We rank sentence pairs by their filtering scores and train a MT system on N percentage of the top ranked data. N is selected based on the development set and we report the best BLEU score. Under this setting, adding a domain filter makes the model use less data (N = 50%  <ref type="figure">Figure 1</ref>: Precision and recall curves of the acceptability filter on our internal JA-ZH filtering test set. The threshold is based on the classifier probability produced by the softmax layer. When threshold set to 0.9, we obtain 97.7% precision parallel sentence pairs at 66.9% recall.</p><p>vs N = 75%), but we do not observe any performance gain, as we suspect that the malformatted but parallel sentence pairs are neither harmful or helpful to the model, and filtering them out makes no difference in performance of the model. High Precision Parallel Corpus Filtering. For analysis purposes, we manually annotate a small set of 320 sentence pairs randomly selected from our original web crawled Japanese-Chinese data set. 24% of the sentence pairs are labeled "not mutual translations." As stated in , neural machine translation models are more sensitive to noise than statistical machine translation models, so having high precision filtering results as training data is necessary. In <ref type="figure">Figure</ref> 1, we show precision and recall curves for our proposed filtering method on this labeled test set, under different threshold settings. The threshold is selected based on the filtering classifier probability produced by the softmax layer. By setting the threshold to 0.9, we are able to obtain 97.7% precision high-quality parallel sentences, while still having 66.9% recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we address the parallel corpus filtering problem in machine translation. We propose a novel filtering method using pre-trained language models. Our method outperforms strong baselines and achieves a new state-of-the-art. We release a large Japanese-Chinese web crawled parallel corpus for the research purposes. Because it is artificial to use synthetic data for training a filter classifier, future work can focus on a better objective that models parallelism more smoothly. Future work also includes extending the method to low-resource languages not covered by multilingual BERT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>8 pre-trained Mixedlarge corpus + GptEncoder + LmTarget Model in https://github.com/dbiir/UER-py</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>GPT. ‡ our unsupervised acceptability + domain-GPT is comparable to top supervised method.</figDesc><table><row><cell>Method</cell><cell>Supervised Unsupervised</cell><cell>10M 100M</cell></row><row><cell>Junczys-Dowmunt (2018) top-1</cell><cell>x</cell><cell>28.62 32.05</cell></row><row><cell>Lu et al. (2018) top-2</cell><cell>x</cell><cell>27.60 31.93</cell></row><row><cell>Lo et al. (2018) top-3</cell><cell>x</cell><cell>27.41 31.88</cell></row><row><cell>Hangya and Fraser (2018)</cell><cell>x</cell><cell>22.96 30.54</cell></row><row><cell>Chaudhary et al. (2019)</cell><cell>x</cell><cell>26.98 30.77</cell></row><row><cell>adequacy (our replication of J-D 2018)</cell><cell>x</cell><cell>27.12 31.20</cell></row><row><cell>+ domain-news (our replication of J-D 2018)</cell><cell>x</cell><cell>28.66 32.01</cell></row><row><cell>+ domain-GPT</cell><cell>x</cell><cell>† 29.09  † 32.11</cell></row><row><cell>supervised acceptability</cell><cell>x</cell><cell>27.09 31.56</cell></row><row><cell>+ domain-GPT</cell><cell>x</cell><cell>28.94 32.03</cell></row><row><cell>unsupervised acceptability</cell><cell>x</cell><cell>27.03 30.65</cell></row><row><cell>+ domain-GPT</cell><cell>x</cell><cell>‡ 28.68  ‡ 32.02</cell></row><row><cell cols="2">-all methods above apply language detection filter beforehand.</cell><cell></cell></row><row><cell cols="2">† our new state-of-the-art combines adequacy (Junczys-Dowmunt, 2018) + our proposed domain-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>BLEU scores of German-English neural MT systems trained on 10 million and 100 million word training data selected by different methods. The scores are averaged BLEU scores across the six test sets from WMT 2018 parallel corpus filtering task. domain-news trains an in-domain language model on news corpus, while domain-GPT uses the pre-trained GPT language model.</figDesc><table><row><cell>Methods</cell><cell cols="2">JA-ZH %  *</cell><cell cols="2">ZH-JA %  *</cell></row><row><cell>unfiltered</cell><cell cols="2">22.92 100</cell><cell cols="2">22.27 100</cell></row><row><cell>Chaudhary et al. (2019)</cell><cell>23.46</cell><cell>75</cell><cell>26.22</cell><cell>70</cell></row><row><cell>adequacy (our replication of J-D 2018)</cell><cell>23.91</cell><cell>90</cell><cell>24.51</cell><cell>90</cell></row><row><cell>+ domain-GPT</cell><cell>24.00</cell><cell>65</cell><cell>-</cell><cell>-</cell></row><row><cell>acceptability</cell><cell>25.53</cell><cell>75</cell><cell>28.54</cell><cell>50</cell></row><row><cell>+ domain-GPT</cell><cell>25.49</cell><cell>50</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">-all methods above apply language detection filter beforehand.</cell><cell></cell><cell></cell><cell></cell></row></table><note>* percentage of raw parallel sentences used for MT training.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://iwslt.org/doku.php?id=open_ domain_translation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">GNOME, Ubuntu, OpenOffice, and KDE data set, from http://opus.nlpl.eu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/dbiir/UER-py 4 https://github.com/huggingface/transformers 5 https://paracrawl.eu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://commoncrawl.org/ 10 Using Python module BeautifulSoup 11 http://mokk.bme.hu/en/resources/hunalign/ 12 Using the Python-based scrapy tool</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their constructive feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Web-Crawled Parallel Data for Japanese-Chinese   This appendix describes our pipeline to extract parallel Japanese-Chinese parallel sentence fragments from the Internet <ref type="figure">(Figure 2</ref>). We start with 5 billion URLs from CommonCrawl. <ref type="bibr">9</ref> We identify Japanese-Chinese parallel webpages by looking at URL structure (step 2). For example, https://www.gotokyo. org/jp/ and https://www.gotokyo.org/cn/ only differ by jp and cn. We download these potentially parallel page pairs (step 3), remove HTML and other markup metadata (step 4), 10 and split into sentence segments. We use off-the-shelf Hunalign 11 for segment alignment (step 5). We filter segment pairs by rough language ID and length ratio (step 6). We obtain 227k URL pairs, 1.4m segment pairs, and 28.7m characters of parallel data (measured on the Chinese side).</p><p>From the 227k URL pairs above, we trace which site pairs yielded the most parallel data. We then run a deep-crawling module on each of the 6000 most-promising sites, 12 and we process the resulting URLs using the rest of the pipeline. Concatenating parallel data from all runs (step 7) and running a simple post-processing filter to remove objectionable content in the text gathered, we obtain around 494m characters of parallel data (measured on the Chinese side).</p><p>We also integrate existing Japanese-Chinese parallel datasets from other publicly available sources for a final parallel data size 527m characters in 20.9m parallel segments. <ref type="table">Table 3</ref> describes the various components of this dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marginbased parallel corpus mining with multilingual sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation via pseudo in-domain data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-based n-gram language difference models for data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogarshi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Martindale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>the International Workshop on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations (ICLR</title>
		<meeting>the Sixth International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lowresource corpus filtering using multilingual sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrated parallel sentence and fragment extraction from comparable corpora: A case study on Chinese-Japanese Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Asian and Low-Resource Language Information Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MMCR4NLP: multilingual multiway corpora repository for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno>abs/1710.01025</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The CMU-Avenue French-English translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Hanneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptation data selection using neural language models: Experiments in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An unsupervised system for parallel corpus filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Hangya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">KenLM: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hérve</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">FastText.zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual conditional cross-entropy filtering of noisy parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Marian: Fast neural machine translation in C++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Neckermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Nikolay Bogoychev, André F. T. Martins, and Alexandra Birch. System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the impact of various types of noise on neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Neural Machine Translation and Generation</title>
		<meeting>the Workshop on Neural Machine Translation and Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><forename type="middle">L</forename><surname>Forcada</surname></persName>
		</author>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Findings of the WMT 2018 shared task on parallel corpus filtering</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">OpenSub-titles2016: Extracting large parallel corpora from movie and TV subtitles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Measuring sentence parallelism using Mahalanobis distances: The NRC unsupervised submissions to the WMT18 parallel corpus filtering shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darlene</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accurate semantic textual similarity for cleaning noisy parallel corpora using semantic machine translation evaluation metric: The NRC supervised submissions to the parallel corpus filtering task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darlene</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Alibaba submission to the WMT18 parallel corpus filtering task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangbin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Marcello Federico. 2020. Findings of the iwslt 2020 evaluation campaign</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Spoken Language Translation (IWSLT)</title>
		<editor>Christian Federmann Jiatao Gu Fei Huang Ajay Nagesh Jan Niehues Elizabeth Salesky Sebastian St uker Marco Turchi Ondrej Bojar</editor>
		<meeting>the 2020 International Conference on Spoken Language Translation (IWSLT)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual BERT?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Filtering and mining parallel data in a joint multilingual space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05791</idno>
		<title level="m">Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from Wikipedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in OPUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parallel corpora for medium density languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dániel</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Péter</forename><surname>Halácsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Kornai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Nagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Amsterdam Studies in the Theory and History of Linguistic Science</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>László Németh, and Viktor Trón</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zipporah: a fast and scalable data cleaning system for noisy webcrawled parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hainan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
