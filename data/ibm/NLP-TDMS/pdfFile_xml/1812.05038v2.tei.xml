<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-Term Feature Banks for Detailed Video Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Long-Term Feature Banks for Detailed Video Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To understand the world, we humans constantly need to relate the present to the past, and put events in context. In this paper, we enable existing video models to do the same. We propose a long-term feature bank-supportive information extracted over the entire span of a video-to augment state-of-the-art video models that otherwise would only view short clips of 2-5 seconds. Our experiments demonstrate that augmenting 3D convolutional networks with a long-term feature bank yields state-of-the-art results on three challenging video datasets: AVA, EPIC-Kitchens, and Charades. Code is available online. 1 1 https://github.com/facebookresearch/ video-long-term-feature-banks Input clip (4 seconds) Target frame arXiv:1812.05038v2 [cs.CV]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What is required to understand a movie? Many aspects of human intelligence, for sure, but memory is particularly important. As a film unfolds there's a constant need to relate whatever is happening in the present to what happened in the past. Without the ability to use the past to understand the present, we, as human observers, would not understand what we are watching.</p><p>In this paper, we propose the idea of a long-term feature bank that stores a rich, time-indexed representation of the entire movie. Intuitively, the long-term feature bank stores features that encode information about past and (if available) future scenes, objects, and actions. This information provides a supportive context that allows a video model, such as a 3D convolutional network, to better infer what is happening in the present (see <ref type="figure">Fig. 1</ref>, 2, and 5).</p><p>We expect the long-term feature bank to improve stateof-the-art video models because most make predictions based only on information from a short video clip, typically 2-5 seconds <ref type="bibr">[5, 33, 46-48, 51, 52, 56]</ref>. The reason for this short-term view is simple: benchmark advances have resulted from training end-to-end networks that use some form of 3D convolution, and these 3D convolutions require dense sampling in time to work effectively. Therefore, to fit in GPU memory the video inputs must be short. <ref type="bibr">Figure 1</ref>. What are these people doing? Current 3D CNN video models operate on short clips spanning only ∼4 seconds. Without observing longer-term context, recognition is difficult. (Video from the AVA dataset <ref type="bibr" target="#b13">[14]</ref>; see next page for the answer.)</p><p>The long-term feature bank is inspired by works that leverage long-range temporal information by using precomputed visual features <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b56">57]</ref>. However, these approaches extract features from isolated frames using an ImageNet pre-trained network, and then use the features as input into a trained pooling or recurrent network. Thus the same features represent both the present and the long-term context. In contrast, we propose to decouple the two: the long-term feature bank is an auxiliary component that augments a standard video model, such as a state-of-the-art 3D CNN. This design enables the long-term feature bank to store flexible supporting information, such as object detection features, that differ from what the 3D CNN computes.</p><p>Integrating the long-term feature bank with 3D CNNs is straightforward. We show that a variety of mechanisms are possible, including an attentional mechanism that relates information about the present (from the 3D CNN) to longrange information stored in the long-term feature bank. We illustrate its application to different tasks with different output requirements: we show results on datasets that require object-level as well as frame-or video-level predictions.</p><p>Finally, we conduct extensive experiments demonstrating that augmenting 3D CNNs with a long-term feature bank yields state-of-the-art results on three challenging video datasets: AVA spatio-temporal action localization <ref type="bibr" target="#b13">[14]</ref>, EPIC-Kitchens verb, noun, and action classification <ref type="bibr" target="#b5">[6]</ref>, and Charades video classification <ref type="bibr" target="#b37">[38]</ref>. Our ablation study establishes that the improvements on these tasks arise from the integration of long-term information.</p><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>...</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long-Term Feature Bank</head><p>3D CNN input <ref type="figure">Figure 2</ref>. The action becomes clear when relating the target frame to the long-range context. Our long-term feature bank provides longterm supportive information that enables video models to better understand the present. (AVA ground-truth label: 'listening to music'.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep networks are the dominant approach for video understanding <ref type="bibr">[5, 21, 33, 39, 46-48, 50, 51, 56]</ref>. This includes the highly successful two-stream networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref> and 3D convolutional networks <ref type="bibr">[5, 33, 46-48, 51, 56]</ref>. In this paper, we use 3D CNNs, but the long-term feature bank can be integrated with other families of video models as well.</p><p>Temporal and relationship models include RNNs that model the evolution of video frames <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b56">57]</ref> and multilayer perceptrons that model ordered frame features <ref type="bibr" target="#b57">[58]</ref>. To model finer-grained interactions, a growing line of work leverages pre-computed object proposals <ref type="bibr" target="#b51">[52]</ref> or detections <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>, and models their co-occurrence <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref>, temporal order <ref type="bibr" target="#b3">[4]</ref>, or spatial arrangement <ref type="bibr" target="#b51">[52]</ref> within a short clip.</p><p>Long-term video understanding with modern CNNs is less explored, in part due to GPU memory constraints. One strategy to overcome these constraints is to use precomputed features without end-to-end training <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b56">57]</ref>. These methods do not optimize features for a target task, and thus are likely suboptimal. Another strategy is to use aggressive subsampling <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b57">58]</ref> or large striding <ref type="bibr" target="#b7">[8]</ref>. TSN <ref type="bibr" target="#b49">[50]</ref> samples 3-7 frames per video. ST-ResNet <ref type="bibr" target="#b7">[8]</ref> uses a temporal stride of 15. To our knowledge, our approach is the first that enjoys the best of three worlds: end-to-end learning for strong short-term features with dense sampling and decoupled, flexible long-term modeling.</p><p>Spatio-temporal action localization is an active research area <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53]</ref>. Most recent approaches extend object detection frameworks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref> to first propose tubelets/boxes in a short clip/frame, and then classify the tubelets/boxes into action classes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b35">36]</ref>. The detected tubelets/boxes can then be optionally linked to form full action tubes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>. In contrast to our method, these methods find actions within each frame or clip independently without exploiting long-term context.</p><p>Information 'bank' representations, such as object bank <ref type="bibr" target="#b25">[26]</ref>, detection bank <ref type="bibr" target="#b0">[1]</ref>, and memory networks <ref type="bibr" target="#b41">[42]</ref> have been used as image-level representations, for video indexing and retrieval, and for modeling information in text corpora. We draw inspiration from these approaches and develop methodologies for detailed video understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Long-Term Feature Bank Models</head><p>For computer vision models to make accurate predictions on long, complex videos they will certainly require the ability to relate what is happening in the present to events that are distant in time. With this motivation in mind, we propose a model with a long-term feature bank to explicitly enable these interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method Overview</head><p>We describe how our method can be used for the task of spatio-temporal action localization, where the goal is to detect all actors in a video and classify their actions. Most state-of-the-art methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b42">43]</ref> combine a 'backbone' 3D CNN (e.g., C3D <ref type="bibr" target="#b45">[46]</ref>, I3D <ref type="bibr" target="#b4">[5]</ref>) with a region-based person detector (e.g., Fast/Faster R-CNN <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref>). To process a video, it is split into short clips of 2-5 seconds, which are independently forwarded through the 3D CNN to compute a feature map, which is then used with region proposals and region of interest (RoI) pooling to compute RoI features for each candidate actor <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>. This approach, which captures only short-term information, is depicted in <ref type="figure">Fig. 3a</ref>.</p><p>The central idea in our method is to extend this approach with two new concepts: (1) a long-term feature bank that intuitively acts as a 'memory' of what happened during the entire video-we compute this as RoI features from detections at regularly sampled time steps; and (2) a feature bank operator (FBO) that computes interactions between the short-term RoI features (describing what actors are doing now) and the long-term features. The interactions may be computed through an attentional mechanism, such as a non-local block <ref type="bibr" target="#b50">[51]</ref>, or by feature pooling and concatenation. Our model is summarized in <ref type="figure">Fig. 3b</ref>. We introduce these concepts in detail next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Long-Term Feature Bank</head><p>The goal of the long-term feature bank, L, is to provide relevant contextual information to aid recognition at the current time step. For the task of spatio-temporal action localization, we run a person detector over the entire video to generate a set of detections for each frame. In parallel, we run a standard clip-based 3D CNN, such as C3D <ref type="bibr" target="#b45">[46]</ref> or I3D <ref type="bibr" target="#b4">[5]</ref>, over the video at regularly spaced intervals, such as every one second. We then use RoI pooling to extract fea-  <ref type="figure">Figure 3</ref>. We contrast our model with standard methods. (a) 3D CNN: vanilla 3D CNNs process a short clip from a video (e.g., 2-5 seconds) and use pooling to obtain a representation of the clip (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Bank Operator</head><p>Our model references information from the long-term features L via a feature bank operator, FBO(S t ,L t ). The feature bank operator accepts inputs S t andL t , where S t is the short-term RoI pooled feature andL t is [L t−w , . . . , L t+w ], a subset of L centered at the current clip at t within 'window' size 2w + 1, stacked into a matrix L t ∈ R N ×d , where N = t+w t =t−w N t . We treat the window size 2w + 1 as a hyperparameter that we cross-validate in our experiments. The output is then channel-wise concatenated with S t and used as input into a linear classifier.</p><p>Intuitively, the feature bank operator computes an updated version of the pooled short-term features S t by relating them to the long-term features. The implementation of FBO is flexible. Variants of attentional mechanisms are an obvious choice and we will consider multiple instantiations in our experiments.</p><p>Batch vs. Casual. Thus far we have assumed a batch setting in which the entire video is available for processing. Our model is also applicable to online, casual settings. In this case,L t contains only past information of window size 2w + 1; we consider both batch and causal modes of operation in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>Backbone. We use a standard 3D CNN architecture from recent video classification work. The model is a ResNet-50 <ref type="bibr" target="#b15">[16]</ref> that is pre-trained on ImageNet <ref type="bibr" target="#b34">[35]</ref> and 'inflated' into a network with 3D convolutions (over space and time) using the I3D technique <ref type="bibr" target="#b4">[5]</ref>. The network structure is modified to include non-local operations <ref type="bibr" target="#b50">[51]</ref>. After inflating the network from 2D to 3D, we pre-train it for video classification on the Kinetics-400 dataset <ref type="bibr" target="#b4">[5]</ref>. The model achieves 74.9% (91.6%) top-1 (top-5) accuracy on the Kinetics-400 <ref type="bibr" target="#b4">[5]</ref> validation set. Finally, we remove the temporal striding for conv 1 and pool 1 following <ref type="bibr" target="#b51">[52]</ref>, and remove the Kinetics-specific classification layers to yield the backbone model. The exact model specification is given in the Appendix. The resulting network accepts an input of shape 32 × H × W × 3, representing 32 RGB frames with spatial size H × W , and outputs features with shape 16 × H/16 × W/16 × 2048. The same architecture is used to compute short-term features S and long-term features L. Parameters are not shared between these two models unless otherwise noted.</p><p>RoI Pooling. We first average pool the video backbone features over the time axis. We then use RoIAlign <ref type="bibr" target="#b14">[15]</ref> with a spatial output of 7 × 7, followed by spatial max pooling, to yield a single 2048 dimensional feature vector for the RoI. This corresponds to using a temporally straight tube <ref type="bibr" target="#b13">[14]</ref>.</p><p>Feature Bank Operator Instantiations. The feature bank operator can be implemented in a variety of ways. We experiment with the following choices; others are possible.</p><p>-LFB NL: Our default feature bank operator FBO NL (S t ,L t ) is an attention operator. Intuitively, we use S t to attend to features inL t , and add the attended information back to S t via a shortcut connection. We use a simple implementation in which FBO NL (S t ,L t ) is a stack of up to three non-local (NL) blocks <ref type="bibr" target="#b50">[51]</ref>. We replace the selfattention of the standard non-local block <ref type="bibr" target="#b50">[51]</ref> with attention between the local features S t and the long-term feature windowL t , illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref>. In addition, our design uses layer normalization (LN) <ref type="bibr" target="#b2">[3]</ref> and dropout <ref type="bibr" target="#b40">[41]</ref> to improve regularization. We found these modifications to be important since our target tasks contain relatively few training videos and exhibit overfitting. The stack of modified      non-local blocks, denoted as NL , is iterated as:</p><formula xml:id="formula_0">S t L t L t LN S t (1) N t × 512 N × 512 N t × N N × 512 N t × 512 N × 512 N × 512 N t × 512~⇥ &lt;</formula><formula xml:id="formula_1">o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A</formula><formula xml:id="formula_2">o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; &lt; l a</formula><formula xml:id="formula_3">o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; &lt; l a</formula><formula xml:id="formula_4">S (1) t = NL θ1 (S t ,L t ), S (2) t = NL θ2 (S (1) t ,L t ), . . .</formula><p>where θ {1,2,... } are learnable parameters. Similar to Wang et al. <ref type="bibr" target="#b51">[52]</ref>, we use a linear layer to reduce the FBO NL input dimensionality to 512 and apply dropout <ref type="bibr" target="#b40">[41]</ref> with rate 0.2. Thus the input of the final linear classifier is 2048 (S t ) + 512 (FBO NL output) = 2560 dimensional.</p><p>-LFB Avg/Max: This version uses a simpler variant in which FBO pool (S t ,L t ) = pool(L t ), where pool can be either average or max pooling. This implementation results in a classifier input that is 2048 (S t ) + 2048 (FBO pool output) = 4096 dimensional. Training. Joint, end-to-end training of the entire model <ref type="figure">(Fig. 3b)</ref> is not feasible due to the computational and memory complexity of back-propagating through the long-term feature bank. Instead, we treat the 3D CNN and detector that are used to compute L as fixed components that are trained offline, but still on the target dataset, and not updated subsequently. We have experimented with alternating optimization methods for updating these models, similar to target propagation <ref type="bibr" target="#b22">[23]</ref>, but found that they did not improve results. Dataset-specific training details are given later. A Baseline Short-Term Operator. To validate the benefit of incorporating long-term information, we also study a 'degraded' version of our model that does not use a longterm feature bank. Instead, it uses a short-term operator that is identical to FBO NL , but only references the information from within a clip: STO(S t ) := FBO NL (S t , S t ). STO is conceptually similar to <ref type="bibr" target="#b51">[52]</ref> and allows for backpropagation. We observed substantial overfitting with STO and thus applied additional regularization techniques. See the Appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on AVA</head><p>We use the AVA dataset <ref type="bibr" target="#b13">[14]</ref> for extensive ablation studies. AVA consists of 235 training videos and 64 validation videos; each video is a 15 minute segment taken from a movie. Frames are labeled sparsely at 1 FPS. The labels are: one bounding box around each person in the frame together with a multi-label annotation specifying which actions the person in the box is engaged in within ±0.5 seconds of the labeled frame. The action label space is defined as 80 'atomic' actions defined by the dataset authors.</p><p>The task in AVA is spatio-temporal action localization: each person appearing in a test video must be detected in each frame and the multi-label actions of the detected person must be predicted correctly. The quality of an algorithm is judged by a mean average precision (mAP) metric that requires at least 50% intersection over union (IoU) overlap for a detection to be matched to the ground-truth while simultaneously predicting the correct actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Next, we describe the object detector, input sampling, and training and inference details used for AVA.</p><p>Person Detector. We use Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> with a ResNeXt-101-FPN <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b54">55]</ref> backbone for person detection. The model is pre-trained on ImageNet <ref type="bibr" target="#b34">[35]</ref> and COCO keypoints <ref type="bibr" target="#b28">[29]</ref>, and then fine-tuned on AVA bounding boxes; see the Appendix for training details. The final model obtains 93.9 AP@50 on the AVA validation set.</p><p>Temporal Sampling. Both short-and long-term features are extracted by 3D CNNs that use 32 input frames sampled with a temporal stride of 2 spanning 63 frames (∼2 seconds in 30 FPS video). Long-term features are computed at one clip per second over the whole video, with a 3D CNN model <ref type="figure">(Fig. 3a)</ref> fine-tuned on AVA.</p><p>Training. We train our models using synchronous SGD with a minibatch size of 16 clips on 8 GPUs (i.e., 2 clips per GPU), with batch normalization <ref type="bibr" target="#b17">[18]</ref> layers frozen. We train all models for 140k iterations, with a learning rate of 0.04, which is decreased by a factor of 10 at iteration 100k and 120k. We use a weight decay of 10 −6 and momentum of 0.9. For data augmentation, we perform random flipping, random scaling such that the short side ∈ [256, 320] pixels, and random cropping of size 224×224. We use both ground-truth boxes and predicted boxes with scores at least 0.9 for training. This accounts for the discrepancy between ground-truth-box distribution and predicted-box distribution, and we found it beneficial. We assign labels of a ground-truth box to a predicted box if they overlap with IoU at least 0.9. A predicted box might have no labels assigned. Since the number of long-term features N differs from clip to clip, we pad zero-vectors for clips with fewer long-term features to simplify minibatch training. Inference. At test time we use detections with scores ≥ 0.85. All models rescale the short side to 256 pixels and use a single center crop of 256×256. For both training and inference, if a box crosses the cropping boundary, we pool the region within the cropped clip. In the rare case where a box falls out of the cropped region, RoIAlign <ref type="bibr" target="#b14">[15]</ref> pools the feature at the boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiments</head><p>Temporal Support. We first analyze the impact of increasing temporal support on models both with and without LFB. For models without LFB, we evaluate a vanilla 3D CNN <ref type="figure">(Fig. 3a</ref>) and a 3D CNN extended with STO (denoted 'STO' in tables). To increase their temporal support, we increase the temporal stride but fix the number of input frames so that the model covers a longer temporal range while still being feasible to train. To increase the temporal support of LFB models, we increase the 'window size' 2w + 1 ofL t . <ref type="table">Table 1a</ref> compares model performance. Increasing temporal support through striding in fact hurts the performance of both '3D CNN' and 'STO'. Temporal convolution might not be suitable for long-term patterns, since long-term patterns are more diverse, and include challenging scene cuts. On the other hand, increasing temporal support by adding an LFB steadily improves performance, leading to a large gain over the original '3D CNN' (22.1 → 25.5). The online (causal) setting shows a similar trend.</p><p>Overall we observe strong improvements given by longrange context even though AVA actions are designed to be 'atomic' and localized within ±0.5 seconds. For the rest of the ablation study, we focus on the batch setting and use a window size of 60 seconds due to the strong performance.</p><p>Feature Decoupling. In <ref type="table">Table 1b</ref>, we compare our decoupled feature approach with prior long-term modeling strategies where a single, pre-computed feature type is used (e.g. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b56">57]</ref>). To do this, we use the same 3D CNN for both short-term and long-term feature bank computation and keep it fixed during training; only the parameters of the FBO and classifier are updated. We consider two choices: a Kinetics-400 <ref type="bibr" target="#b4">[5]</ref> pre-trained 3D CNN ('K400 feat.') and a 3D CNN that is fine-tuned on AVA ('AVA feat.'). Our decoupled approach, which updates the short-term 3D CNN based on the long-term context, works significantly better.</p><p>FBO Function Design. We next compare different FBO function designs in <ref type="table">Table 1c</ref>. We see that a non-local function significantly outperforms pooling on AVA. This is not surprising, since videos in AVA (and videos in general) are multi-actor, multi-action, and can exhibit complex interactions possibly across a long range of time. We expect a more complex function class is required for reasoning through the complex scenes. Nonetheless, pooling still offers a clear improvement. This again confirms the importance of longterm context in video understanding.</p><p>FBO Input Design. What spatial granularity is required for complex video understanding? In <ref type="table">Table 1d</ref>, we compare constructing long-term features using detected objects ('Detection'), regular grids ('Grid'), and non-spatial features ('Global pool'). In the 'Grid' experiments, we divide the feature map into a k × k grid, for k = 2, 4, and average the features within each bin to obtain a localized representation without an object detector (similar to ACRN <ref type="bibr" target="#b42">[43]</ref>). <ref type="table">Table 1d</ref> shows that the actor-level features ('Detection') works better than coarser regular-grid or non-spatial fea-  <ref type="figure">Figure 5</ref>. Example Predictions. We compare predictions made by models using LFB of different window sizes. Through LFB, the model is able to exploit information distant in time, e.g., the zoomed-in frames in example A and C, to improve predictions. We encourage readers to zoom in for details. (Blue: correct labels. Red: incorrect labels. Best viewed on screen.) tures. We believe this suggests a promising future research direction that moves from global pooling to a more detailed modeling of objects/actors in video. Non-Local Block Design. Next, we ablate our NL block design. In <ref type="table">Table 1e</ref>, we see that adding a second layer of NL blocks leads to improved performance. In addition, scaling <ref type="bibr" target="#b48">[49]</ref>, layer normalization <ref type="bibr" target="#b2">[3]</ref>, and dropout <ref type="bibr" target="#b40">[41]</ref> all contribute to good performance. Layer normalization <ref type="bibr" target="#b2">[3]</ref>, which is part of our modified NL design, is particularly important. As found in <ref type="bibr" target="#b50">[51]</ref>, the default embedded Gaussian variant performs similarly to dot product and concatenation. (Later we found that adding a third layer of NL blocks further improves accuracy, but otherwise use two by default.) Model Complexity. Our approach uses two instances of the backbone model: one to compute the long-term features and another to compute the short-term features. It thus uses around 2× more parameters and computation than our baselines. What if we simply use 2× more computation on baselines through an ensemble? We find that both '3D CNN' or 'STO' are not able to obtain a similar gain when ensembled; the LFB model works significantly better <ref type="table">(Table 1f)</ref>. Example Predictions. We qualitatively present a few examples illustrating the impact of LFB in <ref type="figure">Fig. 5</ref>. Specifically, we compare predictions made by models with LFB of different window sizes, from 4 seconds to 10 seconds. We see that when observing only short-term information, the model is confused and not able to make accurate predictions in these cases. When observing more context, e.g., zoomedin frames (example A, C) or frames that give clearer cue (example B, D), an LFB model is able to leverage the information and improve predictions.</p><p>Backbone and Testing. So far for simplicity, we have used a relatively small backbone of R50-I3D-NL and performed center-crop testing. In <ref type="table">Table 1g</ref>, we show that with an R101-I3D-NL backbone, LFB (3L) achieves 26.8 mAP, and with standard testing techniques, 27.7 mAP. We use short side ∈ {224, 256, 320} pixels for 3-scale testing.</p><p>Comparison to Prior Work. Finally we compare with other state-of-the-art methods <ref type="table">(Table 1h</ref>). For fair comparison, we follow Girdhar et al. <ref type="bibr" target="#b8">[9]</ref> and train on both the training and validation set for test set evaluation. 2 For this model we use a 1.5× longer schedule due to the larger data size.</p><p>Our model, using only RGB frames, significantly outperforms all prior work including strong competition winners that use optical flow and large ensembles. Our single model outperforms the best previous single-model entry by Girdhar et al. <ref type="bibr" target="#b8">[9]</ref> with a margin of 5.8 and 6.2 points mAP on validation and test set respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on EPIC-Kitchens</head><p>The long-term feature bank is a generalizable and flexible concept. We illustrate this point on two tasks in the EPIC-Kitchens dataset <ref type="bibr" target="#b5">[6]</ref> for which we store different types of information in the feature bank.</p><p>The EPIC-Kitchens dataset consists of videos of daily activities (mostly cooking) recorded in participants' native kitchen environments. Segments of each video are annotated with one verb (e.g., 'squeeze') and one noun (e.g., 'lemon'). The task is to predict the verb, noun, and the combination (termed action <ref type="bibr" target="#b5">[6]</ref>) in each segment. Performance is measured by top-1 and top-5 accuracy.</p><p>The dataset consists of 39,594 segments in 432 videos. Test set annotations are not released; for validation we split the original training set into a new training/validation split following Baradel et al. <ref type="bibr" target="#b3">[4]</ref>. We train independent models to recognize verbs and nouns, and combine their predictions for actions. For actions, we additionally use a prior based on the training class frequencies of verbs and nouns; see the Appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Long-Term Feature Banks. Recognizing which object a person is interacting with (the noun task) from a short segment is challenging, because the object is often occluded, blurred, or can even be out of the scene. Our LFB is wellsuited for addressing these issues, as the long-term supporting information can help resolve ambiguities. For example, if we know that the person took a lemon from the refrigerator 30 seconds ago, then cutting a lemon should be more likely. Based on this motivation, we construct an LFB that contains object-centric features. Specifically, we use Faster R-CNN to detect objects and extract object features using RoIAlign from the detector's feature maps (see the Appendix for details of our detector).</p><p>On the other hand, for recognizing verbs we use a video model to construct an LFB that captures motion patterns. Specifically, we use our baseline 3D CNN fine-tuned on EPIC-Kitchens verbs to extract clip-level features every 1 second of video. Our default setting uses a window size of 40 seconds for the verb model, and 12 seconds for the noun model, chosen on the validation set.</p><p>Adaptation to Segment-Level Tasks. To adapt our model for segment-level predictions, we replace the RoI pooling by global average pooling, resulting in S ∈ R 1×2048 . For the STO baseline, we use a slightly modified formulation: STO(S t ) := FBO NL (S t , S t ), where S t contains 16 spatially pooled features, each at a temporal location, and S t is S t pooled over the time axis. STO learns to interact with information at different time steps within the short clip. Training and inference procedures are analogous to our experiments on AVA; see the Appendix for details.  <ref type="table">Table 2</ref>. EPIC-Kitchens validation and test server results. Augmenting 3D CNN with LFB leads to significant improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Evaluation</head><p>We now quantitatively evaluate our LFB models. <ref type="table">Table 2</ref> shows that augmenting a 3D CNN with LFB significantly boosts the performance for all three tasks. Using object features for the noun model is particularly effective, leading to 5.7% (26. We also observe that FBO Max and FBO Avg outperform FBO NL on EPIC-Kitchens. We conjecture that this is due to the simpler setting: each video has only one person, doing one thing at a time, without the complicated person-person interactions of AVA. Thus a simpler function suffices.</p><p>On the test set, our method outperforms prior work by a large margin on both the 'seen kitchens (s1)' and 'unseen kitchens (s2)' settings. Our LFB model outperforms the Two-Stream <ref type="bibr" target="#b38">[39]</ref> TSN <ref type="bibr" target="#b49">[50]</ref> baseline by 50% relatively for s1, and almost doubles the performance for s2, in terms of top-1 action accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments on Charades</head><p>Finally we evaluate our approach on the Charades dataset <ref type="bibr" target="#b37">[38]</ref>. The Charades dataset contains 9,848 videos with an average length of 30 seconds. In each video, a person can perform one or more actions. The task is to recognize all the actions in the video without localization.  <ref type="bibr" target="#b51">[52]</ref>) R50-I3D-NL RGB 33.5 -I3D-NL <ref type="bibr" target="#b50">[51]</ref> R101-I3D-NL RGB 37.5 39.5 STRG <ref type="bibr" target="#b51">[52]</ref> R50-I3D-NL RGB 37.5 -STRG <ref type="bibr" target="#b51">[52]</ref> R101-I3D-NL RGB 39.7 -</p><formula xml:id="formula_5">3D CNN R50-I3D-NL RGB 38.3 - 3D CNN ens. R50-I3D-NL RGB 39.5 - STO R50-I3D-NL RGB 39.6 - STO ens. R50-I3D-NL RGB 40.0 - LFB Avg R50-I3D-NL RGB 38.4 - LFB Max R50-I3D-NL RGB 38.6 - LFB NL R50-I3D-NL RGB 40.3 - 3D CNN R101-I3D-NL RGB 40.3 40.8 3D CNN ens. R101-I3D-NL RGB 41.7 - STO R101-I3D-NL RGB 41.0 - STO ens. R101-I3D-NL RGB 42.3 - LFB Avg R101-I3D-NL RGB 40.8 - LFB Max R101-I3D-NL RGB 40.9 - LFB NL</formula><p>R101-I3D-NL RGB 42.5 43.4 <ref type="table">Table 4</ref>. Action recognition accuracy on Charades. (mAP in %)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Details</head><p>We use the RGB frames at 24 FPS provided by the dataset authors. We sample training and testing clips (32 frames) with a temporal stride of 4 following STRG <ref type="bibr" target="#b51">[52]</ref>, resulting in input clips spanning 125 frames (∼5.2 seconds). The LFB is sampled at 2 clips per second. We found a postactivation version of NL to work better on Charades, so we adopt it in the following experiments. Details and full results of both variants are in the Appendix. Other details are identical to the verb model for EPIC-Kitchens.</p><p>Training and Inference. We train 3D CNN models for 24k iterations with a learning rate of 0.02 and weight decay of 1.25e-5. Note that these hyperparameters are different from STRG <ref type="bibr" target="#b51">[52]</ref>, which uses longer schedule (50k iterations), smaller learning rate (0.0025), and larger weight decay (1e-4). <ref type="bibr" target="#b2">3</ref>  <ref type="table" target="#tab_5">Table 3</ref> compares the two settings, and we see that surprisingly our 2× shorter schedule works significantly better. With the new schedule, a simple NL model without proposals (STO) works as well as the full STRG <ref type="bibr" target="#b2">3</ref> The original STRG <ref type="bibr" target="#b51">[52]</ref> uses a batch size of 8. For clear comparison, we use the same batch size as ours <ref type="bibr" target="#b15">(16)</ref>, but adjust the learning rate and schedule according to 'Linear Scaling Rule' <ref type="bibr" target="#b12">[13]</ref>. We verified that the accuracy matches that of the original 4-GPU training. method (37.5% mAP) <ref type="bibr" target="#b51">[52]</ref>. We observe that the benefit of using the short-term operator becomes smaller when using a stronger baseline. In all following experiments we use our 24k schedule as default, and use a 2-stage training approach similar to STRG <ref type="bibr" target="#b51">[52]</ref> for training LFB models; see the Appendix for details. At test time, we sample 10 clips per video, and combine the predictions using max pooling following prior work <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>. We use (left, center, right) 3-crop testing following Wang et al. <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Quantitative Evaluation</head><p>For Charades, we experiment with both ResNet-50-I3D-NL and ResNet-101-I3D-NL <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b50">51]</ref> backbones for a consistent comparison to prior work. <ref type="table">Table 4</ref> shows that LFB models again consistently outperform all models without LFB, including prior state-of-the-art on both validation and test sets. The improvement on Charades is not as large as other datasets, in part due to the coarser prediction task (video-level). <ref type="figure" target="#fig_7">Fig. 6</ref> shows the relative gain of using LFB of different window sizes. <ref type="bibr" target="#b3">4</ref> We see that different datasets exhibit different characteristics. The movie dataset, AVA, benefits from very long context lasting 2+ minutes. To recognize cooking activities (EPIC-Kitchens), context spanning from 15 to 60 seconds is useful. Charades videos are much shorter (∼30 seconds), but still, extending the temporal support to 10+ seconds is beneficial. We conjecture that more challenging datasets in the future may benefit even more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>In conclusion, we propose a Long-Term Feature Bank that provides long-term supportive information to video models. We show that enabling video models with access to long-term information, through an LFB, leads to a large performance gain and yields state-of-the-art results on challenging datasets like AVA, EPIC-Kitchens, and Charades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Backbone Architecture</head><p>We use ResNet-50 I3D <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> with non-local blocks <ref type="bibr" target="#b50">[51]</ref> as the 'backbone' of our model. Following Wang et al. <ref type="bibr" target="#b51">[52]</ref>, the network only downsamples the temporal dimension by a factor of two. <ref type="table">Table 5</ref>   <ref type="table">Table 5</ref>. ResNet-50 NL-I3D <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b50">51]</ref> backbone used in this paper.</p><p>Here we assume input size 32×224×224 (frames×width×height). 'NL: i0, i1, . . . ' in stage 'resj' denotes additional non-local blocks <ref type="bibr" target="#b50">[51]</ref> after block i0, i1, . . . of resj. 3(1)×1×1 denotes that we either use a 3×1×1 or a 1×1×1 convolution. Specifically, we use 3×1×1 for block 0, 2 of res3, block 0, 2, 4 of res4, and block 1 of res5, and use 1×1×1 for the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. AVA Person Detector</head><p>We use Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> with a ResNeXt-101-FPN <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b54">55]</ref> backbone for person detection. The model is pre-trained on ImageNet for classification, and on COCO for keypoint detection. The model obtains 56.9 box AP and 67.0 keypoint AP on COCO keypoints. Model parameters are available in Detectron Model Zoo <ref type="bibr" target="#b10">[11]</ref>. We fine-tune the model on AVA bounding boxes from the training videos for 130k iterations with an initial learning rate of 0.005, which is decreased by a factor of 10 at iteration 100k and 120k. To improve generalization, we train with random scale jittering (from 512 to 800 pixels). The final model obtains 93.9 AP@50 on the AVA validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. LFB vs. Improving Backbones</head><p>A large body of recent research focuses on improving 3D CNN architectures, i.e., improving modeling of short-term patterns. This paper, on the other hand, aims at improving the modeling of long-term patterns. How do these two directions impact video understanding differently?</p><p>We plot the per-class impact of LFB in <ref type="figure">Fig. 7</ref>, the perclass impact of improving backbone in <ref type="figure">Fig. 8</ref>, and compare them in <ref type="figure">Fig. 9</ref>. The error bars are plus/minus one standard error around the mean, computed from 5 runs. We see that they lead to improvement in different action classes. Using LFB leads to improvement in many interactive actions, such as 'play musical instrument' or 'sing to', while improving backbone leads to improvement in more standalone actions, such as 'hand shake' or 'climb'. This suggests that improving long-term modeling (through LFB) and short-term modeling (through improving backbone) are complementary; we believe both are important for future video understanding research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. AVA STO Regularization</head><p>To address the overfitting issue of STO on AVA, we experimented with a number of regularization techniques, as summarized in <ref type="table" target="#tab_8">Table 6</ref>. We found that dropout <ref type="bibr" target="#b40">[41]</ref> was insufficient to regularize an STO, but injecting 'distractors', i.e., randomly sampled features, into the features being attended during training was very effective. We report STO results with 'distractor' training for AVA unless otherwise stated. For STO on other datasets, we did not observe obvious overfitting.   Adding LFB Improving backbone <ref type="figure">Figure 9</ref>. Adding LFB vs. improving backbone. We compare the absolute improvement (in AP) brought by LFB and the improvement brought by improving backbone. We see that they lead to improvement in different action classes. This suggests that improving long-term modeling (through LFB) and short-term modeling (through improving backbone) are complementary; we believe both are important for future video understanding research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix G. Object Detector for LFB of EPIC-Kitchens Noun Model</head><p>We use Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> with ResNeXt-101-FPN <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b54">55]</ref> backbone for the object detector. The detector is pretrained on Visual Genome <ref type="bibr" target="#b21">[22]</ref> with 1600 class labels defined in Anderson et al. <ref type="bibr" target="#b1">[2]</ref>. We fine-tune this model on the 'new' training split (defined in Baradel et al. <ref type="bibr" target="#b3">[4]</ref>) of EPIC-Kitchens for 90k iterations, with random scale jittering (from 512 to 800 pixels). We use an initial learning rate of 0.005, which is decreased by a factor of 10 at itera-tion 60k and 80k. The final model achieves 2.4 AP on the 'new' validation split. The AP is low because with the new train/val split, most of the classes are unseen during training. In addition, most of the classes have zero instance in the new, smaller validation set, and we calculate the average precision of those classes as 0. This is also not comparable to the performance reported in <ref type="bibr" target="#b5">[6]</ref>, where the model is trained on the full training set, and evaluated on the unreleased test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix H. Charades Training Schedule</head><p>We train the models to predict the 'clip-level' labels, i.e., the union of the frame labels that fall into the clip's temporal range. We train the baseline 3D CNNs with the default 24k schedule with learning rate 0.02, which is decreased by a factor of 10 at iteration 20k. To train LFB models, we use a 2-stage approach following STRG <ref type="bibr" target="#b51">[52]</ref>. We first train the model without the FBO using the 24k schedule, and then add FBO, freeze backbone, and train for half of the schedule (12k iterations, so 36k in total). This schedule helps prevent overfitting that was observed when training directly with FBO in one stage. For training STO, we observed worse performance with this 2-stage approach, so we report the STO performance using the default 24k schedule.  <ref type="table">Table 7</ref>. Pre-activation vs. post-activation NL on Charades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix I. Charades NL Block Details</head><p>We experimented with two variants of NL design: a pre-activation (the default variant described in paper), and a post-activation variant, where we move ReLU after the skip connection, and move the layer normalization after the output linear layer. For both variants, LFB consistently outperforms STO and baseline 3D CNN <ref type="table">(Table 7)</ref>. We choose post-activation as default for Charades due to the stronger performance. For AVA and EPIC-Kitchens, we did not observe any noticeable difference between the two variants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2 o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2 o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2 o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2 o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; ⇥ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2 o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; ⇥ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2 o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2 o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2 o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 9 3 y M f R F E 0 u 2 D G 3 M O f Q / 9 v 7 c R r Q = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j w 4 j G C e U C y h N n J b D J m d m a Z 6 R X C k n / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 5 b V m e G 8 S b T U p t O R C 2 X Q v E m C p S 8 k x p O k 0 j y d j S + n f n t J 2 6 s 0 O o B J y k P E z p U I h a M o p N a P R Q J t / 1 K 1 a / 5 c 5 B V E h S k C g U a / c p X b 6 B Z l n C F T F J r u 4 G f Y p h T g 4 J J P i 3 3 M s t T y s Z 0 y L u O K u q W h P n 8 2 i k 5 d 8 q A x N q 4 U k j m 6 u + J n C b W T p L I d S Y U R 3 b Z m 4 n / e d 0 M 4 5 s w F y r N k C u 2 W B R n k q A m s 9 f J Q B j O U E 4 c o c w I d y t h I 2 o o Q x d Q 2 Y U Q L L + 8 S l q X t c C v B f d X 1 X q j i K M E p 3 A G F x D A N d T h D h r Q B A a P 8 A y v 8 O Z p 7 8 V 7 9 z 4 W r W t e M X M C f + B 9 / g C 6 Y Y 9 C &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Our modified non-local block design. Here we plot the first layer S (1) t = NL θ 1 (St,Lt) as an example. '⊗' denotes matrix multiplication, and '⊕' denotes element-wise sum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 → 31 . 8 )</head><label>1318</label><figDesc>absolute improvement over our strong baseline model. On verb recognition, LFB with 3D CNN features results in 3.2% (49.8 → 53.0) improvement and outperforms previous state-of-the-art by Baradel et al. [4] by 12.1% (40.9 → 53.0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Relative improvement of LFB models with different window sizes over vanilla 3D CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Impact of LFB. We compare per-class AP of 3D CNN (22.1 mAP) and LFB model (25.5 mAP) on AVA. LFB leads to larger improvement on interactive actions, e.g., 'sing to', 'play musical instrument', or 'work on a computer'. (Bold: 5 classes with the largest absolute gain. Blue: 5 classes with the largest relative gain. Red: classes with decreased performance. Classes are sorted by frequency. ) s ( a p e r s o n ) ta k e o b je c t f r o m p e r s o n h a n d s h a k e s a il b o a t p u t d o w n li f t/ p ic k u p te x t o n /l o o k a t a c e ll p h o n e li f t ( a p e r s o n ) Impact of improving backbone. We compare per-class AP of 3D CNN with the default backbone (R50-I3D-NL; 22.1 mAP) and a stronger backbone (R101-I3D-NL; 23.0 mAP) on AVA. Improving backbone leads to larger improvement in standalone actions, such as 'crouch/kneel', 'read', or 'hand shake'. (Bold: 5 classes with the largest absolute gain. Blue: 5 classes with the largest relative gain. Red: classes with decreased performance. ) o n /l o o k a t a c e ll p h o n e li f t ( a p e r s o n )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>22.2 22.3 20.0 19.7 17.5 15.7 STO 23.2 23.6 23.3 21.5 20.9 18.5 16.9 LFB (causal) -24.0 24.3 24.6 24.8 24.6 24.2 LFB (batch) -24.2 24.7 25.2 25.3 25.3 25.5</figDesc><table><row><cell>Support (sec.)</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>30</cell><cell>60</cell><cell>mAP</cell><cell></cell><cell>mAP</cell><cell>mAP</cell></row><row><cell>3D CNN</cell><cell cols="5">22.1 (a) Temporal support (mAP in %)</cell><cell></cell><cell></cell><cell>K400 feat. AVA feat. LFB (b) Feature decoupling 19.7 24.3 25.5</cell><cell cols="2">Avg pool Max pool NL (c) LFB operator 23.1 23.2 25.5</cell><cell>Global pool 2 × 2 Grid 4 × 4 Grid Detection (d) LFB spatial design 24.9 25.1 25.1 25.5</cell></row><row><cell></cell><cell></cell><cell>mAP</cell><cell></cell><cell></cell><cell cols="3">params FLOPs mAP</cell><cell></cell><cell>mAP</cell><cell>model</cell><cell>flow</cell><cell>val</cell><cell>test</cell></row><row><cell>1L</cell><cell></cell><cell>25.1</cell><cell>3D CNN</cell><cell></cell><cell>1×</cell><cell>1×</cell><cell>22.1</cell><cell>R50-I3D-NL</cell><cell></cell><cell>AVA [14]</cell><cell>15.6</cell><cell>-</cell></row><row><cell>2L (default)</cell><cell></cell><cell>25.5</cell><cell cols="2">3D CNN ×2</cell><cell>2×</cell><cell>2×</cell><cell>22.9</cell><cell>center-crop (default)</cell><cell>25.8</cell><cell>ACRN [43]</cell><cell>17.4</cell><cell>-</cell></row><row><cell>2L w/o scale</cell><cell></cell><cell>25.2</cell><cell>STO</cell><cell></cell><cell cols="3">1.00× 1.12× 23.2</cell><cell>R101-I3D-NL</cell><cell></cell><cell>RTPR [24]</cell><cell>22.3</cell><cell>-</cell></row><row><cell>2L w/o LN</cell><cell></cell><cell>23.9</cell><cell>STO ×2</cell><cell></cell><cell cols="3">2.00× 2.24× 24.1</cell><cell>center-crop (default)</cell><cell>26.8</cell><cell>9-model ens. [19]</cell><cell>25.6</cell><cell>21.1</cell></row><row><cell>2L w/o dropout</cell><cell></cell><cell>25.4</cell><cell cols="2">LFB (2L)</cell><cell cols="3">2.00× 2.12× 25.5</cell><cell>3-crop</cell><cell>27.1</cell><cell>R50-I3D-NL [19]</cell><cell>19.3</cell><cell>-</cell></row><row><cell cols="2">2L (dot product)</cell><cell>25.5</cell><cell cols="2">LFB (3L)</cell><cell cols="3">2.00× 2.15× 25.8</cell><cell>3-crop+flips</cell><cell>27.4</cell><cell>RTPR [24]</cell><cell>20.5</cell><cell>-</cell></row><row><cell>2L (concat)</cell><cell></cell><cell>25.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3-crop+flips+3-scale</cell><cell>27.7</cell><cell>Girdhar et al. [9]</cell><cell>21.9</cell><cell>21.0</cell></row><row><cell>3L</cell><cell></cell><cell>25.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LFB (R50)</cell><cell>25.8</cell><cell>24.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LFB (best R101)</cell><cell>27.7</cell><cell>27.2</cell></row><row><cell cols="3">(e) LFB NL design</cell><cell cols="4">(f) Model complexity</cell><cell></cell><cell cols="2">(g) Backbone &amp; testing</cell><cell cols="2">(h) Comparison to prior work</cell></row></table><note>Table 1. AVA ablations and test results. STO: 3D CNN with a non-local (NL) short-term operator; LFB: 3D CNN with a long-term feature bank; the LFB operator is a two-layer (2L) NL block by default. We perform ablations on the AVA spatio-temporal action localization. The results validate that longer-term information is beneficial, that the improvement is larger than what would be observed by ensembling, and demonstrate various design choices. Finally, we show state-of-the-art results on the AVA test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Training schedule on Charades. Our 2× shorter schedule works significantly better than the schedule used in STRG<ref type="bibr" target="#b51">[52]</ref>.</figDesc><table><row><cell>iterations / lr / wd</cell><cell cols="2">50k / 0.0025 / 1e-4 [52]</cell><cell cols="3">24k / 0.02 / 1.25e-5</cell></row><row><cell>3D CNN</cell><cell></cell><cell>33.8</cell><cell></cell><cell></cell><cell>38.3</cell></row><row><cell>STO</cell><cell></cell><cell>37.8</cell><cell></cell><cell></cell><cell>39.6</cell></row><row><cell></cell><cell>backbone</cell><cell>modality</cell><cell></cell><cell>train /</cell><cell>trainval</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>val</cell><cell>/ test</cell></row><row><cell cols="2">2-Strm. [39] (from [37]) VGG16</cell><cell cols="2">RGB+Flow</cell><cell>18.6</cell><cell>-</cell></row><row><cell>Asyn-TF [37]</cell><cell>VGG16</cell><cell cols="2">RGB+Flow</cell><cell>22.4</cell><cell>-</cell></row><row><cell>CoViAR [54]</cell><cell>R50</cell><cell cols="3">Compressed 21.9</cell><cell>-</cell></row><row><cell>MultiScale TRN [58]</cell><cell>Inception</cell><cell>RGB</cell><cell></cell><cell>25.2</cell><cell>-</cell></row><row><cell>I3D [5]</cell><cell cols="2">Inception-I3D RGB</cell><cell></cell><cell>32.9</cell><cell>34.4</cell></row><row><cell>I3D [5] (from [51])</cell><cell>R101-I3D</cell><cell>RGB</cell><cell></cell><cell>35.5</cell><cell>37.2</cell></row><row><cell>I3D-NL [51] (from</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>STO with different regularization techniques on AVA (mAP in %).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Test set performance evaluated by ActivityNet server.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For each dataset, we use its best-performing FBO. Standard error is calculated based on 5 runs. The temporal support here considers the support of each clip used for computing L, so Charades's support starts at a higher value due to its larger 3D CNN clip size (∼5.2 seconds).</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Training Schedule for EPIC-Kitchens</head><p>We train the verb models for 36k iterations with 10 −5 weight decay and a learning rate of 0.0003, which is then decreased by 10 times at iteration 28k and 32k. For the noun models, we train for 50k iterations with weight decay 10 −6 and a learning rate of 0.001, which is decreased by 10 times at iteration 40k and 45k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix F. EPIC-Kitchens Inference</head><p>We sample training clips such that the center of the clip falls within a training segment. For testing, we sample one center clip per segment, resize such that the short side is 256 pixels, and use a single center crop of 256×256. We compute the probability of an action as the product of the softmax scores, weighted by a prior µ, i.e. P (action = (v, n)) ∝ µ(v, n)P (verb = v)P (noun = n), where µ is a prior estimated as the count of (v, n) pair divided by count of n in training annotations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detection bank: an object detection based video representation for multimedia event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object level visual reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The EPIC-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10066</idno>
		<title level="m">A better baseline for AVA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tube convolutional neural network (T-CNN) for action detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human centric spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ActivityNet workshop, CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action tubelet detector for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning process in an asymmetric threshold network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Disordered systems and biological organization</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent tubelet proposal and recognition networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Temporal modeling approaches for large-scale youtube-8m video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04555</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attend and interact: Higher-order object interactions for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AMTnet: Action-microtube regression by end-to-end trainable deep architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Online real-time multiple spatiotemporal action localisation and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-toend memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lattice long short-term memory for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Non-local netvlad encoding for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00207</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<title level="m">Convnet architecture search for spatiotemporal feature learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
