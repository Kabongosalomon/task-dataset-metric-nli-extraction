<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WDRN : A Wavelet Decomposed RelightNet for Image Relighting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Densen</forename><surname>Puthussery</surname></persName>
							<email>puthusserydenson@</email>
							<affiliation key="aff0">
								<orgName type="department">College of Engineering</orgName>
								<address>
									<settlement>Trivandrum</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><forename type="middle">P S</forename></persName>
							<email>hrishikeshps@</email>
							<affiliation key="aff0">
								<orgName type="department">College of Engineering</orgName>
								<address>
									<settlement>Trivandrum</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Kuriakose</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Engineering</orgName>
								<address>
									<settlement>Trivandrum</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><forename type="middle">C V</forename></persName>
							<email>jijicv@cet.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">College of Engineering</orgName>
								<address>
									<settlement>Trivandrum</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WDRN : A Wavelet Decomposed RelightNet for Image Relighting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gray Loss</term>
					<term>Illumination</term>
					<term>Relighting</term>
					<term>Wavelet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of recalibrating the illumination settings in an image to a target configuration is known as relighting. Relighting techniques have potential applications in digital photography, gaming industry and in augmented reality. In this paper, we address the one-to-one relighting problem where an image at a target illumination settings is predicted given an input image with specific illumination conditions. To this end, we propose a wavelet decomposed RelightNet called WDRN which is a novel encoder-decoder network employing wavelet based decomposition followed by convolution layers under a muti-resolution framework. We also propose a novel loss function called gray loss that ensures efficient learning of gradient in illumination along different directions of the ground truth image giving rise to visually superior relit images. The proposed solution won the first position in the relighting challenge event in advances in image manipulation (AIM) 2020 workshop which proves its effectiveness measured in terms of a Mean Perceptual Score which in turn is measured using SSIM and a Learned Perceptual Image Patch Similarity score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of recalibrating the illumination settings of an acquired image is widely known as image relighting. Relighting is an emerging technology owing to its applications in augmented reality (AR) and also in casual digital photography. Relighting enabled AR can bring about great changes in the way one perceives digital experiences like online shopping, online teaching, etc. For example, one may wish to visualize whether furniture to be purchased online is suitable for the room. Since the ambient lighting conditions like the direction of illumination, brightness, color temperature etc. may vary from user to user, adaptability to the same is required in the AR visualization tool. Such adaptability can be realized by integrating relighting techniques in the online platform using the AR tool. In first and third person gaming, the ambient lighting of a scene is highly dynamic and changes with time of the day, viewpoint of the avatar etc. There is a scope for using relighting techniques to quickly render the scene graphics to drive the gameplay with higher number of frames per second. <ref type="figure" target="#fig_1">Fig. 1</ref> shows a simple case of relighting where the appearance of the scene is changed drastically when the illuminant is positioned at different azimuthal angles. In digital photography, relighting techniques are used to enhance the perceptual quality of an image. In natural images of outdoor scenes, it is often difficult to control the illumination. Diverse factors affect natural illuminance like time of the day, weather, clouds, objects in the vicinity etc. Due to these and other factors, it is common that outdoor images are poorly lit. Many modern cameras offer the flexibility to control the image lighting by adjusting the shutter speed, aperture, ISO sensitivity etc. However, such tweaks usually require professional expertise and are prone to degradation like blur, grains etc.</p><p>The outdoor images usually have uniform illumination in daytime as sunlight is far-field and heavily scattered. However, indoor images usually have a nonuniform illumination as the objects close to the light source are considerably lit in contrast to the ones far away. The location, directionality and properties of illuminant dictate the appearance of natural indoor images. The formation of shadows and general gradient of illumination are controlled by the location of illuminants in the room.</p><p>Additionally, the nature of object shadow like its size, position etc. varies with the location of the light source. Similarly, the illumination pattern produced by a directional source is quite different from that of an omni-directional light source. Besides, the properties of illuminant like its color temperature, spectral power distribution etc. affect the visual quality of an indoor image. The above factors hold true even for outdoor images when natural lighting is not present. Although indoor photography is flexible, as we have a control on these factors, it is only feasible on a professional scale, like in a digital studio. In a home environment and in casual photography, the location and type of illuminant are mostly fixed and there is little control on these aspects. Relighting finds its applications in areas like these where one would like to change the illumination setting of an image without putting much physical effort or using specialized tools.</p><p>Another area of image manipulation which necessitates relighting is digital image montaging. In image montaging, a certain portion of an image is replaced with a crop taken from a different image. Multiple images can also be fused in a similar manner to generate a montage. For seamless and visually appealing results in a montage, the illumination in images being combined must be same. Since the images used for montaging are usually unrelated, their illumination setting could be different. In this scenario, relighting techniques can be employed to translate the images into the final illumination setting and then apply montaging for superior results.</p><p>In this work, we address a special case of the relighting problem where we describe the solution we proposed as part of the challenge event on one-to-one relighting in Advances in Image Manipulation(AIM) 2020 workshop <ref type="bibr" target="#b6">[6]</ref>. The task of the challenge was to develop a solution that recalibrates the illumination setting of an input scene to a given target setting. To this end, a deep convolutional neural network (CNN) that efficiently learns the illumination setting of the target domain is proposed. The contributions of the proposed work are :</p><p>• A wavelet decomposed encoder-decoder network to solve the relighting problem that can effectively translate an image from a source illumination setting to a target setting. • A novel training loss term called gray loss that drives the network to learn the illumination gradient in target domain images. • Introduced pixel shuffler operations in wavelet based encoder-decoder network for fast training and inference.</p><p>Rest of the paper is organized as follows : In Section 2 we review related works and in Section 3 we describe the proposed methodology. Section 4 details our experiments, Section 5 presents our result analysis and in Section 6 we describe our ablation studies. Finally, Section 7 concludes the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Here we first review the image enhancement techniques, both using conventional and deep learning approaches where the enhanced image is obtained through some form of illumination adjustment. Further, we discuss some of the recently proposed relighting techniques using deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Enhancement</head><p>Conventional Methods Smartphones and casual photography using these devices have brought an increased demand for methods based on various image manipulation techniques like photo enhancement. Image enhancement is one of the fundamental problems in the field of computer vision starting with methods like histogram equalisation for contrast enhancement. Retinex theory of color vision <ref type="bibr" target="#b10">[10]</ref> by Edwin H Land inspired many methods like <ref type="bibr" target="#b7">[7]</ref>[19] that considers images as the pixel-wise product of reflectance and illumination. These works treat image enhancement problem as an illumination estimation problem, where the illumination component is used to enhance the input images. These works were only able to generate very inferior results because of the high non-linearity across the channels and the spatial sensitivity of colour in the image.</p><p>Deep learning based Methods Most of the recent works on photo enhancement is learning based and the first dataset used for the purpose was MIT-Adobe FiveK <ref type="bibr" target="#b0">[1]</ref> introduced by Bychkovsky et al. The dataset contains five sets of 5000 input-output pairs. Each set is a retouched (using Adobe Lightroom) version of the same input image by different professionals. The work was used to address general tone adjustment rather than enhancing an underexposed image. Lore et al. <ref type="bibr" target="#b12">[12]</ref> proposed an auto-encoder architecture for denoising and brightening the low-light images. Many Generative Adversarial Network (GAN) based networks were also developed for image enhancement. Chen et al. <ref type="bibr" target="#b1">[2]</ref> proposed a method that uses a two-way GAN architecture. The network transforms the input image to an enhanced image with characteristics of a reference image. Ignatov et al. <ref type="bibr" target="#b8">[8]</ref>, proposed weakly supervised (no exact image pair) GAN based network that enhances images that are taken using mobile phones to DSLR quality images. The network used DPED dataset <ref type="bibr" target="#b9">[9]</ref> along with many other unpaired HD images.</p><p>Wang proposed a learning based method <ref type="bibr" target="#b16">[16]</ref> that enhanced under exposed images using end-to-end CNN based model. The network used an encoderdecoder architecture, where the encoder was used to extract the local features like contrast, detail sharpness, shadow, highlight etc and global features such as color distribution, average brightness and scene category. For driving the network to learn illumination mapping from under-exposed to the enhanced images, they use three loss functions, smoothness loss on the illumination and color and reconstruction loss on the enhanced image. The network was trained on a novel dataset with 3000 under-exposed images and its ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Relighting</head><p>Most of the above mentioned methods cannot remove or change the illumination setting of an input image; it can only modify the effects of the existing illumination. When it comes to image relighting rather than the overall enhancement, the work focuses on predicting a target illumination setting (light direction and colour temperature) from an input with a different illumination setting.</p><p>One-to-one relighting can be considered as a special case of image relighting, where the task is to manipulate an input image that was captured under certain illumination settings (light source position, direction and color temperature) to make it look like it was taken under different settings.</p><p>Debevec et al. proposed a technique <ref type="bibr" target="#b2">[3]</ref> for rendering the human face images from varying viewpoints and direction of illumination, similar to the problem addressed in our work. Here, they collected images of human face from different viewpoints under diverse direction of illumination. A reflectance function of the skin was modeled to estimate the image when the target viewpoint is different from the input. Their network was able to give considerable performance but it required hundreds of images with the stationary subject under a controlled illumination setting. Hence, they were unable to provide a solution for single RGB image for an unknown object in an unconstrained environment as in oneto-one relighting problem.</p><p>Xu et al. proposed a CNN based method <ref type="bibr" target="#b18">[18]</ref> to relight a scene under a new illumination based on five images captured under pre-defined illumination setting. Unlike exploiting similarity in a single light transport function as in <ref type="bibr" target="#b2">[3]</ref> they try to estimate a non-linear function that generalises the estimation of the above mentioned problem using deep learning based training. Along with three channels(RGB) of the five fused images, they also add two extra channels along with the image, which are 2D coordinates of the light source direction. This method still requires five sparse samples of the same scene in order to predict the scene from a novel light setting.</p><p>Indirectly addressing this problem, Sun et al. proposed a CNN based approach <ref type="bibr" target="#b15">[15]</ref> to relight portrait images that were taken on mobile cameras into user defined illumination setting. They also used a encoder-decoder architecture where the input illumination is predicted and the required illumination of the target is injected at the bottleneck layer between the encoder and the decoder. The work was able to develop a function that can predict diverse illumination but their work was limited to portrait images of human faces.</p><p>The proposed method uses wavelet based end-to-end CNN architecture inspired from Multi-level Wavelet-CNN (MWCNN) <ref type="bibr" target="#b11">[11]</ref> by Liu et al. to learn a mapping function that relights a scene without modeling for the geometry or the reflectance. MWCNN is a fully convolutional encoder-decoder network that was proposed as a general methodology for image restoration. The winners of NTIRE 2020 Challenge on image demoireing <ref type="bibr" target="#b20">[20]</ref> used a method inspired from MWCNN which shows its competence. In this work, to relight an input image to a given target illumination settings, we propose a deep convolutional network using wavelet decomposition followed by convolution layers at various scales utilizing novel loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Under the assumption of a distant illumination source, the scene to be relit under a target direction can be formulated from the light transport function λ(i, θ) and the incident source illuminations from direction θ as :</p><formula xml:id="formula_0">Y i = λ(i, θ)I(θ)dθ<label>(1)</label></formula><p>where I(θ) is the radiance of the incident illumination from direction θ and i is the target image pixel. In a fundamental relighting problem, given multiple images of the scene acquired under varying θ, λ(i, θ) can be estimated and then image corresponding to a new value of θ can be rendered <ref type="bibr" target="#b18">[18]</ref>.</p><p>The problem that we discuss in this paper is slightly different, where we describe the solution proposed as part of the one-to-one relighting challenge at Advances in Image Manipulation (AIM) 2020 workshop <ref type="bibr" target="#b6">[6]</ref>. The task of the challenge was to develop a solution that recalibrates the illumination setting of an input scene to a target setting. The illumination setting in the challenge refers to two aspects -position and color temperature of the light source. Thus for the given problem, the input image is characterized by a fixed light source direction θ 1 and a fixed color temperature T 1 while the target image is characterized by a different direction θ 2 and color temperature T 2 . We employ a deep convolutional neural network (CNN) to learn the complex function F (θ 1 , θ 2 , T 1 , T 2 ) which can render the given scene into the new settings θ 2 , T 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Wavelet Decomposed RelightNet (WDRN)</head><p>Overview The proposed method uses a multi-level encoder-decoder based network that processes the image at different spatial resolutions. The encoder section is used to extract the local features like contrast, sharpness, shadow and global features such as color distribution, brightness, and semantic information. The encoder learns the illumination mapping of the input based on the extracted features. The decoder reconstructs the relit images from the encoder output by progressively upsampling the feature maps to the resolution of the target image.</p><p>Also, feature information from the encoder level is forwarded into the decoder level that operates at the same spatial resolution. The information in the decoder is the image context and the forwarded information is the local and global features. By fusing local and contextual information, the target illumination setting is injected into the input image within the decoder. The detailed description of the encoder and decoder sub-net is given in the following sub-sections.</p><p>The network is termed Wavelet Decomposed RelightNet (WDRN) because it employs wavelet decomposition to process the image at different scales within the encoder-decoder architecture. WDRN is inspired by the work of multi-level wavelet CNN (MWCNN) for image restoration proposed by Liu et al. <ref type="bibr" target="#b11">[11]</ref>. The ability of wavelet transform to obtain a large receptive field without information loss or gridding effect was shown in their work. <ref type="figure" target="#fig_2">Fig. 2</ref> depicts the proposed WDRN architecture for relighting.</p><p>Encoder sub-net The encoder sub-net operates at three different image scales or spatial resolutions and hence is a multi-level network. In each level, the input feature map is decomposed into four sub-bands using discrete wavelet transform (DWT) based subsampling followed by a convolution block for feature extraction. Here, 2D Haar wavelet has been used for decomposing the input to its sub-bands since it is the simplest of its kind. Other types of wavelets can also be employed  <ref type="figure">Fig. 3</ref>. Convolution blocks are used for feature extraction at each level and the block is expanded in <ref type="figure" target="#fig_3">Fig. 4</ref> Fig <ref type="figure">. 3</ref>. The operation of a simple DWT-IDWT based encoder-decoder network. Here the input is subsampled and divided into 4 sub-bands and these sub-bands are further processed using convolution block. The same step is repeated in the subsequent levels of the encoder. In the decoder, based on these feature maps information the relit image is reconstructed into required spatial resolution. here on a trial and error basis as it is difficult to theoretically prove the most suitable wavelet for this operation. The main advantage of this processing step is that a high receptive field is obtained in the network, similar to that with a dilated convolution. There are no trainable parameters in this decomposition step unlike in a subsampling convolution. The network will also benefit from frequency and spatial localization capabilities of the wavelet transformation. <ref type="figure">Fig. 3</ref> depicts the operation of a simple DWT-IDWT based encoder-decoder network. Additionally, in the first encoder level, a space-to-depth transform with pixel shuffler is applied on the decomposed sub-bands to generate a feature map of quarter the area of input and four times the number of channels. There are no trainable parameters in this downscaling operation, rather, it is a simple rearrangement of the subpixels. One can now perform subsequent processing of the original features at a smaller resolution which makes the overall network computationally efficient.</p><p>The convolution block in each encoder level is a series of convolutional layers followed by ReLU activation as depicted in <ref type="figure" target="#fig_3">Fig. 4</ref>. The convolution block is used for feature extraction from the input to the block. It learns the local features of the image like contrast, sharpness, shadow etc. and global features such as color distribution, brightness, and semantic information. The number of convolutional layers and filters are different for each level of the encoder. In level one, there are four convolution layer with 16 filters in each layer. Similarly, in level two there are four convolution layers with 6 filters. In level three there are seven convolution layers each with 256 filters. In the contraction path of the encoder, the filter size is progressively increased to obtain a rich representation of lower scale features.</p><p>Decoder sub-net Similar to the encoder, the expansion path sub-net or decoder is also multi-level and has three different scales of operation. Each level in the decoder is constituted by an inverse discrete wavelet transform (IDWT) based interpolation followed by a convolution block for feature aggregation. The feature output of third level of encoder is the input to the first level in the decoder. In each level, the input features are assumed to be four sub-bands of a wavelet decomposition. With this assumption, the IDWT is computed on the input feature set to interpolate the features to twice their spatial resolution and a quarter of the total input channels. The features in the expansion path represent the contextual information of the image. Since the local and global features are present in different encoder level outputs, these features can be carried forward to the decoder. This is achieved by directly adding the interpolated decoder features with the encoder level outputs with the same spatial resolution in an element-wise manner. This output feature set is then processed through a convolutional block to gradually inject the target domain illumination setting.</p><p>Similar to encoder, the convolution block in a decoder level is constituted by convolutional layers followed by ReLU activations. Convolution block in Level one of the decoder has four convolution layers of 64 filters each. In level two, there are four convolution layers of 16 filters each. At the output of the second level, a depth-to-space transform with pixel shuffler is employed which serves as the inverse operation of space-to-depth at the input. The third level is constituted only of a single IDWT operation. The last IDWT interpolation generates a three channel feature map which is added to the input image to generate the relit image of target illumination settings. As the last IDWT operation should produce a 3-channel image, the total number of channels at the input of level three IDWT should be 12. Since the output of depth-to-space transform has 16 channels, a convolution layer with 12 filters is placed after it to adjust the depth in subsequent layers.</p><p>In general, for efficient illumination recalibration, the network should be able to establish the relationship between distant pixels. This can be realised by using highly dilated convolutions. But for large dilation factors, two adjacent pixels in the predicted feature map are calculated from the completely non-overlapping input feature set and hence leads to spatial information leakage and poor localization in the encoder levels. The proposed WDRN can achieve a high receptive field without this information loss. Moreover, in contrast with MWCNN, the training losses used in WDRN is tailored for the relighting problem. The details of the novel gray loss that we propose for perceptually superior results in relight problem and other losses that we used are detailed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Functions</head><p>Network is trained based on three empirically weighted loss functions as shown in equation 2.</p><formula xml:id="formula_1">L total = αL M AE + βL SSIM + γL gray (2)</formula><p>The MAE loss is the mean absolute error or the L 1 distance between the groundtruth and the predicted images. It is incorporated to generate high fidelity relit images. Mean squared error (MSE) loss was avoided because of the smoothening effect it introduced in our generated images. The MAE is given by :</p><formula xml:id="formula_2">L M AE = 1 W × H × C W −1 i=0 H−1 j=0 C−1 k=0 Y i,j,k −Ŷ i,j,k<label>(3)</label></formula><p>where, W , H and C are the width, height and number of channels of the output, Y is the ground truth image andŶ is the predicted image. The structural similarity (SSIM) <ref type="bibr" target="#b17">[17]</ref> between two images is a measure of their perceptual difference as SSIM incorporates contrast and luminance masking. A high dynamic range can reveal more details in both poorly and heavily lit regions in an image. Optimising for SSIM loss helps the network to render visually appealing images with better dynamic range. SSIM loss is formulated as :</p><formula xml:id="formula_3">L SSIM = 1 − SSIM (Y,Ŷ )<label>(4)</label></formula><p>Gray Loss In relighting problems where the objective is to change the general direction of lighting, the network should be able to recalibrate the gradients in illumination within the image. The objects closer to the target illuminant position should be heavily lit while the ones far away should be poorly lit. The MAE loss and SSIM loss can optimize for the general texture of the image, but not the gradients in illumination. Hence the enhancements like shadow recasting are poorly learned. A novel loss term called gray loss is hence proposed that can overcome these limitations. The proposed gray loss is the L 1 distance between blurred versions of the grayscale components of the relit and ground truth images. The texture details are smoothened out when the images are blurred, leaving behind the illumination information. Since much details are not present in the blurred image, this information is constituted by the general direction of illumination gradients. Thus gray loss ensures that the gradient in illumination along different directions of the ground truth image is learned by the network and generate visually superior results. Gray loss is formulated as :</p><formula xml:id="formula_4">L gray = 1 W × H W −1 i=0 H−1 j=0 (ψ(Y )) i,j − (ψ(Ŷ )) i,j<label>(5)</label></formula><p>where ψ(.) is the Gaussian blur function used to smoothen the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The dataset used in the experiments is the Virtual Image Dataset for Illumination Transfer (VIDIT) <ref type="bibr" target="#b5">[5]</ref>. The dataset contains 390 different scenes which is captured at 40 different illumination settings (8 azimuthal angles and five different colour temperatures 2500K, 4500K etc.) with a total of 15, 600 images. We participated in track 1 -one-to-one relighting in AIM 2020 challenge for Scene Relighting and Illumination Estimation. For the experiments as part of the challenge, we used 390 image pairs from the dataset, where the input image has a fixed illumination setting θ 1 =North, T 1 = 6500K and the target is set at a different illumination setting θ 2 = East, T 2 = 4500K. All the training images are of fixed size 1024 × 1024 × 3. Out of the 390 image pairs, 300 image pairs were used for training, 45 for validation and 45 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>The network was trained on mini-batches of size 10. The model was trained for 150 epochs and employed Adam optimiser with β 1 = 0.9 and β 2 = 0.99. The initial learning rate was 1e −4 which was then decayed by a factor of 0.5 after every 100 epochs. The training was done on 1× Tesla P100 GPU card with 16 GiB memory. The proposed network has 6.4 million trainable parameters. Training process took 2 hours and testing time per image was 0.03 seconds. The various training accuracy and loss plots are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. It can be inferred from the accuracy plot that the network overfits at around 60 epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>In addition to the standard evaluation metrics like peak signal to noise ratio (PSNR) and SSIM, the performance of the proposed WDRN is evaluated using rather new perceptual metrics like Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b21">[21]</ref> and mean perceptual score (MPS). Mean Perceptual Score (MPS) <ref type="bibr" target="#b6">[6]</ref> is the average of the normalized SSIM <ref type="bibr" target="#b17">[17]</ref> and LPIPS score as shown in Eq.6</p><formula xml:id="formula_5">M P S = 0.5(S + (1 − L))<label>(6)</label></formula><p>where S is the average SSIM score on the test set, and L is the average LPIPS score on the test set. <ref type="figure">Fig. 6</ref> shows relit examples corresponding to four input images from VIDIT validation set for one-to-one relighting problem using the proposed WDRN trained with and without gray loss. For certain cases, the input images have better PSNR and/or SSIM than the relit images although the latter is perceptually closer to the ground truth. As evident from the figure, the position of the illuminant θ 2 and the color temperature T 2 of the target image is predicted with considerable visual similarity with the WDRN architecture. However, WDRN failed to inpaint information in the shadows that should have been uncovered with the change in illuminant position. Similarly, WDRN failed to recast the shadows in the target domain image. These two issues can be assumed to be the biggest challenges in relighting problem. Although WDRN trained with and without gray loss have comparable performance in terms of quantitative metrics, they differ to a certain extent visually. While WDRN with gray loss inpainted some information in shadowy areas in rows 1 and 4 of the figure, WDRN without gray loss failed to do so. Similarly, WDRN without gray loss inpainted information in unwanted regions in rows 2 and 4 of the figure, thereby having an incorrect representation of the target domain image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Result Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(d) Ground truth</head><p>(.) <ref type="figure">Fig. 6</ref>. Sample results with the proposed method on the validation set of VIDIT. From the visual inspection of <ref type="figure">Fig. a, b, c and d</ref>, it is evident that the proposed WDRN architecture is able to capture the colour temperature of the target domain to a large extent. Although WDRN trained with and without gray loss have comparable performance in terms of quantitative metrics, the former has superior visual quality.  <ref type="table" target="#tab_1">Table 1</ref> shows the performance comparison of the proposed WDRN with other competing entries in one-to-one relighting challenge of AIM 2020 workshop. We proposed two variants of WDRN in the competition -one trained with gray loss and the other without it. Both variants were able to achieve better MPS than other methodologies. While the method with gray loss obtained the highest MPS of 0.6452, the one without it obtained highest score in SSIM and 2 nd second highest MPS score of 0.6451. Organizers merged both the methods as the architecture followed was same even though the loss functions used were different. Additionally, owing to the network lightness, runtime of WDRN is considerably lower than the immediate runner-ups.</p><p>As future work, the proposed WDRN network can be modified to address the related problems like any-to-any relighting, under-exposure correction etc. In any-to-any relighting, WDRN may be modified to feature an additional encoder to which a guide image from the target domain can be given as an additional input and the illumination properties of the guide image can be injected into the input image in the encoder section itself. Another way to realise this is to modify WDRN to to have additional scalar inputs corresponding to the target illumination settings and integrate them into encoder in a manner demonstrated in <ref type="bibr" target="#b18">[18]</ref>.</p><p>6 Ablation Studies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Wavelet Domain Network</head><p>To find out the effectiveness of wavelet decomposition approach, an ablation study was conducted using a 3-level encoder-decoder network. In the equivalent pixel domain network, wavelet decomposition was replaced with convolutional downsampling with a stride of two. Similarly, wavelet interpolation has been replaced with transposed convolution with an upscale factor of two. The results of the experiment on validation set of VIDIT is reported in <ref type="table" target="#tab_2">Table 2</ref>. It is conclusive that wavelet domain network obtained superior performance in terms of all evaluation metrics of the experiment which proves its effectiveness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Wavelet Decomposition Levels</head><p>To investigate the effect of various levels of wavelet decomposition, an extensive ablation study have been carried out. Experiments were conducted with two, three and four levels of wavelet decomposition in the encoder-decoder architecture. The model training was limited to 60 iterations to avoid the risk of overfitting. <ref type="table" target="#tab_3">Table 3</ref> shows the comparison of the performance obtained with different decomposition levels on the validation set of VIDIT dataset. Notably, the 3-level decomposed network shows superior performance in terms of perceptual metrics like SSIM and LPIPS while the 4-level network achieves highest PSNR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we proposed a novel multi-resolution encoder-decoder network employing wavelet based decomposition called wavelet decomposed RelightNet to address one-to-one image relighting problem. Additionally, a novel gray loss term tailored for the problem resulted in visually superior relit images. The experimental results have proved the effectiveness of the proposed WDRN both qualitatively and in terms of various quantitative parameters. The proposed WDRN can be modified to address other related problems like any-to-any relighting, under-exposure correction etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Equal contribution arXiv:2009.06678v1 [cs.CV] 14 Sep 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>An example of scene relighting for a change in illuminant position. It can be observed that the shadows caste in a, b, c and d are quite different from each other owing to the different relative position of objects and illuminant in each case. Also, the gradient in brightness is different in each case since the region in the scene proximal to the light source is different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The proposed WDRN architecture. There are 3 different processing levels in the network. The subsampling and interpolation are done using DWT and IDWT respectively. The operation of a simple DWT-IDWT based encoder-decoder network is depicted in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Convolution block contains a series of convolutions and ReLU activations that are stacked in sequence. The number of these conv-ReLU blocks varies in different encoder and decoder levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Plots for (a) Accuracy (b) Training Losses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of WDRN with competing entries in scene relighting and illumination estimation challenge, track-1 one-to-one relighting at AIM 2020 workshop. The MPS, used to determine the final ranking, is computed following Eq. (6).</figDesc><table><row><cell>Team</cell><cell>MPS</cell><cell>SSIM</cell><cell>LPIPS</cell><cell>PSNR</cell><cell>Run-time</cell></row><row><cell>Our method(with gray loss)</cell><cell cols="5">0.6452 (1) 0.6310 (2) 0.3405 (1) 17.0717 (2) 0.03s</cell></row><row><cell cols="6">Our method(without gray loss) 0.6451 (2) 0.6362 (1) 0.3460 (3) 16.8927 (6) 0.03s</cell></row><row><cell>Team 2</cell><cell cols="4">0.6436 (3) 0.6301 (3) 0.3430 (2) 16.6801 (8)</cell><cell>13s</cell></row><row><cell>Team 3</cell><cell cols="4">0.6216 (4) 0.6091 (4) 0.3659 (5) 16.8196 (7)</cell><cell>6s</cell></row><row><cell>Team 4</cell><cell cols="4">0.5897 (5) 0.5298 (7) 0.3505 (4) 17.0594 (3)</cell><cell>0.04s</cell></row><row><cell>Team 5</cell><cell cols="4">0.5892 (6) 0.5928 (6) 0.4144 (7) 17.4252 (1)</cell><cell>0.5s</cell></row><row><cell>Team 6</cell><cell cols="4">0.5603 (7) 0.5236 (8) 0.4029 (6) 16.5136 (9)</cell><cell>0.01s</cell></row><row><cell>Team 7</cell><cell cols="5">0.5339 (8) 0.5666 (6) 0.4989 (8) 16.9234 (4) 0.006s</cell></row><row><cell>Team 8</cell><cell cols="4">0.3746 (9) 0.3769 (9) 0.6278 (9) 16.8949 (5)</cell><cell>0.12s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of wavelet domain network Domain MPS SSIM LPIPS PSNR Pixel 0.6918 0.6619 0.2783 17.3934 Wavelet 0.6935 0.6642 0.2771 17.4539</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of wavelet decomposition levels</figDesc><table><row><cell cols="2">Decomposition level MPS SSIM LPIPS PSNR</cell></row><row><cell>2 level</cell><cell>0.6842 0.6486 0.28 17.0962</cell></row><row><cell>3 level</cell><cell>0.6935 0.6642 0.2771 17.4539</cell></row><row><cell>4 level</cell><cell>0.6908 0.661 0.2792 17.5586</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the support of NVIDIA PSG Cluster and Trivandrum Engineering Science and Technology Research Park (TrEST) in providing the computational resource to conduct this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input / output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep photo enhancer: Unpaired learning for image enhancement from photographs with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6306" to="6314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Acquiring the reflectance field of a human face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tchou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Duiker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sarokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques. p. 145156. SIGGRAPH 00</title>
		<meeting>the 27th Annual Conference on Computer Graphics and Interactive Techniques. p. 145156. SIGGRAPH 00<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/344779.344855</idno>
		<ptr target="https://doi.org/10.1145/344779.344855" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scene relighting with illumination estimation in the latent space on an encoder-decoder scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dherse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Everaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Gwizdaa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barthas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05460</idno>
		<title level="m">VIDIT: Virtual image dataset for illumination transfer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AIM 2020: Scene relighting and illumination estimation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops</title>
		<meeting>the European Conference on Computer Vision Workshops</meeting>
		<imprint>
			<publisher>ECCVW</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">WESPE: weakly supervised photo enhancer for digital cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1709.01118</idno>
		<ptr target="http://arxiv.org/abs/1709.01118" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dslr-quality photos on mobile devices with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1704.02470</idno>
		<ptr target="http://arxiv.org/abs/1704.02470" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The retinex</title>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="247" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-level wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<idno>abs/1805.07071</idno>
		<ptr target="http://arxiv.org/abs/1805.07071" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Llnet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<idno>abs/1511.03995</idno>
		<ptr target="http://arxiv.org/abs/1511.03995" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressively-refined reflectance functions from natural illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rendering Techniques</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Frequency-space decomposition and acquisition of light transport under spatially varying illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012</title>
		<editor>Schmid, C.</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="596" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Single image portrait relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<idno>abs/1905.00824</idno>
		<ptr target="http://arxiv.org/abs/1905.00824" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6842" to="6850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep image-based relighting from optimal sparse samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3197517.3201313</idno>
		<ptr target="https://doi.org/10.1145/3197517.3201313" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new low-light image enhancement algorithm using camera response model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3015" to="3022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Slabaugh</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note>Ntire 2020 challenge on image demoireing: Methods and results</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
