<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-02">2 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Waston AI Lab</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">)</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM T.J. Waston Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Waston AI Lab</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pistoia</surname></persName>
							<email>pistoia@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM T.J. Waston Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
							<email>david.d.cox@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Waston AI Lab</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-02">2 Dec 2019</date>
						</imprint>
					</monogr>
					<note>†: Equal contribution</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state-of-the-art models for video action recognition are mostly based on expensive 3D ConvNets. This results in a need for large GPU clusters to train and evaluate such architectures. To address this problem, we present an lightweight and memory-friendly architecture for action recognition that performs on par with or better than current architectures by using only a fraction of resources. The proposed architecture is based on a combination of a deep subnet operating on low-resolution frames with a compact subnet operating on high-resolution frames, allowing for high efficiency and accuracy at the same time. We demonstrate that our approach achieves a reduction by 3 ∼ 4 times in FLOPs and ∼ 2 times in memory usage compared to the baseline. This enables training deeper models with more input frames under the same computational budget. To further obviate the need for large-scale 3D convolutions, a temporal aggregation module is proposed to model temporal dependencies in a video at very small additional computational costs. Our models achieve strong performance on several action recognition benchmarks including Kinetics, Something-Something and Moments-in-time. The code and models are available at https://github.com/IBM/bLVNet-TAM.</p><p>In this paper, we present an efficient and memory-friendly spatio-temporal representation for action recognition, which enables training of deeper models while allowing for more input frames. The first part of our approach is inspired by the Big-Little-Net architecture (bLNet [8]). We propose a new 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current state-of-the-art approaches for video action recognition are based on convolutional neural networks (CNNs). These include the best performing 3D models, such as I3D <ref type="bibr" target="#b0">[1]</ref> and ResNet3D <ref type="bibr" target="#b1">[2]</ref>, and some effective 2D models, such as Temporal Relation Networks (TRN) <ref type="bibr" target="#b2">[3]</ref> and Temporal Shift Modules (TSM) <ref type="bibr" target="#b3">[4]</ref>. A CNN-based model usually considers a sequence of frames as input, obtained through either uniform or dense sampling from a video <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. In general, Longer input sequences yield better recognition results. However, one problem arising for a model requesting more input frames is that the GPU resources required for training and inference also significantly increase in both memory and time. For example, the top-performing I3D models <ref type="bibr" target="#b0">[1]</ref> on the Kinetics <ref type="bibr" target="#b5">[6]</ref> dataset were trained with 64 frames on a cluster of 32 GPUs, and the non-local network <ref type="bibr" target="#b6">[7]</ref> even uses 128 frames as input. Another problem for action recognition is the lack of effective methods for temporal modeling when moving away from 3D spatiotemporal convolutions. While 2D convolutional models are more resource-friendly than their 3D counterparts, they lack expressiveness over time and thus cannot take much benefit from richer input data.</p><p>video architecture that has two network branches with different complexities: one branch processing low-resolution frames in a very deep subnet, and another branch processing high-resolution frames in a compact subnet. The two branches complement each other through merging at the end of each network layer. With such a design, our approach can process twice as many frames as the baseline model without compromising efficiency. We refer to this architecture as "Big-Little-Video-Net" (bLVNet).</p><p>In light of the limited ability of capturing temporal dependencies in bLVNet, we further develop an effective method to exploit temporal relations across frames by a so called "Depthwise Temporal Aggregation Module" (TAM). The method enables the exchange of temporal information between frames by weighted channel-wise aggregation. This aggregation is made learnable with 1×1 depthwise convolution, and implemented as an independent network module. The temporal aggregation module can be easily integrated into the proposed network architecture to progressively learn spatiotemporal patterns in a hierarchical way. Moreover, the module is extremely compact and adds only negligible computational costs and parameters to bLVNet.</p><p>Our main contributions lie in the following two interconnected aspects: <ref type="bibr" target="#b0">(1)</ref> We propose a lightweight video architecture based on dual-path network to learn video features, and (2) we develop a temporal aggregation module to enable effective temporal modeling without the need for computationally expensive 3D convolutions.</p><p>We evaluate our approach on the Kinetics-400 <ref type="bibr" target="#b5">[6]</ref>, Something-Something <ref type="bibr" target="#b8">[9]</ref> and Moments-intime <ref type="bibr" target="#b9">[10]</ref> datasets. The evaluation shows that bLVNet-TAM successfully allows us to train actionclassification models with deeper backbones (i.e., ResNet-101) as well as more (up to 64) input frames, using a single compute node with 8 Tesla V100 GPUs. Our comprehensive experiments demonstrate that our approach achieves highly competitive results on all datasets while maintaining efficiency. Especially, it establishes a new state-of-the-art result on Something-Something and Moments-in-time by outperforming previous approaches in the literature by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Activity classification has always been a challenging research topic, with first attempts reaching back by almost two decades <ref type="bibr" target="#b10">[11]</ref>; deep-learning architectures nowadays achieve tremendous recognition rates on various challenging tasks, such as Kinetics <ref type="bibr" target="#b0">[1]</ref>, ActivityNet <ref type="bibr" target="#b11">[12]</ref>, or Thumos <ref type="bibr" target="#b12">[13]</ref>.</p><p>Most successful architectures in the field are usually based on the so-called two-stream model <ref type="bibr" target="#b13">[14]</ref>, processing a single RGB frame and optical-flow input in two separate CNNs with a late fusion in the upper layers. Over the last years, many approaches extend this idea by processing a stack of input frames in both streams, thus extending the temporal window of the architecture form 1 to up to 128 input frames per stream. To further capture the temporal correlation in the input over time, those architectures usually make use of 3D convolutions as, e.g., in I3D <ref type="bibr" target="#b0">[1]</ref>, S3D <ref type="bibr" target="#b14">[15]</ref>, and ResNet3D <ref type="bibr" target="#b1">[2]</ref>, usually leading to a large-scale parameter space to train.</p><p>Another way to capture temporal relations has been proposed by <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b2">[3]</ref>, and <ref type="bibr" target="#b3">[4]</ref>. Those architectures mainly build on the idea of processing videos in the form of multiple segments, and then fusing them at the higher layers of the networks. The first approach with this pattern was the socalled Temporal Segment Networks (TSN) proposed by Wang et al. <ref type="bibr" target="#b4">[5]</ref>. The idea of TSN has been extended by Temporal Relation Networks (TRN) <ref type="bibr" target="#b2">[3]</ref>, which apply the idea of relational networks to the modeling of temporal relations between observations in videos. Another approach for capturing temporal contexts has been proposed by Temporal Shift Modules (TSM) <ref type="bibr" target="#b3">[4]</ref>. This approach shifts part of the channels along the temporal dimension, thereby allowing for information to be exchanged among neighboring frames. More complex approaches have been tried as well, e.g. in the context of non-local neural networks <ref type="bibr" target="#b6">[7]</ref>. Our temporal aggregation module is based on depthwise 1×1 convolutions to capture temporal dependencies across frames effectively.</p><p>Separate convolutions are considered in approaches such as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> to reduce costly computation in 3D convolutional models. More recently, SlowFast Network <ref type="bibr" target="#b16">[17]</ref> uses a dual-pathway network to process a video at both slow and fast frame rates. The fast pathway is made lightweight, similar to Little Net in our proposed architecture. However, our approach reduces computation based on both a lightweight architecture and low image resolution. Furthermore, the recent work Timeception <ref type="bibr" target="#b17">[18]</ref> applies the concept of "Inception" to temporal domain for capturing long-range temporal dependen-cies in a video. The Timeception layers involve group convolutions at different time scales while our TAM layers only use depthwise convolution. As a result, the Timeception has significantly more parameters than the TAM (10% vs. 0.1% of the total model parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>We aim at developing efficient and effective video representations for video understanding. To address the computational challenge imposed by the desired long input to a model, we propose a new video architecture based on the Big-Little network (bLNet) <ref type="bibr" target="#b7">[8]</ref> for learning video features. We first give a brief recap of bLNet in Section 3.1. We then show, in Section 3.2, how to extend bLNet to an efficient video architecture that allows for seeing more frames with less computation and memory. An example of the proposed network architecture can be found in the supplementary material (Section A).</p><p>To make temporal modeling more effective in our approach, we further develop a temporal aggregation module (TAM) to capture short-term as well as long-term temporal dependencies across frames. Our method is implemented as a separate network module and integrated with the proposed architecture seamlessly to learn a hierarchical temporal representation for action recognition. We detail this method in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recap of Big-Little Network</head><p>The Big-Little Net, abbreviated as bLNet in <ref type="bibr" target="#b7">[8]</ref>, is a CNN architecture for learning strong feature representations by combining multi-scale image information. The bLNet processes an image at different resolutions using a dual-path network, but with low computational loads based on a clever design. The key idea is to have a high-complexity subnet (Big-Net) along with a low-cost one (Little-Net) operate on the low-scale and high-scale parts of an image in parallel. By such a design, the two subnets learn features complementary to each other while using less computation. The two branches are merged at the end of each network layer to fuse the low-scale and high-scale information so as to form a stronger image representation. The bLNet approach demonstrates improvement of model efficiency and performance on both object and speech recognition, using popular architectures such as ResNet, ResNeXt and SEResNeXt. More details on bLNet can be found in the original paper. In this work, we mainly adopt bLResNet-50 and bLResNet-101 as backbone for our proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Big-Little Video Network as Video Representation</head><p>We describe our architecture in the context of 2D convolutions. However our approach is not specific to 2D convolutions and potentially extendable to any architecture based on 3D convolutions.  uses a shared CNN to process each frame independently, so there is no temporal interaction between frames. b) TSN-bLNet is a variant of TSN that uses bLNet <ref type="bibr" target="#b7">[8]</ref> as backbone. It is efficient, but still lacks temporal modeling. c) bLVNet feeds odd and even frames separately into different branches in bLNet. The branch merging at each layer (local fusion) captures short-term temporal dependencies between adjacent frames. d) bLVNet-TAM includes the proposed aggregation module, represented as a red box, which further empowers bLVNet to model long-term temporal dependencies across frames (global fusion).</p><formula xml:id="formula_0">! ! ! "#$%&amp;$%'% ( ) ( * ( + ( ) ( * ! ! ( + ( , ! ! ( -( . ! ! ! ! ! "#$%&amp;$%'% ! ( ) ! ! ! ( * ! ! ! ( + ! ! "#$%&amp;$%'% ( ) ( * ! ! ( -( . ! ! ( + ( , ! ! ! ! ! "#$%&amp;$%'% "#$</formula><p>The approach of Temporal Segment Networks (TSN) <ref type="bibr" target="#b4">[5]</ref> provides a generic framework for learning video representations. With a shared 2D ConvNet as backbone, TSN performs frame-level predictions and then aggregates the results into a final video-level prediction <ref type="figure" target="#fig_1">(Fig. 1a)</ref>). The framework of TSN is efficient and has been successfully adopted by some recent approaches for action recognition such as TRN <ref type="bibr" target="#b2">[3]</ref> and TSM <ref type="bibr" target="#b3">[4]</ref>. Given its efficiency, we also choose TSN as the underlying video framework for our work.</p><p>Let F = {f t |t = 1 · · · n} be a set of sampled input frames from a video. We divide F into two groups, namely odd frames F odd = {f k ∈ F| mod (k, 2) = 0} at half of the input image resolution, and even frames F even = {f k ∈ F| mod (k, 2) = 0} at the input image resolution. For convenience, from now on, F odd is referred to as big frames and F even as little frames. Note that big branch can take either of a pair of frames as input and the other frame goes to the little branch.</p><p>In TSN, all input frames are ordered as a batch of size n, where the t th element corresponds to the t th frame. We denote the input and output feature maps of the t th frame at the k th layer of the model by x k t ∈ R C×W ×H and y k t ∈ R C×W ×H , respectively. Whenever possible, we omit k for clarity. The bLNet can be directly plugged into TSN as the backbone network for learning video-level representation. We refer to this architecture as TSN-bLNet to differentiate it from the vanilla TSN ( <ref type="figure" target="#fig_1">Fig. 1b)</ref>). This network fully enjoys the efficiency of bLNet, cutting the computational costs down by 1.6 ∼ 2 times according to <ref type="bibr" target="#b7">[8]</ref>. Mathematically, the output y t can be written as</p><formula xml:id="formula_1">y t = F (net B ([x t ] 1/2 ) + net L (x t ), θ t ).</formula><p>(1)</p><p>Here [·] s is an operator scaling a tensor up or down by a factor of s in the spatial domain; net B and net L are the Big-Net and Little-Net in the bLNet aforementioned; and θ t are the model parameters. Following <ref type="bibr" target="#b7">[8]</ref>, F indicates an additional residual block applied after merging the big and little branches to stabilize and enhance the combined feature representation.</p><p>The architecture described above only learns features from a single frame, so there are no interactions between frames. Alternatively, we can feed the odd and even frames separately into the big and little branches so that each branch obtains complementary information from different frames. This idea is illustrated in <ref type="figure" target="#fig_1">Fig. 1c</ref>) and the output y t in this case can be expressed by</p><formula xml:id="formula_2">y t = F (net B (⌊x t ⌋ 1/2 ) + net L (x t+1 ), θ t ), if mod (t, 2) = 0 y t−1 , otherwise<label>(2)</label></formula><p>While the modification proposed above is simple, it leads to a new video architecture, which is called Big-Little-Video-Net, or bLVNet for short. The bLVNet makes two distinct differences from TSN-bLNet. Firstly, without increasing any computation, it can take input frames two times as many as TSN-bLNet. We shall demonstrate the benefit of leveraging more frames for temporal modeling in Section 4. Furthermore, the bLVNet has 1.5 ∼ 2.0× fewer FLOPs than TSN while seeing frames twice as many as TSN, thanks to the efficiency of the dual-path network. Secondly, the merging of the two branches in bLVNet now happens on two different frames carrying temporal information.</p><p>We call this type of temporal interaction by local fusion, since it only captures temporal relations between two adjacent frames. In spite of that, local fusion gives rise to a significant performance boost for recognition, as shown later in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal Aggregation Module</head><p>Temporal modeling is a challenging problem for video understanding. Theoretically, adding a recurrent layer such as LSTM <ref type="bibr" target="#b18">[19]</ref> on top of a 2D ConvNet seems like a promising means to capture temporal ordering and long-term dependencies in actions. Nonetheless, such approaches are not practically competent with 3D ConvNets <ref type="bibr" target="#b0">[1]</ref>, which use spatio-temporal filters to learn hierarchical feature representations. One issue with 3D models is that they are heavy in parameters and costly in computation, making them hard to train. Even though some approaches like S3D <ref type="bibr" target="#b14">[15]</ref> and R(2+1)D <ref type="bibr" target="#b15">[16]</ref> alleviates this issue by separating a 3D convolution filter into a 2D spatial component followed by a 1D temporal component, they are in general still more expensive than 2D ConvNet models.</p><p>With the efficient bLVNet architecture described above, our goal is to further improve its spatiotemporal representation by effective temporal modeling. The local fusion in bLVNet only exploits temporal relations between neighbored frames. To address this limitation, we develop a method to capture short-term as well as long-term dependencies across frames. Our basic idea is to fuse temporal information at each time instance by weighted channel-wise aggregation. As detailed below, this idea can be efficiently implemented as a network module to progressively learn spatiotemporal patterns in a hierarchical way.</p><p>Let y t be the output (i.e. neural activation) of the t th frame f t at a layer of the network (see Eq. 2).</p><p>To model the temporal dependencies between f t and its neighbors, we aggregate the activations of all the frames within a temporal range r around f t . A weight is learned for each channel of the activations to indicate its relevance. Specifically, the aggregation results can be written aŝ</p><formula xml:id="formula_3">y t = ReLU ( j=⌊r/2⌋ j=−⌊r/2⌋ w j ⊗ y t+j ),<label>(3)</label></formula><p>where ⊗ indicates the channel-wise multiplication and w j ∈ R C is the weights. The ⊗ is defined as:</p><formula xml:id="formula_4">for a vector v = [v 1 v 2 · · · v C ] and a tensor M = [m 1 m 2 · · · m C ] with C feature channels, v ⊗ M = [v 1 * m 1 v 2 * m 2 · · · v C * m C ].</formula><p>We implement the temporal aggregation as a network module <ref type="figure" target="#fig_3">(Fig. 2)</ref>. It involves three steps as follows,</p><p>1. apply 1×1 depthwise convolution r times to n input tensors to form an output matrix of size r × n; 2. shift the i th row left (or right) by |i − ⌊r/2⌋| positions if i &gt; ⌊r/2⌋ (or i ≤ ⌊r/2⌋) and if needed, pad leading or trailing zero tensors in the front or at the end; 3. perform temporal aggregation along the column to generate the output.</p><p>The aggregation module(TAM), highlighted as a red box in <ref type="figure" target="#fig_1">Fig. 1d)</ref>, is inserted as a separate layer after the local temporal fusion in the bLVNet, resulting in the final bLVNet-TAM architecture. Obviously none of the steps in the implementation above involve costly computation, so the module is fairly fast. A node in the network initially only sees r − 1 neighbors. As the network goes deeper, the amount of context that the node involves in the input grows quickly, similar to how the receptive field of a neuron is enlarged in a CNN. In such a manner, long-range temporal dependencies are thus potentially captured. For this reason, the temporal aggregation is also called global temporal fusion here, as opposed to the local temporal fusion discussed above.</p><p>The work of TSM <ref type="bibr" target="#b3">[4]</ref> has also applied temporal shifting to swap feature channels between neighboring frames. In such a case, TSM can be treated as a special case of our method where the weights are empirically set rather than learned from data. In Section 4.3, we demonstrate that the proposed TAM is more effective than TSM for temporal modeling under different video architectures. TAM is also related to S3D <ref type="bibr" target="#b14">[15]</ref> and R(2+1)D <ref type="bibr" target="#b15">[16]</ref> in that TAM is independent of spatial convolutions. However, TAM is based on depthwise convolution, thus has fewer parameters and less computation than S3D and R(2+1)D.</p><p>The TAM can also be integrated into 3D convolutions such as C3D <ref type="bibr" target="#b19">[20]</ref> and I3D <ref type="bibr" target="#b0">[1]</ref> to further enhance the temporal modeling capability that already exists in these models. Due to the difference in how temporal data is presented between 2D-based and 3D-based models, the temporal shifting now needs to operate on feature channels within a tensor instead of on tensors themselves.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We evaluate our approach on three large-scale datasets for video recognition, including the widely used Something-Something (Version 1 and Version 2) <ref type="bibr" target="#b8">[9]</ref>, Kinetics-400 <ref type="bibr" target="#b5">[6]</ref> and the recent Moments-in-time dataset <ref type="bibr" target="#b9">[10]</ref>. They are herein referred to as SS-V1, SS-V2, Kinetics-400 and Moments, respectively.</p><p>Something-Something is a dataset containing videos of 174 types of predefined human-object interactions with everyday objects. The version 1 and 2 include 108k and 220k videos, respectively. This dataset focuses on human-object interactions in a rather simple setup with no scene contexts to be exploited for recognition. Instead temporal relationships are as important as appearance for reasoning about the interactions. Because of this, the dataset serves as a good benchmark for evaluating the efficacy of temporal modeling, such as proposed in our approach. Kinetics-400 <ref type="bibr" target="#b5">[6]</ref> has emerged as a standard benchmark for action recognition after UCF101 <ref type="bibr" target="#b20">[21]</ref> and HMDB <ref type="bibr" target="#b21">[22]</ref>, but on a significantly larger scale. The dataset consists of 240k training videos and 20k validation videos, with each video trimmed to around 10 seconds. It has a total of 400 human action categories.</p><p>Moments-in-time <ref type="bibr" target="#b9">[10]</ref> is a recent collection of one million labeled videos, involving actions from people, animals, objects or natural phenomena. It has 339 classes and each video clip is trimmed to 3 seconds long.</p><p>Data Augmentation. During training, we follow the data augmentation used in TSN <ref type="bibr" target="#b4">[5]</ref> to augment the video with different sizes spatially and flip the video horizontally with 50% probability. Furthermore, since our models are finetuned on pretrained ImageNet, we normalize the data with the mean and standard deviation of the ImageNet images. The model input is formed by uniform sampling, which first divides a video into n uniform segments and then selects one random frame from each segment as the input.</p><p>During inference, we resize the smaller side of an image to 256 and then crop a centered 224×224 region. The center frame of each segment in uniform sampling is picked as the input. On Something-Something and Moments, our results are based on the single-crop and single-clip setting. On Kinetics-400, we use the common practice of multi-crop and multi-clip for evaluation.</p><p>Training Details. Since all the three datasets are large-scale, we train the models in a progressive way. For each type of backbone (for example, bLResNet-50), we first finetune a base model on Ima-geNet with a minimum input length (i.e. 8×2 in our case) using 50 epochs. We adopt the Nesterov momentum optimizer with an initial weight of 0.01, a weight decay of 0.0005 and a momentum of 0.9. We then finetune a new model with longer input (for example, 16×2) on top of the corresponding base model, but with 25 epochs only. In this case, the initial learning rate is set to 0.01 on Something-Something and 0.005 on Kinetics and Moments. The learning rate is decreased by a factor of 10 at the 10-th and 20-th epoch, respectively.</p><p>This strategy allows to significantly reduce the training time needed for all the models evaluated in our experiments. All our models were trained on a server with 8 GPU cards and a total of 128G GPU memory. We set the total batch size to 64 whenever possible. For models that require more memory to train, we adjust the batch size accordingly to the maximum number allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Something-Something. We first report our results on the validation set of the Something-Something datasets in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>. With a moderately deep backbone bLResNet-50, our approach outperforms all 3D models on SS-V1 while using much fewer input frames (8×2) and being substantially more efficient. TSM <ref type="bibr" target="#b3">[4]</ref> was the previously best approach on Something-Something. Under the same backbone (i.e. ResNet-50), our approach is better than TSM on both SS-V1 and SS-V2 while being more efficient (i.e our 8x2 model has 1.4 times fewer FLOPs than a 8-frame TSM model).</p><p>When empowered with a stronger backbone bLResNet-101, our approach achieves even better results at 32×2 frames (53.1% top-1 accuracy on SS-V1, and 65.2% on SS-V2), establishing a new stateof-the-art on Something-Something. Notably, these results while based on RGB information only, are superior to those obtained from the best two-stream models at no more computational cost. This  strongly demonstrates the effectiveness of our approach for temporal modeling. We further evaluated our models on the test set of Something-Something. Our results are consistently better than the best results reported by the other approaches in comparison including 2-stream models.</p><p>Kinetics-400. Kinetics-400 is one of the most popular benchmarks for action recognition. Currently the best-performed models on this dataset are all based on 3D Convolutions. However, it has been shown in the literature that temporal ordering in this dataset does not seem to be as crucial as RGB information for recognition. For example, as experimented in S3D <ref type="bibr" target="#b14">[15]</ref>, the model trained on normal time-order data performs well on the time-reversed data on Kinetics. In accordance to this, our approach (3 crops and 3 clips) mainly performs on par with or better than the current large-scale architectures, but without outperforming them as clearly as on the Something-Something datasets, where the temporal relations are more essential for an overall understanding of the video content.   Moments. We finally evaluate the proposed architecture on the Moments dataset <ref type="bibr" target="#b9">[10]</ref>, a large-scale action dataset with about three times more training samples than Kinetics-400. Since Moments is relatively new and results reported on it are limited, we only compare our results with those reported in the Moments paper <ref type="bibr" target="#b9">[10]</ref>. As can been seen from <ref type="table" target="#tab_3">Table 4</ref>, our approach outperforms all the singlestream models as well as the ensemble one. We hope our models provide stronger baseline results for future reference on this challenging dataset.</p><p>It is also noted that our model trained with 16×2 frames only produces slightly better top-1 accuracy than the model trained with 8 × 2 frames. We speculate that this has to do with the fact that the Moments clips are only as short as 3 seconds and that there is only a limited impact in choosing a finer temporal granularity on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>In this section, we conduct ablation studies to provide more insights about our main ideas.</p><p>Is temporal aggregation effective?. We validate the efficacy of the proposed temporal aggregation module (TAM), which is considered as a global fusion method (Section 3.3). Local fusion here is referred to the branch merging in the dual path network (Section 3.2). We compare TAM with the temporal shift module used in TSM <ref type="bibr" target="#b3">[4]</ref> in <ref type="table" target="#tab_5">Table 5</ref>  Does seeing more frames help?. One of the main contribution of this work is an efficient video architecture that makes it possible to train deeper models with more input frames using moderate GPU resources. <ref type="figure" target="#fig_4">Fig. 3a)</ref> shows consistent improvement of our approach on SS-V1 as the number of input frames increases. A similar trend in our results can be observed on Kinetics-400 in <ref type="table" target="#tab_2">Table 3</ref>. On the other hand, the almost flattened line from TSN suggests that a model without effective temporal modeling cannot take much of the benefit from longer input frames.</p><p>Memory Usage. We compare the memory usage between our approach based on bLResNet-50 and TSN based on ResNet-50. As shown in <ref type="figure" target="#fig_4">Fig. 3b</ref>), our approach is more memory friendly than TSN, achieving a saving of ∼2 times at the same number of input frames. The larger batch size allowed for training under the same computational budget is critical for our approach to obtain better models and reduce training time. We presented an efficient and memory-friendly video architecture for learning video representations. The proposed architecture allows for twice as many input frames as the baseline while using less computation and memory. This enables training of deeper models with richer input under the same GPU resources. We further developed a temporal aggregation method to capture temporal dependencies effectively across frames. Our models achieve strong performance on several action recognition benchmarks, and establish a state-of-the-art on the Something-Something dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Network Architecture</head><p>Here we details our network architecture for bLVNet-TAM-50 in <ref type="table" target="#tab_6">Table 6</ref>. We follow the notation used in bLNet <ref type="bibr" target="#b7">[8]</ref> (α = 2 and β = 4) but adding the proposed TAM module before branching out to Big-Net and Little-Net and the last shared residual block. As noted before, the two branches work on different frames and then merged every stage; on the other hand, in ResBlock T AM , the TAM module goes through the non-shortcut path. ResBlockB: the first 3 × 3 convolution is with stride 2, and then restoring the size via the bi-linear upsampling. ResBlockL: a 1 × 1 convolution is applied at the end to align the channel size.</p><p>ResBlockT AM : a residual block embedded with temporal aggregation module with r = 3. s2: the stride is set to 2 for the 3 × 3 convolution in the ResBlock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Data Preprocessing</head><p>Here we describe how we convert the video data into images for our training and inference. For the Something-Something dataset, we resize the smaller side of an image to 256 while keeping aspect ratio. For the Kinetics dateset, we resize the smaller side of an image to 331 since its original resolution is higher. For the Moments dataset, we we resize an image to 256×256.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Different architectures for action recognition. a) TSN<ref type="bibr" target="#b4">[5]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Temporal aggregation module (TAM). The TAM takes as input a batch of tensors, each of which is the activation of a frame, and produces a batch of tensors with the same order and dimension. The module consists of three operations: 1) 1×1 depthwise convolutions to learn a weight for each feature channel; 2) temporal shifts (left or right direction indicated by the smaller arrows; the white cubes are padded zero tensors.); and 3) aggregation by summing up the weighted activations from 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Number of input frames v.s. model accuracy and memory usage. (a) A longer input sequence yields better recognition in our proposed bLVNet-TAM on the Something-Something dataset [9], but not in TSN [5] due to limited temporal modeling ability. (b) Compared to TSN, bLVNet-TAM reduces memory usage by ∼2 times under the same number of input frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Recognition Accuracy of Various Models on Something-Something-V1 (SS-V1).</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>Pretrain</cell><cell>Frames</cell><cell>Modality</cell><cell cols="2">Param (10 6 ) FLOPs (10 9 )</cell><cell cols="3">Val Top-1 (%) Top-5 (%) Top-1 (%) Test</cell></row><row><cell>I3D [1]</cell><cell>Inception</cell><cell>ImageNet</cell><cell>64</cell><cell>RGB</cell><cell>12.7</cell><cell>111</cell><cell>45.8</cell><cell>76.5</cell><cell>27.2</cell></row><row><cell>NL I3D + GCN [23]</cell><cell>ResNet-50</cell><cell cols="2">ImageNet 32+32</cell><cell>RGB</cell><cell>303</cell><cell>62.2</cell><cell>46.1</cell><cell>76.8</cell><cell>−</cell></row><row><cell>S3D [15]</cell><cell>Inception</cell><cell>ImageNet</cell><cell>64</cell><cell>RGB</cell><cell>8.77</cell><cell>66</cell><cell>47.3</cell><cell>78.1</cell><cell>−</cell></row><row><cell>ECO-LiteEn [24]</cell><cell cols="2">BNInception+ResNet18 ImageNet</cell><cell>92</cell><cell>RGB</cell><cell>150</cell><cell>267</cell><cell>46.4</cell><cell>−</cell><cell>42.3</cell></row><row><cell>TSN [5]</cell><cell>BNInception</cell><cell>ImageNet</cell><cell>8</cell><cell>RGB</cell><cell>10.7</cell><cell>16</cell><cell>19.5</cell><cell>−</cell><cell>−</cell></row><row><cell>TRN [3]</cell><cell>BNInception BNInception</cell><cell>ImageNet ImageNet</cell><cell>8 8+8</cell><cell>RGB RGB+Flow</cell><cell>18.3 −</cell><cell>16 −</cell><cell>34.4 42.0</cell><cell>− −</cell><cell>33.6 40.7</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>Kinetics</cell><cell>8</cell><cell>RGB</cell><cell>24.3</cell><cell>33</cell><cell>45.6</cell><cell>74.2</cell><cell></cell></row><row><cell>TSM [4]</cell><cell>ResNet-50</cell><cell>Kinetics</cell><cell>16</cell><cell>RGB</cell><cell>24.3</cell><cell>65</cell><cell>47.2</cell><cell>77.1</cell><cell>46.0</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>Kinetics</cell><cell cols="2">16+16 RGB+Flow</cell><cell>−</cell><cell>−</cell><cell>52.6</cell><cell>81.9</cell><cell>50.7</cell></row><row><cell></cell><cell>bLResNet-50</cell><cell>ImageNet</cell><cell>8×2</cell><cell>RGB</cell><cell>25.0</cell><cell>23.8</cell><cell>46.4</cell><cell>76.6</cell><cell>−</cell></row><row><cell></cell><cell>bLResNet-50</cell><cell>SS-V1</cell><cell>16×2</cell><cell>RGB</cell><cell>25.0</cell><cell>47.7</cell><cell>48.4</cell><cell>78.8</cell><cell>−</cell></row><row><cell>bLVNet-TAM</cell><cell>bLResNet-101 bLResNet-101</cell><cell>ImageNet SS-V1</cell><cell>8×2 16×2</cell><cell>RGB RGB</cell><cell>40.2 40.2</cell><cell>32.1 64.3</cell><cell>47.8 49.6</cell><cell>78.0 79.8</cell><cell>− −</cell></row><row><cell></cell><cell>bLResNet-101</cell><cell>SS-V1</cell><cell>24×2</cell><cell>RGB</cell><cell>40.2</cell><cell>96.4</cell><cell>52.2</cell><cell>81.8</cell><cell>−</cell></row><row><cell></cell><cell>bLResNet-101</cell><cell>SS-V1</cell><cell>32×2</cell><cell>RGB</cell><cell>40.2</cell><cell>128.6</cell><cell>53.1</cell><cell>82.9</cell><cell>48.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Recognition Accuracy of Various Models on Something-Something-V2 (SS-V2). : using their pretrained models and code to evaluate under the 1-crop and 1-clip setting for fair comparison : model ensemble of RGB and Flow model, each is evaluated with 3 crops and 10 clips and uses 256 as the shorter side.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>Pretrain</cell><cell>Frames</cell><cell>Modality</cell><cell cols="2">Param (10 6 ) FLOPs (10 9 )</cell><cell cols="4">Val Top-1 (%) Top-5 (%) Top-1 (%) Top-5 (%) Test</cell></row><row><cell>TRN [3]</cell><cell>BNInception BNInception</cell><cell>ImageNet ImageNet</cell><cell>8 8</cell><cell>RGB RGB+Flow</cell><cell>18.3 36.6</cell><cell>16 32</cell><cell>48.8 55.5</cell><cell>77.6 83.1</cell><cell>50.9 56.2</cell><cell>79.3 83.2</cell></row><row><cell></cell><cell>ResNet-50 †</cell><cell>Kinetics</cell><cell>8</cell><cell>RGB</cell><cell>24.3</cell><cell>33</cell><cell>58.9</cell><cell>85.5</cell><cell>−</cell><cell>−</cell></row><row><cell>TSM [4]</cell><cell>ResNet-50 †</cell><cell>Kinetics</cell><cell>16</cell><cell>RGB</cell><cell>24.3</cell><cell>65</cell><cell>61.4</cell><cell>87.0</cell><cell>−</cell><cell>−</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>Kinetics</cell><cell>−</cell><cell>RGB+Flow</cell><cell>−</cell><cell>−</cell><cell>66.0</cell><cell>90.5</cell><cell>66.6</cell><cell>91.3</cell></row><row><cell></cell><cell>bLResNet-50</cell><cell>ImageNet</cell><cell>8×2</cell><cell>RGB</cell><cell>25.0</cell><cell>23.8</cell><cell>59.1</cell><cell>86.0</cell><cell>−</cell><cell>−</cell></row><row><cell></cell><cell>bLResNet-50</cell><cell>SS-V2</cell><cell>16×2</cell><cell>RGB</cell><cell>25.0</cell><cell>47.7</cell><cell>61.7</cell><cell>88.1</cell><cell>−</cell><cell>−</cell></row><row><cell>bLVNet-TAM</cell><cell cols="2">bLResNet-101 ImageNet bLResNet-101 SS-V2</cell><cell>8×2 16×2</cell><cell>RGB RGB</cell><cell>40.2 40.2</cell><cell>32.1 64.3</cell><cell>60.2 61.9</cell><cell>87.1 88.4</cell><cell>− −</cell><cell>− −</cell></row><row><cell></cell><cell>bLResNet-101</cell><cell>SS-V2</cell><cell>24×2</cell><cell>RGB</cell><cell>40.2</cell><cell>96.4</cell><cell>64.0</cell><cell>89.8</cell><cell>−</cell><cell>−</cell></row><row><cell></cell><cell>bLResNet-101 bLResNet-101  *</cell><cell>SS-V2 SS-V2</cell><cell>32×2 32×2</cell><cell>RGB RGB+Flow</cell><cell>40.2 −</cell><cell>128.6 −</cell><cell>65.2 68.5</cell><cell>90.3 91.4</cell><cell>− 67.1</cell><cell>− 91.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>†*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Recognition Accuracy of Various Models on Kinetics-400 (RGB-only).</figDesc><table><row><cell>Net</cell><cell>Backbone</cell><cell>Pretrain</cell><cell cols="3">FLOPs (10 9 ) Top-1 (%) Top-5 (%)</cell></row><row><cell>STC [25]</cell><cell>ResNeXt-101</cell><cell>None</cell><cell>−</cell><cell>68.7</cell><cell>88.5</cell></row><row><cell>ARTNet [26]</cell><cell>ResNet-18</cell><cell>None</cell><cell>23.5×250</cell><cell>69.2</cell><cell>88.3</cell></row><row><cell>C3D [26]</cell><cell>ResNet-18</cell><cell>None</cell><cell>19.6×250</cell><cell>65.6</cell><cell>85.7</cell></row><row><cell>I3D [1]</cell><cell>Inception</cell><cell>ImageNet</cell><cell>108×N/A</cell><cell>71.1</cell><cell>89.3</cell></row><row><cell>S3D [15]</cell><cell>Inception</cell><cell>ImageNet</cell><cell>−</cell><cell>72.2</cell><cell>90.6</cell></row><row><cell>R(2+1)D [16]</cell><cell>ResNet-34</cell><cell>None</cell><cell>−</cell><cell>72.0</cell><cell>90.0</cell></row><row><cell>SlowFast-4×16 [17]</cell><cell>ResNet-50</cell><cell>None</cell><cell>36.1×30</cell><cell>75.6</cell><cell>92.1</cell></row><row><cell>TSN [5]</cell><cell>InceptionV3</cell><cell>ImageNet</cell><cell>142.8×10</cell><cell>72.5</cell><cell>−</cell></row><row><cell>ECO-Lite En [24]</cell><cell cols="2">BNInception+ResNet18 ImageNet</cell><cell>267</cell><cell>70.7</cell><cell>-</cell></row><row><cell>TSM-8 [4]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>42.7×30</cell><cell>74.1</cell><cell>91.2</cell></row><row><cell>TSM-16 [4]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>85.4×30</cell><cell>74.7</cell><cell>−</cell></row><row><cell>bLVNet-TAM-8×2</cell><cell>bLResNet-50</cell><cell>ImageNet</cell><cell>31.1×9</cell><cell>71.0</cell><cell>89.8</cell></row><row><cell>bLVNet-TAM-16×2</cell><cell>bLResNet-50</cell><cell>Kinetics</cell><cell>62.3×9</cell><cell>72.0</cell><cell>90.6</cell></row><row><cell>bLVNet-TAM-24×2</cell><cell>bLResNet-50</cell><cell>Kinetics</cell><cell>93.4×9</cell><cell>73.5</cell><cell>91.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Recognition Accuracy of Various Models on Moments-in-time.</figDesc><table><row><cell>Net</cell><cell>Backbone</cell><cell>Pretrain</cell><cell>Frames</cell><cell>Modality</cell><cell cols="2">Top-1 (%) Top-5 (%)</cell></row><row><cell>SoundNet [10]</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>Audio</cell><cell>7.60</cell><cell>18.0</cell></row><row><cell>TSN [10]</cell><cell cols="2">BNInception ImageNet</cell><cell>16</cell><cell>RGB</cell><cell>24.1</cell><cell>49.1</cell></row><row><cell>TSN [10]</cell><cell>BNInception</cell><cell>−</cell><cell cols="2">16+16 RGB+Flow</cell><cell>25.3</cell><cell>50.1</cell></row><row><cell>TRN [10]</cell><cell>Inception</cell><cell>ImageNet</cell><cell>16</cell><cell>RGB</cell><cell>28.3</cell><cell>53.9</cell></row><row><cell>I3D [10]</cell><cell>ResNet-50</cell><cell>−</cell><cell>16</cell><cell>RGB</cell><cell>29.5</cell><cell>56.1</cell></row><row><cell>Ensemble [10]</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>31.2</cell><cell>57.7</cell></row><row><cell>bLVNet-TAM</cell><cell cols="2">bLResNet-50 ImageNet bLResNet-50 Moments</cell><cell>8×2 16×2</cell><cell>RGB RGB</cell><cell>31.2 31.4</cell><cell>58.3 59.3</cell></row><row><cell></cell><cell>a)</cell><cell></cell><cell></cell><cell></cell><cell>b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>under two different video architectures: TSN and bLVNet proposed in this work. TAM demonstrates clear advantages over TSM, outperforming TSM by over 2% under both architectures. Interestingly, with the here proposed bLVNet baseline with local temporal fusion almost doubles the performance of a TSN baseline, improving the accuracy from 17.4% to 33.6%. On top of that, TAM boosts the performance by another 13% in both cases, suggesting that TAM is complementary to local fusion. This further confirms the significance of temporal reasoning on the Something-Something dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Temporal Modeling on SS-V1.</figDesc><table><row><cell>Net</cell><cell>Backbone</cell><cell cols="3">Local Fusion Global Fusion Top-1 (%)</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>None</cell><cell>None</cell><cell>17.4</cell></row><row><cell>TSN</cell><cell>ResNet-50</cell><cell>None</cell><cell>TSM</cell><cell>43.4</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>None</cell><cell>TAM</cell><cell>46.1</cell></row><row><cell></cell><cell>bLResNet-50</cell><cell></cell><cell>None</cell><cell>33.6</cell></row><row><cell cols="2">bLVNet bLResNet-50</cell><cell></cell><cell>TSM</cell><cell>44.2</cell></row><row><cell></cell><cell>bLResNet-50</cell><cell></cell><cell>TAM</cell><cell>46.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Network configurations of bLVNet-TAM-50 with temporal fusion.</figDesc><table><row><cell>Layers</cell><cell>Spatial output size</cell><cell cols="2">bLVNet-TAM-50</cell></row><row><cell>Convolution</cell><cell>112 × 112</cell><cell cols="2">7 × 7, 64, s2</cell></row><row><cell>TAM-module</cell><cell>112 × 112</cell><cell cols="2">Temporal Aggregation Module (r = 3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3×3, 32</cell></row><row><cell>bL-module</cell><cell>56 × 56</cell><cell>3 × 3, 64, s2</cell><cell>3×3, 32, s2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1×1, 64</cell></row><row><cell>TAM-module</cell><cell>56 × 56</cell><cell cols="2">Temporal Aggregation Module (r = 3)</cell></row><row><cell>bL-module</cell><cell>56 × 56</cell><cell>ResBlock B , 256 ×2</cell><cell>ResBlock L , 128 ×1</cell></row><row><cell></cell><cell>28 × 28</cell><cell cols="2">ResBlock, 256, s2</cell></row><row><cell>TAM-module</cell><cell>28 × 28</cell><cell cols="2">Temporal Aggregation Module (r = 3)</cell></row><row><cell>bL-module</cell><cell>28 × 28</cell><cell>ResBlock B , 512 ×3</cell><cell>ResBlock L , 256 ×1</cell></row><row><cell></cell><cell>14 × 14</cell><cell cols="2">ResBlock, 512, s2</cell></row><row><cell>TAM-module</cell><cell>14 × 14</cell><cell cols="2">Temporal Aggregation Module (r = 3)</cell></row><row><cell>bL-module</cell><cell>14 × 14</cell><cell>ResBlock B , 1024 ×5</cell><cell>ResBlock L , 512 ×1</cell></row><row><cell></cell><cell>14 × 14</cell><cell cols="2">ResBlock, 1024</cell></row><row><cell>ResBlock T AM</cell><cell>7 × 7</cell><cell cols="2">ResBlock T AM , 2048 ×3, s2</cell></row><row><cell>Average pool</cell><cell>1 × 1</cell><cell cols="2">7 × 7 average pooling</cell></row><row><cell>FC, softmax</cell><cell></cell><cell># of classes</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08383</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Big-little net: An efficient multi-scale feature representation for visual and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu ;</forename><surname>Richard) Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mallinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Activitynet: A largescale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing System (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03982</idno>
		<title level="m">Slowfast networks for video recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features With 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Mahdi</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahman</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
