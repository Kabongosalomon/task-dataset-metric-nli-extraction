<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text classification with word embedding regularization and soft similarity measure</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vít</forename><surname>Novotný</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Eniafe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Festus</forename><surname>Ayetiran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Štefánik</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Sojka</surname></persName>
						</author>
						<title level="a" type="main">Text classification with word embedding regularization and soft similarity measure</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Noname manuscript No. (will be inserted by the editor) the date of receipt and acceptance should be inserted later</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Text classification · Soft Cosine Measure · Word Mover&apos;s Distance · Word embedding regularization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since the seminal work of Mikolov et al., word embeddings have become the preferred word representations for many natural language processing tasks. Document similarity measures extracted from word embeddings, such as the soft cosine measure (SCM) and the Word Mover's Distance (WMD), were reported to achieve state-of-the-art performance on the semantic text similarity and text classification.</p><p>Despite the strong performance of the WMD on text classification and semantic text similarity, its super-cubic average time complexity is impractical. The SCM has quadratic worst-case time complexity, but its performance on text classification has never been compared with the WMD. Recently, two word embedding regularization techniques were shown to reduce storage and memory costs, and to improve training speed, document processing speed, and task performance on word analogy, word similarity, and semantic text similarity. However, the effect of these techniques on text classification has not yet been studied.</p><p>In our work, we investigate the individual and joint effect of the two word embedding regularization techniques on the document processing speed and the task performance of the SCM and the WMD on text classification. For evaluation, we use the kNN classifier and six standard datasets: BBCSPORT, TWITTER, OHSUMED, REUTERS-21578, AMAZON, and 20NEWS.</p><p>We show 39% average kNN test error reduction with regularized word embeddings compared to non-regularized word embeddings. We describe a practical procedure for deriving such regularized embeddings through Cholesky factorization. We also show that the SCM with regularized word embeddings significantly outperforms the WMD on text classification and is over 10,000× faster. Keywords Text classification · Soft Cosine Measure · Word Mover's Distance · Word embedding regularization 1 Introduction Word embeddings are the state-of-the-art words representation for many natural language processing (NLP) tasks. Most of these tasks are at the word level, such as the word analogy (Garten et al. 2015) and word similarity, and at the sentence level, such as question answering, natural language inference, semantic role labeling, co-reference resolution, named-entity recognition, and sentiment analysis (Peters et al. 2018), semantic text similarity (Charlet and Damnati 2017). On document-level tasks, such as machine translation (Qi et al. 2018), text classification (Kusner et al. 2015; Wu et al. 2018) and ad-hoc information retrieval (Zuccon et al. 2015; Kuzi et al. 2016), word embeddings provide simple and strong baselines. Document similarity measures, such as the Soft Cosine Measure (SCM) (Sidorov et al. 2014; Charlet and Damnati 2017; Novotný 2018) and the Word Mover's Distance (WMD) <ref type="bibr" target="#b16">(Kusner et al. 2015)</ref> can be extracted from word embeddings. The SCM achieves state-of-the-art performance on the semantic text similarity task (Charlet and Damnati 2017). The WMD outperforms standard methods, such as the VSM, BM25, LSI, LDA, mSDA, and CCG on the text classification task <ref type="bibr" target="#b16">(Kusner et al. 2015)</ref>, and achieves state-of-the-art performance on nine text classification datasets and 22 semantic text similarity datasets with better performance on datasets with shorter documents. The SCM is asymptotically faster than the WMD, but their task performance has never been compared.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Regularization techniques were reported to improve the task performance of word embeddings. We use the quantization technique of <ref type="bibr" target="#b19">Lam (2018)</ref>, which reduces storage and memory cost, and improves training speed <ref type="bibr" target="#b7">(Courbariaux et al. 2016</ref>) and task performance on word analogy and word similarity. We also use the orthogonalization technique of <ref type="bibr" target="#b26">Novotný (2018)</ref>, which improves the document processing speed and the task performance of the SCM on semantic text similarity <ref type="bibr" target="#b5">(Charlet and Damnati 2017)</ref>. However, the effect of these techniques at the document-level (e.g. text classification) has not been studied.</p><p>In our work, we investigate the effect of word embedding regularization on text classification. The contributions of our work are as follows: (1) We show that word embedding regularization techniques that reduce storage and memory costs and improve speed can also significantly improve performance on the text classification task.</p><p>(2) We show that the SCM with regularized word embeddings significantly outperforms the slower WMD on the text classification task. (3) We define orthogonalized word embeddings and we prove a relationship between orthogonalized word embeddings, Cholesky factorization, and the word embedding regularization technique of <ref type="bibr" target="#b26">Novotný (2018)</ref>.</p><p>The rest of the paper is organized as follows: We present related work in Section 2. In Section 3, we discuss the document distance and similarity measures. In Section 4, we discuss word embedding regularization techniques and we prove their properties. Section 5 presents our experiment and Section 6 discusses our results. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Word embeddings represent words in a vector space, where syntactically and semantically similar words are close to each other. Word embeddings can be extracted from word co-occurrence matrices <ref type="bibr" target="#b8">(Deerwester et al. 1990;</ref><ref type="bibr" target="#b30">Pennington et al. 2014</ref>) and from neural network language models <ref type="bibr" target="#b1">(Bengio et al. 2003;</ref><ref type="bibr" target="#b25">Mikolov et al. 2013;</ref><ref type="bibr" target="#b31">Peters et al. 2018)</ref>. Word embeddings extracted from neural network language models have been shown to be effective on several (NLP) tasks, but they tend to suffer from overfitting due to high feature dimensionality. There have been several works that use word embedding regularization to reduce overfitting. <ref type="bibr" target="#b18">Labutov and Lipson (2013)</ref> introduce a technique which re-embeds an existing embedding with the end product being a target embedding. In their technique, they perform optimization of a convex objective. Their objective is a linear combination of the log-likelihood of the dataset under a designated target embedding and the Frobenius norm of a distortion matrix. The Frobenius norm serves as a regularizer that penalizes the Euclidean distance between the initial and the designated target embeddings. To enrich the embedding, they further use external source embedding, which they incorporated into the regularizer, on the supervised objective. Their approach was reported to improve performance on the sentiment classification task.</p><p>The Dropout technique was introduced by <ref type="bibr" target="#b42">Srivastava et al. (2014)</ref> to mitigate the problem of overfitting in neural networks by dropping their neurons and links between neurons during training. During the training, Dropout samples an exponential number of the reduced networks and at test time, it approximates the effect of averaging the previsions of these thinned networks by using a single unthinned network with smaller weights. Learning the Dropout networks involves two major steps: back propagation and unsupervised pre-training. Dropout was successfully applied to a number of tasks in the areas of vision, speech recognition, document classification, and computational biology. <ref type="bibr" target="#b43">Sun et al. (2016)</ref> introduce a sparse constraint into Word2Vec <ref type="bibr" target="#b25">(Mikolov et al. 2013</ref>) in order to increase its interpretability and performance. They added the 1 regularizer into the loss function of the Continuous Bag-of-Words (CBOW). They applied the technique to online learning and to solve the problem of stochastic gradient descent, they employ an online optimization algorithm for regularized stochastic learning -the Regularized Dual Averaging (RDA).</p><p>In their own work, <ref type="bibr" target="#b29">Peng et al. (2015)</ref> experimented with four regularization techniques: penalizing weights (embeddings excluded), penalizing embeddings, word re-embedding and Dropout. At the end of their experiments, they concluded the following: (1) regularization techniques do help generalization, but their effect depends largely on the dataset size. (2) penalizing the 2 -norm of embeddings also improves task performance (3) incremental hyperparameter tuning achieves similar performance, which shows that regularization has mostly a local effect (4) Dropout performs slightly worse than the 2 -norm penalization (5) word re-embedding does not improve task performance.</p><p>Another approach by <ref type="bibr" target="#b41">Song et al. (2017)</ref> uses pre-learned or external priors as a regularizer for the enhancement of word embeddings extracted from neural network language models. They considered two types of embeddings. The first one was extracted from topic distributions generated from unannotated data using the Latent Dirichlet Allocation (LDA). The second was based on dictionaries that were created from human annotations. A novel data structure called the trajectory softmax was introduced for effective learning with the regularizers. Song et al. reported improved embedding quality through learning from prior knowledge with the regularizer.</p><p>A different algorithm was presented by <ref type="bibr" target="#b45">Yang et al. (2017)</ref>. They applied their own regularization to cross-domain embeddings. In contrast to <ref type="bibr" target="#b43">Sun et al. (2016)</ref>, who applied their regularization to the CBOW, the technique of Yang et al. is a regularized skip-gram model, which allows word embeddings to be learned from different domains. They reported the effectiveness of their approach with experiments on entity recognition, sentiment classification and targeted sentiment analysis. <ref type="bibr" target="#b3">Berend (2018)</ref> in his own approach investigates the effect of 1 -regularized sparse word embeddings for identification of multi-word expressions. Berend uses dictionary learning, which decomposes the original embedding matrix by solving an optimization problem.</p><p>Other works that focus solely on reducing storage and memory costs of word embeddings include the following: <ref type="bibr" target="#b13">Hinton et al. (2014)</ref> use a distillation compression technique to compress the knowledge in an ensemble into a single model, which is much easier to deploy. <ref type="bibr" target="#b6">Chen et al. (2015)</ref> present HashNets, a novel framework to reduce redundancy in neural networks. Their neural architecture uses a low-cost hash function to arbitrarily group link weights into hash buckets, and all links within the same hash bucket share a single parameter value. <ref type="bibr" target="#b37">See et al. (2016)</ref> employ a network weight pruning strategy and apply it to Neural Machine Translation (NMT). They experimented with three NMT models, namely the class-blind, the class-uniform and the class-distribution model. The result of their experiments shows that even strong weight pruning does not reduce task performance. FastTest.zip is a compression technique by <ref type="bibr" target="#b15">Joulin et al. (2016)</ref> who use product quantization to mitigate the loss in task performance reported with earlier quantization techniques. In an attempt to reduce the space and memory costs of word embeddings, <ref type="bibr" target="#b38">Shu and Nakayama (2017)</ref> experimented with construction of embeddings with a few basis vectors, so that the composition of the basis vectors is determined by a hash code.</p><p>Our technique is based partly on the recent regularization technique by <ref type="bibr" target="#b19">Lam (2018)</ref>, in which a quantization function was introduced into the loss function of the CBOW with negative sampling to show that high-quality word embeddings using 1-2 bits per parameter can be learned. Lam's technique is based on the work of <ref type="bibr" target="#b7">Courbariaux et al. (2016)</ref>, who employ neural networks with binary weights and activations to reduce space and memory costs. A major component of their work is the use of bit-wise arithmetic operations during computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Distance and Similarity Measures</head><p>The Vector Space Model (VSM) <ref type="bibr" target="#b35">(Salton and Buckley 1988)</ref> is a distributional semantics model that is fundamental to a number of text similarity applications including text classification. The VSM represents documents as coordinate vectors relative to a real inner-product-space orthonormal basis β, where coordinates correspond to weighted and normalized word frequencies. In the VSM, a commonly used measure of similarity for document vectors x and y is the cosine similarity: cosine similarity of x and y = x/ x 2 , y/ y 2 , where x, y = (x) β T (y) β and z 2 is the 2 -norm of z.</p><p>(1)</p><p>The cosine similarity is highly susceptible to polysemy, since distinct words correspond to mutually orthogonal basis vectors. Therefore, documents that use different terminology will always be regarded as dissimilar. To borrow an example from <ref type="bibr" target="#b16">Kusner et al. (2015)</ref>, the cosine similarity of the documents "Obama speaks to the media in Illinois" and "the President greets the press in Chicago" is zero if we disregard stop words.</p><p>The Word Mover's Distance (WMD) and the Soft Cosine Measure (SCM) are document distance and similarity measures that address polysemy. Because of the scope of this work, we discuss briefly the WMD and the SCM in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Mover's Distance</head><p>The Word Mover's Distance (WMD) <ref type="bibr" target="#b16">(Kusner et al. 2015)</ref> uses network flows to find the optimal transport between VSM document vectors. The distance of two document vectors x and y is the following:</p><formula xml:id="formula_0">WMD(x, y) = minimum cumulative cost ∑ i, j f i j c i j of a flow F = ( f i j ) subject to F ≥ 0, ∑ j f i j = (x i ) β ,<label>(2)</label></formula><p>where the cost c i j is the Euclidean distance of embeddings for words i and j. We use the implementation in PyEMD <ref type="bibr">Werman 2008, 2009</ref>) with the best known average time complexity O(p 3 xy log p xy ), where p xy is the number of unique words in x and y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Soft Cosine Measure</head><p>The soft VSM <ref type="bibr" target="#b39">(Sidorov et al. 2014;</ref><ref type="bibr" target="#b26">Novotný 2018)</ref> assumes that document vectors are represented in a nonorthogonal normalized basis β. In the soft VSM, basis vectors of similar words are close and the cosine similarity of two document vectors x and y is the Soft Cosine Measure (SCM):</p><formula xml:id="formula_1">SCM(x, y) = x/ x 2 , y/ y 2 , where x, y = (x) β T S(y) β , and S is a word similarity matrix.<label>(3)</label></formula><p>We define the word similarity matrix S like Charlet and Damnati <ref type="formula" target="#formula_0">(2017)</ref>: s i j = max(t, e i / e i 2 , e j / e j 2 ) o , where e i and e j are the embeddings for words i and j, and o and t are free parameters. We use the implementation in the similarities.termsim module of Gensim (Řehůřek and Sojka 2010). The worst-case time complexity of the SCM is O(p x p y ), where p x is the number of unique words in x and p y is the number of unique words in y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Word Embedding Regularization</head><p>The Continuous Bag-of-Words Model (CBOW) <ref type="bibr" target="#b25">(Mikolov et al. 2013</ref>) is a neural network language model that predicts the center word from context words. The CBOW with negative sampling minimizes the following loss function:</p><formula xml:id="formula_2">J(u o ,v c ) = − log σ ( u o ,v c ) − k ∑ i=1 log σ (− u i ,v c ) , wherev c = 1 2w ∑ −w+i≤i≤w+o,i =o v i ,<label>(4)</label></formula><p>u o is the vector of a center word with corpus position o, v i is the vector of a context word with corpus position i, and the window size w and the number of negative samples k are free parameters. Word embeddings are the sum of center word vectors and context word vectors. To improve the properties of word embeddings and the task performance of the WMD and the SCM, we apply two regularization techniques to CBOW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantization</head><p>Following the approach of Lam (2018), we quantize the center word vector u o and the context word vector v i during the forward and backward propagation stages of the training: a quantized center word vector u o = 1 /3 · sign(u o ) and a quantized context word vector v i = 1 /3 · sign(v i ). <ref type="formula">(5)</ref> Since the quantization function is non-differentiable at certain points, we use Hinton's straight-through estimator (Hinton 2012, Lecture 15b) as the gradient:</p><formula xml:id="formula_3">∇( 1 /3 · sign) = ∇I,</formula><p>where ∇ is the gradient operator and I is the identity function.</p><p>Lam shows that quantization reduces the storage and memory cost and improves the performance of word embeddings on the word analogy and word similarity tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Orthogonalization</head><p>Novotný <ref type="formula" target="#formula_0">(2018)</ref> shows that producing a sparse word similarity matrix S that stores at most C largest values from every column of S reduces the worst-case time complexity of the SCM to O(p x ), where p x is the number of unique words in a document vector x.</p><p>Novotný also claims that S improves the performance of the soft VSM on the question answering task and describes a greedy algorithm for producing S , which we will refer to as the orthogonalization algorithm. The orthogonalization algorithm has three boolean parameters: Sym, Dom, and Idf. Sym and Dom make S symmetric and strictly diagonally dominant. Idf processes columns of S in descending order of inverse document frequency <ref type="bibr" target="#b34">(Robertson 2004)</ref>:</p><formula xml:id="formula_5">inverse document frequency of word w = − log 2 P(w | D) = log 2 |D| |{d ∈ D | w ∈ d}|</formula><p>, where D are documents. <ref type="formula">(7)</ref> In our experiment, we compute the SCM directly from the word similarity matrix S , see Equation <ref type="formula" target="#formula_1">(3)</ref>. However, actual word embeddings must be extracted for many NLP tasks. Novotný shows that the word similarity matrix S can be decomposed using Cholesky factorization. We will now define orthogonalized word embeddings and we will show that the Cholesky factors of S are in fact orthogonalized word embeddings. Notice how the cluster of numerals from E is separated in E due to the parameter value Idf = , which makes common words more likely to be mutually orthogonal.</p><p>Definition 1 (Orthogonalized word embeddings) Let E, E be real matrices with |V | rows, where V is a vocabulary of words. Then E are orthogonalized word embeddings from E, which we denote E ≤ ⊥ E, iff for all i, j = 1, 2, . . . , |V | it holds that e i , e j = 0 =⇒ e i , e j = e i , e j , where e k and e k denote the k-th rows of E and E .</p><p>Theorem 1 Let E be a real matrix with |V | rows, where V is a vocabulary of words, and for all k = 1, 2, . . . , |V | it holds that e k 2 = 1. Let S be a word similarity matrix constructed from E with the parameter values t = −1 and o = 1 as described in Section 3.2. Let S be a word similarity matrix produced from S using the orthogonalization algorithm with the parameter values Sym = and Dom = . Let E be the Cholesky factor of S . Then E ≤ ⊥ E.</p><p>Proof With the parameter values Sym = , Dom = , S is symmetric and strictly diagonally dominant, and therefore also positive definite. The symmetric positive definite matrix S has a unique Cholesky factorization of the form S = E (E ) T . Therefore, the Cholesky factor E exists and is uniquely determined.</p><p>From S = E (E ) T , we have that for all i, j = 1, 2, . . . , |V | such that the sparse matrix S does not contain the value s i j it holds that s i j = e i , e j = 0. Since the implication in the theorem only applies when e i , e j = 0, we do not need to consider this case.</p><p>From S = E (E ) T , o = 1,t = −1, and e k 2 = 1, we have that for all i, j = 1, 2, . . . , |V | such that the sparse matrix S contains the value s i j , it holds that e i , e j = s i j = s i j = max(t, e i / e i 2 , e j / e j 2 ) o = e i , e j . <ref type="figure">Figure 1</ref> shows the extraction of orthogonalized word embeddings E from E: From E, we construct the dense word similarity matrix S and from S, we produce the sparse word similarity matrix S through orthogonalization. From S , we produce the orthogonalized embeddings E through Cholesky factorization.</p><p>With a large vocabulary V , a |V | × |V | dense matrix S may not fit in the main memory, and we produce the sparse matrix S directly from E. Similarly, a |V | × |V | dense matrix E may also not fit in the main memory, and we use sparse Cholesky factorization to produce a sparse matrix E instead. If dense word embeddings are required, we use dimensionality reduction on E to produce a |V | × D dense matrix, where D is the number of dimensions.</p><p>With the parameter value Idf = , words with small inverse document frequency, i.e. common words such as numerals, prepositions, and articles, are more likely to be mutually orthogonal (i.e. e i , e j = 0) than rare words. This is why in <ref type="figure">Figure 1</ref>, the numerals form a cluster in the non-regularized word embeddings E, but they are separated in orthogonalized word embeddings E .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>The experiment was conducted using six standard text classification datasets by employing both the WMD and the SCM with a k Nearest Neighbor (kNN) classifier using both regularized and non-regularized word embeddings. First, we describe briefly our datasets, then our experimental steps. Our experimental code is available online. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>In our experiment, we used the following six standard text classification datasets:</p><p>BBCSPORT The BBCSPORT dataset <ref type="bibr" target="#b10">(Greene and Cunningham 2006)</ref> consists of 737 sport news articles from the BBC sport website in five topical areas: athletics, cricket, football, rugby, and tennis. The period of coverage was during 2004-2005.</p><p>TWITTER The TWITTER dataset (Sanders 2011) consists of 5,513 tweets hand-classified into one of four topics: Apple, Google, Twitter, and Microsoft. The sentiment of every tweet was also hand-classified as either Positive, Neutral, Negative, or Irrelevant.</p><p>OHSUMED The OHSUMED dataset <ref type="bibr" target="#b11">(Hersh et al. 1994</ref>) is a set of 348,566 references spanning 1987-1991 from the MEDLINE bibliographic database of important, peer-reviewed medical literature maintained by the National Library of Medicine (NLM). While the majority of references are to journal articles, there are also a small number of references to letters of the editor, conference proceedings, and other reports. Each reference contains human-assigned subject headings from the 17,000-term Medical Subject Headings (MeSH) vocabulary.</p><p>REUTERS The documents in the REUTERS-21578 collection <ref type="bibr" target="#b21">(Lewis 1997</ref>) appeared on the Reuters newswire in 1987. The documents were assembled and indexed with categories by the personnel of Reuters Ltd. The collection is contained in 22 SGML files. Each of the first 21 files (reut2-000.sgm through reut2-020.sgm) contains 1,000 documents, while the last one (reut2-021.sgm) contains only 578 documents. 20NEWS The 20NEWS dataset <ref type="bibr" target="#b20">(Lang 1995</ref>) is a collection of 18,828 Usenet newsgroup messages partitioned across 20 newsgroups with different topics. The collection has become popular for experiments in text classification and text clustering. For all a j ∈ A, compute the SCM / WMD between a j and b i . 3:</p><p>Choose the k nearest neighbors of b i in A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Assign the majority class of the k nearest neighbors to l i . 5: end for For TWITTER, we use 5,116 out of 5,513 tweets due to unavailability, we restrict our experiment to the Positive, Neutral, and Negative classes, and we subsample the dataset to 3,108 tweets like <ref type="bibr" target="#b16">Kusner et al. (2015)</ref>. For OHSUMED, we use the 34,389 abstracts related to cardiovascular diseases <ref type="bibr" target="#b14">(Joachims 1998</ref>) out of 50,216 and we restrict our experiment to abstracts with a single class label from the first 10 classes out of 23. In the case of REUTERS, we use the R8 subset <ref type="bibr" target="#b4">(Cardoso-Cachopo 2007)</ref>. For AMAZON, we use 5-core reviews from the Books, CDs and Vinyl, Electronics, and Home and Kitchen classes. We preprocess the datasets by lower-casing the text and by tokenizing to longest non-empty sequences of alphanumeric characters that contain at least one alphabetical character. We do not remove stop words or rare words, only words without embeddings. We split each dataset into train and test subsets using either a standard split (for REUTERS and 20NEWS) or following the split size of <ref type="bibr" target="#b16">Kusner et al. (2015)</ref>. See <ref type="table" target="#tab_1">Table 1</ref> for statistics of the preprocessed datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training and Regularization of Word Embeddings</head><p>Using Word2Vec, we train the CBOW on the first 100 MiB of the English Wikipedia (Mahoney 2011) using the same parameters as <ref type="bibr">Lam (2018, Section 4</ref>.3) and 10 training epochs. We use quantized word embeddings in 1,000 dimensions and non-quantized word embeddings in 200 dimensions to achieve comparable performance on the word analogy task. <ref type="bibr" target="#b19">(Lam 2018)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Nearest Neighbor Classification</head><p>We use the VSM with uniform word frequency weighting, also known as the bag of words (BOW), as our baseline. For the SCM, we use the double-logarithm inverse collection frequency word weighting (the SMART dtb weighting scheme) as suggested by <ref type="bibr">Singhal et al. (2001, Table 1</ref>). For the WMD, we use BOW document document vectors like <ref type="bibr" target="#b16">Kusner et al. (2015)</ref>.</p><p>We tune the parameters o ∈ {1, 2, 3, 4} and t ∈ {0, ± 1 /2, 1} of the SCM, the parameter s ∈ {0.0, 0.1, . . . , 1.0} of the SMART dtb weighting scheme, the parameter k ∈ {1, 3, . . . , 19} of the kNN, and the parameters C ∈ {100, 200, 400, 800}, and Idf, Sym, Dom ∈ {, } of the orthogonalization. For each dataset, we hold out 20% of the train set for validation, and we use grid search to find the optimal parameter values. To classify each sample in the test set, we follow the procedure presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Significance Testing</head><p>We use the method of <ref type="bibr" target="#b0">Agresti and Coull (1998)</ref> to construct 95% confidence intervals for the kNN test error. For every dataset, we use Student's t-test at 95% confidence level with q-values <ref type="bibr" target="#b2">(Benjamini and Hochberg 1995)</ref> for all combinations of document similarities and word embedding regularization techniques to find significant differences in kNN test error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion of Results</head><p>Our results are shown in figures 2-11 and in Table 2. In the following subsections, we will discuss the individual results and how they are related.  <ref type="figure">Fig. 3 kNN (k = 1)</ref> confusion matrices and t-SNE document visualizations for the soft VSM with non-regularized (21.82% test error, top) and regularized (2.27% test error, bottom) word embeddings on the BBCSPORT dataset. See https://mir.fi.muni.cz/bbcsport-nonregularized and https://mir.fi.muni.cz/bbcsport-regularized for interactive three-dimensional t-SNE document visualisations.  <ref type="figure">Fig. 4 kNN (k = 9 and 19</ref>) confusion matrices and t-SNE document visualizations for the soft VSM with non-regularized (10.19% test error, top) and regularized (7.22% test error, bottom) word embeddings on the REUTERS dataset. See https://mir.fi.muni.cz/reuters-nonregularized and https://mir.fi.muni.cz/reuters-regularized for interactive three-dimensional t-SNE document visualisations.  <ref type="figure" target="#fig_6">Fig. 5 kNN (k = 15 and 11</ref>) confusion matrices and t-SNE document visualizations for the soft VSM with non-regularized (10.04% test error, top) and regularized (6.33% test error, bottom) word embeddings on the AMAZON dataset. See https://mir.fi.muni.cz/amazon-nonregularized and https://mir.fi.muni.cz/amazon-regularized for interactive three-dimensional t-SNE document visualisations.  <ref type="figure">Fig. 7 kNN (k = 1)</ref>   <ref type="figure">Figure 8</ref> shows 95% interval estimates for the kNN test error. All differences are significant, except for the second and fourth results from the left on the BBCSPORT and TWITTER datasets, the sixth and seventh results from the left on the BBCSPORT and OHSUMED datasets, and the fourth and sixth results from the left on the TWITTER dataset. Although most differences in the kNN test error are statistically significant on the TWITTER dataset, they are relatively minor compared to other datasets. This is because of two reasons: (1) TWITTER is a sentiment analysis dataset, and (2) words with opposite sentiment often appear in similar sentences. Since word embeddings are similar for words that appear in similar sentences, embeddings for words with opposite sentiment are often similar. For example, the embeddings for the words good and bad are mutual nearest neighbors with cosine similarity 0.58 for non-quantized and 0.4 for quantized word embeddings. As a result, word embeddings don't help us separate positive and negative documents. Using a better measure of sentiment in the word similarity matrix S and in the flow cost c i j would improve the task performance of the soft VSM and the WMD. <ref type="figure" target="#fig_2">Figures 2-7</ref> show confusion matrices and t-SNE document visualizations <ref type="bibr" target="#b22">(Maaten and Hinton 2008)</ref> for the soft VSM with non-regularized word embeddings and for the soft VSM with orthogonalized and quantized word embeddings.</p><p>In <ref type="figure" target="#fig_2">Figure 2</ref>, we see that with non-regularized word embeddings, we predict most documents as class C04, followed by classes C10 and C06. When a document representation is uniformly random, then the most common classes in a dataset are most likely to be predicted by the kNN classifier. In the OHSUMED dataset, 2,835 documents belong to the most common class C04, 1,711 documents belong to the second most common class C10, and 1,246 documents belong to the third most common class C06. In contrast to the almost random representations of the soft VSM with non-regularized word embeddings, orthogonalized word embeddings make the true class label much easier to predict. We hypothesize that this is because OHSUMED is a medical dataset. Medical terms are highly specific, so when we search for documents containing similar words, we need to restrict ourselves only to highly similar words, which is one of the effects of using orthogonalized word embeddings.  In <ref type="figure">Figure 4</ref>, we see that with non-regularized word embeddings, most documents from the class Grain are misclassified as the more common class Crude. With regularized word embeddings, the classes are separated. We hypothesize that this is because both grain and crude oil are commodities, which makes the documents from both classes contain many similar words. The classes will become indistinguishable unless we restrict ourselves only to a subset of the similar words, which is one of the effects of using orthogonalized word embeddings.</p><p>In <ref type="figure" target="#fig_5">Figure 6</ref>, we see that with non-regularized word embeddings, messages are often misclassified as a different newsgroup in the same Usenet hierarchy. We can see that the Usenet hierarchies comp.*, rec.*, and rec.sport.* form visible clusters in both the t-SNE document visualization and in the confusion matrix. In <ref type="figure">Figure 7</ref>, we see that with regularized word embeddings, the clusters are separated. We hypothesize that this is because newsgroups in a Usenet hierarchy share a common topic and similar terminology. The terminology of the newsgroups will become difficult to separate unless only highly specific words are allowed to be considered similar, which is one of the effects of using orthogonalized word embeddings. <ref type="figure">Figure 10</ref> shows the average kNN test error ratio between the document similarities and the BOW baseline. This ratio is the lowest for the soft VSM with orthogonalized word embeddings, which achieves 48.86% of the average BOW kNN test error. The average test error ratio between the soft VSM with regularized word embeddings and the soft VSM with non-regularized word embeddings is 60.57%, which is a 39.43% reduction in the average kNN test error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Average Task Performance</head><p>Unlike the soft VSM, the WMD does not benefit from word embedding quantization. This is because of two reasons: (1) the soft VSM takes into account the similarity between all words in two documents, whereas the WMD only considers the most similar word pairs, and (2) non-quantized word embeddings are biased towards positive similarity, see <ref type="figure" target="#fig_6">Figure 11</ref>. With non-quantized word embeddings, embeddings of unrelated words have positive cosine similarity, which makes dissimilar documents less separable. With quantized embeddings, unrelated words have negative cosine similarity, which improves separability and reduces kNN test error. The WMD is unaffected by the bias in non-quantized word embeddings, and the reduced precision of quantized word embeddings increases kNN test error. <ref type="figure">Figure 9</ref> shows the average document processing speed using a single Intel Xeon X7560 2.26 GHz core. Although the orthogonalization reduces the worst-case time complexity of the soft VSM from quadratic to linear, it also makes the word similarity matrix sparse, and performing sparse instead of dense matrix operations causes a 2.73× slowdown compared to the soft VSM with non-orthogonalized word embeddings. Quantization causes a 1.8× slowdown, which is due to the 5× increase in the word embedding dimensionality, since we use 1000-dimensional quantized word embeddings and only 200-dimensional non-quantized word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Document Processing Speed</head><p>The super-cubic average time complexity of the WMD results in an average 819,496× slowdown compared to the soft VSM with orthogonalized and quantized word embeddings, and a 1,505,386× slowdown on the 20NEWS dataset, which has a large average number of unique words in a document. Although <ref type="bibr" target="#b16">Kusner et al. (2015)</ref> report up to 110× speed-up using an approximate variant of the WMD (the WCD), this still results in an average 7,450× slowdown, and a 13,685× slowdown on the 20NEWS dataset. <ref type="table" target="#tab_3">Table 2</ref> shows the optimal parameter values for the soft VSM with orthogonalized word embeddings. The most common parameter value Idf = shows that it is important to store the nearest neighbors of rare words in the word similarity matrix S . The most common parameter values C = 100, o = 4, t = −1, Sym = , and Dom = show that strong orthogonalization, which makes most values in S zero or close to zero, gives the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Parameter Optimization</head><p>Due to the low document processing speed of the WMD and the number of orthogonalization parameters, using parameter optimization on the WMD with regularized embeddings is computationally intractable. Therefore, we do not report results for the WMD with regularized embeddings in figures 8-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Word embeddings achieve state-of-the-art results on several NLP tasks, predominantly at the sentence level, but overfitting is a major issue, especially when applied at the document level with a large number of words. We have shown that regularization of word embeddings significantly improves their performance not only on the word analogy, word similarity, and semantic text similarity tasks, but also on the text classification task.</p><p>We further show that the most effective word embedding regularization technique is orthogonalization and we prove a connection between orthogonalization, Cholesky factorization and orthogonalized word embeddings. With word embedding orthogonalization, the task performance of the soft VSM exceeds the WMD, an earlier known state-of-the-art document distance measure, while being several orders of magnitude faster. This is an important step in bringing application of word embeddings from supercomputers to mobile and embedded systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>T-SNE visualizations of non-regularized word embeddings E (top left) and orthogonalized word embeddings E (top right) for the 40 most common words on the first 100 MiB of the English Wikipedia, and images of the word similarity matrices S (bottom left) and S (bottom right). Word similarity matrix construction uses the parameters o = 1 and t = −1, and orthogonalization uses the parameter values Sym = , Dom = , and Idf = .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>AMAZON</head><label></label><figDesc>The AMAZON dataset<ref type="bibr" target="#b24">(McAuley et al. 2015)</ref> contains 142.8 million product reviews from Amazon spanning 1996-2014. Each product belongs to one of 24 categories, which include Books, Cell Phones &amp; Accessories, Clothing, Shoes &amp; Jewelry, Digital Music, and Electronics, among others. The 5-core subset of the AMAZON dataset contains only those products and users with at least five reviews.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>kNN (k = 9) confusion matrices and t-SNE document visualizations for the soft VSM with non-regularized (50.71% test error, top) and regularized (24.14% test error, bottom) word embeddings on the OHSUMED dataset. See https://mir.fi.muni.cz/ohsumed-nonregularized and https://mir.fi.muni.cz/ohsumed-regularized for interactive three-dimensional t-SNE document visualisations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>kNN (k = 1) confusion matrix (top) and t-SNE document visualization (bottom) for the soft VSM with non-regularized (42.53% test error) word embeddings on the 20NEWS dataset. Notice how the Usenet hierarchies comp.*, rec.*, and rec.sport.* form visible clusters. See https://mir.fi.muni.cz/20news-nonregularized for an interactive three-dimensional t-SNE document visualisation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11</head><label>11</label><figDesc>Histograms of word embedding cosine similarity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 The kNN classifier for the WMD and the SCM with regularized and non-regularized word embeddings Input: Training data A, test data B, neighborhood size k Output: Class labels for the test data, L 1: foreach b i ∈ B do 2:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Statistics of the datasets used for evaluation, where E[p] is the average number of unique words in a document, and |Y | is the number of document classes</figDesc><table><row><cell>Dataset name</cell><cell>Train set size</cell><cell>Test set size</cell><cell cols="2">E[p] |Y |</cell></row><row><cell>BBCSPORT</cell><cell>517</cell><cell cols="2">220 181.0</cell><cell>5</cell></row><row><cell>TWITTER</cell><cell>2,176</cell><cell>932</cell><cell>13.7</cell><cell>3</cell></row><row><cell>OHSUMED</cell><cell>3,999</cell><cell>5,153</cell><cell>89.4</cell><cell>10</cell></row><row><cell>REUTERS</cell><cell>5,485</cell><cell>2,189</cell><cell>56.0</cell><cell>8</cell></row><row><cell>AMAZON</cell><cell>5,600</cell><cell>2,400</cell><cell>86.3</cell><cell>4</cell></row><row><cell>20NEWS</cell><cell>11,293</cell><cell cols="2">7,528 145.0</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>confusion matrix (top) and t-SNE document visualization (bottom) for the soft VSM with regularized (29.97% test error) word embeddings on the 20NEWS dataset. See https://mir.fi.muni.cz/20news-regularized for an interactive three-dimensional t-SNE document visualisation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>WMD</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">BoW</cell><cell>Soft VSM</cell><cell></cell><cell></cell><cell cols="3">WMD (quant.)</cell><cell>Soft VSM (quan.)</cell><cell cols="3">Soft VSM (quan., ortho.)</cell><cell>Soft VSM (ortho.)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>49</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44</cell><cell></cell><cell></cell><cell></cell><cell>44</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>43</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40 40</cell></row><row><cell>test error %</cell><cell>40</cell><cell>23</cell><cell>22</cell><cell>22</cell><cell>32</cell><cell>31</cell><cell>33</cell><cell>31</cell><cell>31 31 31</cell><cell>24 24</cell><cell></cell><cell>29</cell><cell></cell><cell>30</cell><cell>30</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">BBCSPORT 11 2.3 2.3 9.1 Fig. 8 95% interval estimates for the kNN test error on six text classification datasets TWITTER OHSUMED REUTERS 0 10 5.9 9.4 5.3 8.8 7.2 7.4 0.1 1 10 100 10 10 10 10 10 10 doc. similarities per second 3,434,694 1,462,366 0.8898 811,964 0.4720 386,825 536,641 8 0 50 avg. test error % w.r.t. BoW 100</cell><cell>10 80.66</cell><cell>AMAZON 9.0 11 6.3 6.6 14 76.02 75.42</cell><cell>70.18</cell><cell>20NEWS 48.90</cell><cell>48.86</cell></row><row><cell cols="11">Fig. 9 Average document processing speed on one Intel Xeon X7560 core</cell><cell cols="4">Fig. 10 Average kNN test error compared to the BOW</cell></row><row><cell cols="10">6.1 Task Performance on Individual Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Optimal parameter values for the soft VSM with orthogonalized word embeddings. Most common parameter values are bold.</figDesc><table><row><cell>Dataset name</cell><cell>o</cell><cell>s</cell><cell>t</cell><cell>C</cell><cell>k</cell><cell>Idf</cell><cell>Sym Dom</cell></row><row><cell>BBCSPORT</cell><cell cols="4">2 0 −1 100</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>TWITTER</cell><cell cols="5">2 0 −1 400 13</cell><cell></cell><cell></cell></row><row><cell>OHSUMED</cell><cell cols="5">4 0 −1 200 11</cell><cell></cell><cell></cell></row><row><cell>REUTERS</cell><cell cols="5">4 0 −1 100 19</cell><cell></cell><cell></cell></row><row><cell>AMAZON</cell><cell cols="5">4 0 −1 100 17</cell><cell></cell><cell></cell></row><row><cell>20NEWS</cell><cell cols="4">3 0 −1 100</cell><cell>1</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See https://github.com/MIR-MU/regularized-embeddings.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements The authors are grateful to their colleagues who helped make the language precise and engaging, the figures crisp and accessible, and the bibliographical references valid and consistent.</p><p>Funding First author's work was graciously funded by the South Moravian Centre for International Mobility as a part of the Brno Ph.D. talent project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest</head><p>The authors declare that they have no conflict of interest.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Approximate is better than &quot;exact&quot; for interval estimation of binomial proportions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Coull</surname></persName>
		</author>
		<idno type="DOI">DOI10.2307/2685469</idno>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="126" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=944919.944966" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate: A practical and powerful approach to multiple testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2346101" />
	</analytic>
	<monogr>
		<title level="j">JRSSB</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">1 regularization of word embeddings for multi-word expression identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berend</surname></persName>
		</author>
		<idno type="DOI">DOI10.14232/actacyb.23.3.2018.5</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Cybernetica</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="801" to="813" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving methods for single-label text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardoso-Cachopo</surname></persName>
		</author>
		<ptr target="http://web.ist.utl.pt/~acardoso/docs/2007-phd-thesis.pdf" />
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="volume">23</biblScope>
			<pubPlace>Portugal, URL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Instituto Superior Técnico, University of Lisbon</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SimBow at SemEval-2017 task 3: Soft-cosine semantic similarity between questions for community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Charlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Damnati</surname></persName>
		</author>
		<idno type="DOI">DOI10.18653/v1/S17-2051</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="315" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045118.3045361" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning, JMLR.org</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning, JMLR.org<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
	<note>ICML &apos;15</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1602.02830" />
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or −1</title>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<idno type="DOI">10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9</idno>
		<ptr target="https://doi.org/10.1002/(SICI)1097-4571" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
	<note>6%3C391::AID-ASI1%3E3.0.CO;2-9</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining distributed vector representations for words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
		<idno type="DOI">DOI10.3115/v1/W15-1513</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="95" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Practical solutions to the problem of diagonal dominance in kernel document clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<idno type="DOI">DOI10.1145/1143844.1143892</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings 23rd International Conference on Machine learning (ICML &apos;06)</title>
		<meeting>23rd International Conference on Machine learning (ICML &apos;06)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">OHSUMED: An interactive retrieval evaluation and new large test collection for research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hickam</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=188490.188557" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;94)</title>
		<meeting>the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;94)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/~hinton/coursera_lectures.html" />
		<imprint>
			<date type="published" when="2012-10-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://www.dlworkshop.org/65.pdf" />
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">DOI10.1007/BFb0026683</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Machine Learning (ECML)</title>
		<meeting>European Conference on Machine Learning (ECML)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.03651" />
		<title level="m">FastText.zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://dl.acm.org/citation.cfm?id=3045118.3045221" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Query expansion using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kuzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
		<idno type="DOI">DOI10.1145/2983323.2983876</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Information and Knowledge Management (CIKM), ACM</title>
		<meeting>the 25th ACM International Conference on Information and Knowledge Management (CIKM), ACM</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1929" to="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Re-embedding words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P13-2087" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="489" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.05651" />
		<title level="m">Word2Bits -quantized word vectors</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">NewsWeeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">DOI10.1016/B978-1-55860-377-6.50048-7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Machine Learning</title>
		<meeting>the 12th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reuters-21578 text categorization test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="http://www.daviddlewis.com/resources/testcollections/reuters21578" />
	</analytic>
	<monogr>
		<title level="j">AT&amp;T Labs-Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="1997-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">About the test data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://mattmahoney.net/dc/textdata.html" />
		<imprint>
			<date type="published" when="2011-10" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<idno type="DOI">DOI10.1145/2766462.2767755</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, ACM</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, ACM</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1301.3781v3" />
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Implementation notes for the soft cosine measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Novotný</surname></persName>
		</author>
		<idno type="DOI">DOI10.1145/3269206.3269317</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 27th ACM International Conference on Information and Knowledge Management (CIKM &apos;18), Association of Computing Machinery</title>
		<meeting>27th ACM International Conference on Information and Knowledge Management (CIKM &apos;18), Association of Computing Machinery</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1639" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A linear time histogram metric for improved sift matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
		<idno type="DOI">DOI10.1007/978-3-540-88690-7_37</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ECML: Part III</title>
		<meeting>the 10th ECML: Part III<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="495" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and robust earth mover&apos;s distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
		<idno type="DOI">DOI10.1109/ICCV.2009.5459199</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A comparative study on regularization strategies for embedding-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Z</forename></persName>
		</author>
		<idno type="DOI">DOI10.18653/v1/D15-1252</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2106" to="2111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">DOI10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2014 conference</title>
		<meeting>the EMNLP 2014 conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">DOI10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">When and why are pre-trained word embeddings useful for neural machine translation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Sachan</surname></persName>
		</author>
		<ptr target="https://arXiv.org/abs/1804.06323v2" />
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Software framework for topic modelling with large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<idno type="DOI">DOI10.13140/2.1.2393.1847</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding inverse document frequency: on theoretical arguments for IDF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">DOI10.1108/00220410410560582</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="503" to="520" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<idno type="DOI">DOI10.1016/0306-4573(88)90021-0</idno>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sanders-twitter sentiment corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Sanders</surname></persName>
		</author>
		<ptr target="http://www.sananalytics.com/lab/twitter-sentiment/" />
	</analytic>
	<monogr>
		<title level="j">Sanders Analytics LLC</title>
		<imprint>
			<biblScope unit="volume">242</biblScope>
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Compression of neural machine translation models via pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">DOI10.18653/v1/K16-1029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="291" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Compressing word embeddings via deep compositional code learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.01068" />
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Soft similarity and soft cosine measure: Similarity of features in vector space model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<idno type="DOI">DOI10.13053/cys-18-3-2043</idno>
	</analytic>
	<monogr>
		<title level="j">CyS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="504" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modern information retrieval: A brief overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="35" to="43" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning word representations with regularization from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">DOI10.18653/v1/K17-1016</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2627435.2670313" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sparse word embeddings using 1 regularized online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3060832.3061029" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI &apos;16)</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence (IJCAI &apos;16)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2915" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Word Mover&apos;s Embedding: From Word2Vec to Document Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ieh</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Witbrock</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1482" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4524" to="4534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A simple regularization-based algorithm for learning cross-domain word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">DOI10.18653/v1/D17-1312</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2898" to="2904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Integrating and evaluating neural word embeddings in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bruza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<idno>DOI 10.1145/ 2838931.2838936</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Australasian Document Computing Symposium (ADCS), ACM</title>
		<meeting>the 20th Australasian Document Computing Symposium (ADCS), ACM<address><addrLine>Parramatta, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
