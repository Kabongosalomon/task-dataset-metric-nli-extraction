<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Word Segmentation with Rich Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 30 -August 4, 2017. July 30 -August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
							<email>feidong@mymail.sutd.edu.sgyuezhang@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Word Segmentation with Rich Pretraining</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="839" to="849"/>
							<date type="published">July 30 -August 4, 2017. July 30 -August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/P17-1078</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information , such as punctuation, automatic seg-mentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submod-ule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning ( <ref type="bibr" target="#b45">Zheng et al., 2013;</ref><ref type="bibr" target="#b16">Pei et al., 2014;</ref><ref type="bibr" target="#b14">Morita et al., 2015;</ref><ref type="bibr" target="#b4">Chen et al., 2015b;</ref><ref type="bibr" target="#b2">Cai and Zhao, 2016;</ref><ref type="bibr" target="#b39">Zhang et al., 2016b)</ref>. Neural network models have been exploited due to their strength in non-sparse representation learning and non-linear power in feature combination, which have led to advances in many NLP tasks. So far, neural word segmentors have given comparable accuracies to the best statictical models.</p><p>With respect to non-sparse representation, character embeddings have been exploited as a foundation of neural word segmentors. They serve to reduce sparsity of character ngrams, allowing, for example, "(cat) (lie) (in) (corner)" to be connected with "(dog) (sit) (in) * Equal contribution.</p><p>(corner)" ( <ref type="bibr" target="#b45">Zheng et al., 2013)</ref>, which is infeasible by using sparse one-hot character features. In addition to character embeddings, distributed representations of character bigrams <ref type="bibr" target="#b16">Pei et al., 2014</ref>) and words <ref type="bibr" target="#b14">(Morita et al., 2015;</ref><ref type="bibr" target="#b39">Zhang et al., 2016b</ref>) have also been shown to improve segmentation accuracies.</p><p>With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows ( <ref type="bibr" target="#b45">Zheng et al., 2013;</ref><ref type="bibr" target="#b16">Pei et al., 2014;</ref><ref type="bibr" target="#b3">Chen et al., 2015a</ref>), as well as LSTMs on characters <ref type="bibr" target="#b4">(Chen et al., 2015b;</ref><ref type="bibr" target="#b33">Xu and Sun, 2016</ref>) and words <ref type="bibr" target="#b14">(Morita et al., 2015;</ref><ref type="bibr" target="#b2">Cai and Zhao, 2016;</ref><ref type="bibr" target="#b39">Zhang et al., 2016b</ref>). For structured learning and inference, CRF has been used for character sequence labelling models ( <ref type="bibr" target="#b16">Pei et al., 2014;</ref><ref type="bibr" target="#b4">Chen et al., 2015b</ref>) and structural beam search has been used for word-based segmentors <ref type="bibr" target="#b2">(Cai and Zhao, 2016;</ref><ref type="bibr" target="#b39">Zhang et al., 2016b)</ref>.</p><p>Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing ( <ref type="bibr" target="#b0">Andor et al., 2016)</ref>. Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation ( <ref type="bibr" target="#b11">Li and Sun, 2009;</ref><ref type="bibr" target="#b24">Sun and Xu, 2011</ref>), and making use of selfpredictions ( <ref type="bibr" target="#b30">Wang et al., 2011;</ref><ref type="bibr" target="#b12">Liu and Zhang, 2012)</ref>. It has also utilised heterogenous annotations such as POS ( <ref type="bibr" target="#b15">Ng and Low, 2004;</ref><ref type="bibr" target="#b42">Zhang and Clark, 2008)</ref>   <ref type="bibr">[,]</ref> [] SEP state4 <ref type="bibr">[,,]</ref> [] APP state5 <ref type="bibr">[,,]</ref> [] APP state6 <ref type="bibr">[,,]</ref> [] SEP state7 <ref type="bibr">[,,,]</ref> [] APP state8 <ref type="bibr">[,,,]</ref> [ ] FIN state9 <ref type="bibr">[,,,,]</ref> φ [ ] -- <ref type="table">Table 1</ref>: A transition based word segmentation example.</p><p>standards <ref type="bibr" target="#b10">(Jiang et al., 2009</ref>). To our knowledge, such rich external information has not been systematically investigated for neural segmentation. We fill this gap by investigating rich external pretraining for neural segmentation. Following <ref type="bibr" target="#b2">Cai and Zhao (2016)</ref> and <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref>, we adopt a globally optimised beam-search framework for neural structured prediction ( <ref type="bibr" target="#b0">Andor et al., 2016;</ref><ref type="bibr" target="#b46">Zhou et al., 2015;</ref><ref type="bibr" target="#b31">Wiseman and Rush, 2016)</ref>, which allows word information to be modelled explicitly. Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data. We adopt a multi-task learning strategy <ref type="bibr" target="#b6">(Collobert et al., 2011</ref>), casting each external source of information as a auxiliary classification task, sharing a five-character window network. After pretraining, the character window network is used to initialize the corresponding module in our segmentor.</p><p>Results on 6 different benchmarks show that our method outperforms the best statistical and neural segmentation models consistently, giving the best reported results on 5 datasets in different domains and genres. Our implementation is based on LibN3L 1 ( <ref type="bibr" target="#b37">Zhang et al., 2016a</ref>). Code and models can be downloaded from http://gitHub. com/jiesutd/RichWordSegmentor</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Work on statistical word segmentation dates back to the 1990s ( <ref type="bibr" target="#b21">Sproat et al., 1996)</ref>. State-of-the-art approaches include character sequence labeling models ( <ref type="bibr" target="#b35">Xue et al., 2003</ref>) using CRFs (Peng et al., <ref type="bibr">1</ref> https://github.com/SUTDNLP/LibN3L 2004; <ref type="bibr" target="#b44">Zhao et al., 2006</ref>) and max-margin structured models leveraging word features ( <ref type="bibr" target="#b41">Zhang and Clark, 2007;</ref><ref type="bibr" target="#b23">Sun, 2010)</ref>. Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation ( <ref type="bibr" target="#b24">Sun and Xu, 2011;</ref><ref type="bibr" target="#b30">Wang et al., 2011;</ref><ref type="bibr" target="#b12">Liu and Zhang, 2012;</ref><ref type="bibr" target="#b36">Zhang et al., 2013</ref>). Our work belongs to recent neural word segmentation.</p><p>To our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, <ref type="bibr" target="#b24">Sun and Xu (2011)</ref> empirically studied the use of various external resources for enhancing a statistical segmentor, including character mutual information, access variety information, punctuation and other statistical information. Their baseline is similar to ours in the sense that both character and word contexts are considered. On the other hand, their model is statistical while ours is neural. Consequently, they integrate external knowledge as features, while we integrate it by shared network parameters. Our results show a similar degree of error reduction compared to theirs by using external data.</p><p>Our model inherits from previous findings on context representations, such as character windows ( <ref type="bibr" target="#b16">Pei et al., 2014;</ref><ref type="bibr" target="#b3">Chen et al., 2015a</ref>) and LSTMs ( <ref type="bibr" target="#b4">Chen et al., 2015b;</ref><ref type="bibr" target="#b33">Xu and Sun, 2016</ref>). Similar to <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref> and <ref type="bibr" target="#b2">Cai and Zhao (2016)</ref>, we use word context on top of character context. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our model is conceptually simpler and more modularised compared with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Our segmentor works incrementally from left to right, as the example shown in <ref type="table">Table 1</ref>. At each step, the state consists of a sequence of words that have been fully recognized, denoted as W = [w −k , w −k+1 , ..., w −1 ], a current partially recognized word P , and a sequence of next incoming characters, denoted as C = [c 0 , c 1 , ..., c m ], as shown in <ref type="figure">Figure 1</ref>. Given an input sentence, W and P are initialized to [ ] and φ, respectively, and C contains all the input characters. At each step, a decision is made on c 0 , either appending it as a part of P , or seperating it as the beginning of a new word. The incremental process repeats until C is empty and P is null again (C = [ ], P = φ). Formally, the process can be regarded as a state-transition process, where a state is a tuple S = W, P, C, and the transition actions include SEP (seperate) and APP (append), as shown by the deduction system in <ref type="figure">Figure 2</ref> 2 . In the figure, V denotes the score of a state, given by a neural network model. The score of the initial state (i.e. axiom) is 0, and the score of a non-axiom state is the sum of scores of all incremental decisions resulting in the state. Similar to <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref> and <ref type="bibr" target="#b2">Cai and Zhao (2016)</ref>, our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions.</p><p>Different from previous work, the structure of</p><formula xml:id="formula_0">Axiom: S = [ ], φ, C, V = 0 Goal: S = W, φ, [ ], V = V f inal SEP: S = W, P, c 0 |C, V S = W |P, c 0 , C, V = V + Score(S, SEP) APP: S = W, P, c 0 |C, V S = W, P ⊕ c 0 , C, V = V + Score(S, APP)</formula><p>Figure 2: Deduction system, where ⊕ denotes string concatenation.</p><p>our scoring network is shown in <ref type="figure">Figure 1</ref>. It consists of three main layers. On the bottom is a representation layer, which derives dense representations X W , X P and X C for W, P and C, respectively. We compare various distributed representations and neural network structures for learning X W , X P and X C , detailed in Section 3.1. On top of the representation layer, we use a hidden layer to merge X W , X P and X C into a single vector</p><formula xml:id="formula_1">h = tanh(W hW ·X W +W hP ·X P +W hC ·X C +b h ) (1)</formula><p>The hidden feature vector h is used to represent the state S = W, P, C, for calculating the scores of the next action. In particular, a linear output layer with two nodes is employed:</p><formula xml:id="formula_2">o = W o · h + b o (2)</formula><p>The first and second node of o represent the scores of SEP and APP given S, namely Score(S, SEP), Score(S, APP) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation Learning</head><p>Characters. We investigate two different approaches to encode incoming characters, namely a window approach and an LSTM approach. For the former, we follow prior methods ( <ref type="bibr" target="#b35">Xue et al., 2003;</ref><ref type="bibr" target="#b16">Pei et al., 2014</ref>), using five-character window <ref type="figure">Figure 3</ref>, a multi-layer perceptron (MLP) is employed to derive a five-character window vector D C from single-character vector rep-</p><formula xml:id="formula_3">[c −2 , c −1 , c 0 , c 1 , c 2 ] to represent incoming charac- ters. Shown in</formula><formula xml:id="formula_4">resentations V c −2 , V c −1 , V c 0 , V c 1 , V c 2 . D C = MLP([V c −2 ; V c −1 ; V c 0 ; V c 1 ; V c 2 ])<label>(3)</label></formula><p>For the latter, we follow recent work <ref type="bibr" target="#b4">(Chen et al., 2015b;</ref><ref type="bibr" target="#b39">Zhang et al., 2016b</ref>), using a bidirectional LSTM to encode input character sequence. <ref type="bibr">3</ref> In particular, the bi-directional LSTM</p><formula xml:id="formula_5">hidden vector [ ← − h C (c 0 ); − → h C (c 0 )</formula><p>] of the next incoming character c 0 is used to represent the coming characters [c 0 , c 1 , ...] given a state. Intuitively, a five-character window provides a local context from which the meaning of the middle character can be better disambiguated. LSTM, on the other hand, captures larger contexts, which can contain more useful clues for dismbiguation but also irrelevant information. It is therefore interesting to investigate a combination of their strengths, by first deriving a locally-disambiguated version of c 0 , and then feed it to LSTM for a globally disambiguated representation. Now with regard to the single-character vector representation V c i (i ∈ [−2, 2]), we follow previous work and consider both character embedding e c (c i ) and character-bigram embedding e b (c i , c i+1 ) , investigating the effect of each on the accuracies. When both e c (c i ) and e b (c i , c i+1 ) are utilized, the concatenated vector is taken as V c i . Partial Word. We take a very simple approach to representing the partial word P , by using the embedding vectors of its first and last characters, as well as the embedding of its length. Length embeddings are randomly initialized and then tuned in model training. X P has relatively less influence on the empirical segmentation accuracies.</p><formula xml:id="formula_6">X P = [e c (P [0]); e c (P [−1]); e l (LEN(P ))] (4)</formula><p>Word. Similar to the character case, we investigate two different approaches to encoding incoming characters, namely a window approach and an LSTM approach. For the former, we follow prior methods <ref type="bibr" target="#b41">(Zhang and Clark, 2007;</ref><ref type="bibr" target="#b23">Sun, 2010)</ref>, using the two-word window [w −2 , w −1 ] to represent recognized words. A hidden layer is employed to derive a two-word vector X W from single word embeddings e w (w −2 ) and e w (w −1 ).</p><formula xml:id="formula_7">X W = tanh(W w [e w (w −2 ); e w (w −1 )] + b w ) (5)</formula><p>For the latter, we follow <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref> and <ref type="bibr" target="#b2">Cai and Zhao (2016)</ref>, using an uni-directional LSTM on words that have been recognized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pretraining</head><p>Neural network models for NLP benefit from pretraining of word/character embeddings, learning distributed sementic information from large raw texts for reducing sparsity. The three basic elements in our neural segmentor, namely characters, character bigrams and words, can all be pretrained Figure 3: Shared character representation.</p><p>over large unsegmented data. We pretrain the fivecharacter window network in <ref type="figure">Figure 3</ref> as an unit, learning the MLP parameter together with character and bigram embeddings. We consider four types of commonly explored external data to this end, all of which have been studied for statistical word segmentation, but not for neural network segmentors. Raw Text. Although raw texts do not contain explicit word boundary information, statistics such as mutual information between consecutive characters can be useful features for guiding segmentation (Sun and Xu, 2011). For neural segmentation, these distributional statistics can be implicitly learned by pretraining character embeddings. We therefore consider a more explicit clue for pretraining our character window network, namely punctuations ( <ref type="bibr" target="#b11">Li and Sun, 2009</ref>). Punctuation can serve as a type of explicit markup ( <ref type="bibr" target="#b20">Spitkovsky et al., 2010)</ref>, indicating that the two characters on its left and right belong to two different words. We leverage this source of information by extracting character five-grams excluding punctuation from raw sentences, using them as inputs to classify whether there is punctuation before middle character. Denoting the resulting five character window as [c −2 , c −1 , c 0 , c 1 , c 2 ], the MLP in <ref type="figure">Figure 3</ref> is used to derive its representation D C , which is then fed to a softmax layer for binary classification:</p><formula xml:id="formula_8">P (punc) = softmax(W punc · D C + b punc ) (6)</formula><p>Here P (punc) indicates the probability of a punctuation mark existing before c 0 . Standard backpropagation training of the MLP in <ref type="figure">Figure 3</ref> can be done jointly with the training of W punc and b punc . After such training, the embedding V ci and MLP values can be used to initialize the corresponding parameters for D C in the main segmentor, before its training. Automatically Segmented Text. Large texts automatically segmented by a baseline segmentor can be used for self-training (Liu and Zhang, 2012) or deriving statistical features ( <ref type="bibr" target="#b30">Wang et al., 2011</ref>).</p><p>We adopt a simple strategy, taking automatically segmented text as silver data to pretrain the five-character window network. <ref type="figure">Given [c −2 , c −1 , c 0 , c 1 .c 2 ]</ref>, D C is derived using the MLP in <ref type="figure">Figure 3</ref>, and then used to classify the segmentation of c 0 into B(begining)/M(middle)/E(end)/S(single character word) labels.</p><formula xml:id="formula_9">P (silver) = softmax(W silv · D C + b silv ) (7)</formula><p>Here W silv and b silv are model parameters. Training can be done in the same way as training with punctuation. Heterogenous Training Data. Multiple segmentation corpora exist for Chinese, with different segmentation granularities. There has been investigation on leveraging two corpora under different annotation standards to improve statistical segmentation ( <ref type="bibr" target="#b10">Jiang et al., 2009)</ref>. We try to utilize heterogenous treebanks by taking an external treebank as labeled data, training a B/M/E/S classifier for the character windows network.</p><formula xml:id="formula_10">P (hete) = softmax(W hete · D C + b hete ) (8)</formula><p>POS Data. Previous research has shown that POS information is closely related to segmentation <ref type="bibr" target="#b15">(Ng and Low, 2004;</ref><ref type="bibr" target="#b42">Zhang and Clark, 2008)</ref>. We verify the utility of POS information for our segmentor by pretraining a classifier that predicts the POS on each character, according to the character window representation D C . In particular, given [c −2 , c −1 , c 0 , c 1 , c 2 ], the POS of the word that c 0 belongs to is used as the output.</p><formula xml:id="formula_11">P (pos) = softmax(W pos · D C + b pos )<label>(9)</label></formula><p>Multitask Learning. While each type of external training data can offer one source of segmentation information, different external data can be complimentary to each other. We aim to inject all sources of information into the character window representation D C by using it as a shared representation for different classification tasks. Neural model have been shown capable of doing multi-task learning via parameter sharing <ref type="bibr" target="#b6">(Collobert et al., 2011</ref>). Shown in <ref type="figure">Figure 3</ref>, in our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Training</head><p>Input : (x i , y i ) Parameters: Θ Process:</p><formula xml:id="formula_12">agenda ← (S = [ ], φ, X i , V = 0) for j in [0:LEN(X i )] do beam = []</formula><note type="other">forˆyforˆ forˆy in agenda dôdô y = ACTION(ˆ y, SEP) ADD(ˆ y , beam) ˆ y = ACTION(ˆ y, APP) ADD(ˆ y , beam) end agenda ← TOP(beam, B) if y i j / ∈ agenda thenˆy thenˆ thenˆy j = BESTIN(agenda) UPDATE(y i j , ˆ y j ,Θ) return end endˆy endˆ endˆy = BESTIN(agenda) UPDATE(y i , ˆ y,Θ) return case, the output layer for each task is independent, but the hidden layer D C and all layers below D C are shared.</note><p>For training with all sources above, we randomly sample sentences from the Punc./Autoseg/Heter./POS sources with the ratio of 10/1/1/1, for each sentence in punctuation corpus we take only 2 characters (character before and after the punctuation) as input instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoding and Training</head><p>To train the main segmentor, we adopt the global transition-based learning and beam-search strategy of <ref type="bibr" target="#b43">Zhang and Clark (2011)</ref>. For decoding, standard beam search is used, where the B best partial output hypotheses at each step are maintained in an agenda. Initially, the agenda contains only the start state. At each step, all hypotheses in the agenda are expanded, by applying all possible actions and B highest scored resulting hypotheses are used as the agenda for the next step.</p><p>For training, the same decoding process is applied to each training example (x i , y i ). At step j, if the gold-standard sequence of transition actions y i j falls out of the agenda, max-margin update is performed by taking the current best hypothesisˆy hypothesisˆ hypothesisˆy j in the beam as a negative example, and y i j as 843</p><p>Paramater Value Paramater Value α 0.01 size(e c ) 50 </p><formula xml:id="formula_13">λ 10 −8 size(e b ) 50 p 0.2 size(e w ) 50 η 0.2 size(e l ) 20 MLP layer 2 size(X C ) 150 beam B 8 size(X P ) 50 size(h) 200 size(X W ) 100</formula><formula xml:id="formula_14">l(ˆ y j , y i j ) = max((score(ˆ y j ) + η · δ(ˆ y j , y i j ) − score(y i j )), 0),<label>(10)</label></formula><p>where δ(ˆ y j , y i j ) is the number of incorrect local decisions inˆyinˆ inˆy j , and η controls the score margin.</p><p>The strategy above is early-update ( <ref type="bibr" target="#b5">Collins and Roark, 2004</ref>). On the other hand, if the goldstandard hypothesis does not fall out of the agenda until the full sentence has been segmented, a final update is made between the highest scored hypothesisˆypothesisˆ pothesisˆy (non-gold standard) in the agenda and the gold-standard y i , using exactly the same loss function. Pseudocode for the online learning algorithm is shown in Algorithm 1.</p><p>We use Adagrad ( <ref type="bibr" target="#b7">Duchi et al., 2011</ref>) to optimize model parameters, with an initial learning rate α. L2 regularization and dropout ( <ref type="bibr" target="#b22">Srivastava et al., 2014</ref>) on input are used to reduce overfitting, with a L2 weight λ and a dropout rate p. All the parameters in our model are randomly initialized to a value (−r, r), where r = 6.0 f an in +f anout <ref type="bibr">(Ben- gio, 2012</ref>). We fine-tune character and character bigram embeddings, but not word embeddings, acccording to <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Data. We use Chinese Treebank 6.0 (CTB6) ( <ref type="bibr" target="#b34">Xue et al., 2005</ref>) as our main dataset. Training, development and test set splits follow previous work ( <ref type="bibr" target="#b38">Zhang et al., 2014</ref>). In order to verify the robustness of our model, we additionally use SIGHAN 2005 bake-off ( <ref type="bibr" target="#b8">Emerson, 2005)</ref>    <ref type="bibr">4</ref> , automatically segmented using ZPar 0.6 off-theshelf ( <ref type="bibr" target="#b41">Zhang and Clark, 2007)</ref>, the statictics of which are shown in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>For pretraining character representations, we extract punctuation classification data from the Gigaword corpus, and use the word-based ZPar and a standard character-based CRF model ( <ref type="bibr" target="#b27">Tseng et al., 2005</ref>) to obtain automatic segmentation results. We compare pretraining using ZPar results only and using results that both segmentors agree on. For heterogenous segmentation corpus and POS data, we use a People's Daily corpus of 5 months <ref type="bibr">5</ref> . Statistics are listed in <ref type="table" target="#tab_5">Table 3</ref>. Evaluation. The standard word precision, recall and F1 measure <ref type="bibr" target="#b8">(Emerson, 2005</ref>) are used to evaluate segmentation performances. Hyper-parameter Values. We adopt commonly used values for most hyperparameters, but tuned the sizes of hidden layers on the development set. The values are summarized in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Development Experiments</head><p>We perform development experiments to verify the usefulness of various context representations, network configurations and different pretraining methods, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Context Representations</head><p>The influence of character and word context representations are empirically studied by varying the network structures for X C and X W in <ref type="figure">Figure 1</ref>, respectively. All the experiments in this section are performed using a beam size of 8. Character Context. We fix the word representation X W to a 2-word window and compare different character context representations. The results are shown in <ref type="table">Table 4</ref>, where "no char" represents our model without X C , "5-char window" represents a five-character <ref type="bibr">window</ref>   <ref type="table">Table 4</ref>: Influence of character contexts.</p><p>"5-char window + LSTM" represents a combination, detailed in Section 3.1. "-char emb" and "-bichar emb" represent the combined window and LSTM context without character and characterbigram information, respectively. As can be seen from the table, without character information, the F-score is 84.62%, demonstrating the necessity of character contexts. Using window and LSTM representations, the Fscores increase to 95.41% and 95.51%, respectively. A combination of the two lead to further improvement, showing that local and global character contexts are indeed complementary, as hypothesized in Section 3.1. Finally, by removing character and character-bigram embeddings, the F-score decreases to 95.20% and 94.27%, respectively, which suggests that character bigrams are more useful compared to character unigrams. This is likely because they contain more distinct tokens and hence offer a larger parameter space. Word Context. The influence of various word contexts are shown in <ref type="table">Table 5</ref>. Without using word information, our segmentor gives an F-score of 95.66% on the development data. Using a context of only w −1 (1-word window), the F-measure increases to 95.78%. This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors ( <ref type="bibr" target="#b39">Zhang et al., 2016b;</ref><ref type="bibr" target="#b2">Cai and Zhao, 2016)</ref>. This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref>. The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins. Given that character context is what we pretrain, our model relies more heavily  <ref type="table">Table 5</ref>: Influence of word contexts.</p><p>on them.</p><p>With both w −2 and w −1 being used for the context, the F-score further increases to 95.86%, showing that a 2-word window is useful by offering more contextual information. On the other hand, when w −3 is also considered, the F-score does not improve further. This is consistent with previous findings of statistical word segmentation ( <ref type="bibr" target="#b41">Zhang and Clark, 2007)</ref>, which adopt a 2-word context. Interestingly, using a word LSTM does not bring further improvements, even when it is combined with a window context. This suggests that global word contexts may not offer crucial additional information compared with local word contexts. Intuitively, words are significantly less polysemous compared with characters, and hence can serve as effective contexts even if used locally, to supplement a more crucial character context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Stuctured Learning and Inference</head><p>We verify the effectiveness of structured learning and inference by measuring the influence of beam size on the baseline segmentor. <ref type="figure" target="#fig_0">Figure 4</ref> shows the F-scores against different numbers of training iterations with beam size 1,2,4,8 and 16, respectively. When the beam size is 1, the inference is local and greedy. As the size of the beam increases, more global structural ambiguities can be resolved since learning is designed to guide search. A contrast between beam sizes 1 and 2 demonstrates the usefulness of structured learning and inference. As the beam size increases, the gain by doubling the beam size decreases. We choose a beam size of 8 for the remaining experiments for a tradeoff between speed and accuracy. <ref type="table" target="#tab_9">Table 6</ref> shows the effectiveness of rich pretraining of D c on the development set. In particular, by using punctuation information, the F-score increases from 95.86% to 96.25%, with a relative error reduction of 9.4%. This is consistent with   the observation of Sun and Xu (2011), who show that punctuation is more effective compared with mutual information and access variety as semisupervised data for a statistical word segmentation model. With automatically-segmented data 6 , heterogenous segmentation and POS information, the F-score increases to 96.26%, 96.27% and 96.22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation ( <ref type="bibr" target="#b10">Jiang et al., 2009;</ref><ref type="bibr" target="#b30">Wang et al., 2011;</ref><ref type="bibr" target="#b36">Zhang et al., 2013)</ref>. Finally, by integrating all above information via multi-task learning, the F-score is further improved to 96.48%, with a 15.0% relative error reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Pretraining Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Comparision with Zhang et al. (2016b)</head><p>Both our model and <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref> use global learning and beam search, but our network is different. <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref> utilizes the action history with LSTM encoder, while we use partial word rather than action information. Besides, the character and character bigram embeddings are fine-tuned in our model while <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref>  Figure 5: F1 measure against the sentence length.</p><p>We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref>.</p><p>In particular, we cluster the sentences in the development dataset into 6 categories based on their length and evaluate their F1-values, respectively. As shown in <ref type="figure">Figure 5</ref>, the models give different error distributions, with our models being more robust to the sentence length compared with <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref>. Their model is better on very short sentences, but worse on all other cases. This shows the relative advantages of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Final Results</head><p>Our final results on CTB6 are shown in <ref type="table" target="#tab_12">Table 7</ref>, which lists the results of several current state-ofthe-art methods. Without multitask pretraining, our model gives an F-score of 95.44%, which is higher than the neural segmentor of <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref>, which gives the best accuracies among pure neural segments on this dataset. By using multitask pretraining, the result increases to 96.21%, with a relative error reduction of 16.9%. In comparison, Sun and Xu (2011) investigated heterogenous semi-supervised learning on a stateof-the-art statistical model, obtaining a relative error reduction of 13.8%. Our findings show that external data can be as useful for neural segmentation as for statistical segmentation. Our final results compare favourably to the best statistical models, including those using semisupervised learning ( <ref type="bibr" target="#b24">Sun and Xu, 2011;</ref><ref type="bibr" target="#b30">Wang et al., 2011)</ref>, and those leveraging joint POS and syntactic information ( <ref type="bibr" target="#b38">Zhang et al., 2014</ref>). In addition, it also outperforms the best neural models, in particular <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref>*, which is a hybrid neural and statistical model, integrating man-  ual discrete features into their word-based neural model. We achieve the best reported F-score on this dataset. To our knowledge, this is the first time a pure neural network model outperforms all existing methods on this dataset, allowing the use of external data 7 . We also evaluate our model pretrained only on punctuation and auto-segmented data, which do not include additional manual labels. The results on CTB test data show the accuracy of 95.8% and 95.7%, respectivley, which are comparable with those statistical semi-supervised methods ( <ref type="bibr" target="#b24">Sun and Xu, 2011;</ref><ref type="bibr" target="#b30">Wang et al., 2011</ref>). They are also among the top performance methods in <ref type="table" target="#tab_12">Table 7</ref>. Compared with discrete semisupervised methods ( <ref type="bibr" target="#b24">Sun and Xu, 2011;</ref><ref type="bibr" target="#b30">Wang et al., 2011</ref>), our semi-supervised model is free from hand-crafted features. In addition to CTB6, which has been the most commonly adopted by recent segmentation research, we additionally evaluate our results on the SIGHAN 2005 bakeoff and Weibo datasets, to examine cross domain robustness. Different stateof-the-art methods for which results are recorded on these datasets are listed in <ref type="table" target="#tab_14">Table 8</ref>. Most neural models reported results only on the PKU 8 and MSR datasets of the bakeoff test sets, which are in simplified Chinese. The AS and CityU corpora are in traditional Chinese, sourced from Taiwan and <ref type="bibr">7</ref> We did not investigate the use of lexicons ( <ref type="bibr">Chen et al., 2015a,b</ref>) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by <ref type="bibr" target="#b2">Cai and Zhao (2016)</ref>. <ref type="bibr">8</ref> We notice that both PKU dataset and our heterogenous data are based on the news of People's Daily. While the heterogenous data only collect news from Febuary 1998 to June 1998, it does not contain the sentences in the dev and test datasets of PKU.   Hong Kong corpora, respectively. We map them into simplified Chinese before segmentation. The Weibo corpus is in a yet different genre, being social media text. <ref type="bibr" target="#b32">Xia et al. (2016)</ref> achieved the best results on this dataset by using a statistical model with features learned using external lexicons, the CTB7 corpus and the People Daily corpus. Similar to <ref type="table" target="#tab_12">Table 7</ref>, our method gives the best accuracies on all corpora except for MSR, where it underperforms the hybrid model of <ref type="bibr" target="#b39">Zhang et al. (2016b)</ref> by 0.2%. To our knowledge, we are the first to report results for a neural segmentor on more than 3 datasets, with competitive results consistently. It verifies that knowledge learned from a certain set of resources can be used to enhance cross-domain robustness in training a neural segmentor for different datasets, which is of practical importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We investigated rich external resources for enhancing neural word segmentation, by building a globally optimised beam-search model that leverages both character and word contexts. Taking each type of external resource as an auxiliary classification task, we use neural multi-task learning to pre-train a set of shared parameters for character contexts. Results show that rich pretraining leads to 15.4% relative error reduction, and our model gives results highly competitive to the best systems on six different benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: F1 measure against the training epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>and segmentation under different</head><label></label><figDesc></figDesc><table>State Recognized words 

Partial word Incoming chars 
Next Action 
state0 [ ] 
φ 
[] 
SEP 
state1 [ ] 

[] 
SEP 
state2 [] 

[] 
SEP 
state3 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Hyper-parameter values. 

a positive example. The loss function is 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>and NLPCC 2016 shared task for Weibo segmentation (Qiu et al., 2016) as test datasets, where the stan- dard splits are used. For pretraining embedding of</figDesc><table>Source 
#Chars #Words #Sents 
Raw data 
Gigaword 
116.5m 
-
-
Auto seg 
Gigaword 
398.2m 238.6m 12.04m 
Hete. 
People's Daily 10.14m 
6.17m 
104k 
POS 
People's Daily 10.14m 
6.17m 
104k 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Statistics of external data.</head><label>3</label><figDesc></figDesc><table>words, characters and character bigrams, we use 
Chinese Gigaword (simplified Chinese sections) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 : Influence of pretraining.</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head></head><label></label><figDesc>set the embeddings fixed during training.</figDesc><table>10&lt; 
30 
50 
70 
90 
&gt;110 

Sentence length 

0.94 

0.95 

0.96 

0.97 

0.98 

F1-value 

Multitask 
Baseline 
Zhang et al. 2016 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 7 : Main results on CTB6.</head><label>7</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Main results on other test datasets. 

</table></figure>

			<note place="foot" n="2"> An end of sentence symbol /s is added to the input so that the last partial word can be put onto W as a full word before segmentation finishes.</note>

			<note place="foot" n="3"> The LSTM variation with coupled input and forget gate but without peephole connections is applied (Gers and Schmidhuber, 2000)</note>

			<note place="foot" n="6"> By using ZPar alone, the auto-segmented result is 96.02%, less than using results by matching ZPar and the CRF segmentor outputs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insightful comments and the support of NSFC 61572245. We would like to thank Meishan Zhang for his insightful discussion and assisting coding. Yue Zhang is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1231</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1231" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural word segmentation learning for chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1039</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1039" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1168</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1168" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long shortterm memory neural networks for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1141</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1141" />
	</analytic>
	<monogr>
		<title level="m">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1385" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P04-1015" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">111</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The second international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth SIGHAN workshop on Chinese language Processing</title>
		<meeting>the fourth SIGHAN workshop on Chinese language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE-INNS-ENNS International Joint Conference on. IEEE</title>
		<meeting>the IEEE-INNS-ENNS International Joint Conference on. IEEE</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P09-1059" />
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Punctuation as implicit annotations for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/J09-4006" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="505" to="512" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for joint segmentation and pos-tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature-based neural language model and chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/I13-1181" />
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1271" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Morphological analysis for unsegmented languages using recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1276</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1276" />
	</analytic>
	<monogr>
		<title level="j">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Kiat</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Low</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W04-3236" />
	</analytic>
	<monogr>
		<title level="m">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="277" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Max-margin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<idno type="doi">10.3115/v1/P14-1028</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-1028" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Chinese segmentation and new word detection using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C04-1081" />
	</analytic>
	<monogr>
		<title level="m">COLING. page</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">562</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview of the nlpcc-iccpol 2016 shared task: Chinese word segmentation for micro-blog texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Processing of Oriental Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Profiting from mark-up: Hyper-text annotations for guided parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alshawi</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P10-1130" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1278" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A stochastic finitestate word-segmentation algorithm for chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chilin</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/J96-3004" />
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="404" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word-based and characterbased word segmentation models: Comparison and combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C10-2139" />
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1211" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhancing chinese word segmentation using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D11-1090" />
	</analytic>
	<monogr>
		<title level="m">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="970" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P12-1027" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A discriminative latent variable chinese segmenter with hybrid word/character information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A conditional random field word segmenter for sighan bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth SIGHAN workshop on Chinese language Processing</title>
		<meeting>the fourth SIGHAN workshop on Chinese language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two knives cut better than one: Chinese word segmentation with dual decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<idno type="doi">10.3115/v1/P14-</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving chinese word segmentation and pos tagging with semi-supervised methods using large autoanalyzed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="309" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D16-1137" />
	</analytic>
	<monogr>
		<title level="m">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Word segmentation on micro-blog texts with external lexicon and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingrong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Processing of Oriental Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dependencybased gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-2092</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-2092" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">567</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Chinese word segmentation as character tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics and Chinese Language Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="48" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring representations from unlabeled data with co-training for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D13-1031" />
	</analytic>
	<monogr>
		<title level="m">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="311" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Libn3l: a lightweight package for neural nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="doi">10.1145/322234.322243</idno>
		<ptr target="https://doi.org/10.1145/322234.322243" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Character-level chinese dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P14-1125</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-1125" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1326" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transition-based neural word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1040</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1040" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Subword-based tagging by conditional random fields for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genichiro</forename><surname>Kikui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Chinese segmentation with a word-based perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P07-1106" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint word segmentation and pos tagging using a single perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P08-1101" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="888" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<idno type="doi">10.1162/colia00037</idno>
		<ptr target="https://doi.org/10.1162/colia00037" />
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Effective tag set selection in chinese word segmentation via conditional random field modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/Y06-1012" />
		<editor>PACLIC. Citeseer</editor>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D13-1061" />
	</analytic>
	<monogr>
		<title level="m">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A neural probabilistic structured-prediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1117</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1117" />
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1213" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
