<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
							<email>scho@dgist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
							<email>suha.kwak@postech.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Postech</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">DGIST</orgName>
								<orgName type="institution">Kakao Corp</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">DGIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel approach for learning instance segmentation with image-level class labels as supervision. Our approach generates pseudo instance segmentation labels of training images, which are used to train a fully supervised model. For generating the pseudo labels, we first identify confident seed areas of object classes from attention maps of an image classification model, and propagate them to discover the entire instance areas with accurate boundaries. To this end, we propose IRNet, which estimates rough areas of individual instances and detects boundaries between different object classes. It thus enables to assign instance labels to the seeds and to propagate them within the boundaries so that the entire areas of instances can be estimated accurately. Furthermore, IRNet is trained with interpixel relations on the attention maps, thus no extra supervision is required. Our method with IRNet achieves an outstanding performance on the PASCAL VOC 2012 dataset, surpassing not only previous state-of-the-art trained with the same level of supervision, but also some of previous models relying on stronger supervision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation is a task that jointly estimates class labels and segmentation masks of individual objects. As in other visual recognition tasks, supervised learning of Convolutional Neural Networks (CNNs) has driven recent advances in instance segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. Due to the data-hungry nature of deep CNNs, this approach demands an enormous number of training images with groundtruth labels, which are given by hand in general. However, manual annotation of instance-wise segmentation masks is prohibitively time-consuming, which results in existing datasets limited in terms of both class diversity and the amount of annotated data. It is thus not straightforward to learn instance segmentation models that can handle diverse object classes in the real world. * Co-corresponding authors.</p><p>One way to alleviate this issue is weakly supervised learning that adopts weaker and less expensive labels than instance-wise segmentation masks as supervision. Thanks to low annotation costs of weak labels, approaches in this category can utilize more training images of diverse objects, although they have to compensate for missing information in weak labels. For instance segmentation, bounding boxes have been widely used as weak labels since they provide every property of objects except shape <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref>. However, it is still costly to obtain box labels for a variety of classes in a large number of images as they are manually annotated.</p><p>To further reduce the annotation cost, one may utilize image-level class labels for learning instance segmentation since such labels are readily available in large-scale image classification datasets, e.g., ImageNet <ref type="bibr" target="#b44">[45]</ref>. Furthermore, although image-level class labels indicate only the existence of object classes, they can be used to derive strong cues for instance segmentation, called Class Attention Maps (CAMs) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b52">52]</ref>. A CAM roughly estimates areas of each class by investigating the contribution of local image regions to the classification score of the class. However, CAMs cannot be directly utilized as supervision for instance segmentation since they have limited resolution, often highlight only partial areas of objects, and most importantly, cannot distinguish different instances of the same class. To resolve this issue, a recent approach <ref type="bibr" target="#b53">[53]</ref> incorporates CAMs with an off-the-shelf segmentation proposal technique <ref type="bibr" target="#b1">[2]</ref>, which however has to be trained separately on an external dataset with additional supervision.</p><p>In this paper, we present a novel approach for learning instance segmentation using image-level class labels, which outperforms the previous state-of-the-art trained with the same level of supervision <ref type="bibr" target="#b53">[53]</ref> and even some of approaches relying on stronger supervision <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>. Moreover, it requires neither additional supervision nor any segmentation proposals unlike the previous approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b53">53]</ref>. Our method generates pseudo instance segmentation labels of training images given their image-level labels and trains a known CNN model with the pseudo labels. For generating the pseudo labels, it utilizes CAMs, but as mentioned earlier, they can neither distinguish different instances nor find  <ref type="figure">Figure 1</ref>. Overview of our framework for generating pseudo instance segmentation labels.</p><p>entire instance areas with accurate boundaries.</p><p>To overcome these limitations of CAMs, we introduce Inter-pixel Relation Network (IRNet) that is used to estimate two types of additional information complementary to CAMs: a class-agnostic instance map and pairwise semantic affinities. A class-agnostic instance map is a rough instance segmentation mask without class labels nor accurate boundaries. On the other hand, the semantic affinity between a pair of pixels is a confidence score for class equivalence between them. By incorporating instanceagnostic CAMs with a class-agnostic instance map, we obtain instance-wise CAMs, which are in turn enhanced by propagating their attention scores to relevant areas based on the semantic affinities between neighboring pixels. After the enhancement, a pseudo instance segmentation label is generated by selecting the instance label with the highest attention score in the instance-wise CAMS at each pixel. The entire procedure for label synthesis is illustrated in <ref type="figure">Fig. 1</ref>.</p><p>IRNet has two branches estimating an instance map and semantic affinities, respectively. The first branch predicts a displacement vector field where a 2D vector at each pixel indicates the centroid of the instance the pixel belongs to. The displacement field is converted to an instance map by assigning the same instance label to pixels whose displacement vectors point at the same location. The second branch detects boundaries between different object classes. Pairwise semantic affinities are then computed from the detected boundaries in such a way that two pixels separated by a strong boundary are considered as a pair with a low semantic affinity. Furthermore, we found that IRNet can be trained effectively with inter-pixel relations derived from CAMs. Specifically, we collect pixels with high attention scores and train IRNet with the displacements and class equivalence between the collected pixels. Thus, no supervision in addition to image-level class labels is required.</p><p>The contribution of this paper is three-fold: â€¢ We propose a new approach to identify and localize instances with image-level supervision through classagnostic instance maps. This enables instance segmentation without off-the-shelf segmentation proposals.</p><p>â€¢ We propose a new way to learn and predict semantic affinities between pixels with image-level supervision through class boundary detection, which is more effective and efficient than previous work <ref type="bibr" target="#b0">[1]</ref>. â€¢ On the PASCAL VOC 2012 dataset <ref type="bibr" target="#b12">[13]</ref>, our model substantially outperforms the previous state-of-the-art trained with the same level of supervision <ref type="bibr" target="#b53">[53]</ref>. Also, it even surpasses previous models based on stronger supervision like SDI <ref type="bibr" target="#b23">[24]</ref> that uses bounding box labels and SDS <ref type="bibr" target="#b17">[18]</ref>, an early model that uses full supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section reviews semantic and instance segmentation models closely related to our method. We first introduce weakly supervised approaches for the two tasks, and discuss models that are based on ideas similar with the displacement field and pairwise semantic affinity of our framework. Weakly Supervised Semantic Segmentation: For weak supervision of semantic segmentation, various types of weak labels such as bounding boxes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref>, scribbles <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">47]</ref>, and points <ref type="bibr" target="#b2">[3]</ref> have been utilized. In particular, imagelevel class labels have been widely used as weak labels since they require minimal or no effort for annotation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b53">53]</ref>. Most approaches using the image-level supervision are based on CAMs <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b52">52]</ref> that roughly localize object areas by drawing attentions on discriminative parts of object classes. However, CAMs often fail to reveal the entire object areas with accurate boundaries. To address this issue, extra data or supervision have been exploited to obtain additional evidences like saliency <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref>, motion in videos <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref> and classagnostic object proposals <ref type="bibr" target="#b42">[43]</ref>. Recent approaches tackle the issue without external information by mining comple-mentary attentions iteratively <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b48">48]</ref> or propagating CAMs based on semantic affinities between pixels <ref type="bibr" target="#b0">[1]</ref>. Weakly Supervised Instance Segmentation: For instance segmentation, bounding boxes have been widely used as weak labels. Since a bounding box informs the exact location and scale of an object, weakly supervised models using box labels focus mainly on estimating object shapes. For example, in <ref type="bibr" target="#b23">[24]</ref>, GraphCut is incorporated with generic boundary detection <ref type="bibr" target="#b51">[51]</ref> to better estimate object shapes by considering boundaries. Also, in <ref type="bibr" target="#b43">[44]</ref>, an object shape estimator is trained by adversarial learning <ref type="bibr" target="#b15">[16]</ref> so that a pseudo image generated by cutting and pasting the estimated object area to a random background looks realistic. Meanwhile, weakly supervised instance segmentation with image-level class labels has been rarely studied since this is a significantly ill-posed problem where supervision does not provide any instance-specific information. To tackle this challenging problem, a recent approach <ref type="bibr" target="#b53">[53]</ref> detects peaks of class attentions to identify individual instances and combines them with high-quality segmentation proposals <ref type="bibr" target="#b1">[2]</ref> to reveal entire instance areas. However, the performance of the method heavily depends on that of the segmentation proposals, which have to be trained with extra data with highlevel supervision. In contrast, our approach requires neither off-the-shelf proposals nor additional supervision and it surpasses the previous work <ref type="bibr" target="#b53">[53]</ref> by a substantial margin. Pixel-wise Prediction of Instance Location: Pixel-wise prediction of instance location has been proven to be effective for instance segmentation in literature. In <ref type="bibr" target="#b27">[28]</ref> the coordinates of the instance bounding box each pixel belongs to are predicted in a pixel-wise manner so that pixels with similar box coordinates are clustered as a single instance mask. This idea is further explored in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref>, which predict instance centroids instead of box coordinates. Our approach based on the displacement field share the same idea with <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref>, but it requires only image-level supervision while the previous approaches are trained with instancewise segmentation labels. Semantic Affinities Between Pixels: Pairwise semantic affinities between pixels have been used to enhance the quality of semantic segmentation. In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, CNNs for semantic segmentation are incorporated with a differentiable module computing a semantic affinity matrix of pixels, and trained in an end-to-end manner with full supervision. In <ref type="bibr" target="#b3">[4]</ref>, a predicted affinity matrix is used as a transition probability matrix for random walk, while in <ref type="bibr" target="#b5">[6]</ref>, it is embedded into a convolutional decoder <ref type="bibr" target="#b35">[36]</ref> to encourage local pixels to have the same labels during inference. Recently, a weakly supervised model has been proposed to learn pairwise semantic affinities with image-level class labels <ref type="bibr" target="#b0">[1]</ref>. This model predicts a high-dimensional embedding vector for each pixel, and the affinity between a pair of pixels is defined as the similarity between their embedding vectors.  Our approach shares the same motivation with <ref type="bibr" target="#b0">[1]</ref>, but our IRNet can learn and predict affinities more effectively and efficiently by detecting class boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Class Attention Maps</head><p>CAMs play two essential roles in our framework. First, they are used to define seed areas of instances, which are propagated later to recover the entire instance areas as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref>. Second, they are a source of supervision for learning IRNet; by exploiting CAMs carefully, we extract reliable inter-pixel relations, from which IRNet is trained. To generate CAMs for training images, we adopt the method of <ref type="bibr" target="#b52">[52]</ref> using an image classification CNN with global average pooling followed by a classification layer. Given an image, the CAM of a groundtruth class c is computed by</p><formula xml:id="formula_0">M c (x) = Ï† c f (x) max x Ï† c f (x) ,<label>(1)</label></formula><p>where f is a feature map from the last convolution layer of the CNN, x is a 2D coordinate on f , and Ï† c is the classification weights of the class c. Also, CAMs for irrelevant classes are fixed to a zero matrix. We adopt ResNet50 <ref type="bibr" target="#b19">[20]</ref> as the classification network, and reduce the stride of its last downsampling layer from 2 to 1 to prevent CAMs from further resolution drop. As a result, the width and height of CAMs are 1/16 of those of the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inter-pixel Relation Network</head><p>IRNet aims to provide two types of information: a displacement vector field and a class boundary map, both of which are in turn used to estimate pseudo instance masks from CAMs. This section describes the IRNet architecture and the strategy for learning the model using CAMs as supervision. How to use IRNet for pseudo label generation will be illustrated in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">IRNet Architecture</head><p>IRNet has two output branches that predict a displacement vector field and a class boundary map, respectively. Its architecture is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The two branches share the same ResNet50 backbone, which is identical to that of the classification network in Sec. 3. As inputs, both branches take feature maps from all the five levels 1 of the backbone. All the convolution layers of both branches are followed by group normalization <ref type="bibr" target="#b50">[50]</ref> and ReLU except the last layer. Details of both branches are described below. Displacement Field Prediction Branch: A 1Ã—1 convolution layer is first applied to each input feature map, and the number of channels is reduced to 256 if it is larger than that. On top of them, we append a top-down path way <ref type="bibr" target="#b29">[30]</ref> to merge all the feature maps iteratively in such a way that low resolution feature maps are upsampled twice, concatenated with those of the same resolution, and processed by a 1Ã—1 convolution layer. Finally, from the last concatenated feature map, a displacement field is decoded through three 1Ã—1 convolution layers, whose output has two channels. Boundary Detection Branch: We first apply 1Ã—1 convolution to each input feature map for dimensionality reduction. Then the results are resized, concatenated, and fed into the last 1Ã—1 convolution layer, which produces a class boundary map from the concatenated features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inter-pixel Relation Mining from CAMs</head><p>Inter-pixel relations are the only supervision for training IRNet, thus it is important to collect them reliably. We define two kinds of relations between a pair of pixels: the displacement between their coordinates and their class equivalence. The displacement can be easily computed by a simple subtraction, but the class equivalence is not since pixel-wise class labels are not given in our weakly supervised setting.</p><p>Thus, we carefully exploit CAMs to predict pixel-wise pseudo class labels and obtain reliable class equivalence relations from them. The overall procedure of our method is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Since CAMs are blurry and often inaccurate, we first identify areas with confident foreground/background attention scores. Specifically, we collect pixels with attention scores larger than 0.3 as foreground pixels, and smaller than 0.05 as background pixels. Note that we do not care pixels outside of confident areas during the process. Each confident area is then refined by dense CRF <ref type="bibr" target="#b26">[27]</ref> to better estimate object shapes. After that, we construct a pseudo class mapM by choosing the class <ref type="bibr" target="#b0">1</ref> A level means a group of residual units sharing the same output size in <ref type="bibr" target="#b19">[20]</ref>. However, in our backbone, the output sizes of level4 and level5 are identical since the stride of the last downsampling layer is reduced to 1.  with the best score for each pixel. Finally, we sample pairs of neighboring pixels from the refined confident areas, and categorize them into two sets P + and P âˆ’ according to their class equivalence by</p><formula xml:id="formula_1">P = (i, j) | x i âˆ’ x j 2 &lt; Î³, âˆ€i = j ,<label>(2)</label></formula><formula xml:id="formula_2">P + = (i, j) |M (x i ) =M (x j ), (i, j) âˆˆ P ,<label>(3)</label></formula><formula xml:id="formula_3">P âˆ’ = (i, j) |M (x i ) =M (x j ), (i, j) âˆˆ P ,<label>(4)</label></formula><p>where Î³ is a radius limiting the maximum distance of a pair. We further divide P + into P + fg and P + bg , a set of foreground pairs and that of background pairs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Loss for Displacement Field Prediction</head><p>The first branch of IRNet predicts a displacement vector field D âˆˆ R wÃ—hÃ—2 , where each 2D vector points at the centroid of the associated instance. Although ground truth centroids are not given in our setting, we argue that D can be learned implicitly with displacements between pixels of the same class. There are two conditions for D to be a displacement field. First, for a pair of pixel locations x i and x j belonging to the same instance, their estimated centroids must be identical, i.e., x i +D(x i ) = x j +D(x j ). Second, by the definition of centroid, x D(x) = 0 for each instance.</p><p>To satisfy the first condition, we first assume that a pair of nearby pixels (i, j) âˆˆ P + is likely to be of the same instance since they are sampled within a small radius Î³. Then, given such a pair (i, j), our goal is to approximate their image coordinate displacementÎ´(i, j) = x j âˆ’ x i with their difference in D denoted by Î´(i, j) = D(x i ) âˆ’ D(x j ). In the ideal case where Î´ =Î´, it will hold that x i + D(x i ) = x j + D(x j ) for all (i, j) of the same instance. This implies that D(x) is the displacement vector indicating the corresponding centroid. For learning D with the inter-pixel relations obtained in Sec. 4.2, we minimize L 1 loss between Î´(i, j) andÎ´(i, j):</p><formula xml:id="formula_4">L D fg = 1 |P + fg | (i,j)âˆˆP + fg Î´(i, j) âˆ’Î´(i, j) .<label>(5)</label></formula><p>The second condition, on the other hand, is not explicitly encouraged by Eq. <ref type="bibr" target="#b4">(5)</ref>. However, we argue that IRNet can still learn to predict displacement vectors pointing to rough centroids of instances due to the randomness of initial network parameters. Intuitively speaking, initial random displacement vectors are already likely to satisfy the second condition, and the training of IRNet converges to a local minimum that still satisfies the condition. A similar phenomenon is observed in <ref type="bibr" target="#b36">[37]</ref>. Displacement vectors are then further refined by subtracting the mean of D from D.</p><p>Also, we eliminate trivial centroid estimation from background pixels since the centroid of background is indefinite and may interfere with the above process. For the purpose, we minimize the following loss for background pixels:</p><formula xml:id="formula_5">L D bg = 1 |P + bg | (i,j)âˆˆP + bg |Î´(i, j)|.<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Loss for Class Boundary Detection</head><p>Given an image, the second branch of IRNet detects boundaries between different classes, and the output is denoted by B âˆˆ [0, 1] wÃ—h . Although no ground truth labels for class boundaries are given in our setting, we can train the second branch with class equivalence relations between pixels through a Multiple Instance Learning (MIL) objective.</p><p>The key assumption is that a class boundary exists somewhere between a pair of pixels with different pseudo class labels.</p><p>To implement this idea, we express the semantic affinity between two pixels in terms of the existence of a class boundary. For a pair of pixels x i and x j , we define their semantic affinity a ij as:</p><formula xml:id="formula_6">a ij = 1 âˆ’ max kâˆˆÎ ij B(x k )<label>(7)</label></formula><p>where Î  ij is a set of pixels on the line between x i and x j . We utilize class equivalence relations between pixels as supervision for learning a ij . Specifically, the class equivalence between two pixels is represented as a binary label whose value is 1 if their pseudo class labels are the same and 0 otherwise. The affinity is then learned by minimizing cross-entropy between the one-hot vector of the binary affinity label and the predicted affinity in Eq. <ref type="formula" target="#formula_6">(7)</ref>: where three separate losses are aggregated after normalization since populations of P + fg , P + bg , and P âˆ’ are significantly imbalanced in general. Through the loss in Eq. (8), we can learn B implicitly with inter-pixel class equivalence relations. In this aspect, Eq. (8) can be regarded as a MIL objective where Î  ij is a bag of potential boundary pixels.</p><formula xml:id="formula_7">L B = âˆ’ (i,j)âˆˆP + fg log a ij 2|P + fg | âˆ’ (i,j)âˆˆP + bg log a ij 2|P + bg | âˆ’ (i,j)âˆˆP âˆ’ log(1 âˆ’ a ij ) |P âˆ’ | (8) (a) (b) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Joint Learning of the Two Branches</head><p>The two branches of IRNet are jointly trained by minimizing all the losses we defined previously at the same time:</p><formula xml:id="formula_8">L = L D fg + L D bg + L B .<label>(9)</label></formula><p>Note that the above loss is class-agnostic since P + and P âˆ’ only consider class equivalence between pixels rather than their individual class labels. This allows our approach to utilize more inter-pixel relations per class and helps to improve the generalization ability of IRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Label Synthesis Using IRNet</head><p>To synthesize pseudo instance labels, the two outputs D and B of IRNet are converted to a class-agnostic instance map and pairwise affinities, respectively. Among them, semantic affinities can be directly derived from B by Eq. <ref type="formula" target="#formula_6">(7)</ref> as illustrated in <ref type="figure">Fig. 4</ref>, while the conversion of D is not straightforward due to its inaccurate estimation. This section first describes how D is converted to an instance map, then how to generate pseudo instance segmentation labels with the instance map and semantic affinities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Generating Class-agnostic Instance Map</head><p>A class-agnostic instance map I is a w Ã— h 2D map, each element of which is the instance label associated with the element. If D is estimated with perfect accuracy, I can be obtained simply by grouping pixels whose displacement vectors point at the same centroid. However, D often fails to predict the exact offsets to centroids since IRNet is trained with incomplete supervision derived from CAMs. To address this issue, D is refined iteratively by</p><formula xml:id="formula_9">D u+1 (x) = D u (x) + D (x + D u (x)) âˆ€x,<label>(10)</label></formula><p>where u is an iteration index and D 0 is the initial displacement field given by IRNet. Each displacement vector is refined iteratively by adding the displacement vector at the currently estimated centroid location. As displacement vectors near centroids tend to be almost zero in magnitude, the refinement converges within a finite number of iterations. The effect of the refinement is demonstrated in <ref type="figure">Fig. 5</ref>.</p><p>Since centroids estimated via the refined D are still scattered in general, we consider a small group of neighboring pixels, instead of a single coordinate, as a centroid. To this end, we first identify pixels whose displacement vectors in D have small magnitudes, and regard them as candidate centroids since pixels around a true centroid will have near zero displacement vectors. Then each connected component of the candidates is considered as a centroid. Note that the candidates tend to be well grouped into a few connected components since displacement vectors change smoothly within a local neighborhood as can be seen in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Synthesizing Instance Segmentation Labels</head><p>For generating pseudo instance masks, we first combine CAMs with a class-agnostic instance map as follows:</p><formula xml:id="formula_10">M ck (x) = M c (x) if I(x) = k, 0 otherwise,<label>(11)</label></formula><p>whereM ck is the instance-wise CAMs of class c and instance k. Each instance-wise CAM is refined individually by propagating its attention scores to relevant areas. Specifically, the propagation is done by random walk, whose transition probability matrix is derived from the semantic affinity matrix A = [a ij ] âˆˆ R whÃ—wh as follows: <ref type="bibr" target="#b11">(12)</ref> and A â€¢Î² is A to the Hadamard power of Î² and S is a diagonal matrix for row-normalization of A â€¢Î² . Also, Î² &gt; 1 is a hyper-parameter for smoothing out affinity values in A. The random walk propagation with T is then conducted by</p><formula xml:id="formula_11">T = S âˆ’1 A â€¢Î² , where S ii = j a Î² ij</formula><formula xml:id="formula_12">vec(M * ck ) = T t Â· vec(M ck (1 âˆ’ B)),<label>(13)</label></formula><p>where t denotes the number of iterations, is the Hadamard product, and vec(Â·) means vectorization. We penalize scores of boundary pixels by multiplying (1 âˆ’ B) since those isolated pixels do not propagate their scores to neighbors and have overly high scores compared to the others in consequence. Then an instance segmentation label is generated by choosing the combination of c and k that maximizes M * ck (x) for each pixel x. If the maximum score is less than bottom 25%, the pixel is regarded as background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>The effectiveness of our framework is demonstrated on the PASCAL VOC 2012 dataset <ref type="bibr" target="#b13">[14]</ref>, where our framework generates pseudo labels for training images and trains a fully supervised model with the images and their pseudo labels. We evaluate the quality of our pseudo labels as well as the performance of the model trained with them. The evaluation is done for both instance segmentation and semantic segmentation since our pseudo labels can be used to train semantic segmentation models as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Setting</head><p>Dataset: We train and evaluate our framework on the PAS-CAL VOC 2012 <ref type="bibr" target="#b12">[13]</ref> dataset. Although the dataset contains labels for semantic segmentation and instance segmentation, we only exploit image-level class labels. Following the common practice, the training set is expanded by adding image set proposed in <ref type="bibr" target="#b16">[17]</ref>. In total, 10,582 images are used for training, and 1,449 images are kept for validation.  <ref type="table">Table 2</ref>. Quality of pseudo semantic segmentation labels in mIoU, evaluated on the PASCAL VOC 2012 train set. "Prop" means the semantic propagation using predicted affinities.</p><p>Hyperparameter Settings: The radius that limits the search space of pairs Î³ in Eq. (2) is set to 10 when training, and reduced to 5 at inference for conservative propagation. The number of random walk iterations t in Eq. <ref type="formula" target="#formula_0">(13)</ref> is fixed to 256. The hyperparameter Î² in Eq. <ref type="formula" target="#formula_0">(12)</ref> is set to 10. The iterative update of D in Eq. (10) is done 100 times. Network Parameter Optimization: We adopt the stochastic gradient descent for network optimization. Learning rate is initially set to 0.1, and decreases at every iteration with polynomial decay <ref type="bibr" target="#b33">[34]</ref>. The backbone of IRNet is frozen during training, and gradients that displacement field branch receives are amplified by a factor of 10. Comparison to AffinityNet: For a fair comparison, we modified AffinityNet <ref type="bibr" target="#b0">[1]</ref> by replacing its backbone with ResNet50 as in our IRNet. Then we compare IRNet with the modified AffinityNet in terms of the accuracy of pseudo segmentation labels ( <ref type="table">Table 2</ref>) and performance of DeepLab <ref type="bibr" target="#b4">[5]</ref> trained with these pseudo labels <ref type="table">(Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Analysis of Pseudo Labels</head><p>Instance Segmentation labels: A few qualitative examples of pseudo instance segmentation labels are presented in <ref type="figure" target="#fig_4">Fig. 6</ref>, and the contribution of each branch of IRNet to the quality of the labels is analyzed in Table 1. In the case of "CAM" in <ref type="table" target="#tab_2">Table 1</ref>, we directly utilize raw CAMs to generate pseudo labels by thresholding their scores and applying connected component analysis while assuming that there are no instances of the same class attached to each other. In the case of "CAM + Class Boundary" in <ref type="table" target="#tab_2">Table 1</ref>, pseudo labels are obtained in the same manner, but we enhance CAMs by the semantic propagation based on the class boundary map before generating pseudo labels. We evaluated the performance of each method in terms of average precision (AP). For evaluating APs, the score of each detected instance is given as the maximum class score within its mask. As shown in the table, exploiting a class boundary map effectively improves the quality of pseudo labels by more than 25% as it helps to recover the entire areas of objects missing in CAMs. Exploiting a displacement field further improves the performance by 3.6% as it helps to distinguish different instances of the same class.  Semantic Segmentation Labels: A reduced version of our framework, which skips the instance-wise CAM generation step, produces pseudo labels for semantic segmentation. In this aspect, we compare our framework with the previous state-of-the-art in semantic segmentation label synthesis, AffinityNet <ref type="bibr" target="#b0">[1]</ref>, in terms of mean Intersection-over-Union (mIoU). Similar to ours, AffinityNet also conducts the semantic propagation to enhance CAMs using predicted pairwise semantic affinities. <ref type="table">Table 2</ref> compares the quality of our pseudo segmentation labels to that of AffinityNet <ref type="bibr" target="#b0">[1]</ref>. The accuracy of our pseudo labels is substantially higher than that of AffinityNet thanks to the superior quality of pairwise semantic affinities predicted by IRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Mask R-CNN for Instance Segmentation</head><p>We evaluate the performance of an instance segmentation network trained with pseudo labels generated by our framework. For evaluation, we adopt Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>, which is one of the state-of-the-art instance segmentation networks, with ResNet-50-FPN <ref type="bibr" target="#b29">[30]</ref> as its backbone. <ref type="figure">Fig. 10</ref> shows qualitative results of the Mask-RCNN trained with our pseudo labels, and <ref type="table" target="#tab_4">Table 3</ref> compares its performance to those of previous approaches in AP r 2 <ref type="bibr" target="#b17">[18]</ref>. As shown in <ref type="table" target="#tab_4">Table 3</ref>, ours largely outperforms PRM <ref type="bibr" target="#b53">[53]</ref>, which is the state-of-the-art that also uses image-level supervision. Our approach even outperforms SDI <ref type="bibr" target="#b23">[24]</ref>, which uses bounding box supervision, by 1.9%, and SDS <ref type="bibr" target="#b17">[18]</ref>, which uses full supervision, by 2.9% in AP r 50 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">DeepLab for Semantic Segmentation</head><p>We further explore the effectiveness of our framework by training DeepLab v2-ResNet50 <ref type="bibr" target="#b4">[5]</ref> with our pseudo semantic segmentation labels. <ref type="figure">Fig. 11</ref> visualizes semantic segmentation results obtained by our approach and <ref type="table">Table 4</ref> compares ours with other weakly supervised approaches. Our approach outperforms previous arts relying on the same level of supervision, and is even competitive with Box-Sup <ref type="bibr" target="#b7">[8]</ref>, which utilizes stronger bounding box supervision. Also it recovers 88% of its fully supervised counterpart, the upper bound that it can achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Weakly supervised instance segmentation with imagelevel supervision is a significantly ill-posed problem due to the lack of instance-specific information. To tackle this challenging problem, we propose IRNet, a novel CNN ar-chitecture that identifies individual instances and estimates their rough boundaries. Thanks to the evidences provided by IRNet, simple class attentions can be significantly improved and used to train fully supervised instance segmentation models. On the Pascal VOC 2012 dataset, models trained with our pseudo labels achieve the state-of-the-art performance in both instance and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>This appendix provides contents omitted in the regular sections for the sake of brevity. Sec. A.1 describes the centroid detection algorithm of Sec. 5.1 in more detail, and Sec. A.2 introduces the instance and semantic segmentation models trained with our synthetic labels for the final evaluation. Additional qualitative results are then presented in Sec. A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Details of the Centroid Detection Algorithm</head><p>As discussed in Sec. 5.1 of the main paper, a small group of neighboring pixels, instead of a single coordinate, are considered as a centroid in practice. To this end, we first identify pixels whose displacement vectors in D have magnitudes smaller than a certain threshold, and consider them as candidate centroids. Specifically, the set of candidate centroids are defined as:</p><formula xml:id="formula_13">C = x | D(x) 2 &lt; 2.5 =Äˆ 1 âˆªÄˆ 2 âˆª Â· Â· Â· âˆªÄˆ K ,<label>(14)</label></formula><p>whereÄˆ i is a connected component of pixels in C and K is the number of connected components. Then a class-agnostic instance map I is obtained by assigning each pixel a connected component index in the following manner:</p><formula xml:id="formula_14">I(x) = k, if x + D(x) âˆˆÄˆ k , âˆ€x.<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Details of Our Segmentation Networks</head><p>As our framework aims to generate synthetic labels for instance and semantic segmentation, we evaluated the efficacy of our framework by learning fully supervised models for the two tasks with our synthetic labels. Specifically, we adopt Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> for instance segmentation and DeepLab v2 <ref type="bibr" target="#b4">[5]</ref> for semantic segmentation. Both of them are first pretrained on ImageNet <ref type="bibr" target="#b10">[11]</ref> then finetuned with the synthetic labels instead of groundtruth segmentation masks. The rest of this section describes details of the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Mask R-CNN for instance Segmentation</head><p>We use Detectron <ref type="bibr" target="#b14">[15]</ref>, which is the official implementation of <ref type="bibr" target="#b18">[19]</ref>, to implement Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> with ResNet-50-FPN <ref type="bibr" target="#b29">[30]</ref> as its backbone. We directly adopt the default training setting given in the provided source code, except the number of training steps that is adjusted for better adaptation to the PASCAL VOC 2012 dataset <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 DeepLab v2 for Semantic Segmentation</head><p>We manually implement DeepLab v2 <ref type="bibr" target="#b4">[5]</ref> in PyTorch <ref type="bibr" target="#b40">[41]</ref>. Its architecture consists of ResNet-50 <ref type="bibr" target="#b19">[20]</ref> followed by an atrous spatial pyramid pooling module <ref type="bibr" target="#b4">[5]</ref>. The training setting of ours is identical to that of the original model. We also employ the ensemble of multi-scale prediction during evaluation. Specifically, a single input image is converted to a set of 8 images through resizing with 4 different scales {0.5, 1.0, 1.5, 2.0} and horizontal flip, and fed into the segmentation network so that the 8 outputs are aggregated by pixel-wise average pooling.</p><p>We also reproduce the performance of the fully supervised DeepLab v2, which is the upperbound our segmentation model can achieve. Note that, as summarized in <ref type="table">Table 4</ref> of the main paper, upperbound we measured is lower than the performance reported in the original paper <ref type="bibr" target="#b4">[5]</ref> as we did not tune the parameters of dense CRF <ref type="bibr" target="#b26">[27]</ref> carefully. Thanks to the accurate segmentation labels synthesized in our framework, the DeepLab trained with our synthetic labels achieves 89.4% of its fully supervised one on the PAS-CAL VOC 2012 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. More Qualitative Results of Our Approach</head><p>In this section, we provide additional qualitative results of our framework on the PASCAL VOC dataset. Although IRNet is trained with image-level supervision only, it successfully finds accurate class boundary and displacement field to instance centroids which are not directly available in CAMs, and synthesizes accurate instance segmentation masks from CAMs incorporating those two additional information as illustrated in <ref type="figure">Fig. 9</ref>. <ref type="figure">Fig. 10</ref> and <ref type="figure">Fig. 11</ref> show additional instance segmentation and semantic segmentation results of our models, respectively. Thanks to synthetic labels that are able to differentiate attached instances, our models not only find fine object shape, but also detect independent instances that are adjacent and of the same class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAM</head><p>Displacement Field Class Boundary Instance Labels Class Labels <ref type="figure">Figure 9</ref>. Qualitative results of our instance segmentation model on the PASCAL VOC 2012 train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth Ours</head><p>Input Image Ground-truth Ours <ref type="figure">Figure 11</ref>. Qualitative results of our semantic segmentation model on the PASCAL VOC 2012 val set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of IRNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of our inter-pixel relation mining process. (a) CAMs. (b) Confident areas of object classes. (c) Pseudo class label map within a local neighborhood. (d) Class equivalence relations between the center and the others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Deriving pairwise semantic affinities from a class boundary map. (left) Input Image. (center) A class boundary map. (right) Label propagation from the center after random walks. Detecting instance centroids. (left) Input image. (center) An initial displacement field. (right) A refined displacement field and detected centroids.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Examples of pseudo instance segmentation labels on the PASCAL VOC 2012 train set. (a) Input image. (b) CAMs. (c) Displacement field. (d) Class boundary map. (e) Pseudo labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results of our instance segmentation model on the PASCAL VOC 2012 val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results of smenatic segmentation on the PASCAL VOC 2012 val set. (top) Input images. (middle) Groundtruth semantic segmentaton. (bottom) Results of Ours-ResNet50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Quality of our pseudo instance segmentation labels in AP r 50 , evaluated on the PASCAL VOC 2012 train set.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell>mIoU</cell></row><row><cell>CAM</cell><cell></cell><cell></cell><cell>8.6</cell></row><row><cell cols="2">CAM + Class Boundary</cell><cell></cell><cell>34.1</cell></row><row><cell cols="3">CAM + Displacement Field + Class Boundary (Ours)</cell><cell>37.7</cell></row><row><cell>CAM</cell><cell cols="3">Prop. w/ AffinityNet [1] Prop. w/ IRNet (Ours)</cell></row><row><cell>48.3</cell><cell>59.3</cell><cell>66.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Instance segmentation performance on the PASCAL VOC 2012 val set. The supervision types (Sup.) indicate: I-image-level label, B-bounding box, and F-segmentation label.</figDesc><table><row><cell>Method</cell><cell>Sup.</cell><cell>Extra Data / Information</cell><cell>val</cell><cell>test</cell></row><row><cell>SEC [26]</cell><cell>I</cell><cell>-</cell><cell cols="2">50.7 51.7</cell></row><row><cell>AffinityNet [1]</cell><cell>I</cell><cell>-</cell><cell>58.7</cell><cell>-</cell></row><row><cell>PRM [53]</cell><cell>I</cell><cell>MCG [2]</cell><cell>53.4</cell><cell>-</cell></row><row><cell>CrawlSeg [21]</cell><cell>I</cell><cell>YouTube Videos</cell><cell cols="2">58.1 58.7</cell></row><row><cell>MDC [49]</cell><cell>I</cell><cell>Ground-truth Backgrounds</cell><cell cols="2">60.4 60.8</cell></row><row><cell>DSRG [22]</cell><cell>I</cell><cell>MSRA-B [33]</cell><cell cols="2">61.4 63.2</cell></row><row><cell>ScribbleSup [29]</cell><cell>S</cell><cell>-</cell><cell>63.1</cell><cell>-</cell></row><row><cell>BoxSup [8]</cell><cell>B</cell><cell>-</cell><cell cols="2">62.0 64.6</cell></row><row><cell>SDI [24]</cell><cell>B</cell><cell>BSDS [35]</cell><cell cols="2">65.7 67.5</cell></row><row><cell>Upperbound</cell><cell>F</cell><cell>-</cell><cell cols="2">72.3 72.5</cell></row><row><cell>Ours-ResNet50</cell><cell>I</cell><cell>-</cell><cell cols="2">63.5 64.8</cell></row><row><cell cols="5">Table 4. Semantic segmentation performance on the PASCAL</cell></row><row><cell cols="5">VOC 2012 val and test sets. The supervision type (Sup.) indi-</cell></row><row><cell cols="5">cates: I-image-level label, B-bounding box, S-scribble, and F-</cell></row><row><cell>segmentation label.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">AP r means average precision of masks at different IoU thresholds.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 10. Qualitative results of our instance segmentation model on the PASCAL VOC 2012 val set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>ArbelÃ¡ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What&apos;s the Point: Semantic Segmentation with Point Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional random walk networks for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Localitysensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>ArbelÃ¡ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>ArbelÃ¡ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using web-crawled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Instancecut: From edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Proposal-free network for instance-level semantic object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-scale patch aggregation (MPA) for simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semiconvolutional operators for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AutoDiff, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation with weakly-annotated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">From image-level to pixellevel labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to segment via cut-and-paste</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning random-walk label propagation for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weaklyand semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation using class peak response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
