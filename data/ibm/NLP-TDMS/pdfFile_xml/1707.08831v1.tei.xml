<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STN-OCR: A single Neural Network for Text Detection and Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07-27">27 Jul 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bartz</surname></persName>
							<email>christian.bartz@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute</orgName>
								<orgName type="institution">University of Potsdam</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert Straße 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
							<email>haojin.yang@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute</orgName>
								<orgName type="institution">University of Potsdam</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert Straße 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
							<email>meinel@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute</orgName>
								<orgName type="institution">University of Potsdam</orgName>
								<address>
									<addrLine>Prof.-Dr.-Helmert Straße 2-3</addrLine>
									<postCode>14482</postCode>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STN-OCR: A single Neural Network for Text Detection and Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-07-27">27 Jul 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting and recognizing text in natural scene images is a challenging, yet not completely solved task. In recent years several new systems that try to solve at least one of the two sub-tasks (text detection and text recognition) have been proposed. In this paper we present STN-OCR, a step towards semi-supervised neural networks for scene text recognition that can be optimized end-to-end. In contrast to most existing works that consist of multiple deep neural networks and several pre-processing steps we propose to use a single deep neural network that learns to detect and recognize text from natural images in a semi-supervised way. STN-OCR is a network that integrates and jointly learns a spatial transformer network <ref type="bibr" target="#b15">[16]</ref>, that can learn to detect text regions in an image, and a text recognition network that takes the identified text regions and recognizes their textual content. We investigate how our model behaves on a range of different tasks (detection and recognition of characters, and lines of text). Experimental results on public benchmark datasets show the ability of our model to handle a variety of different tasks, without substantial changes in its overall network structure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text is ubiquitous in our daily lifes. Text can be found on documents, road signs, billboards, and other objects like cars or telephones. Automatically detecting and reading text from natural scene images is an important part of systems that can be used for several challenging tasks such as image-based machine translation, autonomous cars or image/video indexing. In recent years the task of detecting text and recognizing text in natural scenes has seen much interest from the computer vision and document analysis community. Furthermore recent breakthroughs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref> in other areas of computer vision enabled the creation of even better scene text detection and recognition systems than before <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">28]</ref>. Although the problem of Optical  <ref type="figure">Figure 1</ref>. Schematic overview of our proposed system. The input image is fed to a single neural network that consists of a text detection part and a text recognition part. The text detection part learns to detect text in a semi-supervised way, by being jointly trained with the recognition part.</p><p>Character Recognition (OCR) can be seen as solved for printed document texts it is still challenging to detect and recognize text in natural scene images. Images containing natural scenes exhibit large variations of illumination, perspective distortions, image qualities, text fonts, diverse backgrounds, etc. The majority of existing research works developed endto-end scene text recognition systems that consist of complex two-step pipelines, where the first step is to detect regions of text in an image and the second step is to recognize the textual content of that identified region. Most of the existing works only concentrate on one of these two steps.</p><p>In this paper, we present a solution that consists of a single Deep Neural Network (DNN) that can learn to detect and recognize text in a semi-supervised way. This is contrary to existing works, where text detection and text recognition systems are trained separately in a fully-supervised way. Recent work <ref type="bibr" target="#b2">[3]</ref> showed that Convolutional Neural Networks (CNNs) are capable of learning how to solve complex multi-task problems, while being trained in an endto-end manner. Our motivation is to use these capabilities of CNNs and create an end-to-end scene text recognition system that behaves more like a human by dividing the task at hand into smaller subtasks and solving these subtask independently from each other. In order to achieve this behavior we learn a single DNN that is able to divide the input im-age into subtasks (single characters, words or even lines of text) and solve these subtasks independently of each other. This is achieved by jointly learning a localization network that uses a recurrent spatial transformer <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">31]</ref> as attention mechanism and a text recognition network (see <ref type="figure">Figure 1</ref> for a schematic overview of the system). In this setting the network only receives the image and the labels for the text contained in that image as input. The localization of the text is learned by the network itself, making this approach semisupervised.</p><p>Our contributions are as follows: (1) We present a system that is a step towards solving end-to-end scene text recognition by integrating spatial transformer networks. (2) We train our proposed system end-to-end in a semi-supervised way. <ref type="bibr" target="#b2">(3)</ref> We demonstrate that our approach is able to reach state-of-the-art/competitive performance on a range of standard scene text detection and recognition benchmarks. (4) We provide our code 1 and trained models 2 to the research community.</p><p>This paper is structured in the following way: In section 2 we outline work of other researchers related to ours. Section 3 describes our proposed system in detail and provides best practices on how to train such a system. We show and discuss our results on standard benchmark datasets in section 4 and conclude our findings in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Over the course of years a rich environment of different approaches to scene text detection and recognition have been developed and published. Nearly all systems use a two-step process for performing end-to-end recognition of scene text. The first step is to detect regions of text and extract these regions from the input image. The second step is to recognize the textual content and return the text strings of these extracted text regions.</p><p>It is further possible to divide these approaches into three broad categories: (1) Systems relying on hand crafted features and human knowledge for text detection and text recognition. (2) Systems using deep learning approaches together with hand crafted features, or two different deep networks for each of the two steps. (3) Systems that do not consist of a two step approach but rather perform text detection and recognition using a single deep neural network. We will discuss some of these systems for each category below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hand Crafted Features</head><p>In the beginning methods based on hand crafted features and human knowledge have been used to perform text detection. These systems used features like MSERs <ref type="bibr" target="#b24">[24]</ref>, Stroke Width Transforms <ref type="bibr" target="#b3">[4]</ref> or HOG-Features <ref type="bibr" target="#b32">[32]</ref> to identify regions of text and provide them to the text recognition stage of the system. In the text recognition stage sliding window classifiers <ref type="bibr" target="#b20">[21]</ref> and ensembles of SVMs <ref type="bibr" target="#b34">[34]</ref> or k-Nearest Neighbor classifiers using HOG features <ref type="bibr" target="#b33">[33]</ref> were used. All of these approaches use hand crafted features that have a large variety of hyper parameters that need expert knowledge to correctly tune them for achieving the best results.</p><p>Deep Learning Approaches More recent systems exchange approaches based on hand crafted features in one or both steps of end-to-end recognition systems by approaches using DNNs. Gómez and Karatzas <ref type="bibr" target="#b4">[5]</ref> propose a text-specific selective search algorithm that, together with a DNN, can be used to detect (distorted) text regions in natural scene images. Gupta et al. <ref type="bibr" target="#b8">[9]</ref> propose a text detection model based on the YOLO-Architecture <ref type="bibr" target="#b25">[25]</ref> that uses a fully convolutional deep neural network to identify text regions. The text regions identified by these approaches can then be used as input for further systems based on DNNs that perform text recognition.</p><p>Bissacco et al. <ref type="bibr" target="#b0">[1]</ref> propose a complete end-to-end architecture that performs text detection using hand crafted features. The identified text regions are binarized and then used as input to a deep fully connected neural network that classifies each found character independently. Jaderberg et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref> propose several systems that use deep neural networks for text detection and text recognition. In <ref type="bibr" target="#b16">[17]</ref> Jaderberg et al. propose a sliding window text detection approach that slides a convolutional text detection model across the image in multiple resolutions. The text recognition stage uses a single character CNN, which is slided across the identified text region. This CNN shares its weights with the CNN used for text detection. In <ref type="bibr" target="#b14">[15]</ref> Jaderberg et al. propose to use a region proposal network with an extra bounding box regression CNN for text detection and a CNN that takes the whole text region as input and performs classification across a pre-defined dictionary of words, making this approach only applicable to one given language.</p><p>Goodfellow et al. <ref type="bibr" target="#b5">[6]</ref> propose a text recognition system for house numbers, that has been refined by Jaderberg et al. <ref type="bibr" target="#b17">[18]</ref> for unconstrained text recognition. This system uses a single CNN, which takes the complete extracted text region as input, and provides the text contained in that text region. This is achieved by having one independent classifier for each possible character in the given word. Based on this idea He et al. <ref type="bibr" target="#b10">[11]</ref> and Shi et al. <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref> propose text recognition systems that treat the recognition of characters from the extracted text region as a sequence recognition problem. He et al. <ref type="bibr" target="#b10">[11]</ref> use a naive sliding window approach that creates slices of the text region, which are used as input to their text recognition CNN. The features produced by the text recognition CNN are used as input to a Recurrent Neural Network (RNN) that predicts the sequence of characters. In our experiments on pure scene text recognition (see section 4.3 for more information) we use a similar approach, but our system uses a more sophisticated sliding window approach, where the choice of the sliding windows is automatically learned by the network and not engineered by hand. Shi et al. <ref type="bibr" target="#b27">[27]</ref> utilize a CNN that uses the complete text region as input and produces a sequence of feature vectors, which are fed to a RNN that predicts the sequence of characters in the extracted text region. This approach generates a fixed number of feature vectors based on the width of the text region. That means for a text region that only contains a few characters, but has the same width as a text region with sufficently more characters, this approach will produce the same amount of feature vectors used as input to the RNN. In our pure text recognition experiments we utilized the strength of our approach to learn to attend to the most important information in the extracted text region, hence producing only as many feature vectors as necessary. Shi et al. <ref type="bibr" target="#b28">[28]</ref> improve their approach by firstly adding an extra step that utilizes the rectification capabilities of Spatial Transformer Networks <ref type="bibr" target="#b15">[16]</ref> for rectifying the extracted text line. Secondly they added a soft-attention mechanism to their network that helps to produce the sequence of characters in the input image. In their work Shi et al. make use of Spatial Transformers as an extra pre-processing step to make it easier for the recognition network to recognize the text in the image. In our system we use the Spatial Transformer as a core building block for detecting text in a semisupervised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-End trainable Approaches</head><p>The presented systems always use a two-step approach for detecting and recognizing text from scene text images. Although recent approaches make use of deep neural networks they are still using a huge amount of hand crafted knowledge in either of the steps or at the point where the results of both steps are fused together. Smith et al. <ref type="bibr" target="#b30">[30]</ref> propose an end-to-end trainable system that is able to detect and recognize text on french street name signs, using a single DNN. In contrast to our system it is not possible for the system to provide the location of the text in the image, only the textual content can be extracted. Furthermore the attention mechanism used in our approach shows a more human-like behaviour because is sequentially localizes and recognizes text from the given image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed System</head><p>A human trying to find and read text will do so in a sequential manner. The first action is to put attention on a line of text, read each character sequentially and then attend to the next line of text. Most current end-to-end systems for scene text recognition do not behave in that way. These</p><formula xml:id="formula_0">I O 1 O 2 Figure 2.</formula><p>Operation method of grid generator and image sampler. First the grid generator uses the N affine transformation matrices A n θ to create N equally spaced sampling grids (red and blue grids on the left side). These sampling grids are used by the image sampler to extract the image pixels at that location, in this case producing the two output images O 1 and O 2 . The corners of the generated sampling grids provide the vertices of the bounding box for each text region that has been found by the network.</p><p>systems rather try to solve the problem by extracting all information from the image at once. Our system first tries to attend sequentially to different text regions in the image and then recognize the textual content of each text region. In order to this we created a simple DNN consisting of two stages: (1) text detection (2) text recognition. In this section we will introduce the attention concept used by the text detection stage and the overall structure of the proposed system. We also report best practices for successfully training such a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Detecting Text with Spatial Transformers</head><p>A spatial transformer proposed by Jaderberg et al. <ref type="bibr" target="#b15">[16]</ref> is a differentiable module for DNNs that takes an input feature map I and applies a spatial transformation to this feature map, producing an output feature map O. Such a spatial transformer module is a combination of three parts. The first part is a localisation network computing a function f loc , that predicts the parameters θ of the spatial transformation to be applied. These predicted parameters are used in the second part to create a sampling grid that defines which features of the input feature map should be mapped to the output feature map. The third part is a differentiable interpolation method that takes the generated sampling grid and produces the spatially transformed output feature map O. We will shortly describe each component in the following paragraphs.</p><p>Localization Network The localization network takes the input feature map I ∈ R C×H×W , with C channels, height H and width W and outputs the parameters θ of the transformation that shall be applied. In our system we use the localization network (f loc ) to predict N two-dimensional affine transformation matrices A n θ , where n ∈ {0, . . . , N − 1}:</p><formula xml:id="formula_1">f loc (I) = A n θ = θ n 1 θ n 2 θ n 3 θ n 4 θ n 5 θ n 6<label>(1)</label></formula><p>N is thereby the number of characters, words or textlines the localization network shall localize. The affine transformation matrices predicted in that way allow the network to apply translation, rotation, zoom and skew to the input image, hence the network learns to produce transformation parameters that can zoom on characters, words or text lines that are to be extracted from the image.</p><p>In our system the N transformation matrices A n θ are produced by using a feed-forward CNN together with a RNN. Each of the N transformation matrices is computed using the hidden state h n for each time-step of the RNN:</p><formula xml:id="formula_2">c = f conv loc (I) (2) h n = f rnn loc (c, h n−1 ) (3) A n θ = g loc (h n )<label>(4)</label></formula><p>where g loc is another feed-forward network, and each transformation matrix A n θ is conditioned on the globally extracted convolutional features (f conv loc ) together with the hidden state of the previously performed time-step.</p><p>The CNN in the localization network used by us is a variant of the well known ResNet by He et al. <ref type="bibr" target="#b9">[10]</ref>. We use a variant of ResNet because we found that with this network structure our system learns faster and more successful, as compared to experiments with other network structures like the VGGNet <ref type="bibr" target="#b29">[29]</ref>. We argue that this is due to the fact that the residual connections of the ResNet help with retaining a strong gradient down to the very first convolutional layers. In addition to the structure we also used Batch Normalization <ref type="bibr" target="#b12">[13]</ref> for all our experiments. The RNN used in the localization network is a Bidirectional Long-Short Term Memory (BLSTM) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> unit. This BLSTM is used to generate the hidden states h n , which in turn are used to predict the affine transformation matrices. We used the same structure of the network for all our experiments we report in section 4. Image Sampling The N sampling grids G n i produced by the grid generator are now used to sample values of the feature map I at their corresponding coordinates u n i , v n j for each n ∈ N . Naturally these points will not always perfectly align with the discrete grid of values in the input feature map. Because of that we use bilinear sampling that extracts the value at a given coordinate by bilinear interpolating the values of the nearest neighbors. With that we define the values of the N output feature maps O n at a given location i, j where i ∈ H o and j ∈ W o :</p><formula xml:id="formula_3">O n ij = Ho h Wo w I hw max(0, 1−|u n i −h|)max(0, 1−|v n j −w|)<label>(6)</label></formula><p>This bilinear sampling is (sub-)differentiable, hence it is possible to propagate error gradients to the localization network by using standard backpropagation.</p><p>The combination of localization network, grid generator and image sampler forms a spatial transformer and can in general be used in every part of a DNN. In our system we use the spatial transformer as the first step of our network. The localization network receives the input image as input feature map and produces a set of affine transformation matrices that are used by the grid generator to calculate the position of the pixels that shall be sampled by the bilinear sampling operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Text Recognition Stage</head><p>The image sampler of the text detection stage produces a set of N regions that are extracted from the original input image. The text recognition stage (a structural overview of this stage can be found in <ref type="figure">Figure 3</ref>) uses each of these N different regions and processes them independently of each other. The processing of the N different regions is handled by a CNN. This CNN is also based on the ResNet architecture as we found that we could only achieve good results if we use a variant of the ResNet architecture for our recognition network. We argue that using a ResNet in the recognition stage is even more important than in the detection BBoxes of text regions <ref type="figure">Figure 3</ref>. The network used in our work consists of two major parts. The first is the localization network that takes the input image and predicts N transformation matrices, that are applied to N identical grids, forming N different sampling grids. The generated sampling grids are used in two ways: (1) for calculating the bounding boxes of the identified text regions (2) for sampling the input image with N sampling grids to extract N text regions. The N extracted text images are then used in the recognition network to perform text recognition. The whole system is trained end-to-end by only supplying information about the text labels for each text region. stage, because the detection stage needs to receive strong gradients from the recognition stage in order to successfully update the weights of the localization network. The CNN of the recognition stage predicts a probability distributionŷ over the label space L ǫ , where L ǫ = L ∪ {ǫ}, with L = {0 − 9a − z} and ǫ representing the blank label. Depending on the task this probability distribution is either generated by a fixed number of T softmax classifiers, where each softmax classifier is used to predict one character of the given word:</p><p>x n = O n (7) y n t = sof tmax(f rec (x n )) (8)</p><formula xml:id="formula_4">y n = T t=1ŷ n t<label>(9)</label></formula><p>where f rec (x) is the result of applying the convolutional feature extractor on the sampled input x.</p><p>Another possibility is to train the network using Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b6">[7]</ref> and retrieve the most probable labeling by settingŷ to be the most probable labeling path π, that is given by:</p><formula xml:id="formula_5">p(π|x n ) = T t=1ŷ n πt , ∀π ∈ L T ǫ (10) y n t = argmaxp(π|x n )<label>(11)</label></formula><formula xml:id="formula_6">y n = B( T t=1ŷ n t )<label>(12)</label></formula><p>with L T ǫ being the set of all labels that have the length T and p(π|x n ) being the probability that path π ∈ L T ǫ is predicted by the DNN. B is a function that removes all predicted blank labels and all repeated labels (e.g. B(-IC-CC-V) = B(II-CCC-C-V-) = ICCV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Training</head><p>The training set X used for training the model consists of a set of input images I and a set of text labels L I for each input image. We do not use any labels for training the text detection stage. This stage is learning to detect regions of text only by using the error gradients obtained by either calculating the cross-entropy loss or the CTC loss of the predictions and the textual labels. During our experiments we found that, when trained from scratch, a network that shall detect and recognize more than two text lines does not converge. The solution to this problem is to perform a series of pre-training steps where the difficulty is gradually increasing. Furthermore we find that the optimization algorithm chosen to train the network has a great influence on the convergence of the network. We found that it is beneficial to use Stochastic Gradient Descent (SGD) for pretraining the network on a simpler task and Adam <ref type="bibr" target="#b19">[20]</ref> for finetuning the already pre-trained network on images with more text lines. We argue that SGD performs better during pre-training because the learning rate η is kept constant during a longer period of time, which enables the text detection stage to explore the input images and better find text regions. With decreasing learning rate the updates in the detection stage become smaller and the text detection stage (ideally) settles on already found text regions. At the same time the text recognition network can start to use the extracted text regions and learn to recognize the text in that regions. While training the network with SGD it is important to note that choosing a too high learning rate will result in divergence of the model early on. We found that using initial learning rates between 1 −5 and 5 −7 tend to work in nearly all cases, except in cases where the network should only be fine-tuned. Here we found that using Adam is the more reliable choice, as Adam chooses the learning rate for each parameter in an adaptive way and hence does not allow the detection network to explore as radically as it does when using SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we evaluate our presented network architecture on several standard scene text detection/recognition datasets. We present the results of experiments for three different datasets, where the difficulty of the task at hand increases for each dataset. We first begin with experiments on the SVHN dataset <ref type="bibr" target="#b23">[23]</ref>, that we used to prove that our concept as such is feasible. The second type of dataset we performed experiments on were datasets for focused scene text recognition, where we explored the performance of our model, when it comes to find and recognize single characters. The third dataset we exerimented with was the French Street Name Signs (FSNS) dataset <ref type="bibr" target="#b30">[30]</ref>, which is the most challenging dataset we used, as this dataset contains a vast amount of irregular, low resolution text lines that are more difficult to locate and recognize than text lines from the SVHN dataset. We begin this section by introducing our experimental setup. We will then present the results and characteristics of the experiments for each of the aforementioned datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Localization Network The localization network used in every experiment is based on the ResNet architecture <ref type="bibr" target="#b9">[10]</ref>. The input to the network is the image where text shall be localized and later recognized. Before the first residual block the network performs a 3×3 convolution followed by a 2×2 average pooling layer with stride 2. After these layers three residual blocks with two 3 × 3 convolutions, each followed by batch normalization <ref type="bibr" target="#b12">[13]</ref>, are used. The number of convolutional filters is 32, 48 and 48 respectively and ReLU <ref type="bibr" target="#b22">[22]</ref> is used as activation function for each convolutional layer. A 2 × 2 max-pooling with stride 2 follows after the second residual block. The last residual block is followed by a 5×5 average pooling layer and this layer is followed by a BLSTM with 256 hidden units. For each time step of the BLSTM a fully connected layer with 6 hidden units follows. This layer predicts the affine transformation matrix, that is used to generate the sampling grid for the bilinear interpolation. As rectification of scene text is beyond the scope of this work we disabled skew and rotation in the affine transformation matrices by setting the according parameters to 0. We will discuss the rectification capabilities of Spatial Transformers for scene text detection in our future work.</p><p>Recognition Network The inputs to the recognition network are N crops from the original input image that represent the text regions found by the localization network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>64px Maxout CNN <ref type="bibr" target="#b5">[6]</ref> 96 ST-CNN <ref type="bibr" target="#b15">[16]</ref> 96.3 Ours 95.2 <ref type="table">Table 1</ref>. Sequence recognition accuracies on the SVHN dataset. When recognizing house number on crops of 64 × 64 pixels, following the experimental setup of <ref type="bibr" target="#b5">[6]</ref> The recognition network has the same structure as the localization network, but the number of convolutional filters is higher. The number of convolutional filters is 32, 64 and 128 respectively. Depending on the experiment we either used an ensemble of T independent softmax classifiers as used in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b16">[17]</ref>, where T is the maximum length that a word may have, or we used CTC with best path decoding as used in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b27">[27]</ref>.</p><p>Implementation We implemented all our experiments using MXNet <ref type="bibr" target="#b1">[2]</ref>. We conduted all our experiments on a work station which has an Intel(R) Core(TM) i7-6900K CPU, 64 GB RAM and 4 TITAN X (Pascal) GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on the SVHN dataset</head><p>With our first experiments on the SVHN dataset <ref type="bibr" target="#b23">[23]</ref> we wanted to prove that our concept works and can be used with real world data. We therefore first conducted experiments similar to the experiments in <ref type="bibr" target="#b15">[16]</ref> on SVHN image crops with a single house number in each image crop, that is centered around the number and also contains background noise. <ref type="table">Table 1</ref> shows that we are able to reach competitive recognition accuracies.</p><p>Based on this experiment we wanted to determine whether our model is able to detect different lines of text that are arranged in a regular grid or placed at random locations in the image. In <ref type="figure" target="#fig_3">Figure 4</ref> we show samples from two purpose build datasets 3 that we used for our other experiments based on SVHN data. We found that our network performs well on the task of finding and recognizing house numbers that are arranged in a regular grid. An interesting observation we made during training on this data was that we were able to achieve our best results when we did two training steps. The first step was to train the complete model from scratch (all weights initialized randomly) and then train the model with the same data again, but this time with the localization network pre-initialized with the weights obtained from the last training and the recognition net initialized with random weights. This strategy leads to better localization results of the localization network and hence improved recognition results.</p><p>During our experiments on the second dataset, created by us, we found that it is not possible to train a model from scratch, that can find and recognize more than two textlines that are scattered across the whole image. It is possible to train such a network by first training the model on easier tasks first (few textlines, textlines closer to the center of the image) and then increase the difficulty of the task gradually.</p><p>In the supplementary material we provide short video clips that show how the network is exploring the image while learning to detect text for a range of different experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Robust Reading Datasets</head><p>In our next experiments we used datasets where text regions are aleady cropped from the input images. We wanted to see whether our text localization network can be used as an intelligent sliding window generator that adopts to irregularities of the text in the cropped text region. Therefore we trained our recognition model using CTC on a dataset of synthetic cropped word images, that we generated using our own data generator, that works similar to the data generator introduced by Jaderberg et al. <ref type="bibr" target="#b13">[14]</ref>. In <ref type="table">Table 2</ref> we report the recognition results of our model on the IC-DAR 2013 robust reading <ref type="bibr" target="#b18">[19]</ref>, the Street View Text (SVT) <ref type="bibr" target="#b32">[32]</ref> and the IIIT5K <ref type="bibr" target="#b20">[21]</ref> benchmark datasets. For evaluation on the ICDAR 2013 and SVT datasets, we filtered all images that contain non-alphanumeric characters and discarded all images that have less than 3 characters as done in <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b32">32]</ref>. We obtained our final results by post-processing the predictions using the standard hunspell english (en-US) dictionary. Overall we find that our model achieves stateof-the-art performance for unconstrained recognition models on the ICDAR 2013 and IIIT5K dataset and competitive performance on the SVT dataset. In <ref type="figure">Figure 5</ref> we show that our model learns to follow the slope of the individual text regions, proving that our model produces sliding windows in an intelligent way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ICDAR 2013 SVT IIIT5K Photo-OCR <ref type="bibr" target="#b0">[1]</ref> 87.6 78.0 -CharNet <ref type="bibr" target="#b17">[18]</ref> 81.8 71.7 -DictNet* <ref type="bibr" target="#b14">[15]</ref> 90.8 80.7 -CRNN <ref type="bibr" target="#b27">[27]</ref> 86.7 80.8 78.2 RARE <ref type="bibr" target="#b28">[28]</ref> 87.5 81.9 81.9 Ours 90.3 79.8 86 <ref type="table">Table 2</ref>. Recognition accuracies on the ICDAR 2013, SVT and IIIT5K robust reading benchmarks. Here we only report results that do not use per image lexicons. (* <ref type="bibr" target="#b14">[15]</ref> is not lexicon-free in the strict sense as the outputs of the network itself are constrained to a 90k dictionary.) <ref type="figure">Figure 5</ref>. Samples from ICDAR, SVT and IIIT5K datasets that show how well our model finds text regions and is able to follow the slope of the words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Preliminary Experiments on the FSNS dataset</head><p>Following our scheme of increasing the difficulty of the task that should be solved by the network, we chose the French Street Name Signs (FSNS) dataset by Smith et al. <ref type="bibr" target="#b30">[30]</ref> to be our third dataset to perform experiments on. The results we report here are preliminary and are only meant to show that our network architecture is also applicable to this kind of data, although it does not yet reach state-of-theart results. The FSNS dataset contains images of french street name signs that have been extracted from Google Streetview. This dataset is the most challenging dataset for our approach as it (1) contains multiple lines of text with varying length embedded in natural scenes with distracting backgrounds and (2) contains a lot of images that do not include the full name of the streets.</p><p>During our first experiments with that dataset we found that our model is not able to converge, when trained on the supplied groundtruth. We argue that this is because the labels of the original dataset do not include any hint on which words can be found in which text line. We therefore changed our approach and started with experiments where we tried to find individual words instead of textlines with <ref type="figure">Figure 6</ref>. Samples from the FSNS dataset, these examples show that our system is able to detect a range of differently arranged text lines and also recognize the content of these words more than one word. We adapted the groundtruth accordingly and used all images that contain a maximum of three words for our experiments, which leaves us with approximately 80 % of the original data from the dataset. <ref type="figure">Figure 6</ref> shows some examples from the FSNS dataset where our model correctly localized the individual words and also correctly recognized the words. Using this approach we were able to achieve a reasonably good character recognition accuracy of 97 % on the test set, but only a word accuracy of 71.8 %. The discrepancy in character recognition rate and word recognition rate is caused by the fact that the model we trained for this task uses independent softmax classifiers for each character in a word. Having a character recognition accuracy of 97 % means that there is a high probability that at least one classifier makes a mistake and thus increases the sequence error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we presented a system that can be seen as a step towards solving end-to-end scene text recognition, using only a single multi-task deep neural network. We trained the text detection component of our model in a semi-supervised way and are able to extract the localization results of the text detection component. The network architecture of our system is simple, but it is not easy to train this system, as a successful training requires extensive pretraining on easier sub-tasks before the model can converge on the real task. We also showed that the same network architecture can be used to reach competitive or state-of-theart results on a range of different public benchmark datasets for scene text detection/recognition.</p><p>At the current state we note that our models are not fully capable of detecting text in arbitrary locations in the image, as we saw during our experiments with the FSNS dataset. Right now our model is also constrained to a fixed number of maximum textlines/characters that can be detected at once, in our future work we want to redesign the network in a way that makes it possible for the network to determine the number of textlines in an image by itself.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 y ho 1  ( 5 )</head><label>315</label><figDesc>provides a structural overview of this network. Grid Generator The grid generator uses a regularly spaced grid G o with coordinates y ho , x wo , of height H o and width W o , together with the affine transformation matrices A n θ to produce N regular grids G n i of coordinates u n i , v n j of the input feature map I, where i ∈ H o and j ∈ W o : During inference we can extract the N resulting grids G n i which contain the bounding boxes of the text regions found by the localization network. Height H o and width W o can be chosen freely and if they are lower than height H or width W of the input feature map I the grid generator is producing a grid that performs a downsampling operation in the next step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Samples from our generated datasets, including BBoxes predicted by our model. Left: Sample from regular grid dataset, Right: Sample from dataset with randomly positioned house numbers.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Bartzi/stn-ocr 2 https://bartzi.de/research/stn-ocr</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">datasets are available here: https://bartzi.de/research/stn-ocr</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Photoocr: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="785" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Textproposals: a textspecific selective search algorithm for word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02619</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multi-digit number recognition from street view imagery using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<idno>ICLR2014</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading scene text in deep convolutional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3501" to="3508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning, NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014, number 8692 in Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014, number 8692 in Lecture Notes in Computer Science</title>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Represenations</title>
		<meeting>the 3rd International Conference on Learning Represenations<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<idno>BMVC 2012-23rd</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">British Machine Vision Association</title>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="127" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2010, number 6494 in Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="770" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end interpretation of the french street name signs dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="411" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.05329</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Recurrent spatial transformer networks</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2010, number 6311 in Lecture Notes in Computer Science</title>
		<editor>K. Daniilidis, P. Maragos, and N. Paragios</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4042" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
