<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regularization Strategy for Point Cloud via Rigidly Mixed Sample</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dogyoon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeha</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyeop</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyeok</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Woo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
							<email>syleee@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Regularization Strategy for Point Cloud via Rigidly Mixed Sample</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation is an effective regularization strategy to alleviate the overfitting, which is an inherent drawback of the deep neural networks. However, data augmentation is rarely considered for point cloud processing despite many studies proposing various augmentation methods for image data. Actually, regularization is essential for point clouds since lack of generality is more likely to occur in point cloud due to small datasets. This paper proposes a Rigid Subset Mix (RSMix) 1 , a novel data augmentation method for point clouds that generates a virtual mixed sample by replacing part of the sample with shape-preserved subsets from another sample. RSMix preserves structural information of the point cloud sample by extracting subsets from each sample without deformation using a neighboring function. The neighboring function was carefully designed considering unique properties of point cloud, unordered structure and non-grid. Experiments verified that RSMix successfully regularized the deep neural networks with remarkable improvement for shape classification. We also analyzed various combinations of data augmentations including RSMix with single and multi-view evaluations, based on abundant ablation studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have achieved outstanding performances in various fields regardless of the data domains, such as image, video, speech, and point cloud. In particular, three-dimensional (3D) point cloud processing is attracting considerable interest following the pioneering network PointNet <ref type="bibr" target="#b21">[22]</ref> development, since point clouds can be applied directly to deep learning without preprocessing. Although various tasks have been successfully addressed using point clouds, such as 3D object shape classification and part segmentation, inherent drawback of deep learning is still less considered in the point cloud domain. Due to <ref type="bibr" target="#b0">1</ref> Project page: https:// github.com/ dogyoonlee/ RSMix the typical nature of deep neural networks (DNNs) that approximates the model from the given data distribution, the trained model tends to be overfitted regardless of the data domain. This lack of generality is a fundamental deep learning problem. One way to alleviate overfitting and generalize the model is data augmentation, which improves diversity of the training data.</p><p>Various data augmentation methods have been recently proposed in the image domain as network regularization strategies, but data augmentation for point clouds has only rarely been considered. Actually, regularization is essential for point clouds since it is easier to be biased to the distribution of training samples than that of image. That is largely because point cloud datasets <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref> are typically considerably smaller and less diverse than image datasets, such as ImageNet <ref type="bibr" target="#b6">[7]</ref> and MSCOCO <ref type="bibr" target="#b19">[20]</ref>, which have millions of training data. For example, ModelNet40 <ref type="bibr" target="#b30">[31]</ref>, one of the most widely used point cloud dataset, includes only 12,311 models with 40 categories. Therefore, it is essential to improve the generality of models for point cloud.</p><p>In the image domain, regional dropout <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref> and mixup-based methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b10">11]</ref> have been proposed as data augmentation strategies, which are different to conventional methods, to generate virtual training samples. These methods are designed to improve generality of the neural network, preventing the model from being significantly affected by only small part of the sample that has discriminative characteristics by eliminating or mixing the part of the data. However, it is difficult to apply this intuition directly to the point clouds due to two inherent properties of point cloud: non-grid and order-invariance. Although Chen et al. <ref type="bibr" target="#b3">[4]</ref> applied the concept of Mixup <ref type="bibr" target="#b33">[34]</ref> to point clouds handling the properties of point cloud with linear interpolation based on optimal assignment, generated samples lost the structural information of the original sample due to distortion. This paper proposes Rigid Subset Mix (RSMix), the shape-preserving data augmentation method for point clouds that can partially mix two samples, preserving the partial shapes of the original samples. We redefine the concept of mask region from image analysis and adapt it to 3D space to extract parts from each sample while preserving structural information of the point cloud. We also define a Rigid Subset (RS) derived from the redefined mask region, a group of adjacent points within a certain distance from a specific query point using a neighboring function to address unique characteristics: unordered structure and non-grid. In contrast to PointMixup <ref type="bibr" target="#b3">[4]</ref>, we can utilize structural information of the original point cloud sample intactly by using RS. In addition, we designed RS scale to vary, to improve diversity of the training sample, and hence increase regularization effects. Furthermore, RSMix can be used in conjunction with the existing data augmentation since it utilizes the part of the given data intactly. In the end, by introducing RS, we can improve generality of DNNs and give attention them to recognize parts of the object. In Section 3.2, we describe in detail how to generate the virtual sample preserving shape of the source sample by extracting RS. In advance, we provide visualized RS samples to be extracted and resultant mixed samples in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We provide the experimental results for shape classification on ModelNet40 <ref type="bibr" target="#b30">[31]</ref> and ModelNet10 <ref type="bibr" target="#b30">[31]</ref> with the most representative DNN approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref> for point clouds. RSMix successfully improved the network performance, outperforming the existing data augmentation methods. Moreover, abundant ablation studies for various combinations of existing data augmentation and RSMix verified that RSMix improved the model regardless of which conventional data augmentation method was employed.</p><p>Meanwhile, we analyzed the experimental results with respect to two evaluation mechanisms to ensure fair comparisons. In fact, although the evaluation methods of shape classification on point cloud are divided into two ways: single and multi-view, many studies present their experimental results without clearly specifying their mechanism. This makes hard to quantitatively compare results among studies. Our experiments show that the results evaluated by single and multi-view approaches have significant differences. Therefore, it is essential to analyze experimental results along the evaluation methods. Sections 4.2 presents analysis with single and multi-view approaches based on ablation studies. To summarize, this paper provides the following major contributions.</p><p>• Shape preserving augmentation. We propose new data augmentation method for point clouds that mixes training samples with preserved structures by using Rigid Subset (RS).</p><p>• Significant improvement. The proposed method remarkably improves DNN performances and robustness for shape classification and outperforms existing data augmentation strategies.</p><p>• Complementary method. RSMix can be used in conjunction with other data augmentation approaches. Abundant ablation studies verify that RSMix can be combined well with other augmentations and further improves the target model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Data Augmentation for Images. Data augmentation is a regularization methods that expands the knowledge range that can be learned from training data by transforming data while retaining the essential sample meaning. Thus, the model becomes less dependent on the specific given data. Various methods have been proposed in the image domain in addition to conventional methods, such as random rotation, flip and crop. Some works have enabled the model to learn spatially distributed representation by removing the part of the data on pixel <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8]</ref> or feature map <ref type="bibr" target="#b9">[10]</ref> basis. Furthermore, several mixup-based methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b10">11]</ref> have been proposed that generate virtual samples by combining the two samples. Mixup <ref type="bibr" target="#b33">[34]</ref> generates virtual training samples by linearly interpolating two images and defining the mixed area ratio as a corresponding label. By introducing the combination between data, Mixup brings out the regularization effect and shows improved performance for several tasks. After Mixup, Verma et al. <ref type="bibr" target="#b27">[28]</ref> extended Mixup by applying the concept to the feature map, and Yun et al. <ref type="bibr" target="#b32">[33]</ref> fusioned the concept of <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b7">[8]</ref> to improve localization and classification ability of the model. In addition, Kim et al. <ref type="bibr" target="#b13">[14]</ref> Method</p><formula xml:id="formula_0">Mix function f M d (x α , x β )</formula><p>Mixup <ref type="bibr" target="#b33">[34]</ref> (1 − λ)x α + λx β Manifold Mixup <ref type="bibr" target="#b27">[28]</ref> (1 − λ)f (x α ) + λf (x β ) CutMix <ref type="bibr" target="#b32">[33]</ref> (1 − M) x α + M x β Puzzle Mix <ref type="bibr" target="#b13">[14]</ref> ( and Harris et al. <ref type="bibr" target="#b10">[11]</ref> utilized saliency maps <ref type="bibr" target="#b13">[14]</ref> and Fourier transform <ref type="bibr" target="#b10">[11]</ref>, respectively, to use semantically representative parts of the data when generating virtual samples. However, these approaches can only be applied to image based models rather than point clouds, because they have different data structures. Therefore, we propose the RSMix, novel mixup-based augmentation strategy for the point cloud that generates virtual samples considering the unique properties of point clouds . Point Cloud Structural Properties. In contrast with images, point clouds have 3D coordinate information, including implicit geometric feature, which is essential to understand them. Due to the unique properties of point cloud: non-grid and unordered structures, it is difficult to extract the local and geometric feature from point clouds. Various networks have been proposed with different structures, such as point-wise multi layer perceptron (MLP) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32]</ref>, convolution <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16]</ref>, graph <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36]</ref>, and spatial partitioning structured <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref> based networks to extract local and geometric features. For example, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15]</ref> extract local and geometric features by applying point-wise grouping, radius-based kernel, graphstructure, and space partitioning tree, respectively. All these networks demonstrated that considering structural information is significantly important for the DNN model to understand point clouds. Therefore, regardless of how data augmentation occurs, structural information of point cloud should be regarded as core characteristics since it is a critical component for the model. Data Augmentation on Point Cloud. Data augmentation has not been extensively explored in the point cloud domain, aside from general conventional methods, such as randomly scaling, rotation, and jittering. Few studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref> dealt with data augmentation in the point cloud domain. Liu et al. <ref type="bibr" target="#b17">[18]</ref> proposed auto-augmentation network for point clouds to find an optimal combination of conventional data augmentation methods corresponding to each sample. Choi et al. <ref type="bibr" target="#b4">[5]</ref> divided the sample into specific partitions and transformed or mixed each part independently. However, there is a limit to diversity of virtual sample because mixing is performed on inter-classes and specified grids are used for partitioning. PointMixup <ref type="bibr" target="#b3">[4]</ref>, which is the closest method to our proposed approach, extends the concept of Mixup <ref type="bibr" target="#b33">[34]</ref> to point clouds through linear interpolation with optimal assignments between two samples. However, the generated samples have distorted structures which lead to the loss of structural information. Structural information is a core point cloud feature since they have no textural information. Therefore, we propose a more general data augmentation method for point clouds that can preserve shape of the original data .</p><formula xml:id="formula_1">1 − z) Π T α x α + z Π T β x β F-Mix [11] (1 − H(G)) x α + H(G) x β PointMixup [4] (1 − λ)x α + λJ φ * (x α , x β )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>Neural networks aim to model function f that describes the true distribution P for given data</p><formula xml:id="formula_2">D = {(x i , y i )} n i=1 , where samples x ∈ X have corresponding labels y ∈ Y.</formula><p>It has been proved through Empirical Risk Minimization <ref type="bibr" target="#b26">[27]</ref> that f can be approximated by minimizing empirical risk R ξ (f ) of the model by computational optimization using loss L and empirical distribution P ξ for given data distribution as</p><formula xml:id="formula_3">R ξ (f ) = L(f (x), y)dP ξ (x, y) = 1 n n i=1 L(f (x i ), y i )</formula><p>(1) In data augmentation, P ξ can be expanded to P ψ with additional augmented data through vicinal risk minimization <ref type="bibr" target="#b2">[3]</ref>,</p><formula xml:id="formula_4">P ψ (x,ỹ) = 1 n n i=1 ψ(x,ỹ|x i , y i ),<label>(2)</label></formula><p>where ψ is a vicinity distribution, i.e., the probability that virtual sample and label pair (x,ỹ), are sampled from the vicinity of given sample and label pair (x i , y i ). For image data, Zhang et al. <ref type="bibr" target="#b33">[34]</ref> designed a vicinal distribution ψ that generated a virtual mixed sample-label pair (x,ỹ) from two paired data (x α , y α ) and (x β , y β ) using mix function f M d , for sample and f M l , for label, as</p><formula xml:id="formula_5">x = f M d (x α , x β ) = (1 − λ)x α + λx β , y = f M l (y α , y β ) = (1 − λ)y α + λy β ,<label>(3)</label></formula><p>where λ ∼ beta distribution Beta(θ, θ), for θ ∈ (0, ∞). <ref type="table" target="#tab_0">Table 1</ref> shows the deformations of f M d in various ways using features from model f <ref type="bibr" target="#b27">[28]</ref> or masking approaches such as binary mask M <ref type="bibr" target="#b32">[33]</ref>, salient data included mask z <ref type="bibr" target="#b13">[14]</ref>, or thresholding mask H <ref type="bibr" target="#b10">[11]</ref> with filtered data G in the frequency domain. However, these mask-based approaches cannot be applied directly to point cloud, since point clouds have no grid and points can exist anywhere in 3D real space. Though Chen et al. <ref type="bibr" target="#b3">[4]</ref> solved this problem by linear interpolation between two point clouds, introducing optimal assignment J φ * , they could not generate virtual samples preserving shape of the original sample. Therefore, our goal is to generate a shape-preserved virtual sample that has combined information from both samples as well as proposing an adapted spatial mask for 3D data. We are inspired by concept of the mask region from image analysis, which preserves the part of the original data intactly.</p><formula xml:id="formula_6">! " " − " ! " − " #$% " ! " ℎ − '()) ! '()) " Rigid Subset ing ℎ ℎ ( ! − " ) (a) Neighbored Rigid Subset (b) Extracted Rigid Subset (c) Inserted Sample 3 ! ℎ ! " − " "</formula><p>Mask as Region of Neighboring Data. CutMix <ref type="bibr" target="#b32">[33]</ref> defined the mix function as</p><formula xml:id="formula_7">f M d (x α , x β ) = (1 − M) x α + M x β ,<label>(4)</label></formula><p>where represents the element-wise multiplication; mask</p><formula xml:id="formula_8">M denotes a d u × d w binary rectangular region represented as [u M , u M +d u ]×[w M , w M +d w ] with mixture ratio, λ = dudw W H , where (u M , w M )</formula><p>is a randomly chosen pixel of the image. To utilize inherent definition of the mask region, we redefine mask M as a group of successive adjacent pixels within distances d u /2 and d w /2 in image. Hence, mask M can be denoted as</p><formula xml:id="formula_9">M = {(u i , w j )| |u i − u c | ≤ d u 2 , |w j − w c | ≤ d w 2 },<label>(5)</label></formula><p>where (u i , w j ) is the (i, j)th pixel for the given image; i = {1, 2, ..., W }; j = {1, 2, ..., H}; and (u c , w c ) is the center of the mask (u M + du 2 , w M + dw 2 ). Thus, the mask can be regarded as an adjacent group of data from a particular point (u c , w c ). We adapt this definition of mask to point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Rigid Subset Mix</head><p>Rigid Subset Mix (RSMix) mixes parts of two point cloud samples by extracting the Rigid Subset (RS), which preserves each samples's shape. RSMix is divided in three steps: neighboring, extraction, and insertion. First, we utilize the redefined mask region concept from Section 3.1 with neighboring function A to prevent deformation of original data. Then we extract RSs from each sample to mix the samples. Finally, we mix two RSs in the insertion step. <ref type="figure" target="#fig_1">Figure 2</ref> shows the RSMix algorithm pipeline. Neighboring Rigid Subset. We define two n sampled point sets normalized in the unit sphere as P t = {p t i | i = 1, 2, ..., n}, where t∈ {α, β}. p t i ∈ R 3 is its Euclidean coordinates, which represents location of the point. We only consider coordinate information since RSMix operates on point-wise coordinates.</p><p>We adapt the regional mask for image to spatial subset of each point sets from given point sets P α and P β , by grouping adjacent points from a certain query point, q t , randomly chosen from P t . These subsets are denoted as S α and S β according to below Equation <ref type="formula" target="#formula_10">(6)</ref>.</p><formula xml:id="formula_10">S t = A(P t ;q t ),<label>(6)</label></formula><p>which are grouped using the specific neighboring function A. We define these subsets as Rigid Subset (RS) since they preserve the sample shape rigidly. We introduce two instantiations for A to retain the original point set shape: K-Nearest Neighbor(KNN) to given q t and Ball-query method that neighboring points in certain distance r rigid from q t as</p><formula xml:id="formula_11">A knn (P t ;q t ) = { p t | p t is KNN of q t , p t ∈ P t }, A ball (P t ;q t ) = { p t | p t − q t ≤ r rigid , p t ∈ P t },<label>(7)</label></formula><p>respectively, where r rigid is sampled from beta distribution Beta(θ, θ), with parameter θ = 1.0 as default, i.e., the uniform distribution, since P t is normalized in the unit sphere. Both A are based on Euclidean distance in 3D space considering the point cloud's unordered structure and free space around them. Each method has different characteristics on neighboring subsets with respect to the density or directional bias of given point sets.</p><p>Meanwhile, we limit |S β | ≤ n max , where n max and | · | denote the upper bound number of points in RS and cardinality for the point set, respectively. We usually set n max = |P t |/2 to preserve at least half of the original point sets. In addition, when using A ball , we randomly sample points in S β along the difference between |S α | and |S β | to maintain the |P α mix |, where P α mix denotes a mixed sample described in below Insertion part. We compare and analyze the two methods for quantitative and visualized results in Section 4.2. Further, we also provide experiments with various θ values in Section 4.2. <ref type="figure" target="#fig_1">Figure 2(a)</ref> shows neighboring the RS from the each sample. Extraction. Neighbored RSs, S α and S β , are used to a generate mixture sample P α mix . To mix two samples, we remove the S α from P α and replace the empty space with S β . Hence, extracted RSs from each point cloud sample to generate mixture samples are denoted as P α − S α and S β as shown in <ref type="figure" target="#fig_1">Figure 2(b)</ref>. Insertion. However, q α and q β are usually different because they are randomly chosen from P α and P β , respectively. Hence, before insertion, S β should be translated by the difference between the two query points. We introduce translation function T β→α to translate S β by q α − q β as</p><formula xml:id="formula_12">T β→α (S β ; q α , q β ) = {p β→α | p β→α = p β + (q α − q β )},<label>(8)</label></formula><p>where p β is a point in S β . Applying T β→α to S β , the translated subset S β→α is denoted as</p><formula xml:id="formula_13">S β→α = T β→α (S β ; q α , q β ).<label>(9)</label></formula><p>Therefore, mixed sample P α mix is defined as</p><formula xml:id="formula_14">P α mix = (P α − S α ) ∪ S β→α ,<label>(10)</label></formula><p>and <ref type="figure" target="#fig_1">Figure 2</ref>(c) describes the inserted mixture sample.</p><p>Thus, mix function f M d (x α , x β ) for RSMix can be expressed using as follow Equation <ref type="formula" target="#formula_15">(11)</ref> using P α and P β instead of x α and x β .</p><formula xml:id="formula_15">f M d (P α , P β ) = (P α − A(P α )) ∪ T β→α (A(P β )),<label>(11)</label></formula><p>where input arguments related to query points q α and q β are omitted for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mixture Ratio λ for Training</head><p>In this Section, we define the mixture ratio λ, the ratio of |S β→α | w.r.t. |P α mix |, to train the network for shape classification. In contrast to A knn or some previous image masks, |P α −S α | and |S β | are often different when using the A ball , since we apply same r rigid to P α and P β , despite of their different densities. Hence, we define</p><formula xml:id="formula_16">λ =      0 , if P α = S α , 0 , if S β = ∅, |S β | / (|P α − S α | + |S β |) , Otherwise,<label>(12)</label></formula><p>To explicitly consider the relation between |P α − S α | and |S β |. Finally, we define the label mix function as</p><formula xml:id="formula_17">f M l (y α , y β ) = (1 − λ)y α + λy β ,<label>(13)</label></formula><p>which is same as in CutMix <ref type="bibr" target="#b32">[33]</ref>, to generate virtual label y for classification training. Detailed implementations are available in the supplementary material with pseudo-code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We evaluate RSMix on ModelNet40 <ref type="bibr" target="#b30">[31]</ref> and ModelNet10 <ref type="bibr" target="#b30">[31]</ref>, which are widely used point cloud classification benchmark datasets. ModelNet40 is small dataset which comprises 12,311 CAD models from 40 man-made object categories, and ModelNet10 is subset of ModelNet40 that includes only 4899 CAD models from 10 categories. We utilized the preprocessed data provided by PointNet <ref type="bibr" target="#b21">[22]</ref> for ModelNet40 with same train-test split, which were 1024 uniformly sampled points on mesh faces according to face area and normalized onto the unit sphere, and preprocessed ModelNet10 similarly. In particular, we ignored normals of samples since they are not available for real-data. Backbone Networks. We considered three representative point-wise DNNs for point cloud: PointNet++ <ref type="bibr" target="#b22">[23]</ref>, DGCNN <ref type="bibr" target="#b28">[29]</ref>, and PointNet <ref type="bibr" target="#b21">[22]</ref> as our backbone network architecture. We applied RSMix to several neural networks to emphasize RSMix is model agnostic.</p><p>Single and Multi-view Evaluations. Single and multiview evaluations are separated depending on whether objects were evaluated from different angles or not. These approaches can be separated into two cases: with or without voting strategy to predict an object multiple times by rotating about an axis. The experiments adopted voting strategy of evaluating an object 12 times, rotating it 30 • on its vertical (y) axis between evaluations. Meanwhile, Mod-elNet40 <ref type="bibr" target="#b30">[31]</ref> has 10 classes with aligned poses/headings. Thus, it is trivial to separate the 10 classes with the remaining 30 classes if we don't do random rotation on test samples when evaluating the model. Hence, there are obvious differences between the results from single and multiview evaluations. Appropriate combinations of augmentation strategies also vary depending on evaluation type. We investigated results from both evaluation strategies and explored optimal combinations of augmentations for different models by abundant ablation studies (Sections 4.2). Implementation details. We implemented RSMix using PointNet++ <ref type="bibr" target="#b22">[23]</ref> and DGCNN <ref type="bibr" target="#b28">[29]</ref> with conventional data augmentation, ConvDA (comprising jittering(σ 2 =0.01); scaling(0.8∼1.25); rotation along the y-axis i.e., gravity axis; and shifting (range=0.1) for the training dataset. Further details are included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Shape Classification</head><p>Evaluations. We evaluate RSMix for shape classification using three backbone networks on ModelNet40 and Mod-elNet10. All experiments were implemented using official codes and results are shown in <ref type="table" target="#tab_2">Table 2</ref>. "Multi" indicates the evaluation with multi-view. To ensure fair comparison given the rotational bias in ModelNet40, we exclude experimental results of point-wise MLP networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> trained without rotational augmentation in <ref type="table" target="#tab_2">Table 2</ref>. Section 4.2 presents a rotation-related ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method #Points Evaluation Accuracy(%) ModelNet40 ModelNet10</head><p>PointNet <ref type="bibr" target="#b21">[22]</ref> 1k 88.   All the results reveal that RSMix improved the network accuracies regardless of network type or evaluation methods, verifying the effectiveness of our shape-preserved mixture approach with significant improvements for Point-Net++ <ref type="bibr" target="#b22">[23]</ref> and DGCNN <ref type="bibr" target="#b28">[29]</ref>, which encode local or geometric features of object through hierarchical grouping or graph structure, respectively. Comparison against PointMixup <ref type="bibr" target="#b3">[4]</ref>. We demonstrate results of RSMix for two evaluation methods against Point-Mixup <ref type="bibr" target="#b3">[4]</ref>, the closest work to us, in <ref type="table" target="#tab_3">Table 3</ref>. We compared pre-aligned and unaligned settings to for single and multi-view accuracies, respectively, since PointMixup <ref type="bibr" target="#b3">[4]</ref> do not specify their evaluation method but each are similar. They follow the PointCNN <ref type="bibr" target="#b18">[19]</ref> setting discriminating pre-aligned and unaligned with horizontal rotation on point cloud samples. They randomly rotate the training point cloud along the y-axis for unaligned settings. For natural evaluation, we do not preprocess the dataset as pre-aligned or unaligned (denoted as Raw in <ref type="table" target="#tab_3">Table 3</ref>). RSMix achieves more competitive performance than PointMixup <ref type="bibr" target="#b3">[4]</ref> for networks that use local information <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>, and further enhances the network's ability to recognize local information.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Neighboring Function. This Section describes the two types of neighboring function, A ball and A knn , employed to extract RSs from point cloud samples. <ref type="figure">Figure 3 and 4</ref> show the differences between these functions under directional bias or different densities between point cloud samples qualitatively. Although A knn extracts the subset pre-  <ref type="table">Table 5</ref>: Ablation studies on evaluation accuracy with single(ACC S (%)) and multi-view(ACC M (%)) for Point-Net++ <ref type="bibr" target="#b22">[23]</ref> on ModelNet40.</p><p>serving shape of the sample based on Euclidean distance d knn , it is prone to be overlapped with the other extracted part, e.g. <ref type="figure">Figure 3</ref>(b), if there is directional bias on the sample. On the other hand, A ball alleviates overlapping by selecting points within the distance, r rigid e.g. <ref type="figure">Figure 3</ref>(a). However, in contrast with A knn , if the density of sample around the query point is high or r rigid is too large, the number of points in RS from A ball must be controlled to maintain the number of points in a mixed sample. Therefore, we randomly sample the extracted points in S β to preserve the overall shape of the extracted part. <ref type="figure">Figure 4</ref> illustrates the difference between processed RS with A ball and A knn depending on density of point cloud. We also compare two methods with quantitative results of single-view evaluation on DGCNN <ref type="bibr" target="#b28">[29]</ref> in <ref type="table" target="#tab_5">Table 4</ref>, using scaling and shift augmentations as ConvDA. Both approaches achieved improved results over existing networks and A ball achieves superior results since it is more robust to directional bias as well as considers the density of point cloud. Therefore, we used A ball as our main neighboring function. Single and multi-view evaluations with various combinations of augmentations. RSMix can be applied in conjunction with existing ConvDA methods to further increase the diversity of mixed data since they are independent approaches. However, some combinations of augmentations can cause excessive deformation on the data sample, reducing the network's ability to recognize objects itself. Therefore, it is essential to analyze various combinations of augmentation strategies. We provide quantitative results in <ref type="table">Table 5</ref> with single and multi-view evaluations for Point-Net++ <ref type="bibr" target="#b22">[23]</ref> on ModelNet40. RandDrop is the data augmentation method proposed in <ref type="bibr" target="#b22">[23]</ref> that randomly drops the points from sample so that network can extract the global features better. The results show that models with RSMix alone achieved the highest accuracy for single-view evaluation. In addition, overall experiments show better results without rotational augmentation for single-view evaluation.  <ref type="table">Table 6</ref>: Ablation studies for DGCNN <ref type="bibr" target="#b28">[29]</ref> on Model-Net40(MN40) and ModelNet10(MN10). Random scaling augmentation was applied as ConvDA.</p><p>However, results with multi-view evaluation reveal that if the model is trained without rotational augmentation, network can be overfitted to directional bias of the dataset, ModelNet40. Hence, rotational augmentation is essential for multi-view evaluation. However, RSMix improves discriminative ability of the model with appropriate combinations with other augmentations regardless of evaluation type, because diversity of datasets increases significantly when RSMix is used in conjunction with rotation and scaling augmentations.</p><p>In addition, we also provide the results for DGCNN <ref type="bibr" target="#b28">[29]</ref> in <ref type="table">Table 6</ref> with single-view evaluation on ModelNet40 and ModelNet10 with scaling augmentation as ConvDA, since single-view evaluation shows better results without rotational augmentation. We also obtained remarkable improvements with RSMix for all presented combinations.</p><p>Therefore, we can notice three things as follows.</p><p>• Single and multi-view evaluation performances differ significantly depending on the presence of rotational biases in the dataset.</p><p>• Rotational augmentation reduces single-view evaluation performance, but must be included when training if evaluation is performed with multi-view evaluation.</p><p>• RSMix successfully improved model generality by appropriate combination with other augmentation strategies regardless of evaluation type.</p><p>Robustness Test. We tested the robustness of RSMix with PointNet++ <ref type="bibr" target="#b22">[23]</ref> to four noisy environments: jitter, rotation, scaling, and DropPoint, in order to verify that our method makes the model robust to noise. <ref type="table" target="#tab_9">Table 7</ref> verifies the impact of RSMix with single-view and multi-view evaluation for 2 cases against the use of the ConvDA. Especially, multi-view evaluation for rotational noisy environment along the y-axis was performed by rotating the sample  along the x-axis for fair comparison. ConvDA includes jittering, shifting, scaling, and rotational augmentations with default settings as same as PointNet++ <ref type="bibr" target="#b22">[23]</ref>. The results in <ref type="table" target="#tab_9">Table 7</ref> reveal that RSMix improves the robustness of model whether or not ConvDA was applied for rotation and Drop-Point noise, since shape and scale of original point cloud were preserved. However, we achieved lower results with multi-view evaluation when only RSMix was applied for scaling noise. The reason is that if scaling noise is large, it is difficult for the model to interpret the data when viewed from different angles since shape is preserved but scale compared with the original data. Meanwhile, results differed greatly depending on the level of noise for jittering noise, where the shape of an object was not preserved. Although RSMix alone cannot improve robustness for multiview evaluation, RSMix provided improvements when jitter noise was small for single-view evaluation regardless of ConvDA usage. However, robustness was reduced for both evaluation methods for large jitter noise when RSMix was applied because it was difficult for subsets extracted from each sample by RSMix to have sufficient shape information since the original sample shape was greatly distorted prior to applying RSMix.</p><p>Various θ values. We introduced beta function Beta(θ, θ), to sample r rigid from the beta distribution when using A ball in Section 3.2. We demonstrate the experimental results for  <ref type="figure">Figure 5</ref>: Ablation studies for various θ values on DGCNN <ref type="bibr" target="#b28">[29]</ref> with single-view evaluation on ModelNet40.</p><p>various θ for DGCNN <ref type="bibr" target="#b28">[29]</ref> on ModelNet40 <ref type="bibr" target="#b30">[31]</ref> in <ref type="figure">Figure 5</ref> to figure out the effect of θ to our model using same experimental settings. Due to a property of beta function, larger θ implies higher probability that r rigid was sampled close to 0.5. However, since we set the n max = half of the number of points in sample, more frequent sampling of large r rigid does not have much effect. Best accuracy was achieved for θ = 1.0 for all cases. Therefore, we set θ = 1.0 as default for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes RSMix, a novel data augmentation method for point clouds, that generates virtual mixed samples from extracted subsets from each point cloud without additional learnable parameters. We extracted the subsets from samples without shape distortion by redefining the rectangular mask for images as a subset of adjacent points from a query point in 3D space. Various experiments verified that RSMix improved deep neural networks to extract discriminative feature effectively by increasing diversity of data. In addition, extensive tests demonstrated that RSMix also improved robustness of the model to various types of noise. We further analyze the two types of evaluation method for shape classification: single and multiview, which are utilized as evaluation metrics for the overall experiments. Experiments verified explicit differences between two methods and necessity of selecting appropriate combination with various data augmentation strategies. Extensive ablation studies also verified generic effectiveness of RSMix with various combinations with existing data augmentations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Qualitative results with RSMix. Purple (left) and yellow (middle) colored points indicate Rigid Subsets to be extracted from each sample to synthesize red and green colored mixed samples (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall pipeline of RSMix. Three steps to synthesize the mixed samples(P α mix ) using Rigid Subset (RS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )Figure 3 :Figure 4 :</head><label>a34</label><figDesc>RSMix with !"## . $ − $ ( RSMix with )** . $ − $ ( ), %, and &amp;'( $ ( ℎ ) Differences depending on directional bias of point cloud. Differences depending on density of point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Various mixup functions for image and point cloud domains.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">Quantitative results for single and Multi-view eval-</cell></row><row><cell cols="5">uations of RSMix on ModelNet40 [31]. We only present the</cell></row><row><cell cols="5">results of PointNet [22] and PointNet++ [23] with rotational</cell></row><row><cell cols="5">augmentation included model for fair comparison.</cell></row><row><cell>Method</cell><cell cols="3">Augmentation ACC.(%) Dataset Align</cell><cell>Eval</cell></row><row><cell cols="2">PointNet++ [23] PointMixup [4]</cell><cell>92.7</cell><cell>Pre-aligned</cell><cell>-</cell></row><row><cell>DGCNN [29]</cell><cell>PointMixup [4]</cell><cell>93.1</cell><cell>Pre-aligned</cell><cell>-</cell></row><row><cell>PointNet [22]</cell><cell>PointMixup [4]</cell><cell>89.9</cell><cell>Unaligned</cell><cell>-</cell></row><row><cell cols="2">PointNet++ [23] PointMixup [4]</cell><cell>91.7</cell><cell>Unaligned</cell><cell>-</cell></row><row><cell>PointNet++ [23]</cell><cell>Ours</cell><cell>92.7</cell><cell>Raw</cell><cell>Single-View</cell></row><row><cell>DGCNN [29]</cell><cell>Ours</cell><cell>93.5</cell><cell>Raw</cell><cell>Single-View</cell></row><row><cell>PointNet [22]</cell><cell>Ours</cell><cell>88.5</cell><cell>Raw</cell><cell>Multi-View</cell></row><row><cell>PointNet++ [23]</cell><cell>Ours</cell><cell>92.1</cell><cell>Raw</cell><cell>Multi-View</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparing RSMix and PointMixup [4] on Model- Net40 [31].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Visualization. Supplementary material provides additional examples synthesized with RSMix.</figDesc><table><row><cell>ConvDA RandDrop RSMix</cell><cell>ACC knn S</cell><cell cols="2">(%) ACC ball S (%)</cell></row><row><cell></cell><cell cols="2">93.0(0.5↑)</cell><cell>93.3(0.8↑)</cell></row><row><cell></cell><cell cols="2">93.3(0.7↑)</cell><cell>93.4(0.8↑)</cell></row><row><cell></cell><cell cols="2">93.4(0.6↑)</cell><cell>93.5(0.7↑)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Quantitative Comparison of neighboring functions for DGCNN [29] on ModelNet40. ACC knn S and ACC ball S indicate single-view accuracy with A knn and A ball .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Robustness test of RSMix with or without ConvDA for PointNet++ [23] on ModelNet40 using Random shift, scaling, rotation, and jitter augmentations as ConvDA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Single-view Evaluation Accuracy(%)</figDesc><table><row><cell>93.6</cell><cell></cell><cell></cell><cell>No Aug No Aug ConvDA ConvDA</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ConvDA+R andDrop ConvDA+RandDrop</cell></row><row><cell>93.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>93.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>93</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>92.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell></row><row><cell></cell><cell></cell><cell>Hyper Parameter</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by the Institute for Information and Communications Technology Promotion (IITP) funded by the Korean Government (MSIP) under Grant 2016-0-00197. This research was also supported by R&amp;D program for Advanced Integratedintelligence for Identification (AIID) through the National Research Foundation of KOREA(NRF) funded by Ministry of Science and ICT (NRF-2018M3E3A1057289).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convpoint: Continuous convolutions for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vicinal risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06374</idno>
		<title level="m">Pointmixup: Augmentation for point clouds</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Part-aware data augmentation for 3d object detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeseok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13373</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiresolution tree networks for 3d point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Prügel-Bennett Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<title level="m">Fmix: Enhancing mixed sample data augmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Momen (e) t: Flavor the moments in learning to classify shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Joseph-Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Zvirin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06962</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling local geometric structure of 3d point clouds using geo-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Octree guided cnn with spherical kernels for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9631" to="9640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointaugment: an auto-augmentation framework for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6378" to="6387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on computer vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequencies of events to their probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vn Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ya Chervonenkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="264" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Linked dynamic graph cnn: Learning on point cloud via linking hierarchical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuangen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clarence</forename><forename type="middle">W</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silva</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10014</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A graph-cnn for 3d point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6279" to="6283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
