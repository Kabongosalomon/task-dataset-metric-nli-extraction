<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Megvii Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Megvii Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Megvii Inc</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
							<email>shiboxin@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Megvii Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video Frame Interpolation (VFI). Most existing flow-based methods first estimate the bidirectional optical flows, then scale and reverse them to approximate intermediate flows, leading to artifacts on motion boundaries. RIFE uses a neural network named IFNet that can directly estimate the intermediate flows from images with much better speed. Based on our proposed leakage distillation loss, RIFE can be trained in an end-toend fashion. Experiments demonstrate that our method is flexible and can achieve impressive performance on several public benchmarks. The code is available at https: //github.com/hzwer/arXiv2020-RIFE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video Frame Interpolation (VFI) aims to synthesize intermediate frames between two consecutive frames of a video and is widely used to improve frame rate and enhance visual quality. VFI also supports various applications like slow-motion generation, video compression <ref type="bibr" target="#b33">[34]</ref>, and novel view synthesis. Moreover, real-time VFI algorithms running on high-resolution videos have many more potential applications, such as increasing the frame rate of video games and live videos, providing video editing services for users with limited computing resources.</p><p>VFI is challenging due to the complex, large non-linear motions and illumination changes in the real world. Recently, flow-based VFI algorithms have offered a framework to address these challenges and achieved impressive results <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b1">2]</ref>. Common approaches for these methods involve two steps: 1) warping the input frames according to approximated optical flows and 2) fusing and refining the warped frames using Convolutional Neural Networks (CNNs).</p><p>According to the way of warping frames, flow-based VFI algorithms can be classified into forward warping based methods and backward warping based methods. Backward warping is more widely used because forward warp- <ref type="figure">Figure 1</ref>: Speed and accuracy trade-off by adjusting model size parameters C and F . We compare our models with prior VFI methods including TOFlow <ref type="bibr" target="#b36">[37]</ref>, Sep-Conv <ref type="bibr" target="#b26">[27]</ref>, MEMC-Net <ref type="bibr" target="#b2">[3]</ref>, DAIN <ref type="bibr" target="#b1">[2]</ref>, CAIN <ref type="bibr" target="#b6">[7]</ref>, Soft-Splat <ref type="bibr" target="#b24">[25]</ref>, BMBC <ref type="bibr" target="#b27">[28]</ref>, DSepConv <ref type="bibr" target="#b5">[6]</ref>, and EDSC <ref type="bibr" target="#b4">[5]</ref> on the Vimeo90K testing set.</p><p>ing suffers from conflicts when multiple source pixels are mapped to the same location, leading to overlapped pixels and holes. Given the input frames I 0 , I 1 , backward warping based methods need to approximate the intermediate flows F t→0 , F t→1 from the perspective of the frame I t that we are expected to synthesize. Common practices <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18]</ref> first compute bi-directional flows from pre-trained off-theshelf optical flow models, then reverse and refine them to generate intermediate flows. However, these methods may have flaws on motion boundaries, as the object position changes from frame to frame. Consequently, previous flowbased VFI methods share two major drawbacks: 1) Requiring additional components: Image depth <ref type="bibr" target="#b1">[2]</ref>, flow refinement <ref type="bibr" target="#b12">[13]</ref> and flow reversal layer <ref type="bibr" target="#b35">[36]</ref> are introduced to compensate for the defects of optical flow reversal. These operations require a large amount computing resources and are not suitable for real-time scenarios.</p><p>2) Lacking direct supervision for the approximated intermediate flows: The whole interpolation system is usually trained with only the final reconstruction loss <ref type="bibr" target="#b27">[28]</ref>.</p><p>There is no other supervision explicitly designed for the arXiv:2011.06294v5 [cs.CV] 18 Mar 2021 flow estimation process, degrading the performance of interpolation.</p><p>We develop a specialized intermediate flow network named IFNet to directly estimate the intermediate flows. This process is commonly believed to be hard <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref>, and few attempts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3]</ref> have been made. IFNet adopts a coarse-to-fine strategy <ref type="bibr" target="#b11">[12]</ref> with progressively increased resolutions: it iteratively updates a flow field via successive IFBlocks. Conceptually, according to the iteratively updated flow fields, we could move corresponding pixels from two input frames to the same location in a latent intermediate frame. Unlike most previous optical flow models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33]</ref>, IFBlocks do not contain expensive operators like cost volume or pyramid feature warping and use 3 × 3 convolution as building blocks.</p><p>Employing strong intermediate supervision is also found to be important. In fact, when training the IFNet end-to-end with later fusion process using the final reconstruction loss, our method produces worse results than previous methods that use complex pipelines and pre-trained flow models in the intermediate flow estimation process. The picture dramatically changes after we add advanced supervision to IFNet. We design a novel leakage distillation loss which employs an overpowered teacher with access to the intermediate frames during training.</p><p>Combining these designs, we propose the Real-time Intermediate Flow Estimation (RIFE). RIFE can achieve satisfactory results when trained from scratch. We illustrate the speed and accuracy trade-off compared with other methods in <ref type="figure">Figure 1</ref>.</p><p>In summary, our contributions are three-fold:</p><p>• We design an efficient IFNet to simplify the flow-based VFI methods. IFNet can be trained from scratch and directly approximate the intermediate flows given two input frames.</p><p>• We provide effective supervision for the intermediate flow estimation, especially a leakage distillation loss, which leads to a more stable convergence and large performance improvement.</p><p>• We use model scaling to obtain models with varying quality and speed trade-offs. Experiments show that RIFE can achieve impressive performance on public benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>We provide a brief overview of the optical flow estimation task, which is the core of most VFI methods. Then, we will review several most related flow-based VFI methods, and cover some inspiring flow-free methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Optical Flow</head><p>Optical flow estimation is a long-standing vision task that aims to estimate the per-pixel motion, which are useful in lots of downstream tasks. Since the milestone work of FlowNet <ref type="bibr" target="#b7">[8]</ref> based on U-net autoencoder <ref type="bibr" target="#b29">[30]</ref>, architectures for optical flow models have evolved for several years, yielding more accurate results while being more efficient, such as FlowNet2 <ref type="bibr" target="#b11">[12]</ref>, PWC-Net <ref type="bibr" target="#b31">[32]</ref> and Lite-FlowNet <ref type="bibr" target="#b10">[11]</ref>. These methods typically adopt an iterative refinement approach and often involve operators like cost volume, pyramidal features, and backward feature warping. Recently Teed et al. <ref type="bibr" target="#b32">[33]</ref> introduce RAFT, which iteratively updates a flow field through a recurrent unit and achieves a remarkable breakthrough in this field. Another important research direction is unsupervised optical flow estimation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref> due to the difficulty of optical flow labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video Frame Interpolation</head><p>Liu et al. <ref type="bibr" target="#b18">[19]</ref> propose a fully convolutional network to estimate voxel flow and generate intermediate frames by sampling. Jiang et al. <ref type="bibr" target="#b12">[13]</ref> propose SuperSlomo using the linear combination of the two bi-directional flows as an initial approximation of the intermediate flows. Then refine them and predict visibility maps that encode occlusion information. Based on SuperSlomo, Reda et al. <ref type="bibr" target="#b28">[29]</ref> propose to synthesize intermediate frames using unsupervised cycle consistency. Bao et al. <ref type="bibr" target="#b1">[2]</ref> propose DAIN using a depth-aware flow projection layer to estimate the intermediate flow as a weighted combination of bidirectional flow. Niklaus et al. <ref type="bibr" target="#b24">[25]</ref> propose SoftSplat to forward-warp frames and their feature map using softmax splatting. Xu et al. <ref type="bibr" target="#b35">[36]</ref> propose QVI to exploit four consecutive frames and flow reversal filter to get the intermediate flows. Liu et al. <ref type="bibr" target="#b17">[18]</ref> further extend QVI with rectified quadratic flow prediction to EQVI. Among these methods, DAIN has been deployed in many software applications, and SoftSplat leads in many public benchmarks.</p><p>Along with flow-based methods, flow-free methods have also achieved remarkable progress in recent years. Meyer et al. <ref type="bibr" target="#b22">[23]</ref> utilize phase information to learn the motion relationship for multiple video frame interpolation. Niklaus et al. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> formulate VFI as a spatially-adaptive convolution whose convolution kernel is generated using a convolution network given the input frames. Cheng et al.</p><p>propose DSepConv <ref type="bibr" target="#b5">[6]</ref> to extend kernel-based method using deformable separable convolution and further propose EDSC <ref type="bibr" target="#b4">[5]</ref> to perform multiple interpolation. Choi et al. <ref type="bibr" target="#b6">[7]</ref> propose an efficient flow-free method named CAIN, which employs the PixelShuffle operator and channel attention to capture the motion information implicitly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first provide an overview of our proposed RIFE. We then describe the efficient design of the major components in RIFE in section 3.2, elaborate on our proposed leakage distillation loss in section 3.3, and explain the training details in section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pipeline Overview</head><p>We illustrate the overview of our proposed RIFE in <ref type="figure" target="#fig_0">Figure 2</ref>. Given a pair of consecutive RGB frames, I 0 , I 1 , our goal is to synthesize an intermediate frame I t at time t = 0.5. We directly estimate the intermediate flows F t→0 and F t→1 by feeding input frames into IFNet. Then we can get two coarse results I t←0 , I t←1 by backward warping the input frames. To remove the artifacts in the warped frames, we feed the input frames, the approximated flow, and warped frames into the fusion process with an encoderdecoder like FusionNet to generate the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Efficient Architecture Design</head><p>RIFE has two major components: 1) efficient intermediate flow estimation with the IFNet and 2) fusion process of the warped frames.</p><p>Intermediate flow estimation. Some previous intermediate flow estimation methods reverse and refine bidirectional flows <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18]</ref> as depicted in <ref type="figure">Figure 3</ref>. The role of our IFNet is to directly and efficiently predict F t→0 , F t→1 given two consecutive input frames I 0 , I 1 .</p><p>To handle the large motion encountered in intermediate flow estimation, we employ a coarse-to-fine strategy with gradually increasing resolutions, as illustrated in <ref type="figure">Figure 4</ref>. Specifically, we first compute a rough prediction of the flow <ref type="figure">Figure 3</ref>: Comparison between previous intermediate estimation approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18]</ref> (left) and IFNet (right). Most existing methods contain two stages: 1) bidirection flow estimation and 2) flow reversal modules. The flow reversal process is usually cumbersome due to the difficulty of handling the changes of object positions. IFNet can directly estimate the intermediate flows.</p><p>on low resolutions, which is believed to capture large motions easier, then iteratively refine the flow fields with gradually increasing resolutions. Following this design, our IFNet has a stacked hourglass structure, where a flow field is iteratively refined via successive IFBlocks operating on increasing resolutions:</p><formula xml:id="formula_0">F i = F i−1 + g i (F i−1 , I i−1 ),<label>(1)</label></formula><p>where F i−1 denotes the current estimation of the intermediate flows from the i − 1-th IFBlock, I i−1 0→t and I i−1 1→t denote the warped input frames using previous approximated flow, and g i represents the ith IFBlock. We use a total of 3 IFBlocks, and each has a resolution parameter, K i . To keep our design simple, each IFBlock has a feed-forward structure consisting of serveral convolutional layers and an up-sampling operator. Except for the layer that outputs the optical flow residuals, the fusion map, and the reconstruction residual, we use PReLU <ref type="bibr" target="#b8">[9]</ref> as the activation function.</p><p>In <ref type="figure">Figure 5</ref>, we provide visual results of our IFNet and compare them with the linearly combined bi-directional optical flows generated by a pre-trained LiteFlowNet <ref type="bibr" target="#b10">[11]</ref>. Our IFNet produces clear and sharp motion boundaries, while linearly combining flow suffers from overlapped pixels and blurring on motion boundaries.</p><p>We compare the runtime of the current state-of-the-art optical flow estimation networks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> and our IFNet in <ref type="table" target="#tab_0">Table 1</ref>. Current flow-based methods usually need to run their flow models twice to get the bi-directional flows. Therefore the intermediate flow estimation in RIFE runs at a faster speed than previous methods, achieving the acceleration of 5 − 20 times.</p><p>Fusion process. With the estimated intermediate flows F t→0 and F t→1 , we can get the coarse reconstructed frames I t←0 and I t←0 by performing backward warping on input frames. To reduce the severe artifacts in the warped frames, we perform a refine and fusion process formulated as:</p><formula xml:id="formula_1">I t = M I t←0 + (1 − M ) I t←1 + ∆,<label>(2)</label></formula><p>where M is a soft fusion map used to fuse these two warped frames, ∆ is the reconstruction residual term used to refine the details in images, is an element-wise multiplier, and (0 ≤ M, ∆ ≤ 1).</p><p>Following the previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b24">25]</ref>, the fusion process includes a context extractor and a FusionNet with an encoder-decoder architecture similar to U-Net. The context extractor and encoder part of the FusionNet have similar architectures, consisting of four convolutional blocks, and each of them is composed of two 3 × 3 convolutional layers, respectively. The decoder part in the FusionNet has four transpose convolution layers. We use sigmoid function to restrict the outputs of FusionNet.</p><p>Specifically, the context extractor first extracts the pyramid contextual features from input frames separately. We denote the pyramid contextual feature as C 0 :</p><formula xml:id="formula_2">{C 0 0 , C 1 0 , C 2 0 , C 3 0 } and C 1 : {C 0 1 , C 1 1 , C 2 1 , C 3 1 }.</formula><p>We then perform backward warping on these features using estimated intermediate flows to produce aligned pyramid features, C t←0 and C t←1 . The warped frames and intermediate flows are fed into the FusionNet, including an encoder and a decoder. The output of i − th encoder block is concatenated with the C i t←0 and C i t←1 before being fed into the next block. The decoder parts finally produce the fusion map M and the reconstruction residual ∆.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Leakage Distillation for IFNet</head><p>Directly approximating the intermediate flows is challenging because of no access to the intermediate frame and the lack of supervision. To address this problem, we add a leakage distillation loss to our IFNet in which the target is the prediction of an overpowered teacher network who has access to the intermediate frame. Specifically, we feed {I GT t , I 0 }, {I GT t , I 1 } to a pre-trained optical flow estimation network to get the intermediate flow prediction {F Leak t→1 , F Leak t→0 }. And the leakage distillation loss L dis is defined as follows:</p><formula xml:id="formula_3">L dis = ||F t→0 − F Leak t→0 || 1 + ||F t→1 − F Leak t→1 || 1 . (3)</formula><p>Following the previous work <ref type="bibr" target="#b32">[33]</ref>, we apply the leakage distillation loss over the full sequence of predictions generated from the iteratively updating process in our IFNet. Our distillation scheme is different from those in semisupervised learning algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref>, where a pre-trained model is used to infer the label of unlabeled data. With the access of the target frame I GT t , our teacher model has a different view of the video clip with the student. Conceptually, the overpowered teacher causes a leakage <ref type="bibr" target="#b16">[17]</ref> where our flow estimator can have access to the information of the target intermediate frame during training, and in the experiments section, we show that this kind of data (target) leakage is beneficial to the training of our whole system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implement Details</head><p>Supervisions. Given a pair of consecutive frames, I 0 , I 1 , our training loss L is a linear combination of the reconstruction loss L rec , adapted census loss <ref type="bibr" target="#b21">[22]</ref> L cen and leakage distillation loss L dis as defined in section 3.3:</p><formula xml:id="formula_4">L = L rec + λ c L cen + λ d L dis ,<label>(4)</label></formula><p>where we set λ c = 1 and λ d = 0.01. The reconstruction loss L rec models the reconstruction quality of the intermediate frame. We denote the synthesized frame byÎ t and the ground-truth frame by I GT t . The reconstruction loss has the formulation of :</p><formula xml:id="formula_5">L rec = ||Î t − I GT t || 1 .<label>(5)</label></formula><p>As the brightness constancy constraint is often violated in realistic situations, census loss is widely used in unsupervised optical flow estimation <ref type="bibr" target="#b14">[15]</ref> methods to address the illumination changes. We adopt the census loss from unsupervised optical flow learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15]</ref> to robustly handle the illumination changes between consecutive frames. The census loss is defined as the soft Hamming distance on census-transformed <ref type="bibr" target="#b37">[38]</ref> image patches. We optimize the census loss L cen between census-transformedÎ t and I GT t with the width of patches as 9.</p><p>Training dataset. We use the Vimeo90K dataset <ref type="bibr" target="#b36">[37]</ref> to train our model. The Vimeo90K dataset has 51,312 triplets for training, where each triplet contains three consecutive video frames with a resolution of 256 × 448. We randomly augment the training data by horizontal and vertical flipping and temporal order reversing during training. We crop every training example to a 224 × 224 patch. In the benchmark experiment of inter-frame interpolation, we train RIFE to predict the middle frame given the frames on both sides.</p><p>Training strategy. We train our system from scratch on the Vimeo90K training set. An officially pre-trained Lite-FlowNet <ref type="bibr" target="#b10">[11]</ref> is used as the overpowered teacher in the leakage distillation.</p><p>Our model is optimized by AdamW <ref type="bibr" target="#b19">[20]</ref> with weight decay 10 −4 for 300 epochs on the Vimeo90K training set. Our training uses a batch size of 48. We gradually reduce the learning rate from 10 −4 to 0 using cosine annealing during the whole training process. Our pipeline is implemented in PyTorch. We train RIFE on four NVIDIA TITAN X (Pascal) GPUs for about 15 hours. The preprocess needs 2 hours in one GPU, and different models can use the same preprocess results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct several experiments to validate our method. We first introduce the benchmarks for evaluation. Then we provide variants of our models with different computational costs to meet different needs in section 4.2. We compare our models with representative stateof-the-art methods, both quantitatively and visually, in section 4.3. An ablation study in section 4.4 is carried out to analyze our design. We show the capability of generating multiple frames using our models in section 4.5. Finally, we discuss some limitations of our method in section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmarks and Evaluation Metrics</head><p>We evaluate our models on four benchmarks, including Middlebury <ref type="bibr" target="#b0">[1]</ref>, UCF101 <ref type="bibr" target="#b30">[31]</ref>, Vimeo90K <ref type="bibr" target="#b36">[37]</ref> and HD <ref type="bibr" target="#b2">[3]</ref>. Following the previous works, we train our models on the Vimeo90K training dataset and directly test it on all these benchmarks.</p><p>Middlebury. The Middlebury (M.B.) benchmark <ref type="bibr" target="#b0">[1]</ref> is widely used to evaluate VFI methods. The image resolution in this dataset is around 640 × 480. We report the average IE of the Middlebury-OTHER set.</p><p>Vimeo90K. There are 3,782 triplets in the Vimeo90K testing set <ref type="bibr" target="#b36">[37]</ref> with resolution of 448 × 256.</p><p>UCF101. The UCF101 dataset <ref type="bibr" target="#b30">[31]</ref> contains videos with a large variety of human actions. There are 379 triplets with a resolution of 256 × 256.</p><p>HD. Bao et al. <ref type="bibr" target="#b2">[3]</ref> collect 11 high-resolution videos for evaluation. The HD benchmark consists of four 1080p, three 720p and four 1280 × 544 videos. The motions in this benchmark are larger than other benchmarks. Following the author of HD benchmark, we use the first 100 frames of each video for evaluation.</p><p>We measure the peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and interpolation error (IE) for <ref type="table">Table 2</ref>: Quantitative comparisons on the UCF101 <ref type="bibr" target="#b30">[31]</ref>, Vimeo90K <ref type="bibr" target="#b36">[37]</ref>, Middlebury-OTHER set <ref type="bibr" target="#b0">[1]</ref>, and HD benchmarks <ref type="bibr" target="#b2">[3]</ref>. The numbers in red and blue represent the best and second-best performance. We report the interpolation runtime for a single 640 × 480 video frame. Some methods are unable to run on 1080p videos due to exceeding the 12 gigabytes of memory available on our graphics card (denoted as "OOM"). , we compile released models and get three times slower on our graphics card. ‡: get 2.72 using officially released model. quantitative evaluation. All the methods are tested on a TITAN X (Pascal) GPU. We calculate the average process time for 100 runs after a warm-up process of 100 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Scaling</head><p>We provide several models with different computation overheads and performance to meet different needs by model scaling. We introduce two hyper-parameters following <ref type="bibr" target="#b9">[10]</ref>: width multiplier and resolution multiplier. Upon our base model RIFE, we apply a 1.5 width multiplier on the number of channels uniformly at each layer to produce a model named RIFE-1.5C. Meanwhile, we can remove a downsample layer from the headers of IFNet and Fusion-Net, which doubles the feature map's resolution that produces a model named RIFE-2F. Together, we can combine these two modifications to produce a model named RIFE-Large (1.5C2F). The performance and runtime for these models is reported in <ref type="table" target="#tab_2">Table 3</ref> and depicted in <ref type="figure">Figure 1</ref>.</p><p>We show that our model is flexible, and simply increasing model capacity can effectively improve model performance. A useful trick is that when using RIFE to process 720p video frames in parallel (batchsize = 4), the total runtime can further drop to half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with Previous Methods</head><p>We report the performance on the benchmarks in <ref type="table">Table 2</ref>. These methods are officially trained on Vimeo90K dataset except for DVF <ref type="bibr" target="#b18">[19]</ref> and SuperSlomo <ref type="bibr" target="#b12">[13]</ref>. RIFE runs considerably faster than other methods with comparable performance. Meanwhile, RIFE needs only 3.1 gigabytes of GPU memory to process 1080p videos, while some algorithms exceed 12 gigabytes. We get a larger version of our model (RIFE-Large) by simple model scaling, which runs about two times faster than the previous state-of-the-art method SoftSplat <ref type="bibr" target="#b24">[25]</ref> with comparable performance. We provide a visual comparison of video clips with large motions from the Vimeo90K testing set in <ref type="figure">Figure 6</ref>, where SepConv <ref type="bibr" target="#b26">[27]</ref> and DAIN <ref type="bibr" target="#b1">[2]</ref> produce ghosting artifacts, and CAIN <ref type="bibr" target="#b6">[7]</ref> causes missing-parts artifacts. Overall, our method can produce more reliable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs (Overlay)</head><p>SepConv DAIN CAIN RIFE (Ours) GT <ref type="figure">Figure 6</ref>: Qualitative comparison on Vimeo90K <ref type="bibr" target="#b36">[37]</ref> testing set. We cut out the objects according to the green boxes and zoom in the results <ref type="bibr" target="#b6">[7]</ref>. While other methods cause various artifacts, our method produces best effects on the moving objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We design an ablation study on losses, intermediate flow estimation, and fusion process, shown in <ref type="table" target="#tab_3">Table 4</ref>. These experiments use the same hyper-parameter setting and evaluation on Vimeo90K <ref type="bibr" target="#b36">[37]</ref> and MiddleBury <ref type="bibr" target="#b0">[1]</ref> benchmark.</p><p>Ablation on the losses. To analyze the contributions of adapted census loss L cen and leakage distillation loss L dis , we train RIFE models from scratch without these losses. We show that these two losses can improve the performance of RIFE, especially L dis . We also notice that the training of the model will become very unstable without L dis .</p><p>IFNet vs. flow reversal. To demonstrate the effectiveness of IFNet, we compare it with previous intermediate flow estimation methods used in SuperSlomo <ref type="bibr" target="#b12">[13]</ref> and EQVI <ref type="bibr" target="#b17">[18]</ref>. Specifically, we use PWC-Net <ref type="bibr" target="#b31">[32]</ref> with officially pre-trained parameters to estimate the bi-directional flows. Then we implement three flow reversal methods, including linearly combination <ref type="bibr" target="#b12">[13]</ref>, using a hidden convolutional layer with 128 channels, and the flow reversal method from EQVI <ref type="bibr" target="#b17">[18]</ref> consisting of a reversal layer and an U-Net filter. The PWC-Net and flow reversal modules are jointly trained with our fusion process. As shown in <ref type="table" target="#tab_3">Table 4</ref>, IFNet is more efficient and accurate in estimating intermediate flows, leading to better interpolation performance.</p><p>Ablation on the fusion process. To study the fusion process design, we remove the fusion map and residual term, resulting in blurry results and performance degradation as in <ref type="table" target="#tab_3">Table 4</ref>. Moreover, we verify the context extractor can improve performance with a small computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generating Multiple Frames</head><p>To interpolate multiple intermediate frames at different time t ∈ (0, 1), we can apply RIFE recursively. Specifically, given any two consecutive input frames I 0 , I 1 , we apply RIFE once to get intermediate frame I 0.5 at t = 0.5. We feed I 0 and I 0.5 to get I 0.25 , and we can repeat this process recursively to interpolate multiple frames. To demonstrate this ability, we provide the visual results for 2×, 4×, 8× settings on images with large motions from the Vimeo90K testing set in <ref type="figure">Figure 7</ref>. We observe that RIFE successfully produces smooth and continuous motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7:</head><p>Interpolating multiple frames on the Vimeo90K testing dataset by applying RIFE recursively. We cut out the moving objects according to the green boxes and zoom in the results. RIFE provides smooth and continuous motions. <ref type="table">Table 5</ref>: Quantitative evaluation for 8× interpolation on the HD benchmark <ref type="bibr" target="#b2">[3]</ref>. Some methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5]</ref> can support interpolation at arbitrary time, while others <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> can be only applied recursively to get multiple frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Recursion 544 × 1280 720p 1080p DAIN <ref type="bibr" target="#b1">[2]</ref> 19. <ref type="bibr" target="#b31">32</ref> 28 To provide a quantitative comparison for 8× interpolation, we further extract 8k th (0 ≤ 8k &lt; 100) of every video from HD benchmark <ref type="bibr" target="#b2">[3]</ref> and use them to interpolate other frames. We divide the HD benchmark into three subsets with different resolution to test these methods. We show the quantitative PSNR between generated frames and frames of the origin videos in <ref type="table">Table 5</ref>. Note that DAIN <ref type="bibr" target="#b1">[2]</ref>, BMBC <ref type="bibr" target="#b27">[28]</ref> and EDSC m <ref type="bibr" target="#b5">[6]</ref> can generate a frame at an arbitrary time between the input ones. But they do not show obvious advantages over recursive frame interpolation methods. Among these methods, DAIN has the best results. However, DAIN's speed is slower than CAIN <ref type="bibr" target="#b6">[7]</ref> and RIFE.</p><p>Overall, RIFE has stable performance and low overhead in the 8× interpolation scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Limitations</head><p>We show that we can get large efficiency and performance improvement by combining proper representations and supervisions. However, our work has some limitations. First, RIFE does not support directly generating frames at arbitrary time. Using additional training data and following techniques proposed in SuperSlomo <ref type="bibr" target="#b12">[13]</ref>, QVI <ref type="bibr" target="#b35">[36]</ref> may be a feasible approach. Second, RIFE focuses on only using two input frames while multi-frame input is proven to improve the effect of frame interpolation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref>. Third, many previous papers point out that SSIM and PSNR are not consistent with human subjective perception <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref>, and optimizing perceptual loss <ref type="bibr" target="#b13">[14]</ref> and LPIPS <ref type="bibr" target="#b38">[39]</ref> may be essential for training a practical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we develop an efficient and flexible algorithm for VFI, named RIFE. With the more accurate intermediate flow estimation and our fusion process, RIFE can effectively process videos of different resolution and interpolate multiple frames between two input frames. The impressive results of the proposed method shed light for future research on real-time flow-based interpolation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of RIFE. Given two input frames I 0 , I 1 , we directly feed them into our efficient IFNet to approximate intermediate flows F t→0 , F t→1 . The fusion process takes the warped frames I t←0 , I t←1 , intermediate flows F t→0 , F t→1 and the input frames I 0 , I 1 as input. Inside the fusion process, a FusionMap and Residual is firstly estimated, then the warped frames are linearly combined according to the FusionMap, and added with the Residual to reconstruct the frame I t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Structure of IFNet. Left: IFNet is composed of three stacked IFBlocks operating at different resolutions. Right: We first warp the two input frames based on current approximated flow F i−1 . Then the warped frames I i−1 and F i−1 are fed into the next IFBlock to approximate a flow residual.Inputs (Overlay)Combination IFNet Visual comparison between linearly combined bi-directional flows generated by a pre-trained Lite-FlowNet<ref type="bibr" target="#b10">[11]</ref> and the intermediate flow approximated by IFNet. IFNet produces clear motion boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Inference time on 720p video. Standard flowbased VFI methods run the flow estimation network twice to obtain bi-directional optical flows.</figDesc><table><row><cell>Method</cell><cell cols="2">PWC-Net LiteFlowNet</cell><cell>FlowNet2</cell><cell>IFNet</cell></row><row><cell cols="2">Runtime 2 × 52ms</cell><cell>2 × 152ms</cell><cell cols="2">2 × 207ms 17ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Increase model complexity by adjusting model size parameters. C denotes the multiplier for the number of channels, and F denotes the resolution multiplier. 2F represents removing the first downsampling layer of IFNet and the first one of FusionNet. The parameter setting of RIFE-Large is 1.5C2F.</figDesc><table><row><cell>Scale Setting</cell><cell>RIFE</cell><cell>1.5C</cell><cell>2F</cell><cell>RIFE-Large</cell></row><row><cell>UCF101 PSNR</cell><cell>35.24</cell><cell>35.26</cell><cell>35.30</cell><cell>35.29</cell></row><row><cell cols="2">Vimeo90K PSNR 35.51</cell><cell>35.76</cell><cell>35.95</cell><cell>36.10</cell></row><row><cell>M.B. IE</cell><cell>1.96</cell><cell>1.96</cell><cell>1.94</cell><cell>1.94</cell></row><row><cell>HD PSNR</cell><cell>31.99</cell><cell>32.04</cell><cell>31.91</cell><cell>32.14</cell></row><row><cell># Parameters  *</cell><cell cols="2">9.8M 20.9M</cell><cell>9.8M</cell><cell>20.9M</cell></row><row><cell>Runtime  *</cell><cell>35ms</cell><cell>50ms</cell><cell>126ms</cell><cell>234ms</cell></row><row><cell>Complexity  *</cell><cell>200G</cell><cell>460G</cell><cell>790G</cell><cell>1780G</cell></row></table><note>*: average runtime of 720p frames (batchsize = 1)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on losses, intermediate flow estimation, and fusion process.</figDesc><table><row><cell>Setting</cell><cell cols="2">Vimeo90K M.B. PSNR IE</cell><cell>Runtime ms (720p)</cell></row><row><cell>w/o L dis and Lcen</cell><cell>34.62</cell><cell>2.37</cell><cell>34</cell></row><row><cell>w/o L dis</cell><cell>34.89</cell><cell>2.29</cell><cell>34</cell></row><row><cell>w/o Lcen</cell><cell>35.38</cell><cell>1.99</cell><cell>34</cell></row><row><cell>RIFE</cell><cell>35.51</cell><cell>1.96</cell><cell>34</cell></row><row><cell>linear combination [13]</cell><cell>34.58</cell><cell>2.25</cell><cell>118</cell></row><row><cell>CNN model</cell><cell>34.89</cell><cell>2.15</cell><cell>121</cell></row><row><cell>reversal layer [36]</cell><cell>35.24</cell><cell>2.06</cell><cell>232</cell></row><row><cell>RIFE</cell><cell>35.51</cell><cell>1.96</cell><cell>34</cell></row><row><cell>w/o fusion map</cell><cell>34.97</cell><cell>2.23</cell><cell>34</cell></row><row><cell>w/o residual</cell><cell>35.03</cell><cell>2.19</cell><cell>34</cell></row><row><cell>w/o context extractor</cell><cell>35.28</cell><cell>2.00</cell><cell>30</cell></row><row><cell>RIFE</cell><cell>35.51</cell><cell>1.96</cell><cell>34</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<idno>baker2011database</idno>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multiple video frame interpolation via enhanced deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08070</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video frame interpolation via deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<meeting>the Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Channel attention is all you need for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<meeting>the Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What matters in unsupervised optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Flavr: Flow-agnostic video representations for fast frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarun</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08512</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Leakage in data mining: Formulation, detection, and avoidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachar</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saharon</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Perlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ori</forename><surname>Stitelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced quadratic video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Upflow: Upsampling pyramid for unsupervised optical flow learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00212</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<meeting>the Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised video interpolation using cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video compression through image interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quadratic video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-parametric local transforms for computing visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
