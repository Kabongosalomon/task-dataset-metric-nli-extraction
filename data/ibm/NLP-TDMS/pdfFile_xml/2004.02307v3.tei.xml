<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EfficientPS: Efficient Panoptic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
						</author>
						<title level="a" type="main">EfficientPS: Efficient Panoptic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Panoptic Segmentation · Semantic Segmenta- tion · Instance Segmentation · Scene Understanding</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding the scene in which an autonomous robot operates is critical for its competent functioning. Such scene comprehension necessitates recognizing instances of traffic participants along with general scene semantics which can be effectively addressed by the panoptic segmentation task. In this paper, we introduce the Efficient Panoptic Segmentation (EfficientPS) architecture that consists of a shared backbone which efficiently encodes and fuses semantically rich multi-scale features. We incorporate a new semantic head that aggregates fine and contextual features coherently and a new variant of Mask R-CNN as the instance head. We also propose a novel panoptic fusion module that congruously integrates the output logits from both the heads of our EfficientPS architecture to yield the final panoptic segmentation output. Additionally, we introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for the popularly challenging KITTI benchmark. Extensive evaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset demonstrate that our proposed architecture consistently sets the new state-of-the-art on all these four benchmarks while being the most efficient and fast panoptic segmentation architecture to date.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Holistic scene understanding plays a pivotal role in enabling intelligent behavior. Humans from an early age are able to  <ref type="figure">Figure 1</ref> Overview of our proposed EfficientPS architecture for panoptic segmentation. Our model predicts four outputs: semantics prediction from the semantic head, and class, bounding box and mask prediction from the instance head. All the aforementioned predictions are then fused in the panoptic fusion module to yield the final panoptic segmentation output. effortlessly comprehend complex visual scenes which forms the bases for learning more advanced capabilities <ref type="bibr" target="#b3">(Bremner and Slater, 2008)</ref>. Similarly, intelligent systems such as robots should have the ability to coherently understand visual scenes at both the fundamental pixel-level as well as at the distinctive object instance level. This enables them to perceive and reason about the environment holistically which facilitates interaction. Such modeling ability is a crucial enabler that can revolutionize several diverse applications including autonomous driving, surveillance, and augmented reality. The components of a scene can generally be categorized into 'stuff' and 'thing' objects. 'Stuff' can be defined as uncountable and amorphous regions such as sky, road and sidewalk, while 'thing' are countable objects for example pedestrians, cars and riders. Segmentation of 'stuff' classes is primarily addressed using the semantic segmentation task, whereas segmentation of 'thing' classes is addressed by the instance segmentation task. Both tasks have garnered a sub-stantial amount of attention in recent recent years <ref type="bibr" target="#b31">Krähenbühl and Koltun, 2011;</ref><ref type="bibr">Silberman et al, 2014;</ref><ref type="bibr" target="#b22">He and Gould, 2014a)</ref>. Moreover, advances in deep learning <ref type="bibr" target="#b8">(Chen et al, 2018b;</ref><ref type="bibr" target="#b77">Zhao et al, 2017;</ref><ref type="bibr" target="#b64">Valada et al, 2016a;</ref><ref type="bibr" target="#b21">He et al, 2017;</ref><ref type="bibr" target="#b41">Liu et al, 2018;</ref><ref type="bibr" target="#b78">Zürn et al, 2019)</ref> have further boosted the performance of these tasks to new heights. However, state-of-the-art deep learning methods still predominantly address theses tasks independently although their objective of understanding the scene at the pixel level establishes an inherent connection between them. More surprisingly, they have also fundamentally branched out into different directions of proposal based methods  for instance segmentation and fully convolutional networks <ref type="bibr" target="#b43">(Long et al, 2015)</ref> for semantic segmentation, even though some earlier approaches <ref type="bibr" target="#b61">(Tighe et al, 2014;</ref><ref type="bibr" target="#b62">Tu et al, 2005;</ref><ref type="bibr" target="#b73">Yao et al, 2012)</ref> have demonstrated the potential benefits in combining them.</p><p>Recently, <ref type="bibr" target="#b29">Kirillov et al (2019b)</ref> revived the need to tackle these tasks jointly by coining the term panoptic segmentation and introducing the panoptic quality metric for combined evaluation. The goal of this task is to jointly predict 'stuff' and 'thing' classes, essentially unifying the separate tasks of semantic and instance segmentation. More specifically, if a pixel belongs to the 'stuff' class, the panoptic segmentation network assigns a class label from the 'stuff' classes, whereas if the pixel belongs to the 'thing' class, the network predicts both which 'thing' class it corresponds to as well as the instance of the object class. <ref type="bibr" target="#b29">Kirillov et al (2019b)</ref> also present a baseline approach for panoptic segmentation that heuristically combines predictions from individual state-ofthe-art instance and semantic segmentation networks in a post-processing step. However, this disjoint approach has several drawbacks including large computational overhead, redundancy in learning and discrepancy between the predictions of each network. Although recent methods have made significant strides to address this task in top-down manner with shared components or in a bottom-up manner sequentially, these approaches still face several challenges in terms of computational efficiency, slow runtimes and subpar results compared to task-specific individual networks.</p><p>In this paper, we propose the novel EfficientPS architecture that provides effective solutions to the aforementioned problems for urban road scene understanding. The architecture consists of our new shared backbone with mobile inverted bottleneck units and our proposed 2-way Feature Pyramid Network (FPN), followed by task-specific instance and semantic segmentation heads with seperable convolutions, whose outputs are combined in our parameter-free panoptic fusion module. The entire network is jointly optimized in an end-to-end manner to yield the final panoptic segmentation output. <ref type="figure">Figure 1</ref> shows an overview of the information flow in our network along with the intermediate predictions and the final output. The design of our proposed EfficientPS is influenced by the goal of achieving superior performance compared to existing methods while simultaneously being fast and computationally more efficient.</p><p>Currently, the best performing top-down panoptic segmentation models <ref type="bibr" target="#b47">(Porzi et al, 2019;</ref><ref type="bibr" target="#b70">Xiong et al, 2019;</ref><ref type="bibr" target="#b32">Li et al, 2018a)</ref> primarily employ the ResNet-101  or ResNeXt-101 <ref type="bibr" target="#b69">(Xie et al, 2017)</ref> architecture with Feature Pyramid Networks <ref type="bibr" target="#b38">(Lin et al, 2017)</ref> as the backbone. Although these backbones have a high representational capacity, they consume a significant amount of parameters. In order to achieve a better trade-off, we propose a new backbone network consisting of a modified EfficientNet <ref type="bibr" target="#b58">(Tan and Le, 2019)</ref> architecture that employs compound scaling to uniformly scale all the dimensions of the network, coupled with our novel 2-way FPN. Our proposed backbone is substantially more efficient as well as effective than its popular counterparts <ref type="bibr" target="#b26">Kaiser et al, 2017;</ref><ref type="bibr" target="#b69">Xie et al, 2017)</ref>. Moreover, we identify that the standard FPN architecture has its limitations to aggregate multi-scale features due to the unidirectional flow of information. While there are other extensions that aim to mitigate this problem by adding bottom-up path augmentation <ref type="bibr" target="#b41">(Liu et al, 2018)</ref> to the outputs of the FPN. We propose our novel 2-way FPN as an alternate that facilities bidirectional flow of information which substantially improves the panoptic quality of 'thing' classes while remaining comparable in runtime. Now the outputs of our 2-way FPN are of multiple scales which we refer to as large-scale features when they have a downsampling factor of ×4 or ×8 with respect to the input image, and small-scale features when they have a downsampling factor of ×16 or ×32. The large-scale outputs comprise of fine or characteristic features, whereas the smallscale outputs contain features rich in semantic information. The presence of these distinct characteristics necessitates processing features at each scale uniquely. Therefore, we propose a new semantic head with depthwise separable convolutions, which aggregates small-scale and large-scale features independently before correlating and fusing contextual features with fine features. We demonstrate that this semantically reinforces fine features resulting in better object boundary refinement. For our instance head, we build upon Mask-R-CNN and augment it with depthwise separable convolutions and iABN sync <ref type="bibr" target="#b52">(Rota Bulò et al, 2018)</ref> layers.</p><p>One of the critical challenges in panoptic segmentation deals with resolving the conflict of overlapping predictions from the semantic and instance heads. Most architectures <ref type="bibr" target="#b28">(Kirillov et al, 2019a;</ref><ref type="bibr" target="#b47">Porzi et al, 2019;</ref><ref type="bibr" target="#b36">Li et al, 2019b;</ref><ref type="bibr">de Geus et al, 2018</ref>) employ a standard post-processing step <ref type="bibr" target="#b29">(Kirillov et al, 2019b</ref>) that adopts instance-specific 'thing' segmentation from the instance head and 'stuff' segmentation from the semantic head. This fusion technique completely ignores the logits of the semantic head while segmenting 'thing' regions in the panoptic segmentation output which is sub-optimal as the 'thing' logits of the semantic head can aid in resolving the conflict more effectively. In order to thoroughly exploit the logits from both heads, we propose a parameter-free panoptic fusion module that adaptively fuses logits by selectively attenuating or amplifying fused logit scores based on how agreeable or disagreeable the predictions of individual heads are for each pixel in a given instance. We demonstrate that our panoptic fusion mechanism is more effective and efficient than other widely used methods in existing architectures.</p><p>Furthermore, we also introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for images in the challenging KITTI benchmark <ref type="bibr" target="#b15">(Geiger et al, 2013)</ref>. As KITTI provides groundtruth for a whole suite of perception and localization tasks, these new panoptic annotations further complement the widely popularly benchmark. We hope that these panoptic annotations that we make publicly available encourages future research in multi-task learning for holistic scene understanding. Furthermore, in order to facilitate comparison, we benchmark previous state-of-the-art models on our newly introduced KITTI panoptic segmentation dataset and the IDD dataset. We perform exhaustive experimental evaluations and benchmarking of our proposed EfficientPS architecture on four standard urban scene understanding datasets including Cityscapes , Mapillary Vistas , KITTI <ref type="bibr" target="#b15">(Geiger et al, 2013)</ref> and Indian Driving Dataset (IDD) <ref type="bibr" target="#b67">(Varma et al, 2019)</ref>.</p><p>Our proposed EfficientPS with a PQ score of 66.4% is ranked first for panoptic segmentation on the Cityscapes benchmark leaderboard without training on coarse annotations or using model ensembles. Additionally, EfficientPS is also ranked second for the semantic segmentation task as well as the instance segmentation task on the Cityscapes benchmark with a mIoU score of 84.2% and an AP of 39.1% respectively. On the Mapillary Vistas dataset, our single Effi-cientPS model achieves a PQ score of 40.5% on the validation set, thereby outperforming all the existing methods. Similarly, EfficientPS consistently outperforms existing panoptic segmentation models on both the KITTI and IDD datasets by a large margin. More importantly, our EfficientPS architecture not only sets the new state-of-the-art on all the four panoptic segmentation benchmarks, but it is also the most computationally efficient by consuming the least amount of parameters and having the fastest inference time compared to previous state-of-the-art methods. Finally, we present detailed ablation studies that demonstrate the improvement in performance due to each of the architectural contributions that we make in this work. Moreover, we also make implementations of our proposed EfficientPS architecture, training code and pre-trained models publicly available.</p><p>In summary, the following are the main contributions of this work:</p><p>1. The novel EfficientPS architecture for panoptic segmentation that incorporates our proposed efficient shared back-bone with our new feature aligning semantic head, a new variant of Mask R-CNN as the instance head, and our novel adaptive panoptic fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A new panoptic backbone consisting of an augmented</head><p>EfficientNet architecture, and our proposed 2-way FPN that both encodes and aggregates semantically rich multiscale features in a bidirectional manner. 3. A novel semantic head that captures fine features and long-range context efficiently as well as correlates them before fusion for better object boundary refinement. 4. A new panoptic fusion module that dynamically adapts the fusion of logits from the semantic and instance heads based on their mask confidences and congruously integrates instance-specific 'thing' classes with 'stuff' classes to compute the panoptic prediction. 5. The KITTI panoptic segmentation dataset that provides panoptic groundtruth annotations for images from the challenging KITTI benchmark dataset. 6. Benchmarking of existing state-of-the-art panoptic segmentation architectures on the newly introduced KITTI panoptic segmentation dataset and IDD dataset. 7. Comprehensive benchmarking of our proposed Effi-cientPS architecture on Cityscapes, Mapilliary Vistas, KITTI and IDD datasets. 8. Extensive ablation studies that compare the performance of various architectural components that we propose in this work with their counterparts from state-of-the-art architectures. 9. Implementation of our proposed architecture and a live demo on all the four datasets is publicly available at http://rl.uni-freiburg.de/research/panoptic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Panoptic segmentation is a recently introduced scene understanding problem <ref type="bibr" target="#b29">(Kirillov et al, 2019b</ref>) that unifies the tasks of semantic segmentation and instance segmentation. There are numerous methods that have been proposed for each of these sub-tasks, however only a handful of approaches have been introduced to tackle this coherent scene understanding problem of panoptic segmentation. Most works in this domain are largely built upon advances made in semantic segmentation and instance segmentation, therefore we first review recent methods that have been proposed for these closely related tasks, followed by state-of-the-art approaches that have been introduced for panoptic segmentation.</p><p>Semantic Segmentation: There has been significant advances in semantic segmentation approaches in recent years. In this section, we briefly review methods that use a single monocular image to tackle this task. Approaches from the past decade, typically employ random decision forests to address this task. <ref type="bibr" target="#b54">Shotton et al (2008)</ref>   <ref type="bibr" target="#b57">Sturgess et al (2009)</ref> further combine appearance-based features with structurefrom-motion features in addition to CRFs to improve the performance. However, 3D features extracted from dense depth maps <ref type="bibr" target="#b75">(Zhang et al, 2010)</ref> have been demonstrated to be more effective than the combined features. <ref type="bibr" target="#b30">Kontschieder et al (2011)</ref> exploit the inherent topological distribution of object classes to improve the performance, whereas <ref type="bibr" target="#b31">Krähenbühl and Koltun (2011)</ref> improve segmentation by pairing CRFs with Gaussian edge potentials. Nevertheless, all these methods employ handcrafted features that do not encapsulate all the high-level and low-level relations thereby limiting their representational ability.</p><p>The significant improvement in performance of classification tasks brought about by Convolutional Neural Network (CNN) based approaches motivated researchers to explore such methods for semantic segmentation. Initially, these approaches relied on patch-wise training that severely limited their ability to accurately segment object boundaries. However, they still perform substantially better than previous handcrafted methods. The advent of end-to-end learning approaches for semantic segmentation lead by the introduction of Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b43">(Long et al, 2015)</ref> revolutionized this field and FCNs still form the base upon which state-of-the-art architecture are built upon today. FCN is an encoded-decoder architecture where the encoder is based on the VGG-16 <ref type="bibr" target="#b55">(Simonyan and Zisserman, 2014)</ref> architecture with inner-product layers replaced with convolutions, and the decoder consists of convolution and transposed convolution layers. The subsequently proposed SegNet (Badrinarayanan et al, 2017) architecture introduced unpooling layers for upsampling as a replacement for transposed convolutions, whereas ParseNet <ref type="bibr" target="#b42">(Liu et al, 2015)</ref> models global context directly as opposed to only relying on the largest receptive field of the network.</p><p>The PSPNet <ref type="bibr" target="#b77">(Zhao et al, 2017)</ref> architecture emphasizes on the importance of multi-scale features and propose pyramid pooling to learn feature representations at different scales. <ref type="bibr" target="#b74">Yu and Koltun (2015)</ref> introduce atrous convolutions to further exploit multi-scale features in semantic segmentation networks. Subsequently, <ref type="bibr" target="#b65">Valada et al (2017)</ref> propose multi-scale residual units with parallel atrous convolutions with different dilation rates to efficiently learn multiscale features throughout the network without increasing the number of parameters. <ref type="bibr" target="#b6">Chen et al (2017b)</ref> propose the Atrous Spatial Pyramid Pooling (ASPP) module that concatenates feature maps from multiple parallel atrous convolutions with different dilation rates and a global pooling layer. ASPP substantially improves the performance of semantic segmentation networks by aggregating multi-scale features and capturing long-range context, however it significantly increases the computational complexity. Therefore, <ref type="bibr" target="#b7">Chen et al (2018a)</ref> propose Dense Prediction Cells (DPC) and <ref type="bibr" target="#b67">Valada et al (2019)</ref> propose Efficient Atrous Spatial Pyramid Pooling (eASPP) that yield better semantic segmentation performance than ASPP while being 10-times more efficient. <ref type="bibr" target="#b34">Li et al (2019a)</ref> suggest that global feature aggregation often leads to large pattern features and also over-smooth regions of small patterns which results in sub-optimal performance. In order to alleviate this problem, the authors propose the use of a global aggregation module coupled with a local distribution module which results in features that are balanced in small and large pattern regions. There are also several works that have been proposed to improve the upsampling in decoders of encoder-decoder architectures. In <ref type="bibr" target="#b8">(Chen et al, 2018b)</ref>, the authors introduce a novel decoder module for object boundary refinement. <ref type="bibr" target="#b59">Tian et al (2019)</ref> propose data-dependent upsampling which accounts for the redundancy in the label space as opposed to simple bilinear upsampling.</p><p>Instance Segmentation: Some of the initial approaches employ CRFs <ref type="bibr" target="#b22">(He and Gould, 2014b)</ref> and minimize integer quadratic relations <ref type="bibr" target="#b61">(Tighe et al, 2014)</ref>. Methods that exploit CNNs with Markov random fields  and recurrent neural networks <ref type="bibr" target="#b50">(Romera-Paredes and Torr, 2016;</ref><ref type="bibr" target="#b49">Ren and Zemel, 2017)</ref> have also been explored. In this section, we primarily discuss CNN-based approaches for instance segmentation. These methods can be categorized into proposal free and proposal based methods.</p><p>Methods in the proposal free category often obtain instance masks from a resulting transformation. <ref type="bibr" target="#b2">Bai and Urtasun (2017)</ref> uses CNNs to produce an energy map of the image and then perform a cut at a single energy level to obtain the corresponding object instances. <ref type="bibr" target="#b40">Liu et al (2017)</ref> employ a sequence of CNNs to solve sub-grouping problems in order to compose object instances. Some approaches exploit FCNs which either use local coherence for estimating instances <ref type="bibr" target="#b12">(Dai et al, 2016)</ref> or encode the direction of each pixel to its corresponding instance centre <ref type="bibr" target="#b63">(Uhrig et al, 2016)</ref>. The recent approach, SSAP <ref type="bibr" target="#b14">(Gao et al, 2019)</ref> uses pixelpair affinity pyramids for computing the probability that two pixels hierarchically belong to the same instance. However, they achieve a lower than proposal based methods which has led to a decline in their popularity.</p><p>In proposal based methods, <ref type="bibr" target="#b18">Hariharan et al (2014)</ref> propose a method that uses Multiscale Combinatorial Grouping  proposals as input to CNNs for feature extraction and then employ an SVM classifier for region classification. Subsequently, <ref type="bibr" target="#b19">Hariharan et al (2015)</ref> propose hypercolumn pixel descriptors for simultaneous detection and segmentation. In recent works, DeepMask <ref type="bibr" target="#b45">(Pinheiro et al, 2015)</ref> uses a patch of an image as input to a CNN which yields a class-agnostic segmentation mask and the likelihood of the patch containing an object. FCIS ) employs position-sensitive score maps obtained from classification of pixels based on their relative positions to perform segmentation and detection jointly. <ref type="bibr" target="#b12">Dai et al (2016)</ref> propose an approach for instance segmentation that uses three networks for distinguishing instances, estimating masks and categorizing objects. Mask R-CNN  is one of the most popular and widely used approaches in the present time. It extends Faster R-CNN for instance segmentation by adding an object segmentation branch parallel to an branch that performs bounding box regression and classification. More recently, <ref type="bibr" target="#b41">Liu et al (2018)</ref> propose an approach to improve Mask R-CNN by adding bottom-up path augmentation that enhances object localization ability in earlier layers of the network. Subsequently, BshapeNet <ref type="bibr" target="#b27">(Kang and Kim, 2018)</ref> extends Faster R-CNN by adding a bounding box mask branch that provides additional information of object positions and coordinates to improve the performance of object detection and instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Panoptic Segmentation:</head><p>In an earlier attempt of unifying semantic and instance segmentation task, <ref type="bibr" target="#b62">(Tu et al, 2005)</ref> uses a Bayesian framework to output scene representation as a parsing graph. Further, some approaches employ auxiliary variables to reason at the segment level <ref type="bibr" target="#b73">(Yao et al, 2012)</ref> and combination of region-level features with per-exemplar sliding window detectors <ref type="bibr" target="#b60">(Tighe and Lazebnik, 2013)</ref> to address the task. Methods such as minimization of an integer quadratic program <ref type="bibr" target="#b61">(Tighe et al, 2014)</ref> and maximization of a posteriori inference <ref type="bibr">(Sun et al, 2013)</ref> have also been explored. Nevertheless, the aforementioned methods due to their complexity and sub-par performance couldn't garner much attention to the task. But later <ref type="bibr" target="#b29">Kirillov et al (2019b)</ref> revived the unification of semantic segmentation and instance segmentation tasks by introducing panoptic segmentation. They propose a baseline model that combines the output of PSPNet <ref type="bibr" target="#b77">(Zhao et al, 2017)</ref> and Mask R-CNN  with a simple post-processing step in which each model processes the inputs independently. The methods that address this task of panoptic segmentation can be broadly classified into two categories: top-down or proposal based methods and bottom-up or proposal free methods. Most of the current stateof-the-art methods adopt the top-down approach. <ref type="bibr">de Geus et al (2018)</ref> propose joint training with a shared backbone that branches into Mask R-CNN for instance segmentation and augmented Pyramid Pooling module for semantic segmentation. Subsequently, <ref type="bibr" target="#b36">Li et al (2019b)</ref> introduce Attentionguided Unified Network that uses proposal attention module and mask attention module for better segmentation of 'stuff' classes. All the aforementioned methods use a similar fusion technique to <ref type="bibr" target="#b29">Kirillov et al (2019b)</ref> for the fusion of 'stuff' and 'thing' predictions.</p><p>In top-down panoptic segmentation architectures, predictions of both heads have an inherent overlap between them resulting in the mask overlapping problem. In order to mitigate this problem, <ref type="bibr" target="#b33">Li et al (2018b)</ref> propose a weakly supervised model where 'thing' classes are weakly supervised by bounding boxes and 'stuff' classes are supervised with image-level tags. Whereas, <ref type="bibr" target="#b39">Liu et al (2019)</ref> address the problem by introducing the spatial ranking module and <ref type="bibr" target="#b32">Li et al (2018a)</ref> propose a method that learns a binary mask to constrain output distributions of 'stuff' and 'thing' explicitly. Subsequently, UPSNet <ref type="bibr" target="#b70">(Xiong et al, 2019)</ref> introduces a parameter-free panoptic head to address the problem of overlapping of instances and also predicts an extra unknown class. More recently, AdaptIS <ref type="bibr" target="#b56">(Sofiiuk et al, 2019)</ref> uses point proposals to produce instance masks and jointly trains with a standard semantic segmentation pipeline to perform panoptic segmentation. In contrast, <ref type="bibr" target="#b47">Porzi et al (2019)</ref> propose an architecture for panoptic segmentation that effectively integrates contextual information from a lightweight DeepLab-inspired module with multi-scale features from a FPN.</p><p>Compared to the popular proposal based methods, there are only a handful of proposal free methods that have been proposed. Deeper-Lab  was the first bottomup approach that was introduced and it employs an encoderdecoder topology to pair object centres for class-agnostic instance segmentation with DeepLab semantic segmentation. <ref type="bibr" target="#b9">Cheng et al (2020)</ref> further builds on Deeper-Lab by introducing a dual-ASPP and dual-decoder structure for each sub-task branch. SSAP <ref type="bibr" target="#b14">(Gao et al, 2019)</ref> proposes to group pixels based on a pixel-pair affinity pyramid and incorporate an efficient graph method to generate instances while jointly learning semantic labeling.</p><p>In this work, we adopt a top-down approach due to its exceptional ability to handle large scale variation of instances which is a critical requirement for segmenting 'thing' classes. We present the novel EfficientPS architecture that incorporates our proposed efficient backbone with our 2-way FPN for learning rich multi-scale features in a bidirectional manner, coupled with a new semantic head that captures fine-features and long-range context effectively, and a variant of Mask R-CNN augmented with depthwise separable convolutions as the instance head. We propose a novel panoptic fusion module to dynamically adapt the fusion of logits from the semantic and instance heads to yield the panoptic segmentation output. Our architecture achieves state-of-the-art results on benchmark datasets while being the most efficient and fast panoptic segmentation architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EfficientPS Architecture</head><p>In this section, we first give a brief overview of our proposed EfficientPS architecture and then detail each of its constituting components. Our network follows the top-down layout as shown in <ref type="figure">Figure 2</ref>. It consists of a shared backbone with  <ref type="figure">Figure 2</ref> Illustration of our proposed EfficientPS architecture consisting of a shared backbone with our 2-way FPN and parallel semantic and instance segmentation heads followed by our panoptic fusion module. The shared backbone is built upon on the EfficientNet architecture and our new 2-way FPN that enables bidirectional flow of information. The instance segmentation head is based on a modified Mask R-CNN topology and we incorporate our proposed semantic segmentation head. Finally, the outputs of both heads are fused in our panoptic fusion module to yield the panoptic segmentation output. a 2-way Feature Pyramid Network (FPN), followed by taskspecific semantic segmentation and instance segmentation heads. We build upon the EfficientNet <ref type="bibr" target="#b58">(Tan and Le, 2019)</ref> architecture for the encoder of our shared backbone (depicted in red). It consists of mobile inverted bottleneck <ref type="bibr" target="#b69">(Xie et al, 2017)</ref> units and employs compound scaling to uniformly scale all the dimensions of the encoder network. This enables our encoder to have a rich representational capacity with fewer parameters in comparison to other encoders or backbones of similar discriminative capability.</p><p>As opposed to employing the conventional FPN <ref type="bibr" target="#b38">(Lin et al, 2017)</ref> that is commonly used in other panoptic segmentation architectures <ref type="bibr" target="#b28">(Kirillov et al, 2019a;</ref><ref type="bibr" target="#b32">Li et al, 2018a;</ref><ref type="bibr" target="#b47">Porzi et al, 2019)</ref>, we incorporate our proposed 2-way FPN that fuses multi-scale features more effectively than its counterparts. This can be attributed to the fact that the information flow in our 2-way FPN is not bounded to only one direction as depicted by the purple, blue and green blocks in <ref type="figure">Figure 2</ref>. Subsequently after the 2-way FPN, we employ two heads in parallel which are semantic segmentation (depicted in yellow) and instance segmentation (depicted in gray and orange) respectively. We use a variant of the Mask R-CNN ) architecture as the instance head and we incorporate our novel semantic segmentation head consisting of dense prediction cells <ref type="bibr" target="#b7">(Chen et al, 2018a</ref>) and residual pyramids. The semantic head consists of three different modules for capturing fine features, long-range contextual features and correlating the distinctly captured features for improving object boundary refinement. Finally, we employ our proposed panoptic fusion module to fuse the outputs of the semantic and instance heads to yield the panoptic segmentation output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Backbone</head><p>The backbone of our network consists of an encoder with our proposed 2-way FPN. The encoder is the basic building block of any segmentation network and a strong encoder is essential to have high representational capacity. In this work, we seek to find a good trade-off between the number of parameters and computational complexity to the representational capacity of the network. EfficientNets <ref type="bibr" target="#b58">(Tan and Le, 2019)</ref> which are a recent family of architectures have been shown to significantly outperform other networks in classification tasks while having fewer parameters and FLOPs. It employs compound scaling to uniformly scale the width, depth and resolution of the network efficiently. Therefore, we choose to build upon this scaled architecture with 1.6, 2.2 and 456 coefficients, commonly known as the EfficientNet-B5 model. This can be easily replaced with any of the EfficientNet models based on the capacity of the resources that are available and the computational budget.</p><p>In order to adapt EfficientNet to our task, we first remove the classification head as well as the Squeeze-and-Excitation (SE) <ref type="bibr" target="#b24">(Hu et al, 2018)</ref> connections in the network. We find that the explicit modelling of interdependencies between channels of the convolutional feature maps that are enabled by the SE connections tend to suppress localization of features in favour of contextual elements. This property is a desired in classification networks, however both are equally important for segmentation tasks, therefore we do not add any SE connections in our backbone. Second, we replace all the batch normalization <ref type="bibr" target="#b25">(Ioffe and Szegedy, 2015)</ref> layers with synchronized Inplace Activated Batch Normalization (iABN sync) (Rota <ref type="bibr" target="#b52">Bulò et al, 2018)</ref>. This enables synchronization across different GPUs, which in turn yields a better estimate of gradients while performing multi-GPU training and the in-place operations frees up additional GPU memory. We analyze the performance of our modified EfficientNet in comparison to other encoders commonly used in state-of-the-art architectures in the ablation study presented in Section 4.4.2.</p><p>Our EfficientNet encoder comprises of nine blocks as shown in <ref type="figure">Figure 2</ref> (in red). We refer to each block in the figure as block 1 to block 9 in the left to right manner. The output of block 2, 3, 5, and 9 corresponds to downsampling factors ×4, ×8, ×16 and ×32 respectively. The outputs from these blocks with downsampling are also inputs to our 2-way FPN. The conventional FPN used in other panoptic segmentation networks aims to address the problem of multi-scale feature fusion by aggregating features of different resolutions in a top-down manner. This is performed by first employing a 1 × 1 convolution to reduce or increase the number of channels of different encoder output resolutions to a predefined number, typically 256. Then, the lower resolution features are upsampled to a higher resolution and are subsequently added together. For example, ×32 resolution encoder output features will be resized to the ×16 resolution and added to the ×16 resolution encoder output features. Finally, a 3 × 3 convolution is used at each scale to further learn fused features which yields the P 4 , P 8 , P 16 and P 32 outputs. This FPN topology has a limited unidirectional flow of information resulting in an ineffective fusion of multi-scale features. Therefore, we propose to mitigate this problem by adding a second branch that aggregates multi-scale features in a bottom-up manner to enable bidirectional flow of information.</p><p>Our proposed 2-way FPN shown in <ref type="figure">Figure 2</ref> consists of two parallel branches. Each branch consists of a 1 × 1 convolution with 256 output filters at each scale for chan-  nel reduction. The top-down branch shown in blue follows the aggregation scheme of a conventional FPN from right to left. Whereas, the bottom-up branch shown in purple, downsamples the higher resolution features to the next lower resolution from left to right and subsequently adds them with the next lower resolution encoder output features. For example, ×4 resolution features will be resized to the ×8 resolution and added to the ×8 resolution encoder output features. Then in the next stage, the outputs from the bottom-up and top-down branches at each resolution are correspondingly summed together and passed through a 3 × 3 depthwise separable convolution with 256 output channels to obtain the P 4 , P 8 , P 16 , and P 32 outputs respectively. We employ depthwise separable convolutions as opposed to standard convolutions in an effort to keep the parameter consumption low. We evaluate the performance of our proposed 2-way FPN in comparison to the conventional FPN in the ablation study presented in Section 4.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Segmentation Head</head><p>Our proposed semantic segmentation head consists of three components, each aimed at targeting one of the critical requirements. First, at large-scale, the network should have the ability to capture fine features efficiently. In order to enable this, we employ our Large Scale Feature Extractor (LSFE) module that has two 3 × 3 depthwise separable convolutions with 128 output filters, each followed by an iABN sync and a Leaky ReLU activation function. The first 3 × 3 depthwise separable convolution reduces the number of filters to 128 and the second 3 × 3 depthwise separable convolution further learns deeper features.</p><p>The second requirement is that at small-scale, the network should be able to capture long-range context. Modules inspired by Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b5">Chen et al (2017a)</ref> that are widely used in state-of-the-art semantic segmentation architectures have been demonstrated to be effective for this purpose. Dense Prediction Cells (DPC) <ref type="bibr" target="#b7">(Chen et al, 2018a)</ref> and Efficient Atrous Spatial Pyramid Pooling (eASPP)  are two variants of ASPP that are significantly more efficient and also yield a better performance. We find that DPC demonstrates a better performance with a minor increase in the number of parameters compared to eASPP. Therefore, we employ a modified DPC module in our semantic head as shown in <ref type="figure">Figure 2</ref>. We augment the original DPC topology by replacing batch normalization layers with iABN sync, and ReLUs with Leaky ReLUs. The DPC module consists of a 3 × 3 depthwise separable convolution with 256 output channels having a dilation rate of (1,6) and extends out to five parallel branches. Three of the branches, each consist of a 3 × 3 dilated depthwise separable convolution with 256 outputs, where the dilation rates are (1,1), (6,21), and (18,15) respectively. The fourth branch takes the output of the dilated depthwise separable convolution with a dilation rate of (18,15), as input and passes it through another 3 × 3 dilated depthwise separable convolution with 256 output channels and a dilation rate of (6,3). The outputs from all these parallel branches are then concatenated to yield a tensor with 1280 channels. This tensor is then finally passed through a 1 × 1 convolution with 256 output channels and forms the output of the DPC module. Note that each of the convolutions in the DPC module is followed by a iABN sync and a Leaky ReLU activation function.</p><p>The third and final requirement for the semantic head is that it should be able to mitigate the mismatch between large-scale and small-scale features while performing feature aggregation. To this end, we employ our Mismatch Correction Module (MC) that correlates the small-scale features with respect to large-scale features. It consists of cascaded 3 × 3 depthwise separable convolutions with 128 output channels, followed by iABN sync with Leaky ReLU and a bilinear upsampling layer that upsamples the feature maps by a factor of 2. <ref type="figure" target="#fig_0">Figures 3 (a)</ref>, 3 (c) and 3 (d) illustrate the topologies of these main components of our semantic head.</p><p>The four different scaled outputs of our 2-way FPN, namely P 4 , P 8 , P 16 and P 32 are the inputs to our semantic head. The small-scale inputs, P 32 and P 16 with downsampling factors of ×32 and ×16 are each fed into two parallel DPC modules. While the large-scale inputs, P 8 and P 4 with downsampling factors of ×8 and ×4 are each passed through two parallel LSFE modules. Subsequently, the outputs from each of these parallel DPC and LSFE modules are augmented with feature alignment connections and each of them is upsampled to x4 scale. These upsampled feature maps are then concatenated to yield a tensor with 512 channels which is then input to a 1 × 1 convolution with N 'stuff'+'thing' output filters. This tensor is then finally upsampled by a factor of 4 and passed through a softmax layer to yield the semantic logits having the same resolution as the input image. Now, the feature alignment connections from the DPC and LSFE modules interconnect each of these outputs by element-wise summation as shown in <ref type="figure">Figure 2</ref>. We add our MC modules in the interconnections between the second DPC and LSFE as well as between both the LSFE connections. These correlation connections aggregate contextual information from small-scale features and characteristic large-scale features for better object boundary refinement. We use the weighted per-pixel log-loss  for training which is given by</p><formula xml:id="formula_0">L pp (Θ ) = − ∑ i j w i j (p * i j ) log p i j ,<label>(1)</label></formula><p>p * i, j is the groundtruth for a given image, p i, j is the predicted probability for the pixel (i, j) being assigned class c ∈ p, w i j = 4 W H if pixel (i, j) belongs to 25% of the worst prediction, and w i j = 0 otherwise. W and H are the width and height of the given input image. The overall semantic head loss is given by</p><formula xml:id="formula_1">L semantic (Θ ) = 1 n ∑ L pp ,<label>(2)</label></formula><p>where n is the batch size. We present in-depth analysis of our semantic head in comparison other semantic heads commonly used in state-of-the-art architectures in Section 4.4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Instance Segmentation Head</head><p>The instance segmentation head of our EfficientPS network shown in <ref type="figure">Figure 2</ref> has a topology similar to Mask R-CNN  with certain modifications. More specifically, we replace all the standard convolutions, batch normalization layers, and ReLU activations with depthwise separable convolution, iABN sync, and Leaky ReLU respectively. Similar to the rest of our architecture, we use depthwise separable convolutions instead of standard convolutions to reduce the number of parameters consumed by the network. This enables us to conserve 2.09 M parameters in comparison to the conventional Mask R-CNN. Mask R-CNN consists of two stages. In the first stage, the Region Proposal Network (RPN) module shown in <ref type="figure" target="#fig_0">Figure  3</ref> (b) employs a fully convolutional network to output a set of rectangular object proposals and an objectness score for the given input FPN level. Subsequently, ROI align  uses object proposals to extract features from FPN encodings by directly pooling features from the n th channel with a 14 × 14 spatial resolution bounded within a bounding box proposal. The features that are extracted then serve as input to the bounding box regression, object classification and mask segmentation networks. The logits output from the mask segmentation networks for each candidate bounding box proposal is then fused with the semantic logits in our proposed panoptic fusion module described in Section 3.4.</p><p>In order to train the instance segmentation head, we adopt the loss functions proposed in Mask R-CNN, i.e. two loss functions for the first stage: objectness score loss and object proposal loss, and three loss functions for the second stage: classification loss, bounding box loss and mask segmentation loss. We take a set of randomly sampled positive matches and negative matches such that |N s | ≤ 256. The objectness score loss L os defined as log loss for a given N s is given by</p><formula xml:id="formula_2">L os (Θ ) = − 1 |N s | ∑ (p * os ,p os )∈N s p * os · log p os + (1 − p * os ) · log(1 − p os ),<label>(3)</label></formula><p>where p os is the output of the objectness score branch of RPN and p * os is the groundtruth label which is 1 if the anchor is positive, and 0 if the anchor is negative. We use the same strategy as Mask R-CNN for defining positive and negative matches. For a given anchor a, if the groundtruth box b * has the largest Intersection over Union (IoU) or IoU(b * , a) &gt; T H , then the corresponding prediction b is a positive match and b is a negative match when IoU(b * , a) &lt; T L . The thresholds T H and T L are pre-defined where T H &gt; T L .</p><p>The object proposal loss L op is a regression loss that is defined only on positive matches and is given by</p><formula xml:id="formula_3">L op (Θ ) = 1 |N s | ∑ (t * ,t)∈N p ∑ (i * ,i)∈(t * ,t) L 1 (i * , i),<label>(4)</label></formula><p>where L 1 is the smooth L1 Norm, N p is the subset of N s positive matches, t * = (t * x ,t * y ,t * w ,t * h ) and t = (t x ,t y ,t w ,t h ) are the parameterizations of b * and b respectively, b * = (x * , y * , w * , h * ) is the groundtruth box, b * = (x, y, w, h) is the predicted bounding box, x, y, w and h are the center coordinates, width and height of the predicted bounding box. Similarly, x * , y * , w * and h * denote the center coordinates, width and height of the groundtruth bounding box. The parameterizations <ref type="bibr" target="#b16">(Girshick, 2015)</ref> are given by</p><formula xml:id="formula_4">t x = (x − x a ) w a ,t y = (y − y a ) h a , t w = log w w a , t h = log h h a ,<label>(5)</label></formula><formula xml:id="formula_5">t * x = (x * − x a ) w a ,t * y = (y * − y a ) h a ,t * w = log w * w a ,t * h = log h * h a ,<label>(6)</label></formula><p>where x a , y a , w a and h a denote the center coordinates, width and height of the anchor a.</p><p>Similar to the objectness score loss L os , the classification loss L cls is defined for a set of K s randomly sampled positive and negative matches such that |K s | ≤ 512. The classification loss L cls is given by</p><formula xml:id="formula_6">L cls (Θ ) = − 1 |K s | N 'thing +1 ∑ c=1 Y * o,c · logY o,c , for(Y * ,Y ) ∈ K s , (7)</formula><p>where Y is the output of the classification branch, Y * is the one hot encoded groundtruth label, o is the observed class, and c is the correct classification for object o. For a given image, it is a positive match if IoU(b * , b) &gt; T n and otherwise a negative match, where b * is the groundtruth box, and b is the object proposal from the first stage. The bounding box loss L bbx is a regression loss that is defined only on positive matches and is expressed as</p><formula xml:id="formula_7">L bbx (Θ ) = 1 |K s | ∑ (T * ,T )∈K p ∑ (i * ,i)∈(T * ,T ) L 1 (i * , i),<label>(8)</label></formula><p>where L 1 is the smooth L1 Norm <ref type="bibr" target="#b16">(Girshick, 2015)</ref>, K p is the subset of K s positive matches, T * and T are the parameterizations of B * and B respectively, similar to Equation <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_3">(4)</ref> where B * is the groundtruth box, and B is the corresponding predicted bounding box. Finally, the mask segmentation loss is also defined only for positive samples and is given by</p><formula xml:id="formula_8">L mask (Θ ) = − 1 |K s | ∑ (P * ,P)∈K s L p (P * , P),<label>(9)</label></formula><p>where L p (P * , P) is given as</p><formula xml:id="formula_9">L p (P * , P) = − 1 |T p | ∑ (i, j)∈T p P * i, j · log P i, j + (1 − P * i, j ) · log(1 − P i, j ),<label>(10)</label></formula><p>where P is the predicted 28 × 28 binary mask for a class with P i, j denoting the probability of the mask pixel (i, j), P * is the 28 × 28 groundtruth binary mask for the class, and T p is the set of non-void pixels in P * . All the five losses are weighed equally and the total instance segmentation head loss is given by</p><formula xml:id="formula_10">L instance = L os + L op + L cls + L bbx + L mask .<label>(11)</label></formula><p>Similar to Mask R-CNN, the gradient that is computed w.r.t to the losses L cls , L bbx and L mask flow only through the network backbone and not through the region proposal network.  </p><formula xml:id="formula_11">(σ (ML A )+σ (ML B )) (ML A +ML B )), where ML B is output of the function f * , σ (·)</formula><p>is the sigmoid function and is the Hadamard product.</p><p>Here, the f * function for given class prediction c (cyclist in this example), zeroes out the score of the c channel of the semantic logits outside the corresponding bounding box. Please note that 16 initial mask logits and 4 instances are just arbitrary number taken for the sake of ease of explanation. The real values can and are much higher than these numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Panoptic Fusion Module</head><p>In order to obtain the panoptic segmentation output, we need to fuse the prediction of the semantic segmentation head and the instance segmentation head. However, fusing both these predictions is not a straightforward task due to the inherent overlap between them. Therefore, we propose a novel panoptic fusion module to tackle the aforementioned problem in an adaptive manner in order to thoroughly exploit the predictions from both the heads congruously. <ref type="figure" target="#fig_1">Figure 4</ref> shows the topology of our panoptic fusion module. We obtain a set of object instances from the instance segmentation head of our network where for each instance, we have its corresponding class prediction, confidence score, bounding box and mask logits. First, we reduce the number of predicted object instances in two stages. We begin by discarding all object instances that have a confidence score of less than a certain confidence threshold. We then resize, zero pad and scale the 28 × 28 mask logits of each object instance to the same resolution as the input image. Subsequently, we sort the class prediction, bounding box and mask logits according to the respective confidence scores. In the second stage, we check each sorted instance mask logit for overlap with other object instances. To do so we compute the sigmoid of the mask logits and threshold it at 0.5 to obtain the corresponding binary mask. Then if the overlap between the binary masks is greater than a given overlap threshold, the mask logits with the highest confidence are retained and the other overlapping mask logits are discarded. After filtering the object instances, we have the class prediction, bounding box prediction and mask logit ML A of each instance. We simultaneously obtain semantic logits with N channels from the semantic head, where N is the sum of N 'stu f f and N 'thing . We then compute a second mask logit ML B for each instance where we select the channel of the semantic logits based on its class prediction. We only keep the logit score of the selected channel for the area within the instance bounding box, while we zero out the scores that are outside this region. In the end, we have two mask logits for each instance, one from instance segmentation head and the other from the semantic segmentation head. We combine these two logits adaptively by computing the Hadamard product of the sum of sigmoid of ML A and sigmoid of ML B , and the sum of ML A and ML B to obtain the fused mask logits FL of instances expressed as</p><formula xml:id="formula_12">FL = (σ (ML A ) + σ (ML B )) (ML A + ML B ),<label>(12)</label></formula><p>where σ (·) is the sigmoid function and is the Hadamard product. We then concatenate the fused mask logits of the object instances with the 'stuff' logits along the channel dimension to generate intermediate panoptic logits. Subsequently, we apply the argmax operation along the channel dimension to obtain the intermediate panoptic prediction. In the final step, we take a zero-filled canvas and first copy the instancespecific 'thing' prediction from the intermediate panoptic prediction. We then fill the empty parts of the canvas with 'stuff' class predictions by copying them from the predictions of the semantic head while ignoring classes that have an area smaller than a predefined threshold called minimum stuff area. This gives us the final panoptic segmentation output. We fuse ML A and ML B instance logits in the aforementioned manner due to the fact that if both logits for a given pixel conform with each other, the final instance score will increase proportionately to their agreement or vice-versa. In case of agreement, the corresponding object instance will dominate or be superseded by other instances as well as the 'stuff' classes score. Similarly, in case of disagreement, the score of the given object instance will reflect the extent of their difference. Simply put, the fused logit score is either adaptively attenuated or amplified according to the consensus. We evaluate the performance of our proposed panoptic fusion module in comparison to other existing methods in the ablation study presented in Section 4.4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we first describe the standard evaluation metrics that we adopt for empirical evaluations, followed by brief descriptions of the datasets that we benchmark on in Section 4.1. We then present extensive quantitative comparisons and benchmarking results in Section 4.3, and detailed ablation studies on the various proposed architectural components in Section 4.4. Finally, we present qualitative comparisons and visualizations of panoptic segmentation on each of the datasets that we evaluate on in Section 4.5 and Section 4.6 respectively.</p><p>We use PyTorch <ref type="bibr" target="#b44">(Paszke et al, 2019)</ref> for implementing all our architectures and we trained our models on a system with an Intel Xenon@2.20GHz processor and NVIDIA TI-TAN X GPUs. We use the standard Panoptic Quality (PQ) metric <ref type="bibr" target="#b29">(Kirillov et al, 2019b)</ref> for quantifying the performance of our models. The PQ metric is computed as</p><formula xml:id="formula_13">PQ = ∑ (p,g)∈T P IoU(p, g) |T P| + 1 2 |FP| + 1 2 |FN| ,<label>(13)</label></formula><p>where T P, FP, FN and IoU are true positives, false positives, false negatives and the intersection-over-union. The IoU is computed as IoU = T P/(T P+FP+FN). We also report the Segmentation Quality (SQ) and Recognition Quality (RQ) metrics computed as</p><formula xml:id="formula_14">SQ = ∑ (p,g)∈T P IoU(p, g) |T P| ,<label>(14)</label></formula><formula xml:id="formula_15">RQ = |T P| |T P| + 1 2 |FP| + 1 2 |FN| .<label>(15)</label></formula><p>Following the standard benchmarking criteria for pantoptic segmentation, we report PQ, SQ and RQ over all the classes in the dataset, and we also report them for the 'stuff' classes (PQ St , SQ St , RQ St ) and the 'thing' classes (PQ Th , SQ Th , RQ Th ). Additionally, for the sake of completeness, we report the Average Precision (AP), mean Intersectionover-Union (mIoU) for both 'stuff' and 'thing' classes, as well as the inference time and FLOPs for comparisons. The implementation of our proposed EfficientPS model and a live demo on various datasets is publicly available at https: //rl.uni-freiburg.de/research/panoptic. Example images from the challenging urban scene understanding datasets that we benchmark on, namely, Cityscapes, KITTI, Mapillary Vistas, and Indian Driving Dataset (IDD). The images show cluttered urban scenes with many dynamic objects, occluded objects, perpetual snowy conditions and unstructured environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We benchmark our proposed EfficientPS for panoptic segmentation on four challenging urban scene understanding datasets, namely, Cityscapes , KITTI (Geiger et al, 2013), Mapillary Vistas , and Indian Driving Dataset <ref type="bibr" target="#b67">(Varma et al, 2019)</ref>. The KITTI benchmark does not provide panoptic annotations, therefore to facilitate this work, we publicly release manually annotated panoptic groundtruth segmentation labels for the popular KITTI benchmark. These four diverse datasets contain images that range from congested city driving scenarios to rural scenes and highways. They also contain scenes in challenging perceptual conditions including snow, motion blur and other seasonal visual changes. We briefly describe the characteristics of these datasets in this section.</p><p>Cityscapes: The Cityscapes dataset  consists of urban street scenes and focuses on semantic understanding of common driving scenarios. It is one of the most challenging datasets for panoptic segmentation due to its sheer diversity as it covers scenes from over 50 European cities recorded over several seasons such as spring, summer and fall. The presence of a large number of dynamic objects further add to its complexity. <ref type="figure">Figure 5 (a)</ref> shows an example image and the corresponding panoptic groundtruth annotation from the Cityscapes dataset. As we see from this example, the scenes are extremely clutterd with many dynamic objects such as pedestrians and cyclists that are often grouped near one and another or partially occluded. These factors make panoptic segmentation, especially segmenting the 'thing' class exceedingly challenging. The widely used Cityscapes dataset recently introduced a benchmark for the task of panoptic segmentation. The dataset contains pixel-level annotations for 19 object classes of which 11 are 'stuff' classes and 8 are instance-specific 'thing' classes. It consists of 5000 finely annotated images and 20000 coarsely annotated images that were captured at a resolution of 2048 × 1024 pixels using an automotive-grade 22 cm baseline stereo camera. The finely annotated images are divided into 2975 for training, 500 for validation and 1525 for testing. The annotations for the test set are not publicly released, they are rather only available to the online evaluation server that automatically computes the metrics and publishes the results. We report the performance of our proposed EfficientPS on both the validation set as well as the test set. We also use the Cityscapes dataset for evaluating the improvement due to the various architectural contributions that we make in the ablation study. We report results on the validation set for our model trained only on the fine annotations and we report the results on the test set from the benchmarking server for our model trained on both the fine and coarse annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI:</head><p>The KITTI vision benchmark suite <ref type="bibr" target="#b15">(Geiger et al, 2013)</ref> is one of the most comprehensive datasets that provides groundtruth for a variety of tasks such as semantic segmentation, scene flow estimation, optical flow estimation, depth prediction, odometry estimation, tracking and road lane detection. However, it still has not expanded its annotations to support the recently introduced panoptic segmentation task. The challenging nature of the KITTI scenes and its potential for benchmarking multi-task learning problems, makes extending this dataset to include panoptic annotations of great interest to the community. Therefore, in this work, we introduce the KITTI panoptic segmentation dataset for urban scene understanding that provides panoptic annotations for a subset of images from the KITTI vision benchmark suite. The annotations for the images that we provide do not intersect with the official KITTI semantic/instance segmentation test set, therefore in addition to panoptic segmentation, they can also be used as supplementary training data for benchmarking semantic or instance segmentation tasks individually.</p><p>Our dataset consists of a total of 1055 images, out of which 855 are used for the training set and 200 are used for the validation set. We provide annotations for 11 'stuff' classes and 8 'thing' classes adhering to the Cityscapes 'stuff' and 'thing' class distribution. In order to create panoptic annotations, we gathered semantic annotations from community driven extensions of KITTI <ref type="bibr" target="#b71">(Xu et al, 2016;</ref><ref type="bibr" target="#b51">Ros et al, 2015)</ref> and combined them with the 200 training images from the KITTI semantic training set. We then manually annotated all the images with instance masks. We do so by manually drawing boundaries around the objects. We use an overlay of RGB and semantic segmentation image to guide the boundary drawing process. The pixels within the drawn boundaries in the semantic segmentation image are then labelled with a unique id to generate the corresponding instance segmentation mask. We create our simple annotation toolbox for labelling. We try to delineate objects as much as humanly possible otherwise treat the object as background or crowd in our annotations scheme. The instance masks are then merged with the semantic annotations to generate the panoptic segmentation ground truth labels. The images in our KITTI panoptic segmentation dataset are a resolution of 1280 × 384 pixels and contain scenes from both residential and inner city scenarios. <ref type="figure">Figure 5 (b)</ref> shows an example image from the KITTI panoptic segmentation dataset and its corresponding panoptic segmentation labels. We observe that the car denoted in teal color pixels and the van are both partially occluded by other 'stuff' classes such that they cause an object instance to be disjoint into two components. We find that scenarios such as these are extremely challenging for the task of panoptic segmentation as the disjoint object mask has to be assigned to the same instance ID. We hope that this dataset encourages innovative solutions to such realworld problems that are uncommon in other datasets and also accelerates research in multi-task learning for urban scene understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapillary Vistas: Mapillary Vistas (Neuhold et al, 2017)</head><p>is one of the largest publicly available street-level imagery datasets that contains pixel-accurate and instance-specific semantic annotations. The novel aspects of this dataset include diverse scenes from over six continents and in a variety of weather conditions, season, time of day, cameras, and viewpoints. It consists of 18,000 images for training, 2,000 images for validation, and 5,000 images for testing. The dataset provides panoptic annotations for 37 'thing' classes and 28 'stuff' classes. The images in this dataset are of different resolutions, ranging from 1024×768 pixels to 4000×6000 pixels. <ref type="figure">Figure 5 (c)</ref> shows an example image and the corresponding panoptic segmentation groundtruth from the Mapillary Vistas dataset. We can see that due to the snowy condition, recognizing distant objects such as the car in this example becomes extremely difficult. Such drastic seasonal changes make this dataset one of the most challenging for panoptic segmentation.</p><p>Indian Driving Dataset: The Indian Driving Dataset (IDD) (Varma et al, 2019) was recently introduced for scene understanding of unstructured environments. Unlike other urban scene understanding datasets, IDD consists of scenes that do not have well-delineated infrastructures such as lanes and sidewalks. It has a significantly more number of 'thing' instances in each scene compared to other datasets and it only has a small number of well-defined categories for traffic participants. The images in this dataset were captured with a front-facing camera mounted on a car and the data was gathered in two Indian cities as well as in their outskirts. IDD consists of a total of 10,003 images, where 6993 are used for training, 981 for validation and 2029 for testing. The images are a resolution of either 1920 × 1080 pixels or 720 × 1280 pixels. We train and evaluate all our models on 720p resolution on this dataset. The annotations are provided in four levels of hierarchy. Existing approaches primarily report their results for level 3, therefore we report the results of our model on the same to facilitate comparison. This level comprises of a total of 26 classes out of which 17 are 'stuff' classes and 9 are instance-specific 'thing' classes. An example image and the corresponding panoptic segmentation groundtruth from the IDD dataset is shown in <ref type="figure">Figure 5 (d)</ref>. We observe that the transition between the road and the sidewalk class is structurally not well defined which often leads to misclassifications. Factors such as this, make evaluating on this dataset uniquely challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Protocol</head><p>We train our network on crops of different resolutions of the input image, namely, 1024 × 2048, 1024 × 1024, 384 × 1280, and 720 × 1280 pixels. We take crops from the full resolution of the image provided in each of the datasets. We perform a limited set of random data augmentations including flipping and scaling within the range of [0.5, 2.0]. We initialize the backbone of our EfficientPS with weights from the Efficient-Net model pre-trained on the ImageNet dataset <ref type="bibr" target="#b53">(Russakovsky et al, 2015)</ref> and initialize the weights of the iABN sync layers to 1. We use Xavier initialization <ref type="bibr" target="#b17">(Glorot and Bengio, 2010)</ref> for the other layers, zero constant initialization for the biases and we use Leaky ReLU with a slope of 0.01. We use the same hyperparameters as <ref type="bibr" target="#b16">Girshick (2015)</ref> for our instance head and additionally set T H = 0.7, T L = 0.3, and T N = 0.5. In our proposed panoptic fusion module, we use a confidence threshold of c t = 0.5, overlap threshold of o t = 0.5 and minimum stuff area of min sa = 2048. We train our model with Stochastic Gradient Descent (SGD) with a momentum of 0.9 using a multi-step learning rate schedule i.e. we start with an initial base learning rate and train the model for a certain number of iterations, followed by lowering the learning rate by a factor of 10 at each milestone and continue training until convergence. We denote the base learning rate lr base , milestones and the total number of iterations ti for each dataset in the following format: {lr base , {milestone, milestone},ti}. respectively. At the beginning of the training, we have a warm-up phase where the lr base is increased linearly from 1 3 · lr base to lr base in 200 iterations. Aditionally, we freeze the iABN sync layers and further train the model for 10 epochs with a fixed learning rate of lr = 10 −4 . The final loss L total that we optimize is computed as</p><formula xml:id="formula_16">L total = L semantic + L instance ,<label>(16)</label></formula><p>where L semantic and L instance are given in Equation <ref type="formula" target="#formula_1">(2)</ref> and Equation <ref type="formula" target="#formula_0">(11)</ref> respectively. We train our EfficientPS with a batch size of 16 on 16 NVIDIA Titan X GPUs where each GPU tends to a single-image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benchmarking Results</head><p>In this section, we report results comparing the performance of our proposed EfficientPS architecture against current state-of-the-art panoptic segmentation approaches. For comparisons on the Cityscapes and Mapillary Vistas datasets, we directly report the performance metrics of the state-of-the-art methods as stated in their corresponding manuscripts. While for KITTI and IDD, we report results for the models that we trained using the official implementations that have been publicly released by the authors after further tuning of hyperparameters to the best of our ability. Note that existing methods have not reported results on KITTI and IDD validation sets. We report results on the validation sets for all the datasets and we additionally report results on the test set for the Cityscapes dataset by evaluating them on the official server. Note that at the time of submission, only the Cityscapes benchmark has the provision to evaluate the results on the test set. On each of the datasets, we report both the single-scale and multi-scale evaluation results. Following standard practise, we perform horizontal flipping and scaling (scales of {0.75, 1, 1.25, 1.5, 1.75, 2}) during the multi-scale evaluations.</p><p>We compare the performance of our proposed EfficientPS against state-of-the-art models on the Cityscapes dataset including WeaklySupervised <ref type="bibr" target="#b33">(Li et al, 2018b)</ref>, TASCNet <ref type="bibr" target="#b32">(Li et al, 2018a)</ref>, Panoptic FPN <ref type="bibr" target="#b28">(Kirillov et al, 2019a)</ref>, AUNet <ref type="bibr" target="#b36">(Li et al, 2019b)</ref>, UPSNet <ref type="bibr" target="#b70">(Xiong et al, 2019)</ref>, DeeperLab , Seamless <ref type="bibr" target="#b47">(Porzi et al, 2019)</ref>, SSAP <ref type="bibr" target="#b14">(Gao et al, 2019)</ref>, AdaptIS <ref type="bibr" target="#b56">(Sofiiuk et al, 2019)</ref>, and Panoptic-DeepLab <ref type="bibr" target="#b9">(Cheng et al, 2020)</ref>. <ref type="table">Table 1</ref> shows the results on the Cityscapes validation set. For a fair comparison, we categorize models in the table separately according to those that report single-scale and multi-scale evaluation, as well as without any pre-training and pre-training on other datasets, namely Mapillary Vistas (Neuhold et al, 2017) denoted as Vistas and Microsoft COCO <ref type="bibr" target="#b37">(Lin et al, 2014</ref>) abbreviated as COCO. We report the performance of all the aforementioned variants of our EfficientPS model. Note that we do not use the Cityscapes coarse annotations, depth data or exploit temporal  We report the benchmarking results on the Cityscapes test set in <ref type="table">Table 2</ref>, where the results were obtained directly from the leaderboard. Note that the official Cityscapes benchmark only reports the PQ, PQ St , PQ Th , SQ and RQ metrics, and ranks the methods primarily based on the standard PQ metric. Our proposed EfficientPS without pre-training on any extra data achieves a PQ of 64.1% which is an improvement of 1.8% over the previous state-of-the-art Panoptic-Deeplab trained only using Cityscapes fine annotations and an improvement of 1.5% in PQ over the Seamless model that also uses extra data. More importantly, our proposed EfficientPS model pre-trained on Mapillary Vistas, sets the new stateof-art on the Cityscapes panoptic benchmark achieving a PQ score of 66.4%. This accounts for an improvement of 0.9% in PQ compared to the previous state-of-the-art Panoptic Deeplab pre-trained on Mapillary Vistas. Moreover, our EfficientPS model ranks second in the semantic segmentation task with a mIoU of 84.2% as well as second in the instance segmentation task with an AP of 39.1%, among all the published methods in the Cityscapes benchmark.</p><p>We compare the efficiency of our proposed EfficientPS architecture against state-of-the-art models in terms of the number of parameters and FLOPs that it consumes as well as the runtime on the Cityscapes dataset. Operations that involve addition and multiplication at their core are only considered while computing FLOPs. We compute the end-toend runtime of inference for our architecture as well as for the state-of-the-art methods whose runtime is not reported in their respective paper. We use a single Nvidia Titan RTX GPU and an Intel Xenon@2.20GHz CPU. We average over 1000 runs on the same image with single scale test. In the case of parallel components in the architecture, maximum runtime among all the components contribute to the total runtime. <ref type="table">Table 3</ref> shows the comparison with the top two topdown and bottom-up panoptic segmentation architectures. Our proposed EfficientPS has a runtime of 166ms for an input image resolution of 1024 × 2048 pixels which makes it faster than the competing methods. We also observe that our EfficientPS architecture consumes the least amount of parameters and FLOPs, thereby making it the most efficient state-of-the-art panoptic segmentation model.</p><p>In <ref type="table" target="#tab_7">Table 4</ref>, we report results on the Mapillary Vistas validation set. The Mapillary Vistas dataset presents a substantial challenge as it contains images from varying seasons, weather conditions and time of day as well as the presence of 65 semantic object classes. Our proposed EfficientPS model exceeds the state-of-the-art for both single-scale and multiscale evaluation. For single-scale evaluation, it achieves an improvement of 0.6% in PQ over the top-down approach Seamless and the bottom-up approach Panoptic-DeepLab. While for multi-scale evaluation, it achieves an improvement of 0.4% in PQ and 3.6% in AP over the previous state-ofthe-art Panoptic-DeepLab. Note that we do not use model ensembles. Our network falls short of the bottom-up approach Panoptic-Deeplab in PQ St score primarily due to the output stride of 16 at which it operates which increases the computational complexity, whereas our EfficientPS uses an output stride of 32, hence is more efficient. On the one hand, bottom-up approaches tend to have a better semantic segmentation ability which is evident from the high PQ St of Panoptic-Deeplab. While on the other hand, top-down approaches tend to have better instance segmentation ability as they can handle large-scale variations in object instances. It would be interesting to investigate architectures that can combine the strengths of the two in future.</p><p>We present results on the KITTI validation set in <ref type="table" target="#tab_8">Table 5</ref>. Our proposed EfficientPS outperforms the previous state-of- the-art Seamless by 1.6% in PQ, 1.2% in AP and 1.5% mIoU for single scale evaluation and 1.5% in PQ, 1.3% in AP and 1.3% in mIoU for multi-scale evaluation. This dataset consists of cluttered and occluded objects that often have object masks split into two or more parts. In these cases context aggregation plays a major role. Hence, the improvement that we observe can be attributed to three factors: the multi-scale feature aggregation in our 2-way FPN due to the bidirectional flow of information, the long-range context being captured by our semantic head, and the adaptive fusion in our panoptic fusion module that effectively leverages the predictions from the individual heads. Finally, we also report results on the Indian Driving Dataset (IDD) largely due to the fact that it contains images of unstructured urban environments and scenes that do not have clear delineated road infrastructure which makes it extremely challenging. <ref type="table">Table 6</ref> presents results on the IDD validation set. Our proposed EfficientPS substantially exceeds the stateof-the-art by achieving a PQ score of 50.1% and 51.1% for single-scale and multi-scale evaluation respectively. This amounts to an improvement of 2.6% in PQ over Seamless and 4% in PQ over UPSNet for multi-scale evaluation. The unstructured scenes in this dataset challenges the ability of models to detect object boundaries of 'stuff' classes such as road and sidewalk. Our EfficientPS achieves a PQ St score of 49.8% for single-scale evaluation which is an improvement of 2.7% over Seamless and this can be attributed to the effectiveness of our proposed semantic head in capturing object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>In this section, we present extensive ablation studies on the various architectural components that we propose in our Ef-ficientPS architecture in comparison to their counterparts employed in state-of-the-art models. Primarily, we study the impact of our proposed network backbone, semantic head and panoptic fusion module on the overall panoptic segmentation performance of our network. We begin with a detailed analysis of various components of our EfficientPS architecture, followed by comparisons of different encoder network topologies and FPN architectures for the network backbone. We then study the impact of different parameter configurations in our proposed semantic head and its comparison with existing semantic head topologies. Finally, we assess the performance of our proposed panoptic fusion module by comparing with different panoptic fusion methods proposed <ref type="table">Table 7</ref> Ablation study on various architectural contributions proposed in our EfficientPS model. The performance is shown for the models trained on Cityscapes fine annotations and evaluated on the validation set. SIH, SH, and PFM denotes depthwise separable Instance Head, Semantic Head, and Panoptic Fusion Module respectively. '-' refers to the standard configuration as <ref type="bibr" target="#b28">Kirillov et al (2019a)</ref>, whereas ' ' refers to our proposed configuration. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively. in the literature. For all the ablative experiments, we train our models on the Cityscapes fine annotations and evaluate it on the validation set. We use the PQ metric as the primary evaluation criteria for all the experiments presented in this section. Nevertheless, we also report the other metrics defined in the beginning of Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Detailed Study on the EfficientPS Architecture</head><p>We first study the improvement due to the various components that we propose in our EfficientPS architecture. Results from this experiment are shown in <ref type="table">Table 7</ref>. The basic model M1 employs the network configuration and panoptic fusion heuristics as <ref type="bibr" target="#b29">Kirillov et al (2019b)</ref>. It uses the ResNet-50 with FPN as the backbone and incorporates Mask R-CNN for the instance head. It employs group norm <ref type="bibr" target="#b68">(Wu and He, 2018)</ref> for the normalization layer. The semantic head of this network is comprised of an upsampling stage which has a 3 × 3 convolution, group norm <ref type="bibr" target="#b68">(Wu and He, 2018)</ref>, ReLU, and ×2 bilinear upsampling. At each FPN level, this upsampling stage is repeated until the feature maps are 1/4 scale of the input. These resulting feature maps are then summed element-wise and passed through a 1 × 1 convolution, followed by ×4 bilinear upsampling, and softmax to yield the semantic segmentation output. This model M1 achieves a PQ of 57.8%, AP of 31.1% and an mIoU score of 74.1%. For the M2 and M3 model, we use BN sync and IABN sync as the normalization layer. Additionally in M3 ReLU is replaced with leakyReLU activation layer. We observe that M3 and M2 obtains a gain of 0.4% and 0.3% over M1 respectively, implying that with a higher batch size of 16 it is better to employ BN sync or iABN sync than group norm as the normalization layer. As M3 has a slight improvement over M2 we build subsequent models based on M3. The next model M4 that incorporates our proposed panoptic fusion module achieves an improvement of 0.6% in PQ, 2.2% in AP and 0.8% in the mIoU score without increasing the number of parameters. This increase in performance demonstrates that the adaptive fusion of semantic and instance head outputs is effective in resolving the inherent overlap conflict. In the M5 model, we replace all the standard convolutions in the instance head with depthwise separable convolutions which reduces the number of parameters of the model by 2.09 M with a drop of 0.2% in PQ, 0.1% drop in AP and mIoU score. However, from the aspect of having an efficient model, a reduction of 5% of the model parameters for a drop of 0.2% in PQ can be considered as a reasonable trade-off. Therefore, we employ depthwise separable convolutions in the instance head of our proposed EfficientPS architecture.</p><p>In the M6 model, we replace the ResNet-50 encoder with our modified EfficientNet-B5 encoder that does not have any squeeze-and-excitation connections, and we replace all the normalization layers and ReLU activations with iABN sync and leaky ReLU. This model achieves a PQ of 59.7% which is an improvement of 1.1% in PQ over the M3 model and a larger improvement is also observed in the mIoU score. The improvement in performance can be attributed to the richer representational capacity of the EfficientNet-B5 architecture. Subsequently in the M7 model, we replace the standard FPN with our proposed 2-way FPN which additionally improves the performance by 1.8% in PQ and 2.7% in AP. The addition of the parallel bottom-up branch in our 2-way FPN enables bidirectional flow of information, thus breaking away from the limitation of the standard FPN.</p><p>Finally, we incorporate our proposed semantic head into the M8 model that fuses and aligns multi-scale features effectively which enables it to achieve a PQ of 63.9%. Although our semantic head contributes to this improvement of 2.4% in the PQ score, it cannot not be solely attributed to the semantic head. This is due to the fact that if we employ standard panoptic fusion heuristics, an improvement in semantic segmentation would only contribute to an increase in PQ st score. However, our proposed adaptive panoptic fusion yields an improvement in PQ th as well, which is evident from the overall improvement in the PQ score. We denote this M8 model configuration as EfficientPS in this work. In the following sections, we further analyze the individual architectural components of the M6 model in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Comparison of Encoder Topologies</head><p>There are numerous network architectures that have been proposed for addressing the task of image classification. Typically, these networks serve as the encoder or feature extractor for more complex tasks such as panoptic segmentation. In this section, we evaluate the performance of our proposed modified EfficientNet-B5 in comparison to five widely employed encoder architectures. For a fair comparison, we keep all the other components of our EfficientPS network the same and only replace encoder. More specifically, we compare with MobileNetV3 <ref type="bibr" target="#b23">(Howard et al, 2019)</ref>, ResNet-50 , ResNet-101 , Xception-71 <ref type="bibr" target="#b10">(Chollet, 2017)</ref>, ResNeXt-101 <ref type="bibr" target="#b69">(Xie et al, 2017)</ref>, and EfficientNet-B5 <ref type="bibr" target="#b58">(Tan and Le, 2019)</ref>. Results from this experiment are presented in <ref type="table" target="#tab_10">Table 8</ref>. We observe that our modified EfficientNet-B5 architecture yields the highest PQ score, closely followed by the ResNeXt-101 architecture. However, ResNext-101 has an additional 56.74 M parameters which is more than twice the number of parameters consumed by our modified EfficientNet-B5 architecture. Similarly, ResNeXt-101 in FLOPs is 385.87 B more. We can see that the other encoder models, especially MobileNetV3, ResNet-50 and Xception-71 have a comparable or fewer parameters and FLOPs than our modified EfficientNet-B5. However they also yield a substantially lower PQ score. Therefore, we employ our modified EfficientNet-B5 as the encoder backbone in our proposed EfficientPS architecture. The computation of FLOPs presented in <ref type="table" target="#tab_10">Table 8</ref> architectures is only for the encoder part of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Evaluation of the 2-way FPN</head><p>In this section, we compare the performance of our novel 2-way FPN with other existing FPN variants. For a fair comparison, we keep all the other components of our EfficientPS network the same and only replace the 2-way FPN in the backbone. We compare with the top-down FPN <ref type="bibr" target="#b38">(Lin et al, 2017)</ref>, bottom-up FPN and PANet FPN variants. We refer to the FPN architecture described in <ref type="bibr" target="#b41">Liu et al (2018)</ref>   mitigate this problem by adding another bottom-up path to the standard FPN in a sequential or parallel manner respectively. We observe that the model with our proposed 2-way FPN demonstrates an improvement of 0.5% in PQ over the model with the PANet FPN. This implies that the parallel information pathways are more likely to capture better multiscale features to predict stuff regions at varying resolutions as well as are able to encode sufficiently rich semantics to precisely predict class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Detailed Study on the Semantic Head</head><p>We construct the topology of our proposed semantic head considering two critical factors. First, since large-scale outputs comprise of characteristic features and small-scale outputs consist of contextual features, they both should be captured distinctly by the semantic head. Second, while fusing small and large-scale outputs, the contextual features need to be aligned to obtain semantically reinforced fine features. In order to demonstrate that these two critical factors are essential, we perform ablative experiments on various configurations of our semantic head incorporated into the M8 model described in Section 4.4.4. Results from this experiment are presented in <ref type="table" target="#tab_12">Table 10</ref>. The output at each level of the 2-way FPN, P 32 , P 16 , P 8 and P 4 are the inputs to our semantic head. In the first M81 model configuration, we employ two cascaded 3 × 3 convolutions, iABN sync and leaky ReLU activation sequentially at each level of the 2-way FPN. The aforementioned series of layers constitute the LSFE module which is followed by a bilinear upsampling layer at each level of the 2-way FPN to yield an output which is 1/4 scale of the input image. These upsampled features are then concatenated and passed through a 1 × 1 convolution and bilinear upsamplig to yield an output which is the same scale as the input image. This M61 model achieves a PQ of 61.6%. In the subsequent M82 model configuration, we replace all the standard 3×3 convolutions with 3 × 3 depthwise separable convolutions in the LSFE module to reduce the number of parameters. This also yields a minor improvement in performance compared to the M81 model, therefore we employ depthwise separable convolutions in all the experiments that follow.</p><p>In the M83 model, we replace the LSFE module in the P 32 level of the 2-way FPN with dense prediction cells (DPC) described in Section 3.2. This M83 model achieves an improvement of 0.6% in PQ and 0.7% in the mIoU score. This can be attributed to the ability of DPC to effectively capture long-range context. In the M84 model, we replace the LSFE module in the P 16 level with DPC and in the subsequent M85 model, we introduce DPC at both P 16 and P 8 levels. We find that the M84 model achieves an improvement of 0.6% in PQ over M63, however the performance drops in the M85 model by 0.5% in PQ when we add the DPC module at the P 8 level. This can be attributed to the fact that DPC consisting of dilated convolutions do not capture characteristic features effectively at this large-scale. The final M86 model is derived from the M84 model to which we add our mismatch correction (MC) module along with the feature correlation connections as described in Section 3.2. This model achieves the highest PQ score of 63.9% which is an improvement of 1.0% compared to the M84 model. This can be attributed to the MC module that correlates the semantically rich contextual features with fine features and subsequently merges them along the feature correlation connection to obtain semantically reinforced features that results in better object boundary refinement.</p><p>Additionally, we present experimental comparisons of our proposed semantic head against those that are used in other state-of-the-art panoptic segmentation architectures. Specifically, we compare against the semantic head proposed by <ref type="bibr" target="#b28">Kirillov et al (2019a)</ref> which we denote as the baseline, UPSNet <ref type="bibr" target="#b70">(Xiong et al, 2019)</ref> and Seamless <ref type="bibr" target="#b47">(Porzi et al, 2019)</ref>. For a fair comparison, we keep all the other components of the EfficientPS architecture the same across different ex- periments while only replacing the semantic head. <ref type="table" target="#tab_13">Table 11</ref> presents the results of this experiment. The semantic head of UPSNet which is essentially a subnetwork comprising of sequential deformable convolution layers ) achieves a PQ score of 62.0% which is an improvement of 0.5% over the baseline model. The semantic head of the Seamless model employs their MiniDL module at each level of the 2-way FPN that further improves the PQ by 0.9% over semantic head of UPSNet. The semantic heads of all these models use the same module at each level of the 2-way FPN output which are of different scales. In contrast, our proposed semantic head that employs a combination of LSFE and DPC modules at different levels of the 2-way FPN achieves the highest PQ score of 63.9% and consistently outperforms the other semantic head topologies in all the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Evaluation of Panoptic Fusion Module</head><p>In this section, we evaluate our proposed Fusion Equation (12) to fuse ML A and ML B to its simple addition and multiplication counterpart. Here, ML A and ML B are the same entity as defined in Section 3.4. At a glance, addition and multiplication operations might seem like a logical choice for fusing the logits to attain adaptive attenuation or amplification according to the consensus. But they are in fact sub-optimal choices with respect to Equation (12). <ref type="table">Table 12</ref> shows the results from this experiment. We observe our proposed fusion strategy achieves the highest performance of 63.9% in PQ. It is 0.5% higher than addition and 1.6% higher than multiplication. In the case of multiplication, the resulting thing logits attain high values in comparison to stuff logits when concatenated together to form intermediate panoptic logits. This leads to over-representation of thing classes, as a result, PQ Th suffers a lot due to an increase in false positives. PQ Th of 56.9% for multiplication is the lowest out of all the strategies.</p><p>Similarly, in the case of addition, the different range values of ML A and ML B results in biased fused logits. Generally, semantic logits have higher values out of the two and hence the fused logits are biased towards ML B . This again doesn't allow optimal adaptive attenuation or amplification. PQ Th for this strategy is 59.3% which is 2.4% higher than multiplication. Clearly, addition is a better strategy than multiplication but is not the best. In contrast to the above strategies, our proposed strategy addresses the aforementioned shortcomings by normalizing the sum of the two logits (ML A +ML B ) based on the sum of their individual confidence ((σ (ML A ) + σ (ML B )) where σ (·) is the sigmoid function. This enables the proposed fusion module to be adaptive, achieving a gain of 1.4% in PQ Th while remaining relatively equal in stuff.</p><p>Next, we evaluate the performance of our proposed panoptic fusion module in comparison to other existing panoptic fusion mechanisms. First, we compare with the panoptic fusion heuristics introduced by Kirillov et al (2019b) which we consider as a baseline as it is extensively used in several panoptic segmentation networks. We then compare with Mask-Guided fusion <ref type="bibr" target="#b32">(Li et al, 2018a)</ref> and the panoptic fusion heuristics proposed in <ref type="bibr" target="#b70">(Xiong et al, 2019)</ref> which we refer to as TASCNet and UPSNet in the results respectively. Once again for a fair comparison, we keep all the other network components the same across different experiments and only change the panoptic fusion mechanism. <ref type="table" target="#tab_14">Table 13</ref> presents results from this experiment. Combining the outputs of the semantic head and instance head that have an inherent overlap is one of the critical challenges faced by panoptic segmentation networks. The baseline approach directly chooses the output of the instance head, i.e, if there is an overlap between predictions of the 'thing' and 'stuff' classes for a given pixel, the baseline heuristic classifies the pixel as a 'thing' class and assigns it an instance ID. This baseline approach achieves the lowest performance of 62.4% in PQ demonstrating that this fusion problem is more complex than just assigning the output from one of the heads. The Mask-Guided fusion method of TASCNet seeks to address this problem by using a segmentation mask. The mask selects which pixel to consider from the instance segmentation output and which pixel to consider from the semantic segmentation output. This fusion approach achieves a PQ of 62.5% which is comparable to the baseline method. Subsequently, the model that employs the UPSNet fusion heuristics achieves a larger improvement with a PQ score of 63.1%. This method computes the panoptic logits by adding the non-overlapping instance segmentation logits ML A to ML B that is obtained using the semantic logits as described in Section 3.4 while concatenating it to stuff logits from semantic segmenation logits. As shown, in previous experiment this is sub-optimal. However, our proposed adaptive fusion method that dynamically fuses the outputs from both the heads while refining the stuff segmentation using semantic head predictions achieves the highest PQ score of 63.9% which is an improvement of 0.8% over the UPSNet method. We also observe a consistently higher performance in all the other metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Evaluations</head><p>In this section, we qualitatively evaluate the panoptic segmentation performance of our proposed EfficientPS architecture in comparison to the state-of-the-art Seamless <ref type="bibr" target="#b47">(Porzi et al, 2019</ref>) model on each of the datasets that we benchmark on. We use the publicly available official implementation of the Seamless architecture to obtain the outputs for the qualitative comparisons. The best performing state-of-the-art model Panoptic-Deeplab does not provide any publicly available implementation or pre-trained models which makes such comparisons infeasible. <ref type="figure" target="#fig_3">Figure 6</ref> presents two examples from the validation sets of each of the urban scene understanding dataset. For each example, we show the input image, the corresponding panoptic segmentation output from the Seamless model and our proposed EfficientPS model. Additionally, we show the improvement and error map where a green pixel indicates that our EfficientPS made the right prediction but the Seamless model misclassified it (improvement of Effi-cientPS over Seamless), a blue pixel indicates that Seamless model made the right prediction but EfficientPS misclassified it, and a red pixel denotes that both models misclassified it with respect to the groundtruth. <ref type="figure" target="#fig_3">Figure 6 (a) and (b)</ref> show examples from the Cityscapes dataset in which the improvement over the Seamless model can be seen in the ability to segment heavily occluded 'thing' class instances. In the first example, the truck far behind on the bridge is occluded by cars and a cyclist, and in the second example, the distant car parked on the left side of the image is only partially visible as the car in the front occludes it. We observe from the improvement maps that our proposed EfficientPS model accurately detect, classify and segment these instances, while the Seamless model misclassifies these pixels. This can be primarily attributed to our 2-way FPN that effectively aggregates multi-scale features to learn semantically richer representations and the panoptic fusion module that addresses the instance overlap ambiguity in an adaptive manner.</p><p>In <ref type="figure" target="#fig_3">Figure 6</ref> (c) and (d), we qualitatively compare the performance on the challenging Mapillary Vistas dataset. We observe that in <ref type="figure" target="#fig_3">Figure 6</ref> (c) the group of people towards left side of the image who are behind the fence are misclassified in the output of the Seamless model and the instances of these people are not detected. Whereas, our EfficientPS model accurately segments each of the instances of the people. Similarly, the distant van on the right side of the image shown in <ref type="figure" target="#fig_3">Figure 6 (d)</ref> is partially occluded by the neighboring cars and is entirely misclassified by the Seamless model. However, our EfficientPS model accurately captures this heavily occluded object instance. In <ref type="figure" target="#fig_3">Figure 6 (c)</ref>, interestingly, the Seamless model misclassifies the cyclist on the road as a pedestrian. We hypothesize that this might be due to the fact that one of the legs of the cyclist is touching the ground and the other leg which is on the pedal of the bicycle is barely visible. Hence, this causes the Seamless model to misclassify the object instance. Whereas, our EfficientPS model effectively leverages both the semantic and instance prediction in our panoptic fusion module to accurately address this ambiguity in the scene. We also observe in <ref type="figure" target="#fig_3">Figure 6</ref> (c) that the EfficientPS model misclassifies the traffic sign fixed on the fence and only partially segments the advertisement board attached to the building near the fence while it accurately segments all the other instances of this class. This is primarily due to the fact that there is a lack of relevant examples for this type of traffic sign which is atypical of those found in the training set. <ref type="figure" target="#fig_3">Figure 6</ref> (e) and (f) show qualitative comparisons on the KITTI dataset. In <ref type="figure" target="#fig_3">Figure 6</ref> (e), we see that the Seamless model misclassifies the bus that is towards the right of the image as a truck although it segments the object coherently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Seamless Output EfficientPS Output Improvement\Error Map In addition to the panoptic segmentation output, we also show the improvement\error map which denotes the pixels that are misclassified by the Seamless model but correctly predicted by the EfficientPS model in green, the pixels that are misclassified by the EfficientPS model but correctly predicted by the Seamless model in blue, and the pixels that are misclassified by both the EfficientPS model and the Seamless model in red. This is primarily due to the fact that there are poles as well as an advertisement board in front of the bus which divides the it into different subregions. This leads the model to predict it as a truck that has a transition between the tractor unit and the trailer. However, our proposed EfficientPS model mitigates this problem with its bidirectional aggregation of multi-scale features that effectively captures contextual information. In <ref type="figure" target="#fig_3">Figure 6</ref> (f), we observe that a distant truck on the right lane is partially occluded by cars behind it which causes the Seamless model to not detect the truck as a new instance, rather it detects the truck and the car behind it as being the same object. This is similar to the scenario observed on the Cityscapes dataset in <ref type="figure" target="#fig_3">Figure 6 (a)</ref>. Nevertheless, our proposed EfficientPS model yields accurate predictions in such challenging scenarios consistently across different datasets. In <ref type="figure" target="#fig_3">Figure 6</ref> (g) and (h), we present examples from the IDD dataset. We can see that our EfficientPS model captures the boundaries of 'stuff' classes more precisely than the Seamless model in both the examples. For instance, the pillar of the bridge in <ref type="figure" target="#fig_3">Figure 6</ref> (g) and the extent of the sidewalk in <ref type="figure" target="#fig_3">Figure 6</ref> (h) are more well defined in the panoptic segmentation output of our EfficientPS model. This can be attributed to the object boundary refinement ability of our semantic head that correlates features of different scales before fusing them. In <ref type="figure" target="#fig_3">Figure 6</ref> (h), the Seamless model misclassifies the auto-rickshaw as a caravan due to the similar visual appearances of these two objects, however our proposed EfficientPS model with our novel panoptic backbone has an extensive representational capacity which enables it to accurately classify objects even with such subtle differences. We observe that although the upper half of the cyclist towards the left of the image is accurately segmented, the front leg of the cyclist is misclassifies as being part of the bicycle. This is a challenging scenario due to the high contrast in this region. We also observe that the boundary of the sidewalk towards the left of the auto rickshaw is misclassified. However, on visual inspection of the groundtruth, it appears that the sidewalk boundary in this region is mislabeled in groundtruth mask, while the model is making a reasonable prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualizations</head><p>We present visualizations of panoptic segmentation results from our proposed EfficientPS architecture on Cityscapes, Mapillary Vistas, KITTI, and Indian Driving Dataset (IDD) in <ref type="figure" target="#fig_4">Figure 7</ref>. The figures show the panoptic segmentation output of our EfficientPS model using single scale evaluation, which is overlaid on the input image. <ref type="figure" target="#fig_4">Figure 7 (a)</ref> and <ref type="bibr">(b)</ref> show examples from the Cityscapes dataset which exhibit complex road scenes consisting of a large number of traffic participants. These examples show challenging scenarios with dynamic as well as static pedestrian groups in close proximity to each other and distant parked cars that are barely visible due to their neighbouring 'thing' class instances. Our proposed EfficientPS architecture effectively addresses these challenges and yields reliable panoptic segmentation results. In <ref type="figure" target="#fig_4">Figure 7</ref> (c) and (d), we present results on the Mapillary Vistas dataset that show drastic viewpoint variations and scenes in different times of day. <ref type="figure" target="#fig_4">Figure 7</ref> (c.iv), (d.i) and (d.iv) show scenes that were captured from uncommon viewpoints from those observed in the training data and <ref type="figure" target="#fig_4">Figure 7 (d.</ref>iii) shows a scene that was captured during nighttime. Nevertheless, our EfficientPS model demonstrates substantial robustness against these perceptual variations.</p><p>In <ref type="figure" target="#fig_4">Figure 7</ref> (e) and (f), we present results on the KITTI dataset which show residential and highway road scenes consisting of several parked and dynamic cars, as well as a large amount of thin structures such as poles. We observe that our EfficientPS model generalizes effectively to these complex scenes even when the network was only trained on the relatively small dataset. <ref type="figure" target="#fig_4">Figure 7</ref> (g) and <ref type="formula">(h)</ref> show examples from the IDD dataset that highlight challenges of an unstructured environment. One such challenge is the accurate segmentation of sidewalks, as the transition between the road and the sidewalk is not well delineated often caused by a layer of sand over asphalt. The examples also show heavy traffic with numerous types of vehicles, motorcycles and pedestrians scattered all over the scene. However, our proposed EfficientPS model shows exceptional robustness in these immensely challenging scenes thereby demonstrating its suitability for autonomous driving applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we presented our EfficientPS architecture for panoptic segmentation that achieves state-of-the-art performance while being computationally efficient. It incorporates our proposed panoptic backbone with a variant of Mask R-CNN augmented with depthwise separable convolutions as the instance head, a new semantic head that captures fine and contextual features efficiently, and our novel adaptive panoptic fusion module. We demonstrated that our panoptic backbone consisting of the modified EfficientNet encoder and our 2-way FPN achieves the right trade-off between performance and computational complexity. Our 2-way FPN achieves effective aggregation of semantically rich multiscale features due to its bidirectional flow of information. Thus in combination with our encoder, it establishes a new strong panoptic backbone. We proposed a new semantic head that employs scale-specific feature aggregation to capture long-range context and characteristic features effectively, followed by correlating them to achieve better object boundary refinement capability. We also introduced our parameter-free panoptic fusion module that dynamically fuses logits from both heads based on their mask confidences and congruously integrates instance-specific 'thing' classes with 'stuff' classes to yield the panoptic segmentation output. Additionally, we introduced the KITTI panoptic segmentation dataset that contains panoptic groundtruth annotations for images from the challenging KITTI benchmark. We hope that our panoptic annotations complement the suite of other perception tasks in KITTI and encourage the research community to develop novel multi-task learning methods that include panoptic segmentation. We presented exhaustive benchmarking results on Cityscapes, Mapillary Vistas, KITTI and IDD datasets that demonstrate that our proposed Effi-cientPS sets the new state-of-the-art in panoptic segmentation while being faster and more parameter efficient than existing state-of-the-art architectures. In addition to being ranked first on the Cityscapes panoptic segmentation leaderboard, our model is ranked second on both the Cityscapes semantic segmentation and instance segmentation leaderboards. We also presented detailed ablation studies, qualitative analysis and visualizations that highlight the improvements that we make to various core modules of panoptic segmentation architectures. To the best of our knowledge, this work is the first to benchmark on all the four standard urban scene understanding datasets that support panoptic segmentation and exceed the state-of-the-art on each of them while simultaneously being the most efficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>Topologies of various architectural components in our proposed semantic head and instance head of our EfficientPS architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4</head><label>4</label><figDesc>Illustration of our proposed Panoptic Fusion Module. Here, ML A and ML B mask logits are fused as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 5 Example images from the challenging urban scene understanding datasets that we benchmark on, namely, Cityscapes, KITTI, Mapillary Vistas, and Indian Driving Dataset (IDD). The images show cluttered urban scenes with many dynamic objects, occluded objects, perpetual snowy conditions and unstructured environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6</head><label>6</label><figDesc>Qualitative panoptic segmentation results of our proposed EfficientPS network in comparison to the state-of-the-art Seamless architecture (Porzi et al, 2019) on different benchmark datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7</head><label>7</label><figDesc>Visual panoptic segmentation results of our proposed EfficientPS model on each of the challenging urban scene understanding datasets that we benchmark on which in total encompasses scenes from over 50 countries. These examples show complex urban scenarios with numerous object instances in multiple scales and with partial occlusion. These scenes also show diverse lighting conditions from dawn to dusk as well as seasonal changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>use randomized decision forests on local patches for classification, whereas Plath et al (2009) fuse local and global features along with Conditional Random Fields(CRFs) for segmentation. As opposed to leveraging appearance-based features, Brostow et al (2008) use cues from motion with random forests.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The training schedule for Cityscapes, Mapillary Vistas, KITTI and IDD are {0.07, {32K, 44K}, 50K}, {0.07, {144K, 176K}, 192K}, {0.07, {16K, 22K}, 25K} and {0.07 ,{108K, 130K}, 144K}</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 Table 2 Table 3</head><label>123</label><figDesc>Performance comparison of panoptic segmentation on the Cityscapes validation set. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively. − denotes that the metric has not been reported for the corresponding method. Comparison of panoptic segmentation benchmarking results on the Cityscapes test set. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively. Comparison of model efficiency with both state-of-the-art topdown and bottom-up panoptic segmentation architectures.</figDesc><table><row><cell cols="2">Mode Network</cell><cell></cell><cell>Pre-training</cell><cell>PQ</cell><cell>SQ</cell><cell>RQ</cell><cell cols="5">PQ Th SQ Th RQ Th PQ St SQ St RQ St AP</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">(%) (%) (%) (%) (%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell></cell><cell cols="2">WeaklySupervised</cell><cell></cell><cell cols="2">47.3 −</cell><cell>−</cell><cell cols="2">39.6 −</cell><cell>−</cell><cell cols="2">52.9 −</cell><cell>−</cell><cell>24.3 71.6</cell></row><row><cell></cell><cell>TASCNet</cell><cell></cell><cell></cell><cell cols="2">55.9 −</cell><cell>−</cell><cell cols="2">50.5 −</cell><cell>−</cell><cell cols="2">59.8 −</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell></cell><cell cols="2">Panoptic FPN</cell><cell></cell><cell cols="2">58.1 −</cell><cell>−</cell><cell cols="2">52.0 −</cell><cell>−</cell><cell cols="2">62.5 −</cell><cell>−</cell><cell>33.0 75.7</cell></row><row><cell></cell><cell>AUNet</cell><cell></cell><cell></cell><cell cols="2">59.0 −</cell><cell>−</cell><cell cols="2">54.8 −</cell><cell>−</cell><cell cols="2">62.1 −</cell><cell>−</cell><cell>34.4 75.6</cell></row><row><cell></cell><cell>UPSNet</cell><cell></cell><cell></cell><cell cols="8">59.3 79.7 73.0 54.6 79.3 68.7 62.7 80.1 76.2 33.3 75.2</cell></row><row><cell>Single-Scale</cell><cell cols="2">DeeperLab Seamless SSAP AdaptIS Panoptic-DeepLab</cell><cell></cell><cell cols="2">56.3 − 60.3 − 61.1 − 62.0 − 63.0 −</cell><cell>− − − − −</cell><cell cols="2">− 56.1 − − 55.0 − 58.7 − − −</cell><cell>− − − − −</cell><cell cols="2">− 63.3 − − − − 64.4 − − −</cell><cell>− − − − −</cell><cell>− 33.6 77.5 − − − 36.3 79.2 35.3 80.5</cell></row><row><cell></cell><cell cols="2">EfficientPS (ours)</cell><cell></cell><cell cols="8">63.9 81.5 77.1 60.7 81.2 74.1 66.2 81.8 79.2 38.3 79.3</cell></row><row><cell></cell><cell>TASCNet</cell><cell></cell><cell>COCO</cell><cell cols="2">59.3 −</cell><cell>−</cell><cell>56</cell><cell>−</cell><cell>−</cell><cell cols="2">61.5 −</cell><cell>−</cell><cell>37.6 78.1</cell></row><row><cell></cell><cell>UPSNet</cell><cell></cell><cell>COCO</cell><cell cols="5">60.5 80.9 73.5 57.0 −</cell><cell>−</cell><cell cols="2">63.0 −</cell><cell>−</cell><cell>37.8 77.8</cell></row><row><cell></cell><cell>Seamless</cell><cell></cell><cell>Vistas</cell><cell cols="2">65.0 −</cell><cell>−</cell><cell cols="2">60.7 −</cell><cell>−</cell><cell cols="2">68.0 −</cell><cell>−</cell><cell>−</cell><cell>80.7</cell></row><row><cell></cell><cell cols="2">Panoptic-Deeplab</cell><cell>Vistas</cell><cell cols="2">65.3 −</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>38.8 82.5</cell></row><row><cell></cell><cell cols="2">EfficientPS (ours)</cell><cell>Vistas</cell><cell cols="8">66.1 82.5 78.9 62.7 81.9 75.2 68.5 82.9 81.6 41.9 81.0</cell></row><row><cell></cell><cell cols="2">Panoptic-DeepLab</cell><cell></cell><cell cols="2">64.1 −</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>38.5 81.5</cell></row><row><cell>Multi-Scale</cell><cell cols="2">EfficientPS (ours) TASCNet M-RCNN + PSPNet UPSNet</cell><cell>COCO COCO COCO</cell><cell cols="8">65.1 82.2 79.0 61.5 81.4 75.4 67.7 82.8 81.7 39.7 80.3 60.4 − − 56.1 − − 63.3 − − 39.1 78.7 61.2 80.9 74.4 54.0 − − 66.4 − − 36.4 80.9 61.8 81.3 74.8 57.6 77.7 70.5 64.8 81.4 39.0 79.2</cell></row><row><cell></cell><cell cols="2">Panoptic-Deeplab</cell><cell>Vistas</cell><cell cols="2">67.0 −</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>42.5 83.1</cell></row><row><cell></cell><cell cols="2">EfficientPS (ours)</cell><cell>Vistas</cell><cell cols="8">67.5 83.2 80.2 63.5 82.2 77.2 70.4 83.9 82.4 43.8 82.1</cell></row><row><cell></cell><cell></cell><cell>Network</cell><cell></cell><cell cols="2">Pre-training</cell><cell></cell><cell>PQ</cell><cell>SQ</cell><cell>RQ</cell><cell cols="2">PQ Th</cell><cell>PQ St</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell></cell><cell></cell><cell>SSAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>58.9</cell><cell>82.4</cell><cell>70.6</cell><cell>48.4</cell><cell>66.5</cell></row><row><cell></cell><cell></cell><cell>TASCNet</cell><cell></cell><cell>COCO</cell><cell></cell><cell></cell><cell>60.7</cell><cell>81.0</cell><cell>73.8</cell><cell>53.4</cell><cell>66.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Panoptic-Deeplab</cell><cell></cell><cell></cell><cell></cell><cell>62.3</cell><cell>82.4</cell><cell>74.8</cell><cell>52.1</cell><cell>69.7</cell></row><row><cell></cell><cell></cell><cell>Seamless</cell><cell></cell><cell>Vistas</cell><cell></cell><cell></cell><cell>62.6</cell><cell>82.1</cell><cell>75.3</cell><cell>56.0</cell><cell>67.5</cell></row><row><cell></cell><cell></cell><cell cols="2">Panoptic-Deeplab</cell><cell>Vistas</cell><cell></cell><cell></cell><cell>66.5</cell><cell>83.5</cell><cell>78.8</cell><cell>58.8</cell><cell>72.0</cell></row><row><cell></cell><cell></cell><cell cols="2">EfficientPS (ours)</cell><cell></cell><cell></cell><cell></cell><cell>64.1</cell><cell>82.6</cell><cell>76.8</cell><cell>56.7</cell><cell>69.4</cell></row><row><cell></cell><cell></cell><cell cols="2">EfficientPS (ours)</cell><cell>Vistas</cell><cell></cell><cell></cell><cell>67.1</cell><cell>83.4</cell><cell>79.6</cell><cell>60.9</cell><cell>71.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">the previous best proposal based approach AdaptIS by 1.9%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">in PQ and 2.0% in AP, while outperforming the best bottom-</cell></row><row><cell>Network</cell><cell></cell><cell>Input Size</cell><cell cols="3">Params. FLOPs Time</cell><cell cols="6">up approach Panoptic-Deeplab by 0.9% in PQ and 3.0% in</cell></row><row><cell></cell><cell></cell><cell>(pixels)</cell><cell>(M)</cell><cell>(B)</cell><cell>(ms)</cell><cell cols="6">AP. Furthermore, our EfficientPS model trained only on the</cell></row><row><cell>DeeperLab UPSNet Seamless</cell><cell></cell><cell>1025 × 2049 1024 × 2048 1024 × 2048</cell><cell>− 45.05 51.43</cell><cell cols="2">− 487.02 202 463 514.00 168</cell><cell cols="6">Cityscapes fine annotations and with multi-scale evaluation achieves an improvement of 1.0% in PQ and 1.2% in AP over Panoptic-Deeplab. We observe a similar trend while</cell></row><row><cell cols="2">Panoptic-Deeplab</cell><cell>1025 × 2049</cell><cell>46.73</cell><cell cols="2">547.49 175</cell><cell cols="6">comparing with models that have been pre-trained with addi-</cell></row><row><cell cols="2">EfficientPS (ours)</cell><cell>1024 × 2048</cell><cell>40.89</cell><cell cols="2">433.94 166</cell><cell cols="6">tional data, where our proposed EfficientPS outperforms the</cell></row><row><cell cols="6">data. Our EfficientPS model trained only on the Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">fine annotations and with single-scale evaluation outperforms</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>former state-of-the-art Panoptic-Deeplab in both single-scale evaluation and multi-scale evaluation. EfficientPS pre-trained on Mapillary Vistas and with single-scale evaluation outper- forms Panoptic-Deeplab in the same configuration by 0.8%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>Performance comparison of panoptic segmentation on the Mapillary Vistas validation set. Note that no additional data was used for training EfficientPS on this dataset other than pre-training the encoder on ImageNet. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively. − denotes that the metric has not been reported for the corresponding method.</figDesc><table><row><cell cols="2">Mode Network</cell><cell>PQ</cell><cell>SQ</cell><cell>RQ</cell><cell cols="4">PQ Th SQ Th RQ Th PQ St SQ St RQ St AP</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell cols="7">(%) (%) (%) (%) (%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell></cell><cell>JSIS-Net</cell><cell cols="7">17.6 55.9 23.5 10.0 47.6 14.1 27.5 66.9 35.8 −</cell><cell>−</cell></row><row><cell>Single-Scale</cell><cell>DeeperLab TASCNet AdaptIS Seamless Panoptic-DeepLab</cell><cell cols="2">32.0 − 32.6 − 35.9 − 37.7 − 37.7 −</cell><cell>− − − − −</cell><cell>− 31.1 − − 31.5 − 33.8 − 30.4 −</cell><cell>− − − − −</cell><cell>− 34.4 − − − 41.9 − − − 42.9 − − 47.4 − −</cell><cell>− 18.5 − 55.3 − − 16.4 50.4 14.9 55.3</cell></row><row><cell></cell><cell>EfficientPS (ours)</cell><cell cols="7">38.3 74.2 48.0 33.9 73.3 43.0 44.2 75.4 54.7 18.7 52.6</cell></row><row><cell>Multi-Scale</cell><cell>TASCNet Panoptic-DeepLab EfficientPS (ours)</cell><cell cols="7">34.3 − 40.3 − 40.5 74.9 49.5 35.0 73.8 44.4 47.7 76.2 56.4 20.8 54.1 − 34.8 − − 33.6 − − 20.4 − − 33.5 − − 49.3 − − 17.2 56.8</cell></row><row><cell cols="5">in PQ and 3.1% in AP, while for multi-scale evaluation it</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">exceeds the performance of Panoptic-Deeplab by 0.5% in</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PQ and 1.3% in AP.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>Performance comparison of panoptic segmentation on the KITTI validation set. Note that no additional data was used for training EfficientPS on this dataset other than pre-training the encoder on ImageNet. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively. Performance comparison of panoptic segmentation on the Indian Driving Dataset (IDD) validation set. Note that no additional data was used for training EfficientPS on this dataset other than pre-training the encoder on ImageNet. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively.</figDesc><table><row><cell cols="2">Mode Network</cell><cell>PQ</cell><cell>SQ</cell><cell>RQ</cell><cell>PQ Th SQ Th RQ Th PQ St SQ St RQ St AP</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell cols="5">(%) (%) (%) (%) (%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell>Single-Scale</cell><cell>Panoptic FPN UPSNet Seamless EfficientPS (ours)</cell><cell cols="5">38.6 70.4 51.2 26.1 68.3 40.1 47.6 71.9 59.2 24.4 52.1 39.1 70.7 51.7 26.6 68.5 40.6 48.3 72.4 59.8 24.7 52.6 41.3 71.7 52.3 28.5 69.2 42.3 50.6 73.6 59.6 25.9 53.8 42.9 72.7 53.6 30.4 69.8 43.7 52.0 74.9 60.9 27.1 55.3</cell></row><row><cell>Multi-Scale</cell><cell>Panoptic FPN UPSNet Seamless EfficientPS (ours)</cell><cell cols="5">39.3 70.8 51.6 26.9 68.7 40.4 48.3 72.4 59.8 24.8 52.8 39.9 71.2 52.0 27.2 68.8 40.8 49.1 72.9 60.2 25.2 53.2 42.2 72.3 52.9 29.1 69.7 42.9 51.8 74.2 60.1 26.6 55.1 43.7 73.2 54.1 30.9 70.2 44.0 53.1 75.4 61.5 27.9 56.4</cell></row><row><cell cols="2">Table 6 Mode Network</cell><cell>PQ</cell><cell>SQ</cell><cell>RQ</cell><cell>PQ Th SQ Th RQ Th PQ St SQ St RQ St AP</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell cols="5">(%) (%) (%) (%) (%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell>Single-Scale</cell><cell>Panoptic FPN UPSNet Seamless EfficientPS (ours)</cell><cell cols="5">45.9 75.9 60.8 46.1 77.8 60.9 45.8 74.9 60.7 27.8 68.1 46.6 76.5 60.9 47.6 78.9 61.1 46.0 75.3 60.8 28.2 68.4 47.7 77.2 61.2 48.9 79.5 61.5 47.1 76.1 61.1 30.1 69.6 50.1 78.4 62.0 50.7 80.6 61.6 49.8 77.1 62.2 31.6 71.3</cell></row><row><cell>Multi-Scale</cell><cell>Panoptic FPN UPSNet Seamless EfficientPS (ours)</cell><cell cols="5">46.7 77.0 61.0 47.3 78.9 61.1 46.4 76.1 61.0 28.9 70.1 47.1 77.9 60.9 47.6 79.8 61.2 46.8 76.9 60.8 29.2 70.6 48.5 78.2 61.9 49.5 80.4 62.2 47.9 77.1 61.7 31.4 71.3 51.1 78.8 63.5 52.6 81.2 65.4 50.3 77.5 62.5 32.9 72.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc>Performance comparison of various encoder topologies employed in the M8 model. Results are shown for the models trained on the Cityscapes fine annotations and evaluated on the validation set. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively. Performance comparison of various FPN architectures employed in the M8 model. Results are shown for the models trained on the Cityscapes fine annotations and evaluated on the validation set. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively.</figDesc><table><row><cell>Encoder</cell><cell cols="2">Params</cell><cell>FLOPs</cell><cell>PQ</cell><cell>SQ</cell><cell>RQ</cell><cell cols="2">PQ Th SQ Th RQ Th PQ St SQ St RQ St AP</cell><cell>mIoU</cell></row><row><cell></cell><cell>(M)</cell><cell></cell><cell>(B)</cell><cell cols="5">(%) (%) (%) (%) (%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell>MobileNetV3</cell><cell>5.40</cell><cell></cell><cell>9.44</cell><cell cols="5">55.8 78.1 70.2 50.4 77.4 67.1 59.8 78.6 72.4 29.1 72.2</cell></row><row><cell>ResNet-50</cell><cell>25.60</cell><cell></cell><cell>172.19</cell><cell cols="5">60.3 80.1 72.6 55.3 79.9 68.9 63.9 80.3 75.3 34.9 76.1</cell></row><row><cell>ResNet-101</cell><cell>44.50</cell><cell></cell><cell>327.99</cell><cell cols="5">61.1 80.3 75.1 56.5 80.1 71.9 64.2 80.5 77.4 35.9 77.2</cell></row><row><cell>Xception-71</cell><cell>27.50</cell><cell></cell><cell>210.38</cell><cell cols="5">62.1 81.1 75.4 58.5 80.9 72.3 64.7 81.2 77.7 36.2 78.1</cell></row><row><cell>ResNeXt-101</cell><cell>86.74</cell><cell></cell><cell>636.84</cell><cell cols="5">63.2 81.2 76.0 59.6 80.4 72.9 65.8 81.7 78.3 36.9 78.9</cell></row><row><cell>Mod. EfficientNet-B5 (Ours)</cell><cell>30.00</cell><cell></cell><cell>250.97</cell><cell cols="5">63.9 81.5 77.1 60.7 81.2 74.1 66.2 81.8 79.2 38.3 79.3</cell></row><row><cell>Table 9 Architecture</cell><cell></cell><cell>PQ</cell><cell>SQ</cell><cell>RQ</cell><cell cols="3">PQ Th SQ Th RQ Th PQ St SQ St RQ St AP</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell cols="7">(%) (%) (%) (%) (%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell>Bottom-Up FPN</cell><cell></cell><cell cols="7">60.4 80.6 73.7 56.3 80.4 69.9 63.4 80.8 76.4 35.2 75.3</cell></row><row><cell>Top-Down FPN</cell><cell></cell><cell cols="7">62.2 80.9 75.7 58.1 80.1 72.4 65.1 81.4 78.0 36.5 78.2</cell></row><row><cell>PANet FPN</cell><cell></cell><cell cols="7">63.1 81.1 75.5 59.4 80.3 72.3 65.8 81.6 77.8 37.1 78.8</cell></row><row><cell cols="2">2-way FPN (Ours)</cell><cell cols="7">63.9 81.5 77.1 60.7 81.2 74.1 66.2 81.8 79.2 38.3 79.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>level features which describe entire objects, whereas the bottom-up FPN model propagates low-level information such as local textures and patterns. The EfficientPS model with the bottom-up FPN achieves a PQ of 60.4%, while the model with the top-down FPN achieves a PQ of 62.2%. Both these models achieve a performance which is 3.2% and 1.4% lower in PQ than our 2-way FPN respectively. A similar trend can also be observed in the other metrics. The lower PQ score of the individual bottom-up FPN and top-down FPN models substantiate the limitation of the unidirectional flow of information in the standard FPN topology. Both the PANet FPN and our proposed 2-way FPN aim to</figDesc><table><row><cell>as PANet</cell></row><row><cell>FPN in which the top-down path is followed by a bottom-up</cell></row><row><cell>path. For each of the FPN variants we use iABN sync and</cell></row><row><cell>leaky ReLU layers instead of BN and Relu layers. The results</cell></row><row><cell>from comparing with various FPN architectures are shown</cell></row><row><cell>in Table 9.</cell></row><row><cell>The top-down FPN model predominantly propagates se-</cell></row><row><cell>mantically high-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10</head><label>10</label><figDesc>Ablation study on our semantic head topology incorporated into the M6 model. Results are shown for the models trained on the Cityscapes fine annotations and evaluated on the validation set. P os is the output of our 2-way FPN at the os pyramid scale level, c k f refers to a convolution layer with f number of filters and k × k kernel size, LSFE refers to Large Scale Feature Extractor and DPC refers to Dense Prediction Cells. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively.</figDesc><table><row><cell>Model P 32</cell><cell>P 16</cell><cell>P 8</cell><cell>P 4</cell><cell>Feature</cell><cell>PQ SQ RQ PQ Th SQ Th RQ Th PQ St SQ St RQ St AP mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Correlation (%) (%) (%) (%) (%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell cols="5">M81 [c 3 128 c 3 128 ] [c 3 128 c 3 128 ] [c 3 128 c 3 128 ] [c 3 128 c 3 128 ] -</cell><cell>61.6 80.6 75.7 57.3 80.4 72.6 64.7 80.7 77.9 36.7 77.2</cell></row><row><cell>M82 LSFE</cell><cell>LSFE</cell><cell>LSFE</cell><cell>LSFE</cell><cell>-</cell><cell>61.7 80.5 75.4 57.9 80.5 72.6 64.5 80.6 77.4 36.6 77.4</cell></row><row><cell>M83 DPC</cell><cell>LSFE</cell><cell>LSFE</cell><cell>LSFE</cell><cell>-</cell><cell>62.3 80.9 75.9 57.9 80.4 72.0 65.6 81.3 78.8 36.8 78.1</cell></row><row><cell>M84 DPC</cell><cell>DPC</cell><cell>LSFE</cell><cell>LSFE</cell><cell>-</cell><cell>62.9 81.0 75.7 59.0 80.5 71.4 65.8 81.4 78.7 37.0 78.6</cell></row><row><cell>M85 DPC</cell><cell>DPC</cell><cell>DPC</cell><cell>LSFE</cell><cell>-</cell><cell>62.4 80.8 75.4 58.8 80.3 72.4 65.1 81.1 77.6 36.7 78.2</cell></row><row><cell>M86 DPC</cell><cell>DPC</cell><cell>LSFE</cell><cell>LSFE</cell><cell></cell><cell>63.9 81.5 77.1 60.7 81.2 74.1 66.2 81.8 79.2 38.3 79.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11</head><label>11</label><figDesc>Performance comparison of various existing semantic head topologies employed in the M8 model. Results are reported for the model trained on the Cityscapes fine annotations and evaluated on the validation set. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively. ML A ) + σ (ML B )) (ML A + ML B ), with Multiply: (ML A ML B ) and Add: (ML A + ML B ) , employed in the M8 model where σ (·) is the sigmoid function and is the Hadamard product. Results are reported for the model trained on the Cityscapes fine annotations and evaluated on the validation set. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively.</figDesc><table><row><cell></cell><cell>Semantic Head</cell><cell>PQ</cell><cell>SQ</cell><cell>RQ</cell><cell>PQ Th SQ Th RQ Th PQ St SQ St RQ St AP</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell cols="5">(%) (%) (%) (%) (%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="5">61.5 80.7 75.6 57.2 80.6 72.5 64.6 80.9 77.9 36.8 77.3</cell></row><row><cell></cell><cell>UPSNet</cell><cell cols="5">62.0 81.0 74.7 58.5 80.5 70.9 64.5 81.3 77.5 35.9 76.1</cell></row><row><cell></cell><cell>Seamless</cell><cell cols="5">62.9 81.1 75.5 58.9 80.4 71.3 65.6 81.6 78.5 36.8 78.5</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">63.9 81.5 77.1 60.7 81.2 74.1 66.2 81.8 79.2 38.3 79.3</cell></row><row><cell cols="6">Table 12 Performance comparison of our proposed adaptive fusion</cell></row><row><cell>(σ (Model</cell><cell cols="5">PQ SQ RQ PQ Th SQ Th RQ Th PQ St SQ St RQ St</cell></row><row><cell></cell><cell cols="5">(%) (%) (%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell>Multiply</cell><cell cols="5">62.3 80.7 76.0 56.9 79.1 71.9 66.3 81.9 79.0</cell></row><row><cell>Add</cell><cell cols="5">63.4 81.4 76.9 59.3 80.4 73.5 66.4 82.0 79.3</cell></row><row><cell>Ours</cell><cell cols="5">63.9 81.5 77.1 60.7 81.2 74.1 66.2 81.8 79.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13</head><label>13</label><figDesc>Performance comparison of our proposed panoptic fusion module with various other panoptic fusion mechanisms employed in the M8 model. Results are reported for the model trained on the Cityscapes fine annotations and evaluated on the validation set. Superscripts St and Th refer to 'stuff' and 'thing' classes respectively.</figDesc><table><row><cell>Model</cell><cell>PQ SQ RQ PQ Th SQ Th RQ Th PQ St SQ St RQ St</cell></row><row><cell></cell><cell>(%) (%) (%) (%) (%) (%) (%) (%) (%)</cell></row><row><cell>Baseline</cell><cell>62.4 80.8 75.4 58.7 80.4 72.6 65.1 81.1 77.4</cell></row><row><cell>TASCNet</cell><cell>62.5 80.9 75.6 58.6 80.5 72.8 65.3 81.2 77.7</cell></row><row><cell>UPSNet</cell><cell>63.1 81.3 76.1 59.5 80.6 73.2 65.7 81.8 78.2</cell></row><row><cell>Ours</cell><cell>63.9 81.5 77.1 60.7 81.2 74.1 66.2 81.8 79.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partly funded by the European Union's Horizon 2020 research and innovation program under grant agreement No 871449-OpenDR and a Google Cloud research grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5221" to="5229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Bremner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
	<note>Theories of infant development</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Loss max-pooling for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7082" to="7091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>arXiv:170605587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8713" to="8724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>arXiv:180202611</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12475" to="12485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ssap: Single-shot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Panoptic segmentation with a joint semantic and instance segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
		<idno>arXiv:180902110</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research de Geus D</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Vision meets robotics: The kitti dataset</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An exemplar-based crf for multi-instance object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition He X</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition He X</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
	<note>Proceedings of the Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>JMLR.org, ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno>arXiv:170603059</idno>
		<title level="m">Depthwise separable convolutions for neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bshapenet: Object detection and instance segmentation with bounding shape masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno>arXiv:181010327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9404" to="9413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structured classlabels in random forests for semantic image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2190" to="2197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno>arXiv:181201192</idno>
		<title level="m">Learning to fuse things and stuff</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<idno>arXiv:190907229</idno>
		<title level="m">Global aggregation then local distribution in fully convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional instanceaware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2359" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentionguided unified network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7026" to="7035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An endto-end network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6172" to="6181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3496" to="3504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>arXiv: 150604579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
	<note>Proceedings of the International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1990" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-class image segmentation using conditional random fields and global classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Plath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Seamless scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8277" to="8286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multimodal interaction-aware motion prediction for autonomous street crossing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno>arXiv:180806887</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">End-to-end instance segmentation with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6656" to="6664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="312" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Vision-based offline-online perception paradigm for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bakhtiary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Semantic texton forests for image categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition Silberman N</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition Silberman N<address><addrLine>Sontag D, Fergus R</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="616" to="631" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv:14091556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adaptis: Adaptive instance selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7355" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Combining appearance and structure from motion features for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ph ;</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Bs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference Sun M</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1370" to="1383" />
		</imprint>
	</monogr>
	<note>Relating things and stuff via objectproperty interactions</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>arXiv:190511946</idno>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3126" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Finding things: Image parsing with regions and per-exemplar detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3001" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Scene parsing with object instances and occlusion ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3748" to="3755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="140" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Convoluted mixture of deep experts for robust semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W ;</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International conference on intelligent robots and systems (IROS) workshop, state estimation and terrain perception for all terrain mobile robots Valada A</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Robotics: Science and systems (RSS 2016) workshop, are the sceptics right? Limits and potentials of deep learning in robotics</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Adapnet: Adaptive semantic segmentation in adverse environmental conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4644" to="4651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Incorporating semantic and geometric priors in deep pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Radwan</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Learning and Inference in Robotics: Integrating Structure, Priors and Models at Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Idd: A dataset for exploring problems of autonomous navigation in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<idno>1007/s11263-019-01188-y</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1743" to="1751" />
		</imprint>
	</monogr>
	<note>special Issue: Deep Learning for Robotic Vision</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8818" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multimodal information fusion for urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Davoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="349" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<idno>arXiv:190205093</idno>
		<title level="m">Deeperlab: Single-shot image parser</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>arXiv:151107122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Semantic segmentation of urban scenes using dense depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="708" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Self-supervised visual terrain classification from unsupervised acoustic feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zürn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<idno>arXiv:191203227</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
