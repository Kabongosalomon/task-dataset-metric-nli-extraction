<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Practical Text Classification With Large Pre-Trained Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Kant</surname></persName>
							<email>kantneel@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
							<email>raulp@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Yakovenko</surname></persName>
							<email>nyakovenko@nvidia.com</email>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
							<email>bcatanzaro@nvidia.com</email>
							<affiliation key="aff3">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Practical Text Classification With Large Pre-Trained Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-emotion sentiment classification is a natural language processing (NLP) problem with valuable use cases on realworld data. We demonstrate that large-scale unsupervised language modeling combined with finetuning offers a practical solution to this task on difficult datasets, including those with label class imbalance and domain-specific context. By training an attention-based Transformer network (Vaswani et al. 2017) on 40GB of text (Amazon reviews) (McAuley et al. 2015) and fine-tuning on the training set, our model achieves a 0.69 F1 score on the SemEval Task 1:E-c multidimensional emotion classification problem (Mohammad et al. 2018), based on the Plutchik wheel of emotions <ref type="bibr" target="#b15">(Plutchik 1979)</ref>. These results are competitive with state of the art models, including strong F1 scores on difficult (emotion) categories such as Fear (0.73), Disgust (0.77) and Anger (0.78), as well as competitive results on rare categories such as Anticipation (0.42) and Surprise (0.37). Furthermore, we demonstrate our application on a real world text classification task. We create a narrowly collected text dataset of real tweets on several topics, and show that our finetuned model outperforms general purpose commercially available APIs for sentiment and multidimensional emotion classification on this dataset by a significant margin. We also perform a variety of additional studies, investigating properties of deep learning architectures, datasets and algorithms for achieving practical multidimensional sentiment classification. Overall, we find that unsupervised language modeling and finetuning is a simple framework for achieving high quality results on realworld sentiment classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recent work has shown that language models -both RNN variants like the multiplicative LSTM (mLSTM) <ref type="bibr" target="#b8">(Krause et al. 2016)</ref>, as well as the attention-based Transformer network <ref type="bibr" target="#b21">(Vaswani et al. 2017</ref>) -can be trained efficiently over very large datasets, and that the resulting models can be transferred to downstream language understanding problems, often matching or exceeding the previous state of the art approaches on academic datasets. However, how well do these models perform on practical text classification problems, with real world data?</p><p>Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>In this work, we train both mLSTM and Transformer language models on a large 40GB text dataset <ref type="bibr" target="#b9">(McAuley et al. 2015)</ref>, then transfer those models to two text classification problems: binary sentiment (including Neutral labels), and multidimensional emotion classification based on the Plutchik wheel of emotions <ref type="bibr" target="#b15">(Plutchik 1979)</ref>. We examine our performance on these tasks, both against large academic datasets, and on an original text dataset that we compiled from social media messages about several specific topics, such as video games.</p><p>We demonstrate that our approach matches the state of the art on the academic datasets without domain-specific training and without excessive hyper-parameter tuning. Meanwhile on the social media dataset, our approach outperforms commercially available APIs by significant margins, even when those models are re-calibrated to the test set.</p><p>Furthermore, we notice that 1) the Transformer model generally out-performs the mLSTM model, especially when fine-tuning on multidimensional emotion classification, and 2) fine-tuning the model significantly improves performance on the emotion tasks, both for the mLSTM and the Transformer model. We suggest that our approach creates models with good generalization to increasingly difficult text classification problems, and we offer ablation studies to demonstrate that effect.</p><p>It is difficult to fit a single model for text classification across domains, due to unknown words, specialized context, colloquial language, and other differences between domains. For example, words such as war and sick are not necessarily negative in the context of video games, which are significantly represented in our dataset. By training a language model across a large text dataset, we expose our model to many contexts. Perhaps a small amount of downstream transfer is enough to choose the right context features for emotion classification in the appropriate setting.</p><p>Our work shows that unsupervised language modeling combined with finetuning offers a practical solution to specialized text classification problems, including those with large category class imbalance, and significant human label disagreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Supervised learning is difficult to apply to NLP problems because labels are expensive. Following <ref type="bibr" target="#b18">(Radford, Józefowicz, and Sutskever 2017)</ref>, <ref type="bibr" target="#b17">(Radford et al. 2018)</ref> and <ref type="bibr" target="#b2">(Dai and Le 2015)</ref>, we train unsupervised text models on large amounts of unlabelled text data, and transfer the model features to small supervised text problems. The supervised text classification problem used for transfer is binary sentiment on the Stanford Sentiment Treebank (SST) <ref type="bibr" target="#b20">(Socher et al. 2013)</ref>.</p><p>Some of these binary text examples are subtle. Prior works show that unsupervised language models can learn nuanced features of text, such as word ordering and double negation, just from the underlying task of next-word prediction. However, while this includes difficult examples, it does not necessarily represent sentiment on practical text problems.</p><p>• The source material (professionally written movie reviews) does not include colloquial language. • The dataset excludes Neutral sentiment texts and those with weak directional sentiment. • The dataset does not include dimensions of sentiment apart from Positive and Negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Plutchik's Wheel of Emotions</head><p>We focus our multidimension emotion classification on Plutchik's wheel of emotions <ref type="bibr" target="#b15">(Plutchik 1979)</ref>. This taxonomy, in use since 1979, aims to classify human emotions as a combination of four dualities: Joy -Sadness, Anger -Fear, Trust -Disgust, and Surprise -Anticipation. According to the basic emotion model <ref type="bibr" target="#b4">(Ekman 2013)</ref>, while humans experience hundreds of emotions, some emotions are more fundamental than others. The commercial general purpose emotion classification API that we compare against, IBM's Watson 1 , offers classification scores for the Joy, Sadness, Fear, Disgust and Anger emotions -all present in Plutchik's wheel ( <ref type="figure" target="#fig_0">Fig. 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SemEval Multidimension Emotion Dataset</head><p>The Se-mEval Task 1:E-c problem <ref type="bibr" target="#b13">(Mohammad et al. 2018</ref>) offers a training set of 6,857 tweets, with binary labels for the eight Plutchik categories, plus Optimism, Pessimism, and Love. This dataset was created through a process of text selection and human labeling. We show our results on this dataset and compare it to the current state of the art performance.</p><p>While it is not possible to report rater agreement on these categories for the compilation of the dataset, the authors note that 2 out of 7 raters had to agree for a positive label to be applied, as requiring larger agreement caused a scarcity of  <ref type="bibr" target="#b15">(Plutchik 1979)</ref>. labels for some categories. This indicates that some of the categories had significant rater disagreement between the human raters. The dataset also included a substantial degree of label class imbalance, with some categories like Anger (37%), Disgust (38%), Joy (36%) and Sadness (29%) represented often in the dataset, while others like Trust (5%) and Surprise (5%) present much less frequently <ref type="figure">(Fig.2</ref>). This class imbalance and human rater disagreement is not uncommon for real world text classification problems 2 .</p><p>Company Tweet Dataset In addition to the SemEval tweet dataset, we wanted to see how our model would perform on a similar but domain-specific task: Plutchik emotion classification on tweets relevant to a particular company. We collected tweets on a variety of topics, including:</p><p>• Video game tweets • Tweets about the company stock</p><p>We submitted the first batch of 4,000 tweets to human 2 We submitted the SemEval training set for re-labeling using our rater instructions. See <ref type="figure">Fig.3</ref> for an estimate of rater disagreement over the SemEval training set.  . We applied positive labels for each category where 2 out of 5 raters agreed. This is slightly less permissive than the 2 out of 7 raters used by SemEval, because we did not have a budget for 7 raters per tweet. After the first pass, we noticed that random sampling led to some categories being severely under-sampled, below 5% of tweets. Thus we employed a bootstrapping technique to pre-classify tweets by category using our current model, and choose tweets with more likely emotion tweets for classification. See Active Learning section for details. We also sampled 5,000 tweets balanced by source category, since video game tweets have much more emotion, thus dominated the bootstrapped selections.</p><p>Henceforth, we refer to the combined company tweets dataset consisting of: • 4,021 random tweets • 5,024 tweets selected for higher emotion content • 4,281 tweets selected for source category balance Finetuning Recent work has shown promising results using unsupervised language modeling, followed by transfer learning to natural language tasks (Radford, Józefowicz, and Sutskever 2017), <ref type="bibr" target="#b17">(Radford et al. 2018</ref>). Furthermore, these models benefit when the entire model is fine-tuned on the transfer task, as demonstrated in <ref type="bibr" target="#b6">(Howard and Ruder 2018)</ref>. Specifically, these methods have beaten the state of the art on binary sentiment classification. These models have also attained the best overall score on the GLUE Benchmark 4 <ref type="bibr" target="#b22">(Wang et al. 2018)</ref>, comprised of a variety of text understanding tasks, including entailment and question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>We use a larger batch size with shorter sequence length, specifically a global batch of 512 and sequence length 64 tokens (tokenized with a 32,000 BPE vocabulary, as detailed in Characters and Subword Units. The shorter sequence length works well because the transfer target are tweets, which are short pieces of text. We trained our language model on the Amazon Reviews dataset <ref type="bibr" target="#b9">(McAuley et al. 2015)</ref> rather than other large datasets like BooksCorpus <ref type="bibr" target="#b23">(Zhu et al. 2015)</ref>, because reviews are rich in emotional context.</p><p>We also train an mLSTM network on the same dataset, based on the model from <ref type="bibr" target="#b16">(Puri et al. 2018)</ref>.</p><p>We chose to compare these particular models because they work in fundamentally different ways and because they collectively hold state of the art results on many significant academic NLP benchmarks. We wanted to test these models on difficult classification problems with real-world data.</p><p>Unsupervised Pretraining. The language modeling objective can be summarized as a maximum likelihood estimation problem for a sequence of tokens. We treat our model as a function with two parts: an encoder f e and decoder f d . The encoder forms the bulk of the model, including the token embedding dictionary as the first module. The decoder is simply a softmax linear layer that projects the encoder output into the dimension equal to the vocabulary size. The objective to maximize is as follows.</p><formula xml:id="formula_0">− log p(x 0 , . . . , x n ) = − n t=1 log p(x t |x t−1 , . . . , x 0 ) p(x t |x t−1 , . . . , x 0 ) = f d (h l t )</formula><p>where h l t is a hidden layer activation in the final layer of f e , indexed 1 . . . l for timestep t.</p><p>The model is tasked with predicting the next token given all of the ones prior by outputting a probability distribution over the vocabulary of tokens. Doing this for each timestep t produces each term in the sum of the log-likelihood formulation, and so maximizing the correct probabilities is a way to understand the joint probability distribution of sequences in this corpus of text.</p><p>Characters and Subword Units. While <ref type="bibr" target="#b18">(Radford, Józefowicz, and Sutskever 2017)</ref>, <ref type="bibr" target="#b5">(Gray, Radford, and Kingma 2017)</ref> and <ref type="bibr" target="#b16">(Puri et al. 2018</ref>) have shown state of the art results for language modeling and task transfer with character-level mLSTM models, we found that our  <ref type="bibr" target="#b17">(Radford et al. 2018</ref>) uses a bytepair encoding vocabulary with 40,000 word pieces for their state of the art results on language transfer tasks with a Transformer model. Our work closely follows their model. Supervised Finetuning. After the pretraining, we initialize a new decoder f † d to be exclusively trained on the supervised problem. Depending on the task, this decoder may be a single linear layer with activation or an MLP. We also retain the original decoder f d and continue to train it by using language modeling as an auxiliary loss when finetuning on the new corpus. Error signals from both decoders are backpropagated into the language model. The differences between the hyperparameters for finetuning and language modeling are described in <ref type="table" target="#tab_3">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELMo Baseline</head><p>We also compare our language models to ELMo <ref type="bibr" target="#b14">(Peters et al. 2018</ref>), a contextualized word representation based on a deep bidirectional language model, trained on large text corpus. We use a publicly available pretrained Multihead vs. Single Head Finetuning Decoders The tweet datasets are an example of a multilabel classification problem. We can formulate the problem for the finetuning decoder, f † d as either a collection of single binary problems or multiple problems put together.</p><p>The single binary problem formulation allows for a focus on one class and end-to-end optimization will only have one error signal. However, because the label classes are imbalanced in all categories, this may lead to a sparse gradient signal for the positive label, which may impact recall and precision. Increasing the size of f † d to more than one linear layer leads to rapid overfitting and lower validation performance.</p><p>The combined binary problems formulation (henceforth described as multihead) allows for a richer error signal that propagates more information through the encoder f e and sentiment representation in f † d . In this setup, constructing a Multilayer network is far more useful, and can be thought of as specifically creating sentiment features to be used at the final layer to predict the presence of the individual emotions. We find that the inclusion of easier, more balanced label categories improves performance on harder ones in <ref type="table" target="#tab_7">Table  7</ref>. However, the easier categories have slightly lower performance because the network is not being optimized for only those categories.</p><p>Thresholding Supervised Results For both the multihead MLP and the single linear layer instantiating of f † d , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as t * = 0.5. This makes sense since the label classes for most categories are very imbalanced. For thresholding, we take a dataset of tweets and split it into training (70%), thresholding (10%) and validation (20%) sets. At each epoch of finetuning on the training set, we calculate validation accuracy and save predictions on the threshold set on the epoch for which this is maximized.</p><p>To threshold, we search the discretized version of [0, 1]: the linear space T = { i 200 : 1 ≤ i ≤ 200} for the positive label threshold for each category. We denoted the threshold which gave the best score on the threshold set as t * .</p><p>IBM Watson and Google NLP 6 both offer commercial APIs for binary sentiment analysis, producing scalar values that correspond to a continuous [-1,+1] sentiment score. We applied our thresholding procedure to these scores. In the case of classification with neutrals we create two thresholds 0 &lt; t * 1 &lt; t * 2 &lt; 1 which we individually optimized jointly over T as well. With the finetuning procedure, we found success with a decoder f † d = MLP(64, 2), whose two output unitsŷ p ,ŷ n are probability estimates of the positive and negative labels y p , y n . These units both have sigmoid activations, since we denote a neutral as y p = y n = 0. To threshold these predictions, we searched the cartesian product T × T to determine 0 &lt; t * p , t * n &lt; 1.</p><p>Active Learning We hypothesized that we could achieve greater precision and recall on our datasets if our class label were more equally balanced. To this end, we employed an active learning procedure to select unlabeled tweets to be labeled. The algorithm consisted of first finetuning a language model f = (f e , f d , f † d ) on labeled tweets for 5 epochs. At peak validation accuracy, we obtain predictions P ∈ R 8×nu , for Plutchik sentiment on the unlabeled tweets.</p><p>From the labeled dataset, we calculate the negative class percentage for each category v ∈ R 8 . Then we obtain category a weighting parameter w = 10 × (v − 0.5) so that w i ∈ [−5, 5] for i ∈ 1 . . . 8. Then, we get scores for each unlabeled point as weighted features: s = e w P ∈ R nu . This way, positive predictions for sentiment categories are weighted by how much they would contribute towards balancing all of the class distributions. The scores s are used as weights in a weighted uniform random sampler, and from this, we sampled 5,000 tweets to be labeled.</p><p>We found that overall, the method produced tweets with more emotion. Not only was the positive class balance averaged across label categories higher (11.2% compared to 8.2% for random sampling), but the percentage of tweets which had no emotion was dramatically lower: 35.6% compared to 52.1% for random sampling <ref type="table" target="#tab_1">(Table 2)</ref>. We hence achieved better class balance than the dataset prior to the augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary Sentiment Tweets</head><p>For binary sentiment, we compare our model on two tasks: the academic SST dataset, which consists of a balanced set of Positive and Negative labels, and the company tweets dataset, which consists of a balance between Positive, Neutral and Negative labels. See <ref type="table" target="#tab_5">Table 5</ref>.</p><p>While the Transformer gets close but does not exceed the state of the art on the SST dataset, it exceeds both the mL-STM and ELMo baseline as well as both Watson and Google Sentiment APIs on the company tweets. This is despite optimally calibrating the API results on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Label Emotion Tweets</head><p>The IBM Watson API offers multi-label emotion predictions for five categories: Anger, Disgust, Fear, Joy and Sadness. We compare our models to Watson on these categories for both the SemEval dataset and the company tweets in <ref type="table" target="#tab_7">Table 7</ref>. We find that our models outperform Watson on every emotion category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SemEval Tweets</head><p>We submitted our finetuned Transformer model to the SemEval Task1:E-C challenge, as seen in Table 6. These results were computed by the organizers on a golden test set, for which we do not have access to the truth labels. Our model achieved the top macro-averaged F1 score among all submission, with competitive but lower scores for the micro-average F1 an the Jaccard Index accuracy 8 . This suggests that our model out-performs the other top submission on rare and difficult categories, since macroaverage weighs performance on all classes equally, and the most common categories of Joy, Anger, Disgust and Optimism get relatively higher F1 scores across all models. <ref type="bibr">8</ref> SemEval 2018 results can be seen at http://alt.qcri.org/semeval2018/. Our entry is #1 in the postevaluation period for Task1:E-C, as of October 2018.  We also compare the deep learning architectures of the Transformer and mLSTM on this dataset in <ref type="table" target="#tab_7">Table 7</ref> and find that the Transformer outperforms the mLSTM across Plutchik categories.</p><p>The winner of the Task1:E-c challenge <ref type="bibr" target="#b1">(Baziotis et al. 2018</ref>) trained a bidirectional LSTM with an 800,000 word embedding vocabulary derived from training word vectors <ref type="bibr" target="#b12">(Mikolov et al. 2013</ref>) on a dataset of 550 million tweets. Similarly, the second place winner of the SemEval leaderboard trained a word-level bidirectional LSTM with attention, as well as including non-deep learning features into their ensemble <ref type="bibr" target="#b10">(Meisheri and Dey 2018)</ref>. Both submissions used training data across SemEval tasks, as well as additional training data outside of the training set.</p><p>In comparison, we demonstrate that finetuning can be as effective on this task, despite training only on 7,000 tweets. Furthermore, out language modeling took place on the Amazon Reviews dataset, which does not contain emoji, hashtags or usernames. We would expect to see improvements if our unsupervised dataset contained emoji, for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Plutchik on Company Tweets</head><p>Our models gets lower F1 scores on the company tweets dataset than on equivalent Se-mEval categories. As with the SemEval challenge tweets, the Transformer outperformed the mLSTM. These results are shown in <ref type="table" target="#tab_7">Table 7</ref>. Both models performed significantly better than the Watson API on all categories for which Watson supplies predictions.</p><p>We could not conclusively determine whether the singlehead or the multihead Transformer will perform better on a given task. Thus we recommend trying both methods on a new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>Classification Performance by Dataset Size We would have liked to label more data for the company tweets dataset, and thus looked into how much extra labeling contributes to finetuned model performance accuracy.</p><p>First, let us explain the difference between micro and macro averaging of the F1 scores. We can summarize the F1 scores of categories c ∈ C (or any other metric M ) through macro and micro averaging to obtain M . The macro method weights each class equally by averaging the metric calculated on each individual class. The micro method accounts for the class imbalances in each category by aggregating all of the true/false positives/negatives first, and then calculating an overall metric.</p><formula xml:id="formula_1">M macro = 1 |C| c ∈ C M (T P c , T N c , F P c , F N c ) M micro = M (T P , T N , F P , F N ) T P = c ∈ C T P c , T N = c ∈ C T N c . . .</formula><p>In one experiment, we decreased the size of the training dataset and observed the resulting macro and micro averaged F1 scores across all categories on company tweets. The results are shown in <ref type="figure">Fig. 2a</ref>. We observe that the macro average is more sensitive to dataset size and falls more quickly than the micro average. The interpretation of this is that categories with worse class imbalance (which consequently influence macro more than micro average) benefit more from having a larger training dataset size. This suggests that we may obtain substantially better results with more data in the harder categories.</p><p>We conducted a related experiment that focused on the difference in category performance when using a single head versus a multihead decoder f † d . We apply the two architectures at different training dataset sizes for three different label categories: Anger, Anticipation and Trust, which we categorize as low, medium and high difficulty, respectively. As seen in <ref type="figure">Fig. 2b</ref> it appears that the difference between the single and multihead becomes more pronounced for more difficult categories, as well as for smaller dataset sizes.</p><p>We do not have enough data to make a firm conclusion, but this study suggests that we could get more out of the labeled data that we have, by studying which categories benefit from single head and multihead decoders. All categories benefit from more training data, but some categories benefit from from marginal labeled data than others. This suggests further and more rigorous study of the boostrapping methods we used to select tweets for our human labeling budget, as described in the Active Learning section. Following a similar process, we required 2 out of 5 raters for a positive label, and in the case of binary sentiment labels (Positive, Neutral, Negative), we rounded toward polarized sentiment and away from Neutral labels in the case of a 2/3 split. Applying the SemEval-trained Transformer directly to our company tweets dataset gets reasonably good results (0.338 macro average), also validating that our labeling technique is similar to that of SemEval.</p><p>Looking at rater agreement by dataset ( <ref type="figure">Fig. 3)</ref>, we see that Plutchik category labels contain large rater disagreement, even among vetted raters who passed the golden set test. Furthermore, datasets with more emotions (the SemEval dataset and our active learning sampled company tweets) contain higher Plutchik disagreement than random company tweets. This is likely because raters tend to apply the "No Emotion" label when they are not sure about a category. As <ref type="table" target="#tab_1">Table  2</ref> shows, the SemEval and active company tweets datasets contain fewer no-emotion tweets than other datsets.</p><p>It would be interesting to analyze rater disagreement by category, how much this effects classifier convergence, whether getting 7+ ratings per tweet helps classifier convergence, and also whether this work could benefit from estimating rater quality via agreement with the crowd, as proposed in <ref type="bibr" target="#b7">(Khetan, Lipton, and Anandkumar 2017)</ref>. However this analysis is not straightforward, as the truth data is itself collected through human labeling.</p><p>Alongside classifier convergence by dataset size <ref type="figure">(Fig.2b)</ref>, we think that this could be an interesting area a future research.</p><p>Difficult tweets and challenging contexts. There is not sufficient space for a thorough analysis, but we wanted to suggest why general purpose APIs may not work well on our company tweets dataset. <ref type="table" target="#tab_0">Table 1</ref> samples the largest binary sentiment disagreements between human raters and the Wat-son API. For simplicity, we restrict examples to video game tweets, which comprise 19.1% of our test set. As we can see, all of these examples appear to ascribe negative emotion to generally negative terms which, in a video game context, do not indicate negative sentiment.</p><p>Our purpose is not to castigate the Watson or the GCL APIs. Rather, we propose that it may not be possible to provide context-independent emotion classification scores that work well across text contexts.</p><p>It may work better in practice, on some tasks, to train a large unsupervised model and to use a small amount of labeled data to finetune on the context present in the specific dataset. We would like to quantify this further in future work.</p><p>Recent work <ref type="bibr" target="#b23">(Yang et al. 2017)</ref> shows that training an RNN with multiple softmax outputs leads to a much improved BPC on language modeling, especially for diverse datasets and models with large vocabularies. This is because the multiple softmaxes are able to capture a larger number of distinct contexts in the text than a single output.</p><p>Perhaps our Transformer also captures the features relevant to a large number of distinct contexts, and the finetuning is able to select the most significant of these features, while ignoring those features that -while adding value in general -are not appropriate in a video game setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult text classification tasks. We noticed that the finetuning was especially effective with the Transformer network, when transferring to downstream tasks with noisy labels and specialized context.</p><p>We think that this framework makes it easy to customize a text classification model on niche tasks. Unsupervised language modeling can be done on general text datasets, and requires no labels. Meanwhile downstream task transfer works well enough, even on small amounts of domain-specific labelled data, to be accessible to most academics and small organization.</p><p>It would be great to see this approach applied to a variety of practical text classification problems, much as <ref type="bibr">(Radford</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Plutchik's wheel of emotions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a) Comparison of macro and micro averages of F1 scores across categories on the company tweets dataset. b) F1 Scores for different categories on different dataset sizes for single head vs. multihead decoder.Dataset Quality and Human Rater Agreement The Se-mEval dataset<ref type="bibr" target="#b13">(Mohammad et al. 2018</ref>) applies a positive label for every category where 2 out of 7 vetted raters agree. The reason is for the dataset to contain difficult and subtle examples of sentiments, not just those examples where everyone agrees. The raters also have a tendency to under-label categories, especially when presented multiple options.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Difficult video game tweets.</figDesc><table><row><cell>Tweet</cell><cell>Watson Sad Binary</cell><cell>Joy</cell><cell>Fear</cell><cell cols="2">GCL Binary Binary Ours</cell></row><row><cell>Encouraging collaboration among players in Sea of Thieves &lt;url&gt;</cell><cell>-0.302 0.229</cell><cell>0.194</cell><cell>0.150</cell><cell>-0.80</cell><cell>Pos</cell></row><row><cell>got my first kill on Fortnite all by myself I'm geeked &lt;emoji&gt;perioddddd.</cell><cell>-0.847 0.003</cell><cell>0.666</cell><cell>0.225</cell><cell>+0.60</cell><cell>Neu</cell></row><row><cell>Far Cry 5 "Lost On Mars" Gameplay Walkthrough -DLC2: &lt;url&gt;via @YouTube</cell><cell>-0.909 0.047</cell><cell>0.015</cell><cell>0.873</cell><cell>+0.00</cell><cell>Neu</cell></row><row><cell>NEW SUBMACHINE GUN IS INSANE! -Fortnite Best Moments 39 (Fortnite</cell><cell>-0.936 0.821</cell><cell>0.178</cell><cell>0.056</cell><cell>-0.10</cell><cell>Pos</cell></row><row><cell>Funny Fails &amp; WTF Moments) &lt;url&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Label class balance (as percent) for SemEval and company tweet datasets.</figDesc><table><row><cell></cell><cell>Size</cell><cell cols="4">Anger Anticipation Disgust Fear Joy</cell><cell cols="4">Sad Surprise Trust Ave/None</cell></row><row><cell>SemEval</cell><cell>6,858</cell><cell>37.2</cell><cell>14.3</cell><cell>38.0</cell><cell cols="2">18.2 36.2 29.4</cell><cell>5.3</cell><cell>5.2</cell><cell>23.0/2.9</cell></row><row><cell cols="2">(Random) 4,021</cell><cell>7.8</cell><cell>14.7</cell><cell>5.2</cell><cell cols="2">1.7 21.9 3.4</cell><cell>4.3</cell><cell>6.6</cell><cell>8.2/52.1</cell></row><row><cell>(Active)</cell><cell>5,024</cell><cell>22.0</cell><cell>10.2</cell><cell>12.3</cell><cell cols="2">5.6 19.7 6.3</cell><cell>7.1</cell><cell>6.5</cell><cell>11.2/35.6</cell></row><row><cell>(All)</cell><cell>13,326</cell><cell>11.7</cell><cell>12.9</cell><cell>6.8</cell><cell cols="2">2.9 20.6 4.2</cell><cell>5.0</cell><cell>7.6</cell><cell>8.9/47.0</cell></row><row><cell cols="5">raters on the FigureEight 3 platform, with rules similar to</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">those used by SemEval, which also used the FigureEight</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">platform for human labeling. Specifically, we verified that</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">raters passed our golden set (answering 70% of test ques-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tions correctly)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Inter-rater agreement. Humans don't always agree, even on binary sentiment.</figDesc><table><row><cell>Dataset</cell><cell>Judgments</cell><cell cols="2">Binary (3 choices) (8 choices) Plutchik</cell></row><row><cell>SemEval</cell><cell>20,514</cell><cell>77.3%</cell><cell>61.1%</cell></row><row><cell>Company (random)</cell><cell>20,005</cell><cell>80.7%</cell><cell>67.3%</cell></row><row><cell>Company (active)</cell><cell>25,017</cell><cell>79.0%</cell><cell>52.3%</cell></row><row><cell>Company (balanced)</cell><cell>23,812</cell><cell>80.0%</cell><cell>71.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for language modeling and finetuning phases. We compute the BPC equivalent over word pieces, following<ref type="bibr" target="#b11">(Mikolov et al. 2012)</ref>. For the remainder of this work, our Transformer models use 32,000 word pieces 5 .Recent work (Al-Rfou et al. 2018) has shown that it is possible to train a character level Transformer that is up to 64 layers deep and which beat state of the art BPC over large text datasets. However this requires intermediate layer losses, and other auxiliary losses for optimal convergence.</figDesc><table><row><cell></cell><cell>Language Modeling</cell><cell>Finetuning</cell></row><row><cell>global</cell><cell>512 (size 64 on 8 GPUs)</cell><cell>32</cell></row><row><cell>batch size</cell><cell></cell><cell></cell></row><row><cell>sequence</cell><cell>64 -kept short because tar-</cell><cell>max(batch)</cell></row><row><cell>length</cell><cell>geting tweet application</cell><cell></cell></row><row><cell>optimizer</cell><cell cols="2">ADAM</cell></row><row><cell>lr</cell><cell>2 × 10 −4</cell><cell>1 × 10 −5</cell></row><row><cell>(schedule)</cell><cell>(cosine decay after linear</cell><cell>(constant after 1/2 epoch lin-</cell></row><row><cell></cell><cell>warmup on 2000 iterations)</cell><cell>ear warmup)</cell></row><row><cell>Decoder</cell><cell>R dh×32000</cell><cell>Binary: MLP(1024 → n c )</cell></row><row><cell>module</cell><cell></cell><cell>with PReLU and 0.3 dropout</cell></row><row><cell></cell><cell></cell><cell>Multiclass: MLP(4096 →</cell></row><row><cell></cell><cell></cell><cell>2048 → 1024 → n c ) with</cell></row><row><cell></cell><cell></cell><cell>PReLU and 0.3 dropout</cell></row><row><cell># Epochs</cell><cell>1</cell><cell>5</cell></row><row><cell>Loss</cell><cell>L LM = Softmax Cross En-</cell><cell>Sigmoid Binary Cross En-</cell></row><row><cell></cell><cell>tropy</cell><cell>tropy +0.02 · L LM</cell></row><row><cell cols="3">Transformer model benefits from modeling language</cell></row><row><cell cols="3">through subword units. Using a byte-pair-encoding (BPE)</cell></row><row><cell cols="3">(Sennrich, Haddow, and Birch 2015) of various sized we</cell></row><row><cell cols="3">notice that a 32,000 word-piece vocabulary achieves a</cell></row><row><cell cols="3">better bits per character (BPC) loss over one epoch of the</cell></row><row><cell cols="3">Amazon Reviews dataset (McAuley et al. 2015) than a small</cell></row><row><cell>vocabulary.</cell><cell></cell><cell></cell></row></table><note>By comparison,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ELMo model from the authors. During finetuning, text is embedded with ELMo before being passed into a decoder f † d . Error signals are backpropagated into the ELMo language model. Unlike our other models, we do not use an auxiliary language modeling loss during finetuning, as the ELMo language model is bidirectional.Finetuning the ELMo model substantially improves accuracy on our tasks, thus we include only finetuned ELMo results.</figDesc><table><row><cell>5 Library</cell><cell>for</cell><cell>BPE</cell><cell>available</cell><cell>in</cell><cell>open</cell><cell>source:</cell></row><row><cell cols="4">https://github.com/google/sentencepiece</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Binary sentiment accuracy. The SST dataset includes Positive and Negative labels. Other datasets include Neutral labels. Third party results (Watson and Google) thresholded on the test set.</figDesc><table><row><cell></cell><cell cols="2">SST (acc) Company</cell><cell>-/=/+</cell></row><row><cell>Transformer (finetune)</cell><cell>90.9%</cell><cell>81.2%</cell><cell>88.2/73.5/81.9</cell></row><row><cell>mLSTM (finetune)</cell><cell>90.4%</cell><cell>78.2%</cell><cell>87.0/69.3/78.3</cell></row><row><cell>8k mLSTM(Puri et al. 2018)</cell><cell>93.8%</cell><cell>77.3%</cell><cell>86.0/67.4/78.6</cell></row><row><cell>(Gray, Radford, and Kingma 2017)</cell><cell>93.1%</cell><cell>-</cell><cell>-</cell></row><row><cell>ELMo (finetuned)</cell><cell>79.9%</cell><cell>71.4%</cell><cell>81.7/60.1/72.4</cell></row><row><cell>ELMo+BiLSTM+Attn (Wang et al. 2018)</cell><cell>91.6%</cell><cell>-</cell><cell>-</cell></row><row><cell>Watson API</cell><cell>84.4%</cell><cell>56.7%</cell><cell>42.9/54.0/73.3</cell></row><row><cell>Google Sentiment (GCL) API</cell><cell>81.3%</cell><cell>62.5%</cell><cell>69.6/54.0/63.8</cell></row><row><cell>Class Balance</cell><cell>50.0/50.0</cell><cell>-</cell><cell>22.4/46.0/31.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison on SemEval Task 1:E-c challenge. Official results on the golden test set [truth labels hidden]. 7</figDesc><table><row><cell></cell><cell cols="3">Accuracy Micro F1 Macro F1 (Jaccard)</cell></row><row><cell>Transformer (ours)</cell><cell>0.577</cell><cell>0.690</cell><cell>0.561</cell></row><row><cell>(Baziotis et al. 2018)</cell><cell>0.595</cell><cell>0.709</cell><cell>0.542</cell></row><row><cell>(Meisheri and Dey 2018)</cell><cell>0.582</cell><cell>0.694</cell><cell>0.534</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Transformer vs. mLSTM on Plutchik Tweet Categories (F1 Score). MH: Multi Head, SH: Single Head Anger Anticipation Disgust Fear Joy Sadness Surprise Trust Average</figDesc><table><row><cell></cell><cell>Transformer (MH)</cell><cell>.684</cell><cell>.486</cell><cell>.441</cell><cell>.400 .634</cell><cell>.333</cell><cell>.269</cell><cell>.300</cell><cell>.443</cell></row><row><cell>Company</cell><cell>Transformer (SH)</cell><cell>.679</cell><cell>.491</cell><cell>.371</cell><cell>.400 .675</cell><cell>.286</cell><cell>.210</cell><cell>.279</cell><cell>.424</cell></row><row><cell></cell><cell>mLSTM (SH)</cell><cell>.636</cell><cell>.426</cell><cell>.319</cell><cell>.232 .609</cell><cell>.260</cell><cell>.201</cell><cell>.284</cell><cell>.371</cell></row><row><cell></cell><cell>ELMo (MH)</cell><cell>.515</cell><cell>.306</cell><cell>.325</cell><cell>.086 .489</cell><cell>.182</cell><cell>.161</cell><cell>.182</cell><cell>.281</cell></row><row><cell></cell><cell>Watson</cell><cell>.358</cell><cell>-</cell><cell>.179</cell><cell>.086 .520</cell><cell>.096</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Transformer (MH)</cell><cell>.779</cell><cell>.413</cell><cell>.769</cell><cell>.723 .850</cell><cell>.712</cell><cell>.360</cell><cell>.240</cell><cell>.606</cell></row><row><cell>Semeval</cell><cell>Transformer (SH)</cell><cell>.774</cell><cell>.425</cell><cell>.765</cell><cell>.735 .832</cell><cell>.699</cell><cell>.373</cell><cell>.247</cell><cell>.606</cell></row><row><cell></cell><cell>mLSTM (SH)</cell><cell>.668</cell><cell>.189</cell><cell>.691</cell><cell>.535 .763</cell><cell>.557</cell><cell>.103</cell><cell>.000</cell><cell>.438</cell></row><row><cell></cell><cell>ELMo (MH)</cell><cell>.506</cell><cell>.215</cell><cell>.351</cell><cell>.172 .540</cell><cell>.348</cell><cell>.164</cell><cell>.239</cell><cell>.317</cell></row><row><cell></cell><cell>Watson</cell><cell>.498</cell><cell>-</cell><cell>.331</cell><cell>.149 .684</cell><cell>.359</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.ibm.com/watson/services/natural-languageunderstanding/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.figure-eight.com/ 4 https://gluebenchmark.com/leaderboard</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://cloud.google.com/natural-language/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>et al. 2018) and <ref type="bibr" target="#b3">(Devlin et al. 2018</ref>) have applied language modeling and transfer to a variety of academic text understanding problems on the GLUE Benchmark.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<idno>abs/1808.044449</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">NTUA-SLP at semeval-2018 task 1: Predicting affective content in tweets with deep attentive rnns and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolovou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paraskevopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ellinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
		<idno>abs/1804.06658</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1511.01432</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An argument for basic emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gpu kernels for block-sparse weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fine-tuned language models for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno>abs/1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning from noisy singly-labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>abs/1712.04577</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multiplicative LSTM for sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<idno>abs/1609.07959</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<title level="m">Image-based recommendations on styles and substitutes. SIGIR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tcs research at semeval-2018 task 1: Learning robust representations using multiattention architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meisheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
		<meeting>The 12th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="291" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Unpublished Manuscript</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1310.4546</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semeval-2018 Task 1: Affect in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bravo-Marquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salameh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Semantic Evaluation</title>
		<meeting>International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1802.05365</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Emotions: A general psychoevolutionary theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large scale language modeling: Converging on 40gb of text in four hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yakovenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>abs/1808.01371</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Improving language understanding by generative pre-training</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1704.01444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno>abs/1508.07909</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CoRR abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>abs/1804.07461</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno>abs/1711.03953</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Breaking the softmax bottleneck: A high-rank RNN language model. CoRR abs/1506.06724</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
