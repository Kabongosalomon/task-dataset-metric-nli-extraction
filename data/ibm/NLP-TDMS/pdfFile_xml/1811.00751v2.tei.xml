<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
							<email>hui.li02@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing irregular text in natural scene images is challenging due to the large variance in text appearance, such as curvature, orientation and distortion. Most existing approaches rely heavily on sophisticated model designs and/or extra fine-grained annotations, which, to some extent, increase the difficulty in algorithm implementation and data collection. In this work, we propose an easy-to-implement strong baseline for irregular scene text recognition, using offthe-shelf neural network components and only word-level annotations. It is composed of a 31-layer ResNet, an LSTMbased encoder-decoder framework and a 2-dimensional attention module. Despite its simplicity, the proposed method is robust. It achieves state-of-the-art performance on irregular text recognition benchmarks and comparable results on regular text datasets. Code is available at: https : //tinyurl.com/ShowAttendRead</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Text information in images is of indispensable value in semantic visual understanding. Reading text in natural scene, however, compared to traditional OCR, is still a challenging problem. One of the main reasons is the potential irregularity and diversity of in text shape and layout, which can be curved, oriented or distorted. With the application of deep neural networks, the performance of regular (mostly horizontal) text recognition has been improved rapidly. Taking the ICDAR 2013 benchmark <ref type="bibr" target="#b9">(Karatzas et al. 2013</ref>) as example, the best-reported accuracy ) has been 94.4%, to our knowledge. Nonetheless, most regular text recognizers <ref type="bibr" target="#b22">(Shi, Bai, and Yao 2017;</ref><ref type="bibr" target="#b27">Wang and Hu 2017)</ref> treat text as horizontal lines, which makes them difficult to be extended directly to irregular text. The performance of existing irregular text recognizers is far from being satisfactory. For instance, the current top-performing approach <ref type="bibr" target="#b24">(Shi et al. 2018</ref>) only achieves 76.1% accuracy on the ICDAR 2015 benchmark <ref type="bibr" target="#b10">(Karatzas et al. 2015)</ref>.</p><p>Existing irregular text recognizers can be roughly categorized into three groups: rectification based <ref type="bibr" target="#b23">(Shi et al. 2016;</ref><ref type="bibr" target="#b15">Liu et al. 2016;</ref><ref type="bibr" target="#b17">Liu, Chen, and Wong 2018)</ref>, attention based <ref type="bibr" target="#b30">(Yang et al. 2017;</ref><ref type="bibr" target="#b1">Cheng et al. 2017</ref>) and multidirection encoding based  approaches. * The first two authors equally contributed to this work. C. Shen is the corresponding author.  <ref type="bibr" target="#b24">(Shi et al. 2018)</ref> irregular text recognizers. The second column gives the predictions of our approach and the heat map by aggregating attention weights at all character decoding steps; the third column demonstrates the rectified images and the corresponding predictions using the authors' implementation. Rectification based methods may encounter difficulties when the input image is severely curved or distorted. In contrast, we do not transform images and propose a tailored 2D attention module to localize individual characters in a weaklysupervised manner.</p><p>The rectification based methods attempt to transform irregular text patches into regular ones and then recognize them using regular text recognizers. However, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, severe distortions or curvatures give rise to difficulties for rectification. <ref type="bibr" target="#b30">(Yang et al. 2017)</ref> proposed an attention mechanism to select local 2D features when decoding individual characters. Nevertheless, it needs extra characterlevel annotations to supervise the attention network and a multi-task strategy to learn better visual features. ) stated that both rectification and attention based approaches are somewhat difficult to be directly trained on irregular text. They designed a sophisticated framework that needs to encode arbitrarily-oriented text in four directions.</p><p>Alternatively, we go back to the conventional attention based encoder-decoder framework. Our proposed model is composed of a 31-layer ResNet, an LSTM-based encoderdecoder framework and a tailored 2-dimensional attention module. In contrast to <ref type="bibr" target="#b30">(Yang et al. 2017)</ref>, our model only needs word-level annotations, which enables us to make full use of both real and synthetic data for training without using character-level annotations. Built upon standard NN modules, the main architecture can be implemented by around 100 lines of code. Despite its simplicity, our method outperforms previous methods on irregular text datasets by a large margin, and achieves comparable results on regular text. To our best knowledge, we are the first one that uses 2D attention in irregular text recognition with only wordlevel annotations needed. As demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>, our 2D attention module is more flexible and robust in handling sophisticated text layout.</p><p>For regular text, it is presented in <ref type="bibr" target="#b13">(Lee and Osindero 2016;</ref><ref type="bibr" target="#b23">Shi et al. 2016</ref>) that the 1D attention based encoder-decoder framework is able to align between input subsequences and decoded characters. Our approach extends this framework by replacing 1D attention with a tailored 2D attention mechanism, in order to handle the complicated spatial layout of irregular text. Inspired by the success of the Show-Attendand-Tell model <ref type="bibr" target="#b29">(Xu et al. 2015)</ref> on image captioning, our model is also based on a 2D attention based encoder-decoder structure, which is referred to as Show-Attend-and-Read (SAR). Note that <ref type="bibr" target="#b29">(Xu et al. 2015)</ref> is designed for image caption, while ours is used for text recognition.</p><p>The main contributions of this work is three-fold: 1) We setup an easy-to-implement strong baseline for recognizing irregular text in natural scene images, which is made up of off-the-shelf neural components such as CNNs, LSTMs and attention mechanisms. The proposed model can be trained end-to-end without pre-training. All the training examples are synthetic or from public real data. We will release the code and data used for training.</p><p>2) Compared to existing irregular text recognizers, our proposed approach does not rely on sophisticated designs (including spatial transformation, hierarchical attention or multi-directional encoding) to handle text distortions. Alternatively, we simply use a 2D attention mechanism to deal with irregular text, which selects local features for individual characters. Moreover, our proposed attention module does not require additional pixel-level or character-level supervision information, which is weakly supervised by the crossentropy loss on the final predictions. The attention mechanism is also tailored to consider neighborhood information and boosts the recognition performance.</p><p>3) Note that many irregular text recognizers perform relatively worse on regular text. In contrast, due to its flexibility and robustness, the proposed approach not only significantly outperforms existing approaches on irregular text, but also achieves favorable performance on regular text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Early Work Scene text recognition has drawn lots of attentions during recent years and made significant progress in performance. Early approaches mainly work in a bottom-up fashion <ref type="bibr" target="#b28">(Wang, Babenko, and Belongie 2011;</ref><ref type="bibr" target="#b19">Mishra, Alahari, and Jawahar 2012b;</ref><ref type="bibr" target="#b20">Phan et al. 2013;</ref><ref type="bibr" target="#b31">Yao et al. 2014)</ref>, in which individual characters are detected firstly via sliding window or connected components, and then integrated into a word by dynamic programming or graph models. Character detection or separation by itself, however, is not a completely-solved problem due to complicated background or cursive fonts. Alternatively, <ref type="bibr" target="#b7">(Jaderberg et al. 2015a</ref>) considered text recognition as a multi-class classification problem, which assigned a distinct label to each word in a 90ksized dictionary. Apparently, it is difficult to extend the approach to words out of the dictionary. Regular Text Recognition <ref type="bibr" target="#b6">(He et al. 2016b</ref>) and <ref type="bibr" target="#b22">(Shi, Bai, and Yao 2017</ref>) considered words as one-dimensional sequences of varying lengths, and employed RNNs to model the sequences without explicit character separation. A Connectionist Temporal Classification (CTC) layer was adopted to decode the sequences. <ref type="bibr" target="#b27">(Wang and Hu 2017)</ref> proposed a Gated Recurrent Convolutional Neural Network (GR-CNN) with CTC for regular text recognition. Inspired by the sequence-to-sequence framework for machine translation, <ref type="bibr" target="#b13">(Lee and Osindero 2016)</ref> and <ref type="bibr" target="#b23">(Shi et al. 2016)</ref> proposed to recognize text using an attention-based encoderdecoder framework. In this manner, RNNs are able to learn the character-level language model hidden in the word strings from the training data. A 1D soft-attention model was adopted to select relevant local features during decoding characters. The RNN+CTC and sequence-to-sequence frameworks serve as two meta-algorithms that are widely used by subsequent text recognition approaches. Both models can be trained end-to-end and achieve considerable improvements on regular text recognition.  observed that the frame-wise maximal likelihood loss, which is conventionally used to train the encoder-decoder framework, may be confused and misled by missing or superfluity of characters, and thus degrade the recognition accuracy. To this end, they proposed "Edit Probability" to handle this misalignment problem. ) presented a binary convolutional encoder-decoder network (B-CEDNet) together with a bidirectional recurrent neural network (Bi-RNN) for recognizing regular text images, and achieved significant speed-up. The whole framework needs to be trained in two stage, and requires pixel-level annotations. <ref type="bibr" target="#b14">(Li, Wang, and Shen 2017)</ref> combined a Faster-RCNN based text detector and a 1D attention based recognizer into an end-to-end trainable system. Irregular Text Recognition The rapid progress on regular text recognition has given rise to increasing attention on recognizing irregular ones. <ref type="bibr" target="#b24">(Shi et al. 2018)</ref> and <ref type="bibr" target="#b23">(Shi et al. 2016)</ref>   <ref type="figure">Figure 2</ref>: Overview of the proposed framework for irregular text recognition. The input image is firstly fed into a 31-layer ResNet, which results in a 2D feature map. Next, an LSTM model encodes the feature map column by column, and the last hidden state is considered as a holistic feature of the input image. Another LSTM model is used to decode the holistic feature into a sequence of characters. At each time step of decoding, an attention module computes a weighted sum of 2D features (glimpse), depending on the current hidden state of the LSTM decoder. The irregularity of text is implicitly handled by the 2D attention module, in a weakly supervised manner. and then recognized it using a 1D attentional sequence-tosequence model. <ref type="bibr" target="#b15">(Liu et al. 2016</ref>) also removed text distortions via STN, and used the RNN + CTC framework for sequence recognition. Instead of rectifying the entire distorted text image as in <ref type="bibr" target="#b24">(Shi et al. 2018;</ref><ref type="bibr" target="#b15">Liu et al. 2016)</ref>, <ref type="bibr" target="#b17">(Liu, Chen, and Wong 2018)</ref> presented a Character-Aware Neural Network (Char-Net) to detect and rectify individual characters, which, however, requires extra character-level annotations. Moreover, a sophisticated hierarchical attention mechanism was designed for accurate feature extraction, which consists of a recurrent RoIWarp layer and a character-level attention layer. <ref type="bibr" target="#b30">(Yang et al. 2017)</ref> introduced an auxiliary dense character detection task into the encoder-decoder network to handle the irregular text. Pixel-level character/non-character annotations are required to train the network. <ref type="bibr" target="#b1">(Cheng et al. 2017)</ref> asserted that there are "attention drifts" in traditional attention model and proposed a Focusing Attention Network (FAN) that is composed of an attention network for character recognition and a focusing network to adjust the attention drift. This work also needs to be trained with characterlevel bounding box annotations.  applied LSTMs in four directions to encode arbitrarily-oriented text. A filtering mechanism was designed to integrate these redundant features and reduce irrelevant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>We describe the architecture of our model in this section. As presented in <ref type="figure">Figure 2</ref>, the whole model consists of two main parts: a ResNet CNN for feature extraction and a 2Dattention based encoder-decoder model. It takes an image as input and outputs a varying length sequence of characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet CNN</head><p>The designed 31-layer ResNet <ref type="bibr" target="#b5">(He et al. 2016a</ref>) is presented in <ref type="table" target="#tab_2">Table 1</ref>. For each residual block, we use the projection shortcut (done by 1 × 1 convolutions) if the input and output dimensions are different, and use the identity shortcut  if they have the same dimension. All the convolutional kernel size is 3 × 3. Besides two 2 × 2 max-pooling layers, we also use a 1 × 2 max-pooling layer as in <ref type="bibr" target="#b22">(Shi, Bai, and Yao 2017)</ref>, which reserves more information along the horizontal axis and benefits the recognition of narrow shaped characters (e.g., 'i', 'l'). The resulting 2D feature maps (denoted as V of size H × W × D where D is the number of channels) will be used: 1) to extract holistic feature for the whole image; 2) as the context for the 2D attention network.</p><p>To keep their original aspect ratios, we resize input images to a fixed height and a varying width. Hence, the width of the obtained feature map, W , also varies w.r.t. aspect ratios.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Attention based Encoder-Decoder</head><p>Sequence-to-sequence models have been widely used in machine translation, speech recognition and text recognition (Sutskever, Vinyals, and Le 2014) <ref type="bibr" target="#b3">(Chorowski et al. 2015</ref>) <ref type="bibr" target="#b1">(Cheng et al. 2017)</ref>. In this work, we adopt a 2D attention based encoder-decoder network for irregular text recognition. Without transforming original text images, the proposed attention module is able to accommodate text of arbitrary shape, layout and orientation. Encoder As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the encoder is a 2-layer LSTM model with 512 hidden state size per layer. At each time step, the LSTM encoder receives one column of the 2D features maps followed by max-pooling along the vertical axis, and updates its hidden state h t . After W steps, the final hidden state of the second LSTM layer, h W , is regarded as a fixed-size representation (holistic feature) of the input image, and provided for decoding. Decoder As shown in <ref type="figure">Figure 4</ref>, the decoder is another LSTM model with 2 layers and 512 hidden state size per layer. The encoder and decoder do not share parameters. Initially, the holistic feature h W is fed into the decoder LSTM, at time step 0. Then a "START" token is input into LSTM at step 1. From step 2, the output of the previous step is fed into LSTM until the "END" token is received. All the LSTM inputs are represented by one-hot vectors, followed by a linear transformation Ψ(). During training, the inputs of decoder LSTMs are replaced by the ground-truth character sequence. The outputs are computed by the following transformation:</p><formula xml:id="formula_0">y t = ϕ(h t , g t ) = softmax(W o [h t ; g t ])<label>(1)</label></formula><p>where h t is the current hidden state and g t is the output of the attention module. W o is a linear transformation, which embeds features into the output space of 94 classes, in corresponding to 10 digits, 52 case sensitive letters, 31 punctuation characters, and an "END" token.</p><p>2D Attention Traditional 2D attention modules <ref type="bibr" target="#b29">(Xu et al. 2015)</ref> treat each location independently, neglecting their 2D spatial relationships. In order to take neighborhood information into account, we propose a tailored 2D attention mech- <ref type="figure">Figure 4</ref>: The structure of the LSTM decoder used in this work. The holistic feature h W , a "START" token and the previous outputs are input into LSTM subsequently, terminated by an "END" token. At each time step t, the output y t is computed by ϕ() with the current hidden state and the attention output as inputs.</p><formula xml:id="formula_1">2D convolution feature map V . . . h Holistic Feature ' T h ' 1 T h + () () () () 1 y 1 T y − T y T y 1 g T g 1 T g + START ' 0 h ' 1 h ' 1 T h + () () ' 1 h LSTM LSTM () () A A LSTM () () LSTM () A A ' T h LSTM () () LSTM () A A END w</formula><p>anism as follows:</p><formula xml:id="formula_2">           eij = tanh(Wvvij + p,q∈N ijW p−i,q−j · vpq + W h h t ), αij = softmax(w T e · eij), gt = i,j αijvij, i = 1, . . . , H, j = 1, . . . , W.</formula><p>( <ref type="formula">2)</ref> where v ij is the local feature vector at position (i, j) in V, and N ij is the eight-neighborhood around this position; h t is the hidden state of decoder LSTMs at time step t, to be used as the guidance signal; W v , W h andWs are linear transformations to be learned; α ij is the attention weight at location (i, j); and g t is the weighted sum of local features, denoted as a glimpse. Compared to traditional attention mechanisms, we add a term p,q∈NijW p−i,q−j · v pq when computing the weight of v ij . We can see from <ref type="bibr">Figure</ref> 5 that the computation of (2) can be accomplished by a series of convolution operations. Hence it is easy to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we perform extensive experiments to verify the effectiveness of the proposed method. We first show the datasets used for training and test, and then demonstrate the implementation details. Our model is compared with state-of-the-art methods on a number of public benchmark datasets, including both regular and irregular text in natural scene images. We also conduct ablation studies to analyze the impact of model hyper-parameters and training data on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>The following datasets are used in our experiments: Synthetic Datasets There are two public available synthetic datasets that are widely used to train text recognizers: the 9-million synthetic data (refer to as Syn90k) released by  <ref type="figure">Figure 5</ref>: The computation of the proposed 2D attention mechanism can be simply implemented by convolutions, where W v v ij + p,q∈NijW p−i,q−j · v pq is accomplished by a 3 × 3 convolution. The sizes of intermediate results are also demonstrated. The operation "tile" duplicates the input 1 × 1 × d vector H × W times. <ref type="bibr" target="#b7">(Jaderberg et al. 2015a</ref>) and the 8-million synthetic words (refer to as SynthText) proposed by <ref type="bibr" target="#b4">(Gupta, Vedaldi, and Zisserman 2016)</ref>. Images in Syn90k are generated based on 90k generic English words, while word instances in Syn-thText are from the Newsgroup20 lexicon <ref type="bibr" target="#b12">(Lang 1995)</ref>. Although they cover a huge number of word instances, the proportion of special characters like punctuations is relatively small. To compensate the lack of special characters, we synthesize additional 1.6-million word images (denoted as SynthAdd) using the synthetic engine proposed by <ref type="bibr" target="#b4">(Gupta, Vedaldi, and Zisserman 2016)</ref>. Special characters are randomly inserted to the words in the aforementioned two lexicons. IIIT 5K-Words (IIIT5K) (Mishra, Alahari, and Jawahar 2012a) contains 5000 word patches cropped from natural scene images found by Google image search, 2000 for training and 3000 for test. Text instances in these images are nearly horizontal. Each image associates with a 50-word lexicon and a 1000-word lexicon individually. Street View Text (SVT) <ref type="bibr" target="#b28">(Wang, Babenko, and Belongie 2011)</ref> consists of 647 word patches cropped from Google Street View for test. They are nearly horizontal, but with noise, blur and low-resolution. Each image is associated with a 50-word lexicon. ICDAR 2013 (IC13) <ref type="bibr" target="#b9">(Karatzas et al. 2013)</ref> has 848 cropped word patches for training and 1095 for test. To fairly compare with previous results, we remove images that contain non-alphanumeric characters, which results in 1015 test patches. Words in this dataset are also mostly regular. No lexicon is provided. ICDAR 2015 (IC15) <ref type="bibr" target="#b10">(Karatzas et al. 2015)</ref> contains word patches cropped from incidental scene images captured under arbitrary angles. Hence most word patches in this dataset are irregular (oriented, perspective or curved). It contains 4468 patches for training and 2077 for test. No lexicon is associated. Street View Text Perspective (SVTP) <ref type="bibr" target="#b20">(Phan et al. 2013)</ref> consists of 639 word patches, which are cropped from sideview snapshots in Google Street View and encounter severe perspective distortions. All patches are used for test, with a 50-word lexicon and a Full lexicon for each image. CUTE80 (CT80) <ref type="bibr" target="#b21">(Risnumawan et al. 2014)</ref> contains 288 curved text images for test, with high resolution. No lexicon is associated. COCO-Text (COCO-T) <ref type="bibr" target="#b26">(Veit et al. 2016</ref>) contains more than 62k legible word patches cropped from COCO images, including machine printed and handwritten, regular and irregular text. There are 42618 patches for training, 9896 for validation and 9837 for test. No lexicon is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>The proposed model is implemented in Torch. All experiments are conducted on an NVIDIA Titan X GPU with 12GB memory. We simply use the cross-entropy loss for training. Without any pre-training, the whole network is endto-end trained using the ADAM optimizer <ref type="bibr" target="#b11">(Kingma and Ba 2014)</ref>. We use a batch size of 32 at training time. The learning rate is set to 10 −3 initially, with a decay rate of 0.9 every 10000 iterations until it reaches 10 −5 .</p><p>Iteratively, we construct distinct data groups with 120k patches randomly sampled from Syn90k, 120k from Syn-thText, 80k from SynthAdd and approximately 50k training data from all the aforementioned public real datasets. Each group is trained for 2 epochs, and our algorithm converges after using 20 groups. In total, 2.4 million patches from Syn90k, 2.4 million from SynthText and 1.6 million from SynthAdd are used in the whole training process. The height of input images is resized to 48 pixels, and the width is calculated according to the original aspect ratio, but no larger than 160 and no smaller than 48 pixels.</p><p>At test time, for images with height larger than width, we will rotate the image by 90 degrees clockwise and anticlockwise respectively, and recognize them together with the original image. A recognition score will be calculated by averaging the output probabilities. The top-scored one will be chosen as the final recognition result. We use beam search for LSTM decoding, which keeps the top-k candidates with the highest accumulative scores, where k is empirically set to 5 in our experiments. Compared to greedy decoding that only picks the highest scored character at each time step, beam search brings an approximately 0.5% improvement to the recognition accuracy, in our practice. The test speed is 15ms per patch in average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>In this section, we evaluate our model on several regular and irregular text benchmarks, and compare the performance with other state-of-the-art methods. For datasets with lexicons provided, we simply select from lexicon the one with the minimum edit distance to the predicted word. The recognition results are summarized in <ref type="table">Table 2</ref>.</p><p>On irregular text datasets (i.e., IC15, SVTP, CT80 and COCO-T), our approach outperforms the compared methods by a large margin. In particular, our approach gives accuracy <ref type="table">Table 2</ref>: Recognition accuracy (in percentages) on public benchmarks, including both regular and irregular text. "50", "1k", and "Full" are lexicon sizes, where "Full" means a combined lexicon of all images in the dataset. "None" means lexicon-free. The approaches marked with "*" are trained with both word-level and character-level annotations. In each column, the best performing result is shown in bold font, and the second best result is shown with underline. Our approach outperforms all the compared methods on all irregular text benchmarks, and achieves comparable performance on regular text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Regular <ref type="table" target="#tab_2">Text  Irregular Text  IIIT5K  SVT  IC13 IC15  SVTP  CT80 COCO-T  50</ref> 1k  . Actually, our model performs the best or the second best on 5 of the 6 evaluated regular text settings. The superiority of our method is more significant when there is no lexicon, such as in IIIT5K and SVTP. It demonstrates the practicality of our proposed approach in realistic scenarios where lexicon is rarely provided. Examples of 2D attention heat maps when decoding individual characters are visualized in <ref type="figure" target="#fig_2">Figure 6</ref>. Although learned in a weakly supervised manner, the attention module can still approximately localize characters being decoded, extract discriminative local features and finally help text recognition. Note that the proposed attention module is trained without character-level annotations.</p><p>Some failure cases are also presented in <ref type="figure">Figure 8</ref>. There are a variety of reasons for failure, such as blurry, partial occlusion, extreme distortion, uneven lighting condition, uncommon fonts, vertical text, etc. Scene text recognition still has a long way to be completely solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>In order to analyze the impact of different model hyperparameters and training data on the recognition performance, we perform a series of ablation studies as presented in <ref type="table" target="#tab_6">Table 3</ref>. All the evaluated models are trained from scratch and tested on benchmarks without lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Parameters</head><p>We firstly reduce by 50% the number of channels of all convolutional layers expect the last layer, which lowers the accuracy by 2 to 4 percentages. The down- sampling ratio of the proposed ResNet is 1/8 vertically and 1/4 horizontally, which results in a feature map of maximum size 6 × 40. Here we further divide the vertical and/or horizontal down-sampling ratios by 2, and obtain worse performance. These results shows that the volume of feature maps should be sufficiently large to encode a large variety of visual information for text recognition.</p><formula xml:id="formula_3">O P T I M U S T A R B U W Y N D H A C H E V R O R O B I N S p</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Modules</head><p>The proposed 2D attention model is respectively replaced by the traditional 2D counterpart with the term p,q∈NijW p−i,q−j · v pq removed from Equation (2) and a 1D attention module that considers feature maps as 1D sequences. By aggregating neighborhood information, the proposed 2D attention model outperforms the traditional 2D one by 1 to 2 percentages. Both of the 2D attention modules performs better than the 1D one in most cases, which shows their robustness for both regular and irregular text recognition. We compare the proposed and the traditional 2D attention heat maps in <ref type="figure">Figure 7</ref>. The proposed model presents better performance on character localization and recognition.</p><p>LSTM Parameters By cutting down by half the hidden state size of both encoder and decoder LSTMs, we receive degraded recognition accuracies. The performance degradation is more serious when we use 1 layer of LSTMs, instead of 2 layers. Relatively, the number of LSTM layers presents a stronger impact on the performance.</p><p>Training Data The performance drops without using the real image training data. However, because of our simple model design, we can make full use of current public available dataset to train the model for better performance. Moreover, it should be note that a large number of methods (as indicated with "*" in <ref type="table">Table 2</ref>) use extra character level annotations, which is another kind of additional information. The requirement of character-level annotations prevents them from using real data with only word-level annotations. <ref type="figure">Figure 8</ref>: Failure cases of our model. "GT" stands for the ground-truth annotation, and "Pred" is the predicted result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we present a simple yet strong baseline for irregular text recognition. The proposed framework is built upon off-the-shelf neural network modules, including a ResNet CNN, an LSTM encoder-decoder and a 2D tailored attention module. Without any extra supervision information, the proposed attention mechanism is capable of select local features for decoding characters. Being robust to different forms of text layouts, our approach performs well for both regular and irregular text.</p><p>As to future works, the proposed framework can be extended in several ways. Firstly, the LSTM encoder-decoder is possible to be replaced by CNNs for sequence modeling, which will further ease the training process. Secondly, the proposed 2D attention module can be seen as a special case of graph neural networks, where edges of the graph are defined on 8-neighborhoods. Straightforwardly, we can apply the attention mechanism on graphs with more complex structures, to incorporate with the rich context information. Finally, to better learn visual features and speedup the training process, we can also add a word classification head apart from the LSTM decoder.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The comparison of our proposed 2D attention based and the rectification based</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The structure of the LSTM encoder used in this work. v :,i represents the ith column of the 2D feature map V. At each time step, a column feature is firstly max pooled along the vertical direction, and then fed into LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of 2D attention weights at individual decoding time steps, which shows that our 2D attention model can be trained to approximately localize characters without character-level annotations. For space reasons, some of the decoding results are truncated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The configuration of the 31-layer ResNet for feature extraction. "Conv" stands for Convolutional layers, with kernel size and output channels presented. The stride and padding for convolutional layers are all set to "1". For Maxpooling layers, "k" means kernel size, and "s" represents stride. No padding for Max-pooling layers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies by changing model hyper-parameters and training data. The first row shows the original proposed model configuration. Those changed parameters are shown in bold font. The models are evaluated on benchmarks without using any lexicon. Reducing the size of CNN and LSTM models has negative impacts on the recognition performance. Using the traditional 2D attention or 1D attention modules, instead of our proposed attention mechanism, also degrades the accuracy. 5% (78.9% to 86.4%) on SVTP-None and 10.1% (79.5% to 89.6%) on CT80. Note that neither SVTP or CT80 provides training data, which lowers the chance of over-fitting. Meanwhile, the proposed method still achieves state-of-the-art performance on regular text datasets (i.e., IIIT5K, SVT and IC13)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Model Configuration</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training</cell><cell>CNN</cell><cell>Down-sampling</cell><cell>Attention</cell><cell cols="7">LSTM Hidden state IIIT5K SVT IC13 IC15 SVTP CT80 COCO-T</cell></row><row><cell>data</cell><cell>channels</cell><cell>ratio</cell><cell>module</cell><cell>layers</cell><cell>size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>×1</cell><cell>1/8, 1/4</cell><cell>2D proposed</cell><cell>2</cell><cell>512</cell><cell>95.0</cell><cell>91.2 94.0 78.8</cell><cell>86.4</cell><cell>89.6</cell><cell>66.8</cell></row><row><cell></cell><cell>×1/2</cell><cell>1/8, 1/4</cell><cell>2D proposed</cell><cell>2</cell><cell>512</cell><cell>92.7</cell><cell>88.7 92.0 75.6</cell><cell>81.3</cell><cell>86.8</cell><cell>62.6</cell></row><row><cell></cell><cell>×1</cell><cell>1/16, 1/4</cell><cell>2D proposed</cell><cell>2</cell><cell>512</cell><cell>93.8</cell><cell>90.3 92.7 77.4</cell><cell>84.5</cell><cell>89.2</cell><cell>64.8</cell></row><row><cell></cell><cell>×1</cell><cell>1/16, 1/8</cell><cell>2D proposed</cell><cell>2</cell><cell>512</cell><cell>94.0</cell><cell>90.6 93.1 76.2</cell><cell>83.7</cell><cell>87.5</cell><cell>63.7</cell></row><row><cell>Synth+Real</cell><cell>×1</cell><cell>1/8, 1/8</cell><cell>2D proposed</cell><cell>2</cell><cell>512</cell><cell>93.6</cell><cell>89.3 92.5 76.1</cell><cell>82.8</cell><cell>87.5</cell><cell>63.3</cell></row><row><cell></cell><cell>×1</cell><cell>1/8, 1/4</cell><cell>2D traditional</cell><cell>2</cell><cell>512</cell><cell>94.0</cell><cell>90.1 92.3 77.2</cell><cell>84.3</cell><cell>87.5</cell><cell>64.2</cell></row><row><cell></cell><cell>×1</cell><cell>1/8, 1/4</cell><cell>1D</cell><cell>2</cell><cell>512</cell><cell>93.0</cell><cell>89.9 90.2 76.6</cell><cell>83.6</cell><cell>84.7</cell><cell>65.4</cell></row><row><cell></cell><cell>×1</cell><cell>1/8, 1/4</cell><cell>2D proposed</cell><cell>1</cell><cell>512</cell><cell>89.7</cell><cell>87.2 87.4 70.6</cell><cell>76.4</cell><cell>80.6</cell><cell>60.1</cell></row><row><cell></cell><cell>×1</cell><cell>1/8, 1/4</cell><cell>2D proposed</cell><cell>2</cell><cell>256</cell><cell>94.0</cell><cell>89.3 92.8 76.8</cell><cell>83.7</cell><cell>86.5</cell><cell>63.8</cell></row><row><cell>OnlySynth</cell><cell>×1</cell><cell>1/8, 1/4</cell><cell>2D proposed</cell><cell>2</cell><cell>512</cell><cell>91.5</cell><cell>84.5 91.0 69.2</cell><cell>76.4</cell><cell>83.3</cell><cell>−</cell></row><row><cell>increases of 7.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5086" to="5094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AON: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading scene text in deep convolutional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting>Adv. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ICDAR 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Doc. Anal. Recog</title>
		<meeting>Int. Conf. Doc. Anal. Recog</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<title level="m">ICDAR 2015 robust reading competition. In Proc. Int. Conf. Doc. Anal. Recog</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<meeting>the Twelfth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards end-to-end text spotting with convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5238" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Star-net: A spatial attention residue network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vis. Conf</title>
		<meeting>British Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeezedtext: A real-time scene text recognition by binary convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Char-net: A character-aware neural network for distorted scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vis. Conf</title>
		<meeting>British Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Topdown and bottom-up cues for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1601.07140</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gated recurrent convolution neural network for ocr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to read irregular text with attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3280" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
