<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Min-Entropy Latent Model for Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE, Pengxu</roleName><forename type="first">Wan</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Wei</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Qixiang</forename><forename type="middle">Ye</forename></persName>
						</author>
						<title level="a" type="main">Min-Entropy Latent Model for Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Weakly Supervised Learning</term>
					<term>Object Detection</term>
					<term>Min-Entropy Latent Model</term>
					<term>Recurrent Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised object detection is a challenging task when provided with image category supervision but required to learn, at the same time, object locations and object detectors. The inconsistency between the weak supervision and learning objectives introduces significant randomness to object locations and ambiguity to detectors. In this paper, a min-entropy latent model (MELM) is proposed for weakly supervised object detection. Min-entropy serves as a model to learn object locations and a metric to measure the randomness of object localization during learning. It aims to principally reduce the variance of learned instances and alleviate the ambiguity of detectors. MELM is decomposed into three components including proposal clique partition, object clique discovery, and object localization. MELM is optimized with a recurrent learning algorithm, which leverages continuation optimization to solve the challenging non-convexity problem. Experiments demonstrate that MELM significantly improves the performance of weakly supervised object detection, weakly supervised object localization, and image classification, against the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S UPERVISED object detection has made great progress in recent years <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>, as concluded in the object detection survey <ref type="bibr" target="#b6">[7]</ref>. This can be attributed to the availability of large datasets with precise object annotations and deep neural networks capable of absorbing the annotation information, especially. Nevertheless, annotating a bounding-box for each object in large datasets is laborious, expensive, or even impractical. It is also not consistent with cognitive learning, which requires solely the presence or absence of a class of objects in a scene, instead of bounding-boxes that indicate the precise locations of all objects.</p><p>Weakly supervised learning (WSL) refers to methods that rely on training data with incomplete annotations to learn recognition models. Weakly supervised object detection (WSOD) requires solely the image-level annotations indicating the presence or absence of a class of objects in images to learn detectors <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b28">[29]</ref>. It can leverage rich Web images with tags to learn object-level models.</p><p>To tackle the WSOD problem, existing approaches often resort to latent variable learning or multi-instance learning (MIL) by using redundant object proposals as inputs. The learning objective is designed to choose a true instance from redundant object proposals of each image to minimize the image classification loss. Due to the unavailability of objectlevel annotations, WSOD approaches require to collect instances from redundant proposals, as well as learning detectors that compromise the appearance of various objects. It typically requires solving a non-convex model and thus is challenged by the local minimum problem.</p><p>In the learning procedure of weakly supervised deep F. <ref type="bibr">Wan</ref> detection networks (WSDDN) <ref type="bibr" target="#b21">[22]</ref>, a representative WSOD approach, the problem has been observed, i.e., the collected instances switch among different object parts with great randomness, <ref type="figure" target="#fig_0">Fig. 1</ref>. Various object parts were capable of minimizing image classification loss, but experienced difficulty in optimizing object detectors due to their appearance ambiguity. Recent approaches have used image segmentation <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, context information <ref type="bibr" target="#b23">[24]</ref>, and instance classifier refinement <ref type="bibr" target="#b26">[27]</ref> to empirically regularize the learning procedure. However, the issues about principally reducing localization randomness and alleviating the local minimum remain unresolved. In this paper, we propose a clique-based min-entropy latent model (MELM) <ref type="bibr" target="#b0">1</ref> to collect instances with minimum randomness, motivated by a classical thermodynamic principle: Minimizing entropy results in minimum randomness of a system. Min-entropy is used as a model to learn object locations and a metric to measure the randomness of localization during learning. MELM is concluded as three components: (1) Instance (object and object part) collection with a clique partition module; <ref type="bibr" target="#b1">(2)</ref> Object clique discovery with a global min-entropy model; (3) Object localization with a local minentropy model, <ref type="figure">Fig. 2</ref>. A clique is defined as a set of object proposals which are spatially related (i.e., overlapping with each other) and class related (i.e., having similar object class scores), <ref type="figure">Fig. 3</ref>. The introduction of proposal cliques can facilitate reducing the redundancy of region proposals and optimizing min-entropy models.</p><p>With the clique partition module and min-entropy models, we can collect instances with minimum randomness, activate true object extent, and suppress object parts, <ref type="figure" target="#fig_0">Fig.  1</ref>. MELM is deployed as a clique partition module and network branches concerning object clique discovery and object localization on top of a deep convolutional neural network (CNN). Based on the global and local min-entropy models, we adopt a recurrent strategy to train detectors and pursue true object extent using solely image-level supervision. This is based on the priori that in deep networks the image classification task and object detection task are highly correlated, which allows MELM to recurrently transfer the weak supervision, i.e., image category annotations, to object locations. By accumulating multiple iterations, MELM discovers multiple objects, if such exist, from a single image. MELM is first proposed in our CVPR paper <ref type="bibr" target="#b30">[31]</ref> and is promoted both theoretically and experimentally in this full version. The contributions of this paper include: (1) A min-entropy latent model that is integrated with deep networks to effectively collect instances and principally minimize the localization randomness during weakly supervised learning. (2) A clique partition module that facilitates instance collection, object extent activation, and object part suppression. (3) A recurrent learning algorithm that formulates image classification and object detection as a predictor and a corrector, respectively, and leverages continuation optimization to solve the challenging non-convexity problem. (4) State-of-the-art performance of weakly supervised detection, localization, and image classification.</p><p>The remainder of this paper can be concluded as follows. Related works are described in Section 2 and the proposed method is presented in Section 3. Experimental results are given in Section 4. We conclude this paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>WSOD was often solved with a pipelined approach, i.e., an image was first decomposed into object proposals, with which clustering <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, latent variable learning <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref> or multiple instance learning <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[32]</ref> was used to perform proposal selection and classifier estimation. With the rise of deep learning, pipelined approaches have been evolving into multiple instance learning (MIL) networks <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b37">[38]</ref>.</p><p>Clustering. Various clustering methods were based on a hypothesis that a class of object instances shape a single compact cluster while the negative instances form multiple diffuse clusters. With such a hypothesis, Wang et al. <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> calculated clusters of object proposals using probabilistic latent Semantic Analysis (pLSA) on positive samples, and employed a voting strategy on these clusters to determine positive sub-categories. Bilen and Song <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> leveraged clustering to initialize latent variables, i.e., object regions, part configurations and sub-categories, and learn object detectors based on the initialization. Clustering is a simple but effective method. The disadvantage lies in that a true positive cluster could incorporate significant noise if the objects are surrounded by clutter backgrounds.</p><p>Latent Variable Learning. Latent SVM <ref type="bibr" target="#b25">[26]</ref> learned object locations and detectors using an Expectation-Maximization-like algorithm. Probabilistic Latent Semantic Analysis <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> learned object locations in a latent space.</p><p>Various latent variable methods were required to solve the non-convexity problem. They often got stuck in a poor local minimum during learning, e.g., falsely localizing object parts or backgrounds. To pursue a stronger minimum, object symmetry and class mutual exclusion information <ref type="bibr" target="#b11">[12]</ref>, Nesterov's smoothing <ref type="bibr" target="#b16">[17]</ref>, and convex clustering <ref type="bibr" target="#b13">[14]</ref> were introduced to the optimization function. These approaches can be regarded as regularization which enforces the appearance similarity among objects.</p><p>Multiple Instance Learning (MIL). A major approach for tackling WSOD is to formulate it as an MIL problem <ref type="bibr" target="#b7">[8]</ref>, which treats each training image as a "bag" and iteratively selects high-scored instances from each bag when learning detectors. However, MIL remains puzzled by random poor solutions. The multi-fold MIL <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> used division of a training set and cross validation to reduce the randomness and thereby prevented training from prematurely locking onto erroneous solutions. Hoffman et al. <ref type="bibr" target="#b20">[21]</ref> trained detectors with weak annotations while transferring representations from extra object classes using full supervision (bounding-box annotation) and joint optimization. To reduce the randomness of positive instances, bag splitting was used during the optimization procedure of MILinear <ref type="bibr" target="#b24">[25]</ref>.</p><p>MIL has been updated to MIL networks <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>, where the convolutional filters behave as detectors to activate regions of interest on the deep feature maps <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>. The beam search <ref type="bibr" target="#b41">[42]</ref> was used to localize objects by leveraging spatial distributions and informative patterns captured in the convolutional layers. To alleviate the non-convexity problem, Li et al. <ref type="bibr" target="#b22">[23]</ref> adopted progressive optimization as regularized loss functions. Tang et al. <ref type="bibr" target="#b26">[27]</ref> proposed to refine instance classifiers online by propagating instance labels to spatially overlapped instances. Diba et al. <ref type="bibr" target="#b27">[28]</ref> proposed weakly supervised cascaded convolutional networks (WCCN). It learned to produce a class activation map and then selected the best object locations on the map by minimizing the segmentation loss.</p><p>MIL networks <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref> report state-of-the-art performance, but are misled by the inconsistency between data annotations and learning objectives. With image-level annotations, they are capable of learning effective representations for image classification. Without object-level annotation, however, their localization ability is limited. The convolutional filters learned with image-level supervision incorporate redundant patterns, e.g., object parts and backgrounds, which cause localization randomness and model ambiguity.</p><p>Recent methods leveraged online instance classifier re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>In weakly supervised learning, the inconsistency between the supervision (image-level annotation) and the objective (object-level classifier) introduces significant randomness to object localization and ambiguity to detectors. We aim at reducing this randomness to facilitate the collection of instances. To this end, we analyze two factors that cause such randomness: proposal redundancy and location uncertainty. 1) It is known that the objective functions of WSOD models are typically non-convex <ref type="bibr" target="#b7">[8]</ref> and have many local minima. The redundant proposals deteriorate them by introducing more local minima and larger searching space. 2) As the object locations are uncertain, the learned instances may switch among object parts, i.e., local minima. To reduce the proposal redundancy, we firstly partition the redundant object proposals into cliques and collect instances which are spatially related (i.e., overlapping with each other) and class related (i.e., having similar object class scores). To minimize localization randomness, we design a global min-entropy model that reflects class and spatial distributions of object cliques. By optimizing the global minentropy model, discriminative cliques containing objects and object parts are discovered, <ref type="figure">Fig. 2</ref>, and the cliques which lack discriminative information are suppressed. The discovered cliques are used to activate true object extent.</p><p>To localize objects in the discovered cliques, a local minentropy latent model is defined. By optimizing the local min-entropy model pseudo-objects are estimated and their spatial neighbors are estimated as hard negatives. Such pseudo-objects and hard negatives estimated under the min-entropy principle have minimized randomness during learning, and further improve the performance of object localization, <ref type="figure">Fig. 2</ref>. MELM is deployed as a clique partition module and two network branches concerning object clique discovery and object localization, <ref type="figure">Fig. 4</ref>. During learning, it leverages a clique partition module to smooth the objective function and a continuation optimization method to solve the challenging non-convexity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Min-Entropy Latent Model</head><p>Let x ∈ X denote an image and y ∈ Y denote labels indicating if x contains an object or not, where Y = {1, 0}. y = 1 indicates that there is at least one object of positive class in the image (positive image) while y = 0 indicates an image without the object of positive class (negative image). h denoting an object proposal (location) is a latent variable and H denoting object proposals in an image is the solution space. H c denoting proposal clique is a subset of H. θ denotes the network parameters. The min-entropy latent model (MELM) with object locations h * and network parameters θ * to be learned, is defined as</p><formula xml:id="formula_0">{h * , θ * } = arg min h,θ E (X ,Y) (h, θ) = arg min h,θ E (X ,Y) (H c , θ) + λE (X ,Y,Hc) (h, θ) ⇔ arg min h,θ L (X ,Y) (H c , θ) + λL (X ,Y,Hc) (h, θ) ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposals</head><p>Cliques Object Cliques <ref type="figure">Fig. 3</ref>. The proposals of high scores are selected and dynamically partitioned into same cliques if they are spatially related (i.e., overlapping with each other) and class related (i.e., having similar object class scores). Clique partition targets at collecting object/object parts and activating true object extent.</p><p>where E (X ,Y) (H c , θ) and E (X ,Y,Hc) (h, θ) are the global and local entropy models which respectively serve for object clique discovery and object localization, <ref type="figure">Fig. 4</ref>. λ is a regularization weight.</p><formula xml:id="formula_1">L (X ,Y) (H c , θ) and L (X ,Y,Hc) (h, θ) are loss functions based on E (X ,Y) (H c , θ) and E (X ,Y,Hc) (h, θ), respectively.</formula><p>Given image-level annotations, i.e., the presence or absence of a class of objects in images, the learning objective of MELM is to find a solution that disentangles object instances from noisy object proposals with minimum image classification loss and localization randomness. To this end, MELM is decomposed into three components including clique partition, object clique discovery, and object localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Clique partition</head><p>Noting that the localization randomness usually occurs among high-scored proposals, we empirically select a set of high-scored (top-200) proposalsH to construct the cliques, whereH ⊆ H.</p><p>The proposal cliques are the minimum sufficient cover toH which satisfy the following formulations, as</p><formula xml:id="formula_2">   C c=1 H c =H ∀c = c , H c ∪ H c = ∅,<label>(2)</label></formula><p>where c, c ∈ {1, ..., C} and C is the number of proposal cliques. To partition cliques, the proposals are sorted by their object scores and the following two steps are iteratively performed: 1) Construct a clique using the proposal of highest object score but not belonging to any clique. 2) Find the proposals that overlap with any proposal in the clique larger than a threshold τ and merge them into the clique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Object clique discovery with global min-entropy</head><p>During the learning procedure, it is required that the cliques evolve with minimum randomness. At the same time, it is required to discover discriminative cliques containing objects and object parts. The network parameters fine-tuned with such cliques can activate true object extent. To this end, a global min-entropy model is defined as</p><formula xml:id="formula_3">H * c = arg min Hc E (X ,Y) (H c , θ) = arg min Hc − log c p (y, H c ; θ),<label>(3)</label></formula><p>where p (y, H c ; θ) is the class probability of a clique H c defined on the object score s (y, h; θ), as</p><formula xml:id="formula_4">p (y, H c ; θ) = exp 1/ |H c | h∈Hc s (y, h; θ) c y exp 1/ |H c | h∈Hc s (y, h; θ) ,<label>(4)</label></formula><p>where |·| calculates proposal number in a clique. s (·) denotes the last FC layer in the object clique discovery branch that outputs object scores for proposals.</p><p>To ensure that the discovered cliques can best discriminate the positive images from negative ones, we further introduce a classification-related weight w Hc . Based on the prior that the object class probabilities of proposals are correlated with their image class probabilities, the global min-entropy is then defined as</p><formula xml:id="formula_5">E (X ,Y) (H c , θ) = − log c w Hc p (y, H c ; θ),<label>(5)</label></formula><p>where w Hc , defined as</p><formula xml:id="formula_6">w Hc = p (y, H c ; θ) y p (y, H c ; θ) ,<label>(6)</label></formula><p>is the classification-related weight of clique H c . Eq. <ref type="formula" target="#formula_5">(5)</ref> belongs to the Aczél and Daróczy (AD) entropy <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> family and is derivable. Eq. <ref type="formula" target="#formula_6">(6)</ref> shows that when y = 1, w Hc ∈ [0, 1] is positively correlated to object score of the positive class in a clique, but negatively correlated to scores of all other classes.</p><p>With above definitions, we implement an object clique discovery branch on top of the network, <ref type="figure">Fig. 4</ref>, and define a loss function to learn network parameters, as</p><formula xml:id="formula_7">L (X ,Y) (H c , θ) = yE (X ,Y) (H c , θ) − (1 − y) h log (1 − p (y, h; θ)).<label>(7)</label></formula><p>For positive images, y = 1, the second term is zero and only the global min-entropy term is optimized. For negative images, y = 0, the first term is zero and the second term (image classification loss) is optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Object localization with local min-entropy</head><p>The cliques discovered by the global min-entropy model constitute good initialization for object localization, but nonetheless incorporate random false positives, e.g., object parts and/or partial objects with backgrounds. This is caused by the learning objective of object clique discovery, which selects proposals to discriminate positive images from negative ones but does not consider how to precisely localize objects. A local min-entropy latent model is then defined to localize objects based on the discovered cliques, as</p><formula xml:id="formula_8">h * = arg min h∈H * c E (X ,Y,H * c ) (h, θ) ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_9">E (X ,Y,Hc) (h, θ) = − h∈Ω h * w h p (y, h; θ) log p (y, h; θ) (9)</formula><p>Object Score Heatmap  <ref type="figure">Fig. 4</ref>. MELM is deployed as a clique partition module and two network branches for object clique discovery and object localization. These two network branches are unified with feature learning and optimized with a recurrent learning algorithm. "M1", "M2" and "M3" are heatmaps about proposal scores without min-entropy, with global min-entropy, and with local min-entropy, respectively. N is the number of object categories. also belongs to the AD entropy <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> family and is also derivable. Different from Eq. (5) which considers the sum of the proposal probabilities globally to predict the image labels, Eq. (9) is designed to locally discriminate each proposal to be positive or negative. w h is defined as</p><formula xml:id="formula_10">w h = h∈Ω h * g (h, h * )p (y, h; θ) p (y, h; θ) h∈Ω h * g (h, h * ) ,<label>(10)</label></formula><p>where Ω h * denotes neighborhoods of h * in the clique. g (h, h * ) = e −a(1−O(h,h * )) 2 is a Gaussian kernel function with parameter a. O (h, h * ) is the IoU of two proposals. The Gaussian kernel function returns a high value when O (h, h * ) is large, and a low value when O (h, h * ) is small. With Eq. (10), we define a "soft" proposal labeling strategy for object localization, which is validated to be less sensitive to noises <ref type="bibr" target="#b46">[47]</ref> compared to the hard thresholding approach defined in <ref type="bibr" target="#b30">[31]</ref>.</p><p>Accordingly, the loss function of the object localization branch is defined as</p><formula xml:id="formula_11">L (X ,Y,Hc) (h, θ) =E (X ,Y,H * c ) (h, θ) .<label>(11)</label></formula><p>According to the definition of w h , the proposals close to h * tend to be true objects, and those far from h * , i.e., O(h; h * ) &lt; 0.5, are hard negatives. Optimizing the loss function produces sparse object proposals of high object probability p(y, h; θ) and suppresses object parts in clique H * c . During the learning procedure, the localization capability of detectors is progressively improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Implementation</head><p>MELM is implemented with an integrated deep network, with a clique partition module and two network branches added on top of the FC layers, <ref type="figure">Fig. 4</ref>. The first network branch, designated as the object clique discovery branch, has a global min-entropy layer, which defines the distribution Clique partition: <ref type="bibr" target="#b5">6</ref>:</p><p>Hc ← Clique partition using Eq. <ref type="formula" target="#formula_2">(2)</ref> 7:</p><p>Object clique discovery: <ref type="bibr" target="#b7">8</ref>:</p><formula xml:id="formula_12">H * c ← Optimize E (X ,Y) (Hc, θ) using Eq. (5) 9:</formula><p>L (X ,Y) (Hc, θ) ← Compute using Eq. <ref type="formula" target="#formula_7">(7)</ref> 10:</p><p>Object localization: <ref type="bibr" target="#b10">11</ref>:</p><formula xml:id="formula_13">h * ← Optimize E (X,Y,H * c ) (h, θ) using Eq. (8) 12:</formula><p>L (X ,Y,Hc) (h, θ) ← Compute using Eq. <ref type="formula" target="#formula_0">(11)</ref> 13:</p><p>Network parameter update: <ref type="bibr">14:</ref> θ ← Back-propagate by miniminzing Eq. <ref type="formula" target="#formula_7">(7)</ref> and Eq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>s(h) ← Update object score using parameters θ 16: end for of object probability and targets at finding candidate object cliques by optimizing the global entropy and the image classification loss. The second branch, designated as the object localization branch, has a local min-entropy layer and a soft-max layer. The local min-entropy layer classifies the object candidates in a clique into pseudo objects 2 and hard negatives by optimizing the local entropy and pseudo object detection loss.</p><p>In the learning phase, object proposals are firstly generated for each image. An ROI-pooling layer atop the convolutional layer (CONV5) is used for efficient feature extraction for these proposals. The MELMs are optimized with a recurrently learning algorithm, which uses forward propagation to select sparse proposals as object instances, and back-2. Pseudo objects are the instantaneously learned objects.</p><p>propagation to optimize the network parameters with the gradient defined in Appendix. The object probability of each proposal is recurrently aggregated by multiplying by the object probability learned in the preceding iteration. In the detection phase, the learned object detectors, i.e., the parameters for the soft-max and FC layers, are used to classify proposals and localize objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Learning</head><p>The objective of model learning is transferring the image category supervision to object locations with min-entropy constraints, i.e., minimum localization randomness.</p><p>Recurrent Learning. A recurrent learning algorithm is implemented to transfer the image-level (weak) supervision using an integrated forward-and back-propagation procedure, <ref type="figure" target="#fig_1">Fig. 5(a)</ref>. In a feed-forward procedure, the minentropy latent models discover object cliques and localize objects which are used as pseudo-objects for detector learning. With the learned detectors the object localization branch assigns all proposals new object probability, which is used to aggregate the object scores with an elementwise multiply operation in the next learning iteration. In the back-propagation procedure, the object clique discovery and object localization branches are jointly optimized with an SGD algorithm, which propagates gradients generated with image classification loss and pseudo-object detection loss. With forward-and back-propagation procedures, the network parameters are updated and the image classifiers and object detectors are mutually enforced. The recurrent learning algorithm is described in Alg. 1.</p><p>Accumulated Recurrent Learning. <ref type="figure" target="#fig_1">Fig. 5(b)</ref> shows the proposed accumulated recurrent learning (ARL). In ARL, we add multiple object localization branches, which may localize objects different from those discovered by previous branches. We thus accumulates objects from all previous branches. Doing so not only endows this approach the capability to localize multiple objects in a single image but also improves the robustness about object appearance diversity by learning various objects with multiple detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model analysis</head><p>With the clique partition module and recurrent learning, MELM implements the idea of continuation optimization <ref type="bibr" target="#b47">[48]</ref> to alleviate the non-convexity problem.</p><p>In continuation optimization, a complex non-convex objective function is denoted as E(θ), where θ denotes the model parameters. Optimizing E(θ) is to find the solution</p><formula xml:id="formula_15">θ * = arg min θ E(θ).<label>(12)</label></formula><p>While directly optimizing Eq. (12) causes local minimum solutions, a smoothed function E(θ, λ) is introduced to approximate E(θ) and facilitate the optimization, as</p><formula xml:id="formula_16">E(θ, λ) = E(θ) − λE(θ),<label>(13)</label></formula><p>where λ ∈ [0, 1] controls the smoothness of the approximate function E(θ, λ) and E(θ) is a correction function. The traditional continuation method traces an implicitly defined curve from a starting point (θ 0 , 1) to a solution point (θ * , 0), where θ 0 is the solution of E(θ, λ) when λ=1.  During the procedure, if E(θ, λ) is smooth and its solution is close to E(θ), we need only to fill the gap between them. This is done by defining a consequence of predictions and corrections to iteratively approximate the original objective function and approach the globally optimal solution θ * .</p><p>The objective function of MELM, defined in Eq. <ref type="formula" target="#formula_0">(1)</ref>, is to find the solution {h * , θ * },</p><formula xml:id="formula_17">{h * , θ * } = arg min h,θ E (X ,Y) (h, θ).<label>(14)</label></formula><p>For the complexity and non-convexity of E (X ,Y) (h, θ), we propose to optimize an approximate function,</p><formula xml:id="formula_18">E (X ,Y) (H c , θ) = E (X ,Y) (h, θ) − λE (X ,Y,Hc) (h, θ),<label>(15)</label></formula><p>which corresponds to Eq. (1). E (X ,Y) (H c , θ) is defined by the clique partition module and is smoother than E (X ,Y) (h, θ). This is achieved by reducing the solution space from thousands of proposals to tens of cliques in each image and averaging the class probability of all proposals in each clique, as defined by Eq. <ref type="figure">(4)</ref>.</p><p>With the approximate function defined, we explore recurrent predictions and corrections to optimize the model. The gap between E (X ,Y) (H c , θ) and E (X ,Y) (h, θ) is that the former is defined to discover object cliques but the latter to localize objects. As the solution of E (X ,Y) (h, θ) (object) is included in the solution of E (X ,Y) (H c , θ) (clique), the gap can be simply filled by designing a correction model E (X ,Y,Hc) (h, θ) to localize the object in the clique. With recurrent learning, the original objective function is thus progressively approximated.</p><p>Accordingly, the weakly supervised learning problem is decomposed into an object clique discovery problem (prediction) and object localization problem (correction). The non-convex optimization problem is turned into a proximate problem, which is easier to be optimized <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Object Detection</head><p>By optimizing the min-entropy latent models, we obtain object detectors, which are applied to detect objects from test images. The detection procedure involves feature extraction and object localization <ref type="figure">Fig. 4</ref>. With redundant object proposals extracted by the Selective Search <ref type="bibr" target="#b50">[51]</ref> or the EdgeBox method <ref type="bibr" target="#b51">[52]</ref>, a test image is fed to the feature extraction module, and then a ROI-pooling layer is used to extract features for each proposal. The detector outputs object scores for each proposal and a Non-Maximum Suppression (NMS) procedure is used to remove the overlapped proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>The PASCAL VOC 2007, 2010, 2012 datasets <ref type="bibr" target="#b52">[53]</ref>, the ILSVRC 2013 dataset <ref type="bibr" target="#b53">[54]</ref>, and the MSCOCO 2014 dataset <ref type="bibr" target="#b54">[55]</ref> are used to evaluate the proposed approach. In what follows, the datasets and experimental settings are first described. The evaluation of the model and comparison with the state-of-the-art approaches are then presented.  <ref type="bibr" target="#b0">[1]</ref>, which was used for training and test, respectively. Although it has more training images, the number of images for each object category is much less than that in the VOC datasets. The MSCOCO 2014 dataset contains 80 object categories, with challenging aspects including multiple objects, multiple classes, and small objects. On the PASCAL VOC and ILSVRC 2013 datasets the mean average precision (mAP) is used for evaluation. On the MSCOCO 2014 dataset the mAP under multiple IoUs is used. CNN Models. MELM is implemented with two popular CNN models pre-trained on the ImageNet ILSVRC 2012 dataset. The first CNN model VGG-CNN-F (VGGF for short) <ref type="bibr" target="#b55">[56]</ref> has a similar architechture as the AlexNet <ref type="bibr" target="#b56">[57]</ref> which has 5 convolutional layers and 3 fully connected layers. The second CNN model is VGG16 <ref type="bibr" target="#b57">[58]</ref>, which has 13 convolutional layers and 3 fully connected layers. For these two CNN models, we replaced the spatial pooling layer after the last convolution layer with the ROI-pooling layer as <ref type="bibr" target="#b1">[2]</ref>. The FC8 layer in the two CNN models was removed and the MELM model was added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Object Proposals. The Selective Search <ref type="bibr" target="#b50">[51]</ref> or Edge-Boxes method <ref type="bibr" target="#b51">[52]</ref> was used to extract about 2000 object proposals for each image. As the conventional object detection task, we used the fast setting when generating proposals by Selective Search. We also removed the proposals whose width or height are less than 20 pixels.</p><p>Learning settings. Following <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, the input images were re-sized into 5 scales {480, 576, 688, 864, 1200} with respect to the larger side, height or width. The scale of a training image was randomly selected and each image was randomly flipped. In this way, each test image was augmented into 10 images. For recurrent learning, we employed the SGD algorithm with momentum 0.9, weight decay 5e-4, and batch size 1. The model iterated 20 epochs where the learning rate was 5e-3 for the first 15 epochs and 5e-4 for the last 5 epochs. The output scores of each proposal from the 10 augmented images were averaged. <ref type="figure">Fig. 6</ref> shows that in discovered cliques discriminative objects and object parts were collected and the proposals which lack discriminative information were suppressed. With the proposals about objects and object parts, the global minentropy model could activate object extent during the backpropagation procedure. It can also be seen that the true object in a clique can be precisely localized after the recurrent learning procedure. <ref type="figure">Fig. 7</ref> shows the object cliques from different learning epochs. It can be seen that in the early training stage (Epoch 2), the object clique collected the object extent, i.e, object and object parts. This ensured the object extent activation by the object clique discovery branch. The object localization branch further suppressed the object parts in the object clique (Epoch 4). MELM finally activated the true object extent, suppressed the object part and detected objects accurately (Epoch 20). <ref type="figure" target="#fig_3">Fig. 8a</ref> shows the evolution of global and local entropy, suggesting that our approach optimizes the min-entropy objective during learning. <ref type="figure" target="#fig_3">Fig. 8b</ref> provides the gradient evolution of the FC layers. In the early learning epochs, the gradient of the global min-entropy module was slightly larger than that of the local min-entropy module, suggesting that the network focused on optimizing the image classifiers. As learning proceeded, the gradient of the global minentropy module decreased such that the local min-entropy module dominated the training of the network, indicating that the object detectors were being optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Effect and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Clique Affect</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Randomness Analysis</head><p>To evaluate the effect of min-entropy, the randomness of object locations was evaluated with localization accuracy and localization variance. Localization accuracy was calculated by weighted averaging the overlaps between the ground-truth object boxes and the learned object boxes, by using p(y, h; θ) as the weight. Localization variance was defined as the weighted variance of the overlaps by using p(y, h; θ) as the weight. <ref type="figure" target="#fig_3">Fig. 8c</ref> and <ref type="figure" target="#fig_3">Fig. 8d</ref> show that the proposed MELM had significantly greater localization accuracy and lower localization variance than WSDDN. This strongly indicates that our approach effectively reduces localization randomness during weakly supervised learning.</p><p>Such an effect was further illustrated in <ref type="figure">Fig. 9</ref>, where we compared WSDDN with MELM by the localization accuracy and localization variance during the learning. As shown in <ref type="figure">Fig. 9</ref>, MELM significantly reduced the localization randomness and achieved higher localization accuracy than WSDDN. Take the "bicycle" in <ref type="figure">Fig. 9</ref> for example. In the early training epochs, both WSDDN and MELM failed to localize the objects. In the following training epochs MELM reduced the randomness and achieved high localization accuracy. In contrast, WSDDN switched among object parts and failed to localize the true objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Ablation Experiments</head><p>Baseline. The baseline approach was derived by simplifying Eq. (7) to solely model the global entropy E (X ,Y) (H c , θ). This is similar to WSDDN without the spatial regulariser <ref type="bibr" target="#b21">[22]</ref> where the single learning objective is to minimize the image classification loss. This baseline, referred to as "MELM-base" in <ref type="table" target="#tab_5">Table 1</ref>, achieved 31.5% mAP using the VGGF network.</p><p>Clique Effect. By dividing the object proposals into cliques, the "MELM-base" approach was promoted to "MELM-base+Clique". <ref type="table" target="#tab_5">Table 1</ref> shows that the introduction of proposal cliques improved the detection performance by  <ref type="figure">Fig. 9</ref>. Comparison of the learned object locations by WSDDN <ref type="bibr" target="#b21">[22]</ref> and MELM. The yellow boxes in the first column denote ground-truth objects. The white boxes denote the learned object locations and the blue boxes denote the high-scored proposals. It can be seen that for WSDDN the learned object locations evolved with large randomness, i.e., switch among the proposals around the objects. In contrast the object locations learned by MELM are consistent and have small randomness, which is quantified by the localization variance curves in the last column. (Best viewed in color) 2.4% (from 31.5% to 33.9%). That occurred because using partitioned cliques reduced the solution space of the latent variable learning, thus readily reducing the redundancy of object proposals and facilitating a better solution. We also conducted experiments with different τ values, which controls the clique size as defined in Sec. 3.2.1, and summarized the results in <ref type="table">Table.</ref> 2. Accordingly, we empirically set τ to be 0.7 in other experiments. Min-entropy models. We denoted the min-entropy models by "MELM-D" and "MELM-L" in <ref type="table" target="#tab_5">Table 1</ref>, which respectively corresponded to object clique discovery and object localization. We trained the models by simply cascading the object clique discovery and object localization branches, without using the recurrent learning. <ref type="table" target="#tab_5">Table 1</ref> shows that with VGGF we achieved 33.6% and 36.0% mAP for object clique discovery and object localization branches, which improved the baseline "MELM-base" by 2.1% and 5.5%. For VGG16, "MELM-L" significantly improved the "MELM-base+Clique" from 29.5% to 40.1%, with a 10.6% margin at most. This fully demonstrated that the min-entropy models and their implementation with object clique discovery and object localization branches were pillars of our approach. Recurrent learning. In <ref type="table" target="#tab_5">Table 1</ref>, the recurrent learning algorithms "MELM-D+RL" and "MELM-L+RL", respectively achieved 34.5% and 42.6% mAP, improving the "MELM-L" (without recurrent learning) by 0.5% and 2.4%. When using VGG16, "MELM-D+RL" and "MELM-L+RL" respectively achieved 34.5% and 42.6% mAP, improving the "MELM-L" by 1.9% and 2.5%. These improvements showed that with recurrent learning, <ref type="figure">Fig. 4</ref>, the object clique discovery and object localization branches benefited from each other and thus were mutually enforced.</p><p>Accumulated recurrent learning. The models with accumulated recurrent learning were denoted by "MELM-D+ARL", "MELM-L1+ARL", and "MELM-L2+ARL" in Table 1. In the learning procedure, the high scored proposals were accumulated into the next branch. When using two object localization branches, "MELM-L2-ARL" significantly   improved the mAP of "MELM-L-RL" from 42.6% to 46.4% (+3.8%). It further improved the mAP from 46.4% to 47.3% (+0.9%) when using three branches, but did not significantly improve when using four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance and Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">PASCAL VOC datasets</head><p>Weakly Supervised Object Detection.   <ref type="bibr" target="#b28">[29]</ref> train/val 39.0 WCCN <ref type="bibr" target="#b27">[28]</ref> trainval/test 37.9 OICR <ref type="bibr" target="#b26">[27]</ref> trainval/test 37.9 Self-Taught <ref type="bibr" target="#b28">[29]</ref> trainval 42.0% mAP). To further improve the detection performance, we re-trained a Fast-RCNN detector using learned pseudo objects and a ResNet-101 network, and achieved 49.0% mAP. <ref type="table" target="#tab_10">Table 5</ref> compared the detection performance of MELM with the state-of the-art approaches on the VOC 2010 and Specifically, the detection performance for "bicycle" (+4.5%), "cow" (+8.5%), "dining-table" (+14.7%), "dog" (+9.6%) significantly improved, which shows the general effectiveness of MELM Despite of the average good performance, our approach failed on the "person" class, as shown in the last image of <ref type="figure" target="#fig_0">Fig. 10(a)</ref>. "Person" is one of the most challenging class as people often involve great appearance variance from clothes, poses, and occlusions. Furthermore, the definition for ??person?? is not consistent. A "person" could be defined as a pedestrian, a head-and-shoulder, or just a human face. Given such ambiguous definition, what the algorithm can do is to localize the most discriminative part of a "person", e.g., the face. We also note that although the performance of "person" decreased, the average performance for all class significantly increased.</p><p>For the object classes with large appearance variance, we observed that the algorithm correctly classified the object regions but often failed to precisely localize them, i.e., the IoU between the learned bounding boxes and the groundtruth is smaller than 0.5. When using the "pointing localization" metric <ref type="bibr" target="#b36">[37]</ref>, the "person" class achieved 97.1% localization accuracy, which shows potential to practical applications. <ref type="figure" target="#fig_0">Fig. 10</ref> shows some of the detection examples. It can be seen that MELM precisely localize objects from clutter background and correctly localized multiple object regions in a single image.</p><p>Weakly Supervised Object Localization. The Correct Localization (CorLoc) metric <ref type="bibr" target="#b17">[18]</ref> was employed to evaluate the localization accuracy. CorLoc is the percentage of images for which the region of highest object score has at least 0.5 interaction-over-union (IoU) with the ground-truth object region. This experiment was done on the trainval set because the region selection exclusively worked in the training process. It can be seen in <ref type="table" target="#tab_8">Table 4</ref> that with VGGF model, the mean CorLoc of MELM respectively outperformed the stateof-the-art WSDDN <ref type="bibr" target="#b21">[22]</ref> and WCCN <ref type="bibr" target="#b27">[28]</ref>   <ref type="bibr" target="#b55">56</ref>.7%). Noticeably, on the "bus", "car", "chair", and "table" classes, MELM outperformed the compared state-of-the-art methods up to 7∼15%. This shows that the clique-based min-entropy strategy is more effective than the image segmentation strategy used in WCCN.</p><p>Image Classification. The object clique discovery and object localization components highlighted informative regions and suppressed disturbing backgrounds, which also benefited image classification. As shown in Tab. 7, with the VGGF model, MELM achieved 87.8% mAP. With the VGG16 model, MELM achieved 93.1% mAP, which respectively outperformed WSDDN <ref type="bibr" target="#b21">[22]</ref> and WCCN <ref type="bibr" target="#b27">[28]</ref> up to 3.4% (93.1% vs. 89.7%) and 2.2% (93.1% vs. 90.9%). It is noteworthy that MELM outperformed the VGG16 network, specifically trained for image classification, by 3.8% mAP (93.1% vs. 89.3%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Large-scale datasets</head><p>On the ILSVRC2013 dataset with 200 object classes, <ref type="table" target="#tab_10">Table  5</ref>, MELM with VGGF outperformed the WCCN approach by 3.6% (13.4% vs. 9.8%). On the MS COCO 2014 dataset, we evaluated the image classification, pointing localization, and object detection performance and compared it with the state-of-the-arts. The evaluation metrics for image classification included macro/micro precision (P-C and P-O), macro/micro recall (R-C and R-O), macro/micro F1measure (F1-C and F1-O) <ref type="bibr" target="#b63">[64]</ref>. It can be seen in <ref type="table">Table.</ref> 6 that for image classification MELM outperformed SPN <ref type="bibr" target="#b36">[37]</ref> by 23.1% (79.1% vs. 56%). For pointing localization, MELM outperformed SPN by 9.8% (65.1% vs. 55.3%). For object detection, MELM outperformed WSDDN. With these experiments, we set new baselines for weakly supervised object detection on large-scale datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we proposed an effective deep min-entropy latent model (MELM) for weakly supervised object detection (WSOD). MELM was deployed as three components of clique partition, object clique discovery, and object localization, and was unified with the deep learning framework in an integrated manner. By partitioning and discovering cliques, MELM provided a new way to learn latent object regions from redundant object proposals. With the minentropy principle, it can principally reduce the variance of positive instances and alleviate the ambiguity of detectors. With the recurrent learning algorithm, MELM improved the performance of weakly supervised detection, weakly supervised localization, and image classification, in striking contrast with state-of-the-art approaches. The underlying reality is that min-entropy results in minimum randomness of an information system and the recurrent learning takes advantages of continuation optimization, which provides fresh insights for weakly supervised learning problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>For succinct representation, we denote E (X ,Y) (H c , θ), E (X ,Y,Hc) (h, θ), L (X ,Y) (H c , θ), and L (X ,Y,Hc) (h, θ) as E (H c , θ), E(h, θ), L (H c , θ), and L (h, θ), respectively.</p><p>Derivation for object clique discovery. Given the object score s (y, h; θ) as the input of the entropy models, its gradient can be computed as </p><p>where the partial derivation of E (H c , θ) with respect to p (y, h; θ) is computed as  </p><p>The partial derivation of L(h, θ) with respect to s (y, h; θ) is calculated with Eq. (18) and Eq. <ref type="bibr" target="#b18">(19)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Evolution of object locations during learning. Blue boxes denote proposals of high object probability and white ones detected objects. It can be seen that our approach reduces localization randomness and learns object extent. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Flowchart of (a) the recurrent learning algorithm and (b) unfolded accumulated recurrent learning algorithm. The solid lines denote network connections and dotted lines denote forward-only connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Visualization of the clique partition, object clique discovery, and object localization results. (a) Bounding boxes of different colors denote proposals from different cliques. (b) Score maps of cliques and objects. (Best viewed in color) Evolution of cliques. (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .</head><label>8</label><figDesc>Gradient, entropy, and localization on the PASCAL VOC 2007 trainval set. (a) The evolution of entropy. (b) The evolution of gradient. (c) Localization accuracy. (d) Localization variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Object detection examples on the PASCAL VOC 2012 and MS COCO 2014 datasets. Yellow bounding boxes denote ground-truth annotations, green boxes correct detection results and red boxes false detection results. (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>∂E</head><label></label><figDesc>(H c , θ) ∂p (y , h ; θ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>18 )</head><label>18</label><figDesc>where H is the clique including h . The partial derivation of p (y, h; θ) with respect to s (y, h; θ) is computed as∂p (y , h ; θ) ∂s (y, h; θ) = −s (y , h ; θ) s (y, h; θ) , h = h or y = y , s (y , h ; θ) − s(y, h; θ) 2 ,otherwise. (Derivation for object localization. In Eq. (11), the term w h p (y, h; θ) is used as a pseudo label for h, which does not back-propagate gradients. Therefore, the derivation for object localization can be simply computed as ∂L(h, θ) ∂s (y, h; θ) = y ,h ∂L(h, θ) ∂p (y , h ; θ) ∂p (y , h ; θ) ∂s (y, h; θ) = y ,h ∈{H * 1 ,H * 2 ,...} w h ∂p (y , h ; θ) ∂s (y, h; θ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>FC(4096) CONVs with ROI Pooling Object Score Region Proposals Input Image Element-wise Multiplication Network Connection Forward-only Connection Local Min-Entropy Object Detection Loss Softmax Image Classification Loss Global Min-Entropy Image Label Object Clique Discovery Object Localization</head><label></label><figDesc></figDesc><table><row><cell>FC(4096)</cell><cell>FC(N)</cell><cell>M1</cell><cell>Clique Partition</cell><cell>M2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Object Probability</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC(N+1)</cell><cell>M3</cell><cell>Pseudo Object Label</cell></row><row><cell>M1</cell><cell></cell><cell></cell><cell>M2</cell><cell></cell><cell>M3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1 Recurrent Learning Input:</head><label></label><figDesc>Image x ∈ X , image label y ∈ Y, and object proposals</figDesc><table /><note>h ∈ H Output: Network parameters θ 1: Initialize object score s (h) = s(y, h; θ) = 1 for all h 2: for i = 1 to M axIter do3: φh ← Compute deep features for all h through forward score4: φh ← φ h · s(h) Aggregate features by object score5:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Image CNN Layers Object Clique Discovery Object Localization Image Classifier Min-Entropy Latent Model (MELM) Object Score Object Detector Clique Partition Object Probability (a) Image CNN Layers Object Clique Discovery Object Localization Image Classifier Object Localization ... Image CNN Layers Object Clique Discovery Object Localization Image Classifier Object Localization Object Score Object Probability Object Score Object Score Object Detector Object Detector Object Probability ... ... ... Clique Partition Clique Partition</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Datasets. The VOC datasets have 20 object categories. The VOC 2007 datasets contains 9963 images which are divided into three subsets: 5011 for train and val, and 4952 for test. The VOC 2010 dataset contains 19740 images of which 10103 for train and val, and 9637 for test. The VOC 2012 dataset contains 22531 images which are divided into three subsets: 11540 for train and val, and 10991 for test. The ILSVRC 2013 detection dataset is more challenging for object detection as it has 200 object categories, containing 464278 images where 424126 image for train and val, and 40152 images for test. For comparison with the previous works, we split the val set of ILSVRC 2013 detection dataset into val1 and val2 as in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 1</head><label>1</label><figDesc>Detection mean average precision (%) on the PASCAL VOC 2007 test set. Ablation experimental results of MELM.</figDesc><table><row><cell>CNN</cell><cell>Method</cell><cell>mAP</cell></row><row><cell></cell><cell>MELM-base</cell><cell>31.5</cell></row><row><cell></cell><cell>MELM-base+Clique</cell><cell>33.9</cell></row><row><cell>VGGF</cell><cell>MELM-D MELM-L</cell><cell>33.6 36.0</cell></row><row><cell></cell><cell>MELM-D+RL</cell><cell>34.1</cell></row><row><cell></cell><cell>MELM-L+RL</cell><cell>38.4</cell></row><row><cell></cell><cell>MELM-base+Clique</cell><cell>29.5</cell></row><row><cell></cell><cell>MELM-D</cell><cell>32.6</cell></row><row><cell></cell><cell>MELM-L</cell><cell>40.1</cell></row><row><cell>VGG16</cell><cell>MELM-D+RL MELM-L+RL</cell><cell>34.5 42.6</cell></row><row><cell></cell><cell>MELM-D+ARL</cell><cell>37.4</cell></row><row><cell></cell><cell>MELM-L1+ARL</cell><cell>46.4</cell></row><row><cell></cell><cell>MELM-L2+ARL</cell><cell>47.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2</head><label>2</label><figDesc>Detection mean average precision (%) on the PASCAL VOC 2007 val set. Performance with different clique sizes (controlled by τ ) of MELM.</figDesc><table><row><cell>τ</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>1</cell></row><row><cell>mAP</cell><cell>32.6</cell><cell>34.3</cell><cell>34.4</cell><cell>35.3</cell><cell>33.5</cell><cell>34.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3</head><label>3</label><figDesc>Detection mean average precision (%) on the PASCAL VOC 2007 test set. Comparison of MELM to the state-of-the-arts. Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP 41.0 45.0 19.1 1.0 34.0 16.0 21.3 32.5 43.4 21.9 19.7 21.5 22.3 36.0 18.0 25.4 Multi-fold MIL [11] 39.3 43.0 28.8 20.4 8.0 45.5 47.9 22.1 8.4 33.5 23.6 29.2 38.5 47.9 20.3 20.0 35.8 30.8 41.0 20.1 30.2 PDA [23] 49.7 33.6 30.8 19.9 13.0 40.5 54.3 37.4 14.8 39.8 9.4 28.8 38.1 49.8 14.5 24.0 27.1 12.1 42.3 39.7 31.0 LCL+Context [16] 48.9 42.3 26.1 11.3 11.9 41.3 40.9 34.7 10.8 34.7 18.8 34.4 35.4 52.7 19.1 17.4 35.9 33.3 34.8 46.5 31.</figDesc><table><row><cell>CNN</cell><cell></cell><cell></cell></row><row><cell>MILinear [25]</cell><cell>41.3 39.7 22.1 9.5</cell><cell>3.9</cell></row><row><cell>VGGF/</cell><cell></cell><cell></cell></row><row><cell>AlexNet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4</head><label>4</label><figDesc>Correct localization rate (%) on the PASCAL VOC 2007 trainval set. Comparison of MELM to the state-of-the-arts.</figDesc><table><row><cell>CNN</cell><cell>Method</cell><cell>mAP</cell></row><row><cell></cell><cell>MILinear [25]</cell><cell>43.9</cell></row><row><cell></cell><cell>LCL+Context [16]</cell><cell>48.5</cell></row><row><cell></cell><cell>PDA [23]</cell><cell>49.8</cell></row><row><cell>VGGF/</cell><cell>WCCN [28]</cell><cell>52.6</cell></row><row><cell>AlexNet</cell><cell>Multi-fold MIL [11]</cell><cell>54.2</cell></row><row><cell></cell><cell>WSDDN [22]</cell><cell>54.2</cell></row><row><cell></cell><cell>ContextNet [24]</cell><cell>55.1</cell></row><row><cell></cell><cell>MELM</cell><cell>58.4</cell></row><row><cell></cell><cell>PDA [23]</cell><cell>52.4</cell></row><row><cell>VGG16</cell><cell>WSDDN [22] WCCN [28]</cell><cell>53.5 56.7</cell></row><row><cell></cell><cell>MELM</cell><cell>61.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>compared</cell></row></table><note>challenging WSOD task. MELM using multiple networks (MELM-Ens.) outperformed OICR-Ens. (47.8% mAP vs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5</head><label>5</label><figDesc>Detection mean average precision (%) on the PASCAL VOC 2010, 2012, and the ILSVRC 2013 datasets. Comparison of MELM to the state-of-the-arts.</figDesc><table><row><cell>Dataset</cell><cell>CNN</cell><cell>Method</cell><cell>Dataset Splitting</cell><cell>mAP</cell></row><row><cell></cell><cell></cell><cell>PDA [23]</cell><cell>train/val</cell><cell>21.4</cell></row><row><cell></cell><cell>VGGF/</cell><cell>WCCN [28]</cell><cell>trainval/test</cell><cell>28.8</cell></row><row><cell>PASCAL VOC 2010</cell><cell>AlexNet VGG16</cell><cell>MELM MELM PDA [23] WCCN [28] MELM</cell><cell>train/val trainval/test train/val trainval/test train/val</cell><cell>35.6 36.3 30.7 39.5 37.1</cell></row><row><cell></cell><cell></cell><cell>MELM</cell><cell>trainval/test</cell><cell>39.9</cell></row><row><cell></cell><cell></cell><cell>PDA [23]</cell><cell>train/val</cell><cell>22.4</cell></row><row><cell></cell><cell></cell><cell>MILinear [25]</cell><cell>train/val</cell><cell>23.8</cell></row><row><cell></cell><cell>VGGF/ AlexNet</cell><cell>WCCN [28] ContextNet [24] OICR-VGGM [27]</cell><cell>trainval/test trainval/test trainval/test</cell><cell>28.4 35.3 34.6</cell></row><row><cell></cell><cell></cell><cell>MELM</cell><cell>train/val</cell><cell>36.2</cell></row><row><cell>PASCAL</cell><cell></cell><cell>MELM</cell><cell>trainval/test</cell><cell>36.4</cell></row><row><cell>VOC</cell><cell></cell><cell>PDA [23]</cell><cell>train/val</cell><cell>29.1</cell></row><row><cell>2012</cell><cell></cell><cell>Self-Taught</cell><cell></cell><cell></cell></row><row><cell></cell><cell>VGG16</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 6</head><label>6</label><figDesc>Detection and localization performance (%) on MSCOCO 2014. Comparison of MELM to the state-of-the-arts. OICR [27], and TS 2 C [59] by 4.5% (42.4% vs. 37.9%), 4.1% (42.4% vs. 38.3%), 4.5% (42.4% vs. 37.9%) and 2.4% (42.4% vs. 40.0%).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Image Classification</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>mAP</cell><cell>F1-C</cell><cell>P-C</cell><cell>R-C</cell><cell>F1-O</cell><cell>P-O</cell><cell>R-O</cell></row><row><cell>CAM [61]</cell><cell></cell><cell>54.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SPN [37]</cell><cell></cell><cell>56.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ResNet-101 [62]</cell><cell>75.2</cell><cell>69.5</cell><cell>80.8</cell><cell>63.4</cell><cell>74.4</cell><cell>82.2</cell><cell>68.0</cell></row><row><cell cols="2">MELM-VGG16</cell><cell>79.1</cell><cell>72.0</cell><cell>79.3</cell><cell>68.6</cell><cell>76.8</cell><cell>82.5</cell><cell>71.9</cell></row><row><cell></cell><cell cols="6">Pointing Localization (with class prediction)</cell><cell></cell></row><row><cell cols="3">Method WeakSup [34]</cell><cell cols="2">Pronet [63]</cell><cell>DFM [42]</cell><cell cols="2">SPN [37]</cell><cell>MELM</cell></row><row><cell>mAP</cell><cell>41.2</cell><cell></cell><cell>43.5</cell><cell></cell><cell>49.2</cell><cell>55.3</cell><cell></cell><cell>65.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Object Detection</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>CNN</cell><cell></cell><cell>mAP@.5</cell><cell cols="3">mAP@[.5,.95]</cell></row><row><cell>WSDDN [22]</cell><cell></cell><cell cols="2">VGGF</cell><cell></cell><cell>10.1</cell><cell></cell><cell cols="2">3.1</cell></row><row><cell>MELM</cell><cell></cell><cell cols="2">VGGF VGG16</cell><cell></cell><cell>11.9 18.8</cell><cell></cell><cell cols="2">4.1 7.8</cell></row><row><cell cols="9">VOC 2012 datasets. It can be seen that MELM usually</cell></row><row><cell cols="9">outperformed the state-of-the-art approaches. On the VOC</cell></row><row><cell cols="9">2010 dataset, MELM with VGGF significantly outperformed</cell></row><row><cell cols="9">WCCN [28] by 7.5% (36.3% vs. 28.8%) with a VGGF model,</cell></row><row><cell cols="9">and was comparable to it with a VGG16 model. On the</cell></row><row><cell cols="9">VOC2012 dataset, with a VGGF model, MELM respectively</cell></row><row><cell cols="9">outperformed WCCN [28] and OICR [27] by 8.0% ( 36.4%</cell></row><row><cell cols="9">vs. 28.4%) and 1.8% (36.4% vs. 34.6%). With a VGG16 model,</cell></row><row><cell cols="9">MELM respectively outperformed WCCN [28], Self-Taught</cell></row><row><cell>[29],</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 7</head><label>7</label><figDesc>Image classification mAP (%) on the PASCAL VOC 2007 test set. Comparison of MELM to the state-of-the-arts.</figDesc><table><row><cell>CNN</cell><cell>Method</cell><cell>mAP</cell></row><row><cell></cell><cell>MILinear [25]</cell><cell>72.0</cell></row><row><cell>VGGF/ AlexNet</cell><cell>AlexNet [57] WSDDN [22] WCCN [28]</cell><cell>82.4 85.3 87.8</cell></row><row><cell></cell><cell>MELM</cell><cell>87.8</cell></row><row><cell></cell><cell>VGG16 [58]</cell><cell>89.3</cell></row><row><cell>VGG16</cell><cell>WSDDN [22] WCCN [28]</cell><cell>89.7 90.9</cell></row><row><cell></cell><cell>MELM</cell><cell>93.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>by 4.2% (58.4% vs. 54.2%) and 5.8% (58.4% vs. 52.6%). With the VGG16 model, it respectively outperformed the state-of-the-art WSDDN [22] and WCCN [28] by 7.9% (61.4% vs. 53.5%) and 4.7% (61.4% vs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>∂L(H c , θ) ∂s (y, h; θ) = − p (y , h ; θ) ∂p (y , h ; θ) ∂s (y, h; θ) ,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">∂L(H c , θ)</cell><cell>∂p (y , h ; θ)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>y ,h</cell><cell cols="3">∂p (y , h ; θ)</cell><cell>∂s (y, h; θ)</cell></row><row><cell>=</cell><cell>y ,h</cell><cell>y</cell><cell cols="2">∂E (H c , θ) ∂p (y , h ; θ)</cell><cell>+</cell><cell>1</cell><cell>y − 1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the NSFC under Grant 61836012, 61671427, and 61771447, and Beijing Municipal Science and Technology Commission under Grant Z181100008918014. Qixiang Ye is the corresponding author.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagannath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From bow to cnn: Two decades of texture representation for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02165</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scene recognition and weakly supervised object localization with deformable part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Megha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svetlana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1307" to="1314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-fold mil training for weakly supervised object lcalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Gokberk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cordelia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. Workshop</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2409" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="203" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brit. Mach. Vis. Conf. (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1997" to="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Jae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stefanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tinne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1081" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiqiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieniu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Largescale weakly supervised object localization via latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiqiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Junge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1371" to="1385" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stefanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>31st Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1611" to="1619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bogdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vittorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised object recognition and localization with invariant high order features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yimeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tsuhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brit. Mach. Vis. Conf. (BMVC)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parthipan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="343" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detector discovery in the wild: Joint multiple instance and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Judy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="797" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Andrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shengjin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Hsuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vadim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maxime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Minsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised large scale object localization with multiple instance learning and bag splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiqiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dacheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieniu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="405" to="416" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Selflearning scene-specific pedestrian detectors using a progressive latent model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2057" to="2066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3059" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5131" to="5139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4294" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image colocalization by mimicking a good detectors confidence score distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Min-entropy latent model for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1297" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relaxed multiple-instance svm with application to object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1224" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3460" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Is object localization for free? weakly supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization using size estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="105" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Soft proposal networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization using things and stuff transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y. Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-phase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weakly supervised localization using deep feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="714" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pcl: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SRN: side-output residual network for object symmetry detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Charakterisierung der entropien positiver ordnung und der shannonschen entropie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aczél</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Daróczy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mathematica Academiae Scientiarum Hungarica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="95" to="121" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Entropy-based latent structured output prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Label refinery: Improving imagenet classification through label progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02641</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Allgower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Georg</surname></persName>
		</author>
		<title level="m">Numerical Continuation Methods</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Mach. Learn</title>
		<meeting>ACM Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mollifying networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Repres</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">J</forename><surname>Rr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Sande Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Theo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Wm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Piotr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Ki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Andrew</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Ts2c:tight box mining with surrounding segmentation context for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="434" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pronet: Learning to propose object-specific boxes for cascaded neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3485" to="3493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">A unified view of multi-label performance measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00288</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
