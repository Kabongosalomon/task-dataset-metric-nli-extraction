<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VinVL: Revisiting Visual Representations in Vision-Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-11">March 11, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥♠</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
						</author>
						<title level="a" type="main">VinVL: Revisiting Visual Representations in Vision-Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-11">March 11, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model OSCAR <ref type="bibr" target="#b20">[21]</ref>, and utilize an improved approach OSCAR+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. Code, models and pre-extracted features are released at https://github.com/pzzhang/VinVL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>♥ Microsoft Corporation</head><p>♠ University of Washington † indicates equal contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vision language pre-training (VLP) has proved effective for a wide range of vision-language (VL) tasks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b20">21]</ref>. VLP typically consists of two stages: <ref type="bibr" target="#b0">(1)</ref> an object detection model is pre-trained to encode an image and the visual objects in the image to feature vectors, and (2) a crossmodal fusion model is pre-trained to blend text and visual features. While existing VLP research focuses mainly on improving the cross-modal fusion model, this paper focuses on improving the object-centric visual representations and presents a comprehensive empirical study to demonstrate that visual features matter in VL models.</p><p>Among the aforementioned work, a widely-used object detection (OD) model <ref type="bibr" target="#b1">[2]</ref> is trained on the Visual Genome dataset <ref type="bibr" target="#b15">[16]</ref>. The OD model provides an object-centric representation of images, and has been used in many VL models as a black box. In this work, we pre-train a large-scale object-attribute detection model based on the ResNeXt-152 C4 architecture (short as X152-C4). Compared to the OD model of <ref type="bibr" target="#b1">[2]</ref>, the new model is better-designed for VL tasks, and is bigger and trained on much larger amounts of data, combining multiple public object detection datasets, including COCO <ref type="bibr" target="#b24">[25]</ref>, OpenImages (OI) <ref type="bibr" target="#b16">[17]</ref>, Objects365 <ref type="bibr" target="#b30">[31]</ref>   <ref type="table">Table 1</ref>: Uniform improvements on seven VL tasks by replacing visual features from Anderson et al. <ref type="bibr" target="#b1">[2]</ref> with ours. The NoCaps baseline is from VIVO <ref type="bibr" target="#b8">[9]</ref>, and our results are obtained by directly replacing the visual features. The baselines for rest tasks are from OSCAR <ref type="bibr" target="#b20">[21]</ref>, and our results are obtained by replacing the visual features and performing OSCAR+ pre-training. All models are BERT-Base size. As analyzed in Section 5.2, the new visual features contributes 95% of the improvement. <ref type="figure">Figure 1</ref>: Predictions from an X152-FPN model trained on OpenImages (Left) and our X152-C4 model trained on four public object detection datasets (Right). Our model contains much richer semantics, such as richer visual concepts and attribute information, and the detected bounding boxes cover nearly all semantically meaningful regions. Compared with those from the common object classes in typical OD models (Left), the rich and diverse region features from our model (Right) are crucial for vision-language tasks. For concepts detected by both models, e.g., "boy", attributes from our model offer richer information, e.g., "young barefoot shirtless standing surfing smiling little playing looking blond boy". There are object concepts that are detected by our model but not by the Open-Images model, including fin, wave, foot, shadow, sky, hair, mountain, water, (bare, tan, light, beige) back, (blue, colorful, floral, multi colored, patterned) trunk, sand, beach, ocean, (yellow, gold) bracelet, logo, hill, head, (black, wet) swim trunks, black, wet swim trunks. Compared to the R101-C4 model of <ref type="bibr" target="#b1">[2]</ref>, our model produces more accurate object-attribute detection results and better visual features for VL applications; see Appendix A for the full pictures and predictions from <ref type="bibr" target="#b1">[2]</ref>.</p><p>and Visual Genome (VG) <ref type="bibr" target="#b15">[16]</ref>. As a result, our OD model achieves much better results on a wide range of VL tasks, as shown in <ref type="table">Table 1</ref>. Compared to other typical OD models, such as X152-FPN trained on OpenImages, our new model can encode a more diverse collection of visual objects and concepts (e.g., producing visual representations for 1848 object categories and 524 attribute categories), as illustrated by an example in <ref type="figure">Figure 1</ref>.</p><p>To validate the effectiveness of the new OD model, we pre-train a Transformer-based cross-modal fusion model OSCAR+ <ref type="bibr" target="#b20">[21]</ref> on a public dataset consisting of 8.85 million text-image pairs, where the visual representations of these images are produced by the new OD model and are fixed during OSCAR+ pre-training.</p><p>We then fine-tune the pre-trained OSCAR+ for a wide range of downstream tasks, including VL understanding tasks such as VQA <ref type="bibr" target="#b7">[8]</ref>, GQA <ref type="bibr" target="#b12">[13]</ref>, NLVR2 <ref type="bibr" target="#b34">[35]</ref>, and COCO text-image retrieval <ref type="bibr" target="#b24">[25]</ref>, and VL generation tasks such as COCO image captioning <ref type="bibr" target="#b24">[25]</ref> and NoCaps <ref type="bibr" target="#b0">[1]</ref>. Our results show that the object-centric representations produced by the new OD model significantly improve the performance across all the VL tasks, often by a large margin over strong baselines using the classical OD model <ref type="bibr" target="#b1">[2]</ref>, creating new state of the arts on all these tasks, including GQA on which none of the published pre-trained models has surpassed the deliberately designed neural state machine (NSM) <ref type="bibr" target="#b11">[12]</ref>. We will release the new OD model to the research community.</p><p>The main contributions of this work can be summarized as follows: (i) We present a comprehensive empirical study to demonstrate that visual features matter in VL models. (ii) We have developed a new object detection model that can produce better visual features of images than the classical OD model <ref type="bibr" target="#b1">[2]</ref> and substantially uplifts the state-of-the-art results on all major VL tasks across multiple public benchmarks. (iii) We provide a detailed ablation study of our pre-trained object detection model to investigate the relative contribution to the performance improvement due to different design choices regarding diversity of object categories, visual attribute training, training data scale, model size, and model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Improving Vision (V) in Vision Language (VL)</head><p>Deep learning-based VL models typically consist of two modules: an image understanding module Vision and a cross-modal understanding module VL:</p><formula xml:id="formula_0">(q, v) = Vision(Img), y = VL(w, q, v),<label>(1)</label></formula><p>where Img and w are the inputs of the vision and language modalities, respectively. The output of the Vision module consists of q and v. q is the semantic representation of the image, such as tags or detected objects, and v the distributional representation of the image in a high-dimensional latent space represented using e.g., the box or region 1 features produced by a VG-pre-trained Faster-RCNN model <ref type="bibr" target="#b1">[2]</ref>. Most VL models use only the visual features v, while the recently proposed OSCAR <ref type="bibr" target="#b20">[21]</ref> model shows that q can serve as anchors for learning better vision-language joint representations and and thus can improve the performance on various VL tasks. w and y of the VL module of Equation (1) vary among different VL tasks. In VQA, w is a question and y is an answer to be predicted. In text-image retrieval, w is a sentence and y is the matching score of a sentence-image pair. In image captioning, w is not given and y is a caption to be generated. Inspired by the great success of pre-trained language models to various natural language processing tasks, vision-language pre-training (VLP) has achieved remarkable success in improving the performance of the cross-modal understanding module VL by (1) unifying vision and language modeling VL with Transformer and (2) pre-training the unified VL with large-scale text-image corpora. However, most recent works on VLP treat the image understanding module Vision as a black box and leave the visual feature improvement untouched since the development of the classical OD model <ref type="bibr" target="#b1">[2]</ref> three years ago, despite that there has been much research progress on improving object detection by 1) developing much more diverse, richer, and larger training datasets (e.g. OpenImages and Objects 365), 2) gaining new insights in object detection algorithms such as feature pyramid network <ref type="bibr" target="#b22">[23]</ref>, one-stage dense prediction <ref type="bibr" target="#b23">[24]</ref>, and anchor-free detectors <ref type="bibr" target="#b36">[37]</ref>, and 3) leveraging more powerful GPUs for training bigger models.</p><p>In this work, we focus on improving Vision for better visual representations. We developed a new OD model by enriching the visual object and attribute categories, enlarging the model size and training on a much larger OD dasetset, and thus advanced the state of the arts on a wide range of VL tasks. We detail how the new OD model is developed in the rest of this section and then describe the use of OSCAR+ for VL pre-training in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Object Detection Pre-training</head><p>To improve the OD model for VL tasks, we utilize four public object detection datasets. As most datasets do not have attribute annotations, we adopt a pre-training and fine-tuning strategy to build our OD model. We first pre-train an OD model on a large-scale corpus consisting of four public datasets, and then fine-tune the model with an additional attribute branch on Visual Genome, making it capable of detecting both objects and attributes.</p><p>Data. <ref type="table" target="#tab_2">Table 2</ref> summarizes the statistics of the four public datasets used in our object detection pre-training, including COCO, OpenImagesV5 (OI), Objects365V1, and Visual Genome (VG). These datasets have complementary characters, and are extremely unbalanced in terms of data size, object vocabulary, and the number of annotations in each class. For example, the VG dataset has a rich and diverse set of annotations for both objects and their attributes with an open vocabulary. But its annotations are noisy and suffer from the missing-annotation problem. The COCO dataset, on the other hand, is very well annotated. But the coverage of visual objects and attributes is much lower than that in VG although we use both its 80 object classes and 91 stuff classes to include as diverse visual concepts as possible. We take the following steps to build a unified corpus by combining the four datasets.</p><p>1. First of all, to enhance visual concepts of tail classes, we perform class-aware sampling for Open-Images and Objects365 to get at least 2000 instances per class, resulting in 2.2M and 0.8M images, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>To balance the contribution of each dataset, we merge the four datasets with 8 copies of COCO (8×0.11M), 8 copies of VG (8×0.1M), 2 copies of class-aware sampled Objects365 (2×0.8M) and one copy of the class-aware sampled OpenImages (2.2M).</p><p>3. To unify their object vocabularies, we use the VG vocabulary and its object aliases as the base vocabulary, merge a class from the other three datasets into a VG class if their class names or aliases match, and add a new class if no match is found.  Model Architecture (FPN vs C4). Although <ref type="bibr" target="#b22">[23]</ref> shows that the FPN model outperforms the C4 model for object detection, recent studies <ref type="bibr" target="#b13">[14]</ref> demonstrate that FPN does not provide more effective region features for VL tasks than C4, which is also confirmed by our experimental results 2 . We thus conduct a set of carefully designed experiments, as to be detailed in Appendix E, and find two main reasons for this. The first is that all layers in the C4 model used for region feature extraction are pre-trained using the ImageNet dataset while the multi-layer-perceptron (MLP) head of the FPN model are not. It turns out that the VG dataset is still too small to train a good enough visual features for VL tasks and using ImageNet-pre-trained weights is beneficial. The second is due to the different network architectures (CNN vs. MLP). The convolutional head used in C4 has a better inductive bias for encoding visual information than the MLP head of FPN. Therefore, in this study we use C4 architecture for VLP.</p><p>Model Pre-Training. Following the common practice in object detection training, we freeze the first convolution layer, the first residual block, and all the batch-norm layers. We also use several data augmentation methods, including horizontal flipping and multi-scale training. To train a detection model with the X152-C4 architecture, we initialize the model backbone from an ImageNet-5K checkpoint <ref type="bibr" target="#b39">[40]</ref> and train for 1.8M iterations with a batch size of 16 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Injecting attribute information into the model</head><p>Following <ref type="bibr" target="#b1">[2]</ref>, we add an attribute branch to the pre-trained OD model, and then fine-tune the OD model on VG to inject attribute information (524 classes). Since the object representations are pre-trained in the object detection pre-training stage, we can focus the VG fine-tuning on learning attributes by picking a much larger attribute loss weight 1.25, compared to 0.5 used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>. Thus, our fine-tuned model significantly outperforms previous models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> in detecting objects and attributes on VG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Efficient region feature extractor for VL tasks</head><p>With a richer set of visual objects and attributes, the classical class-aware non-maximal suppression (NMS) post-processing takes a significantly larger amount of time to remove overlapped bounding boxes, making the feature extraction process extremely slow. To improve the efficiency, we replace the class-aware NMS with the class-agnostic NMS that only conducts the NMS operation once 3 . We also replace the timeconsuming conv layers with dilation=2 used in <ref type="bibr" target="#b1">[2]</ref> with conv layers without dilation. These two replacements make the region feature extraction process much faster than that in <ref type="bibr" target="#b1">[2]</ref> without any accuracy drop on VL downstream tasks. We report the end-to-end inference time of VL models with different vision models on a Titan-X GPU and a CPU with a single thread in <ref type="table" target="#tab_2">Table 21</ref> in Appendix F. In summary, the pre-trained OD model serves as the image understanding module, as in Equation <ref type="formula" target="#formula_0">(1)</ref>, to produce vision presentations (q, v) for downstream VL tasks. Here, q is the set of detected object names (in text) and v is the set of region features. Each region feature is denoted as (v, z), wherev is a P -dimensional representation from the input of the last linear classification layer of the detection head ( i.e., P = 2048) and z is a R-dimensional position encoding of the region (i.e., R = 6) 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OSCAR+ Pre-training</head><p>The success of VLP lies in the use of a unifying model architecture for a wide range of VL tasks and the large-scale pre-training of the unified model using objectives that correlate with the performance metrics of these downstream VL tasks. In this study we pre-train an improved version of OSCAR <ref type="bibr" target="#b20">[21]</ref>, known as OSCAR+ models, to learn the joint image-text representations using image tags as anchors for image-text alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-training corpus</head><p>We build our pre-training corpus based on three types of existing vision and VL datasets: (1) image captioning datasets with human-annotated captions as w and machine-generated 5 image tags as q, including COCO <ref type="bibr" target="#b24">[25]</ref>, Conceptual Captions (CC) <ref type="bibr" target="#b31">[32]</ref>, SBU captions <ref type="bibr" target="#b27">[28]</ref> and flicker30k <ref type="bibr" target="#b41">[42]</ref>; <ref type="bibr" target="#b1">(2)</ref> visual QA datasets with questions as w and human-annotated answers as q, including GQA <ref type="bibr" target="#b12">[13]</ref>, VQA <ref type="bibr" target="#b7">[8]</ref> and VG-QAs; (3) image tagging datasets with machine-generated 6 captions as w and human-annotated tags as q, including a subset of OpenImages (1.67M images). In total, the corpus contains 5.65 million unique images, 8.85 million text-tag-image triples. The detailed statistics are presented in <ref type="table" target="#tab_11">Table 17</ref> in the Appendix. The size of the pre-training corpus could have been significantly increased by combining large-scale image tagging datasets, such as the full set of OpenImages (9M images) and YFCC (92M images). We leave it to future work to leverage much larger corpora for model pre-training.   <ref type="formula" target="#formula_0">(1)</ref> ). COCO-IR metric is Image-to-Text retrieval R@1 at COCO 1K test set. Blue indicates the best result for a task and Black indicates the runner-up.</p><formula xml:id="formula_1">Loss (w, q/q , v) (w/w , q, v) 3-way contrastive w /q All q's (OSCAR) q'</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training Objectives</head><p>There are two terms in the OSCAR+ pre-training loss as in Equation <ref type="formula" target="#formula_2">(2)</ref>.</p><formula xml:id="formula_2">L Pre-training = L MTL + L CL3 .<label>(2)</label></formula><p>L MTL is the Masked Token Loss defined on the text modality (w and q), following closely <ref type="bibr" target="#b20">[21]</ref>. (See Appendix B.2 for details.) L CL3 is a novel 3-way Contrastive Loss. Different from the binary contrastive loss used in OSCAR <ref type="bibr" target="#b20">[21]</ref>, the proposed 3-way Contrastive Loss to effectively optimize the training objectives used for VQA <ref type="bibr" target="#b40">[41]</ref> and text-image matching <ref type="bibr" target="#b5">[6]</ref>  </p><formula xml:id="formula_3">x ( w caption , q, v tags&amp;image ) or ( w, q Q&amp;A , v image )<label>(3)</label></formula><p>To compute contrastive losses, negative examples need to be constructed. We construct two types of negative (unmatched) triplets for the two types of training samples, respectively. One is the polluted "captions" (w , q, v) and the other the polluted "answers" (w, q , v). To classify whether a caption-tags-image triplet contains a polluted caption is a text-image matching task. To classify whether a question-answerimage triplet contains a polluted answer is an answer selection task for VQA. Since the encoding of [CLS] can be viewed as a representation of the triplet (w, q, v), we apply a fully-connected (FC) layer on top of it as a 3-way classifier f (.) to predict whether the triplet is matched (c = 0), contains a polluted w (c = 1), or contains a polluted q (c = 2). The 3-way contrastive loss is defined as</p><formula xml:id="formula_4">L CL3 = −E (w,q,v;c)∼D log p(c|f (w, q, v)),<label>(4)</label></formula><p>where the dataset (w, q, v; c) ∈D contains 50% matched triples, 25% w-polluted triples, and 25% qpolluted triples. For efficient implementation, the polluted w is uniformly sampled from all w's (captions and questions) and q is uniformly sampled from all q's (tags and answers) in the corpus. As demonstrated in <ref type="table" target="#tab_4">Table 3</ref>, when only the answer-polluted triplets are used, i.e., (w, q , v) with q sampled from q's from QA corpus, the contrastive loss simulates closely the objective for the VQA task but not the text-image retrieval task. As a result, the pre-trained model can be effectively adapted to VQA, but not so to text-image retrieval. By contrast, the proposed 3-way contrastive loss transfers well to both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-trained models</head><p>We pre-train two model variants, denoted as OSCAR+ B and OSCAR+ L , which are initialized with parameters θ BERT of BERT base (L = 12, H = 768, A = 12) and large (L = 24, H = 1024, A = 16), respectively, where L is the number of layers, H the hidden size, and A the number of self-attention heads.</p><p>To ensure that the image region features have the same input embedding size as BERT, we transform the position-augmented region features using a linear projection via matrix W. The trainable parameters are θ = {θ BERT , W}. OSCAR+ B is trained for at least 1M steps, with learning rate 1e −4 and batch size 1024. OSCAR+ L is trained for at least 1M steps, with learning rate 3e −5 and batch size 1024. The sequence length of language tokens [w, q] and region features v are 35 and 50, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adapting to VL Tasks</head><p>We adapt the pre-trained models to seven downstream VL tasks, including five understanding tasks and two generation tasks. Each task poses different challenges for adaptation. This section briefly introduces the tasks and our fine-tuning strategy. We refer the readers to Appendix C for details.</p><p>VQA &amp; GQA These two are the most widely used understanding task for evaluating VL models in the research community. The tasks require the model to answer natural language questions based on an image. In this study, we perform experiments on the widely-used VQA v2.0 dataset <ref type="bibr" target="#b7">[8]</ref> and GQA dataset <ref type="bibr" target="#b12">[13]</ref>, Following the setting of <ref type="bibr" target="#b1">[2]</ref>, for each question, the model picks an answer from a shared answer set (i.e., 3, 129 candidates for VQA, 1, 852 candidates for GQA). When adapting a VLP model to the VQA task, we construct the input by concatenating a given question, object tags and object region features, and then feed the [CLS] output from OSCAR+ to a task-specific linear classifier with a softmax layer for answer prediction.</p><p>Image Captioning &amp; NoCaps The captioning task is to generate a natural language caption for an image. This is the most widely used VL generation task in the research community -the Image Captioning Leaderboard 8 hosts more than 260 models as of December 10, 2020. To enable caption generation, we fine-tune OSCAR+ using the seq2seq objective. Each training sample is converted to a triplet consisting of a caption, a set of image region features, and a set of object tags. We randomly mask out 15% of the caption tokens, and use the encoding of the remaining context (the triplet) to predict the masked tokens. Similar to VLP <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45]</ref>, the self-attention mask is constrained such that a caption token can only attend to the tokens before its position to simulate a uni-directional generation process. All caption tokens have full attentions to image regions and object tags but not the other way around. During inference, we first encode the image regions, object tags, and a special token [CLS] as input. Then the model starts to generate a caption by feeding in a [MASK] token and sampling a token from a vocabulary based on the token probability output. Next, the [MASK] token in the previous input sequence is replaced with the sampled token and a new [MASK] is appended for the next word prediction. The generation process terminates when the model outputs the [STOP] token or the generated sentence exceeds a pre-defined max length. We perform image captioning experiments on the COCO image captioning dataset <ref type="bibr" target="#b24">[25]</ref>. Novel Object Captioning at Scale [1] extends the image captioning task to test a model's capability of describing novel objects from the Open Images dataset <ref type="bibr" target="#b16">[17]</ref> which are unseen in the training corpus. Following the restriction guideline of NoCaps, we use the predicted Visual Genome and Open Images labels to form the input tag sequences, and directly train OSCAR+ on COCO without the initialization from pre-training. VIVO <ref type="bibr" target="#b8">[9]</ref> proposed a VLP technique by only using image tagging data, and achieved SOTA results on NoCaps by fine-tuning on COCO captions. We reproduced VIVO with only one change, i.e., replacing its original vision model with our new vision model, and improved the VIVO performance significantly (short as VinVL+VIVO), as reported in <ref type="table" target="#tab_13">Table 9</ref>.</p><p>Image(-to-Text) Retrieval &amp; Text(-to-Image) Retrieval Both tasks require the model to calculate a similarity score between an image and a sentence. Thus, the task is widely used to directly measure the quality of the cross-modal VL representation. Following <ref type="bibr" target="#b20">[21]</ref>, we formulate the task as a binary classification problem, where given a matched image-text pair, we randomly select a different image or a different sentence to form an unmatched pair. The representation of [CLS] is used as the input to a classifier to predict a score indicating how likely the given pair is matched. In testing, the predicted score is used to rank a given image-text pairs of a query. Following <ref type="bibr" target="#b18">[19]</ref>, we report the top-K retrieval results on both the 1K and 5K COCO test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLVR2</head><p>The dataset is developed for joint reasoning about natural language and images <ref type="bibr" target="#b34">[35]</ref>. The task is to determine whether a text description is true about a pair of images. For fine-tuning, we first construct two input sequences, each containing the concatenation of the given text description and one of the images, and then two [CLS] outputs from OSCAR+ are concatenated to form the input to a binary classifier for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments &amp; Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>To account for model parameter efficiency, we group the SoTA models in three categories: (i) SoTA S indicates the best performance achieved by small models prior to the Transformer-based VLP models. (ii) SoTA B indicates the best performance produced by VLP models of a similar size to BERT base. (iii) SoTA L indicates the best performance yielded by VLP models that have a similar size to BERT large. <ref type="table" target="#tab_7">Table 4</ref> gives an overview of the results of OSCAR+ with VINVL(short for VINVL) on seven VL tasks, compared to previous SoTAs 9 . VINVLoutperforms previous SoTA models on all tasks 10 , often by a significantly large margin. The result demonstrates the effectiveness of the region features produced by the new OD model.   <ref type="bibr" target="#b42">[43]</ref>, GQA is from NSM <ref type="bibr" target="#b11">[12]</ref>, NoCaps is from VIVO <ref type="bibr" target="#b8">[9]</ref>, NLVR2 is from VILLA <ref type="bibr" target="#b6">[7]</ref>, the rest tasks are from OSCAR <ref type="bibr" target="#b20">[21]</ref>.   <ref type="table">Table 6</ref>: Evaluation results on GQA.</p><formula xml:id="formula_5">∆ 1.77 ↑ 1.67 ↑ 3.47 ↑ 1.48 ↑ 0.7 ↓ 0.5 ↑ 0.9 ↑ 0.7 ↑ 5.9 ↑ 0.7 ↑ 1.3 ↑ 0.7 ↑ 0.5 ↑ 1.9 ↑ 0.6 ↑ 0.3 ↑ 2.91 ↑ 2.51 ↑</formula><p>In <ref type="table" target="#tab_9">Tables 5 to 11</ref>, we report the detailed results for each downstream task, respectively. (i) The VQA results are shown in <ref type="table" target="#tab_9">Table 5</ref>, where our single OSCAR+ B model outperforms the best ensemble model (In-terBERT large <ref type="bibr" target="#b21">[22]</ref>) on the VQA leaderboard as of Dec. 12, 2020 11 . (ii) The GQA results are shown in <ref type="table">Table 6</ref>, where OSCAR+w/VINVLis the first VLP model that outperforms the neural state machine (NSM) <ref type="bibr" target="#b11">[12]</ref> which contains some sophisticated reasoning components deliberately designed for the task.</p><p>(iii) The Image Captioning results on the public "Karpathy" 5k test split are shown in <ref type="table" target="#tab_11">Table 7</ref>. <ref type="table" target="#tab_12">Table 8</ref> shows on a concise version of the COCO image captioning online leaderboard 12 . The online testing setting <ref type="bibr" target="#b8">9</ref> All the (single-model) SoTAs are from the published results. For all the tables in this paper, Blue indicates the best result for a task, and gray background indicates results produced by VINVL. <ref type="bibr" target="#b9">10</ref> The only exception is B@4 on image captioning. <ref type="bibr" target="#b10">11</ref>         <ref type="table" target="#tab_9">Table 5</ref>, Image Captioning in <ref type="table" target="#tab_11">Table 7</ref>, NoCaps in <ref type="table" target="#tab_13">Table 9</ref>, Image-Text Retrieval in <ref type="table" target="#tab_14">Table 10</ref>, NLVR2 in <ref type="table" target="#tab_15">Table 11</ref>), we show that OSCAR+ B can match or outperform previous SoTA large models, and OSCAR+ L substantially uplifts the SoTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Analysis</head><p>We select the VQA task for the ablation study because its evaluation metric is well-defined and the task has been used as a testbed for all VLP models. To assist our analysis, we create a local validation set, vqa-dev, out of the standard validation set to select the best model during training for evaluation. vqa-dev contains randomly sampled 2K images and their corresponding questions, amounting to 10.4K image-QA pairs in total. Except for <ref type="table" target="#tab_7">Table 4</ref> and 5, all our VQA results are reported on this vqa-dev set. Unless otherwise specified, the reported STD is half of the difference of two runs of the VQA training with different random seeds.</p><p>In VQA, the VL model y = VL(w, q, v) has w as the question and y as the answer. We focus on studying the effect of visual features v produced by different Vision models Vision(Img) to better understand their relative contribution in the VQA performance. To eliminate the impact of using different tags q, we use the same tags in the VQA models of OSCAR <ref type="bibr" target="#b20">[21]</ref>. All the ablation experiments are conducted using models of the BERT-base size.</p><p>How much do the V and VL matter to the SoTA? <ref type="table" target="#tab_2">Table 12</ref> shows the VQA results with different vision models, i.e., R101-C4 model from <ref type="bibr" target="#b1">[2]</ref> and our X152-C4 model pre-trained with 4 datasets (VinVL), and with different VLP methods, i.e., no VLP, OSCAR <ref type="bibr" target="#b20">[21]</ref>    <ref type="bibr" target="#b30">[31]</ref>, which is obtained by pre-training on Objects365. How much do data and model sizes matter to the new vision model? The improvement of VQA from R101-C4 <ref type="bibr" target="#b1">[2]</ref> to VinVL (ours) in <ref type="table" target="#tab_2">Table 12</ref> is a compound effect of increasing model size (from R101-C4 to X152-C4) and data size (from VG to our merged four OD datasets). <ref type="table" target="#tab_4">Table 13</ref> shows the ablation of the two factors without VLP. Although VG's large object and attribute vocabulary allows to learn rich semantic concepts, VG does not contain large amounts of annotations for effective training of deep models. Vision models trained using the merged four OD datasets perform much better than VG-only-trained models, and the improvement is larger with the increase of the model size. <ref type="bibr" target="#b13">14</ref> How much does OD model architecture matter? The choice of model architecture affects the VQA performance. <ref type="table" target="#tab_4">Table 13</ref> shows that R50-FPN under-performs R50-C5 when they are trained only on VG; but the performance gap diminishes when both are trained on the merged dataset (4Sets). A detailed comparison between FPN and C4 architectures is presented in Appendix E.</p><p>How much does OD pre-training matter for object detection tasks? <ref type="table" target="#tab_7">Table 14</ref> presents the object detection results on COCO and the object-attribute detection results on VG (1594 object classes, 524 attribute classes). The results show that OD pre-training benefits the object detection tasks. Note that the mAP on VG is much lower than that on typical OD datasets (such as COCO) due to two reasons: (1) VG contains a large number of object classes with limited and extremely unbalanced annotations, (2) there are many missing annotations in the VG evaluation data. <ref type="bibr" target="#b14">15</ref> Although the mAP numbers are low, the detection result using X152-C4 is reasonably good; see Appendix A for more visualizations. We also see that FPN models <ref type="bibr" target="#b13">14</ref> The R101-C4 model in <ref type="table" target="#tab_4">Table 13</ref> is exactly the VG-pre-pretrained model from <ref type="bibr" target="#b1">[2]</ref>. We do not train this model on our merged OD dataset because this model architecture is old-fashioned and is slow to train. <ref type="bibr" target="#b14">15</ref> As a reference, the R101-C4 model from <ref type="bibr" target="#b1">[2]</ref> on VG with 1600 objects and 400 attributes has mAP of 8.7/7.8 evaluated in our code, whereas it was reported as 10.2/7.8 due to differences in OD evaluation pipeline.   <ref type="table" target="#tab_9">Table 15</ref>), we initialize the OD training with an ImageNet-pre-trained classification model, and use maximal 50 region features per image as input to the VL fusion module. For the ImageNet pre-trained classification model (the second column in <ref type="table" target="#tab_9">Table 15</ref>), we use all the grid features (maximal 273) for each image <ref type="bibr" target="#b15">16</ref> . The results show that • In general, vocabularies with richer objects lead to better VQA results: VG-obj &lt; ImageNet &lt; VG w/o attr. The VG-obj vocabulary contains 79 of 80 COCO classes (only missing potted plant) and 313 of 500 OpenImagesV5 classes, and is a good approximation of common object classes of typical OD tasks. However, our results show that this vocabulary is not rich enough for VL tasks because it misses many important visual concepts (e.g., sky, water, mountain, etc.) which are crucial for VL tasks, as also illustrated by the comparison of detected regions in <ref type="figure">Figure 1</ref>. <ref type="bibr" target="#b16">17</ref> . • Attribute information is crucial to VL tasks: models trained with attributes (VG and 4Sets→VG) are significantly better than those without attributes. • Even for the small vision model R50-C4, vision pre-training improves visual features for VQA, i.e., 4Sets→VG is the best performer.</p><p>In <ref type="table" target="#tab_23">Table 16</ref>, we use different kinds of region proposals to extract image features. COCO groundtruth object regions (GT-Obj, 80 classes) and object-stuff regions (GT-Obj&amp;Stuff, 171 classes) are perfect in terms of localization, but their vocabulary sizes are limited. Regions proposed by VG-trained models ( <ref type="bibr" target="#b1">[2]</ref> and VinVL) are imperfect in localization but using a larger vocabulary. For the VQA task, COCO GT boxes are much worse than the proposals generated by VG-trained models. The result demonstrates the difference between the typical OD tasks and the OD tasks in VL: OD in VL requires much richer visual semantics to align with the rich semantics in the language modality. This further echoes our claim that an image understanding module trained using richer vocabularies performs better for VL tasks.   A Qualitative study of three pre-trained vision models</p><p>We apply three (pre-trained) object detection models on the image in <ref type="figure">Figure 1</ref>  Detections from R101-C4 trained on VG by Anderson et al. <ref type="bibr" target="#b1">[2]</ref>. There are obviously wrong detections, marked in red. See <ref type="figure">Figure 3 (</ref>  <ref type="table" target="#tab_11">Table 17</ref> shows the statistics of image and text of the pre-training corpora. In our ablation study, we use corpora of three different sizes: 'Small', 'Medium', 'Large'. Different from OSCAR <ref type="bibr" target="#b20">[21]</ref>, we make use of image tagging datasets OpenImages, by generating captions using OSCAR's image captioning model to form triplets of (generated caption, image tags, image features) for OSCAR+ pre-training. By self-training technique, our pre-training corpora can be scaled to a much larger amount by making use of large-scale image tagging datasets, e.g., OpenImages (9M) and YFCC (92M).  </p><formula xml:id="formula_6">L MTL = −E (v,h)∼D log p(h i |h \i , v)<label>(5)</label></formula><p>This is the same MTL as in OSCAR <ref type="bibr" target="#b20">[21]</ref> and similar to the masked language model used by BERT. The masked word or tag needs to be recovered from its surrounding context, with additional image information to help ground the learned word embeddings in the vision context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-way Contrastive Loss: A Loss Mimics</head><p>Text-Image Retrieval and Visual Question Answering Simultaneously. We present our 3-way contrastive loss in Section 3.2 in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Ablation of the two new techniques</head><p>Effect of self-training: Leveraging Image Tagging data. In <ref type="figure">Figure 4</ref>, we show the effect of self-training by making use of tagging data in OSCAR+, by fine-tuning OSCAR+ pre-training checkpoints on VQA. Compared with "OSCAR+, Small; VinVL" (green), "OSCAR+, Medium; VinVL" (yellow) adds the 1.7M OpenImages Tagging data into pre-training and its performance gets improved significantly, demonstrating the effect of self-training by making use of tagging data. As baselines, we also provide performance of OSCAR and OSCAR+ with image features from <ref type="bibr" target="#b1">[2]</ref>, which clearly demonstrates that the new image features pre-trained by VinVL matter significantly in the VL pre-training and VL downstream tasks.</p><p>Effect of the new 3-way contrastive loss. As illustrated in <ref type="table" target="#tab_4">Table 3</ref>, with the new 3-way contrastive loss, the VQA performance is the same as the OSCAR pre-training, while the Text-Image Retrieval performance improves significantly compared with the OSCAR pre-training. <ref type="figure">Figure 4</ref>: Effect of OSCAR+ pre-training corpus size and effect of self-training by making use of tagging data in OSCAR+. Each curve, with legend "VLP, Corpus; VisionFeature", denotes a VLP experiment where the VLP method is either OSCAR or OSCAR+, the VLP pre-training Corpus is Small/Medium/Large (defined in <ref type="table" target="#tab_11">Table 17</ref>), and VisionFeature is either our new vision features (VinVL for short) or those from <ref type="bibr" target="#b1">[2]</ref> ([2] for short). X-axis denotes the pre-training iterations of OSCAR+ checkpoints. Y-axix is the vqa-dev accuracy of a VQA model initialized from the corresponding pre-training checkpoint and fine-tuned with a fixed scheme. Compared with "OSCAR+, Small; VinVL" (green), "OSCAR+, Medium; VinVL" (yellow) adds the 1.7M OpenImages Tagging data into the pre-training and its performance gets improved significantly, demonstrating the effect of self-training by making use of tagging data. The "OSCAR+, Large; VinVL" (blue) further scales up the pre-training corpus by adding Google Conceptual Captions and SBU datasets with generated tags and its performance gets further improved, demonstrating the effect of OSCAR+ pre-training corpus size. As baselines, we also provide performance of OSCAR and OSCAR+ with image features from <ref type="bibr" target="#b1">[2]</ref>, which clearly demonstrates that our new image features (VinVL) matter significantly in the VL pre-training and VL downstream tasks.</p><p>Overall improvement from OSCAR to OSCAR+. We point out that the improvement from OSCAR to OSCAR+ with image features from <ref type="bibr" target="#b1">[2]</ref> is minor, because (1) we only add 1.7M OpenImages' tagging data to enlarge the pre-training corpus, which is a small portion compared with OSCAR's original pre-training corpus (i.e., Large\OI, 3.98M images and 7.18M image-caption pairs), and (2) the new 3-way contrastive loss has more significant improvements in Text-Image Retrieval tasks than that in the VQA task, as illustrated in <ref type="table" target="#tab_4">Table 3</ref>. We would expect much more significant improvements when we scale up the OSCAR+'s pre-training corpus to a much larger scale by adding large scale image tagging datasets, e.g., OpenImages (9M) and YFCC (92M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Downstream Tasks Fine-tuning</head><p>We follow the downstream task fine-tuning recipes in OSCAR <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 VQA</head><p>Given an image and a question, the task is to select the correct answer from a multi-choice list, it requires the model to answer natural language questions based on an image. Here we conduct experiments on the widely-used VQA v2.0 dataset <ref type="bibr" target="#b7">[8]</ref>, which is built on the MSCOCO <ref type="bibr" target="#b24">[25]</ref> images. Following <ref type="bibr" target="#b1">[2]</ref>, for each question, the model picks the corresponding answer from a shared set of 3, 129 candidates. When fine-tuning on the VQA task, the input sequence contains the concatenation of a given question, object tags and object region features, and then the [CLS] output from OSCAR+ is fed to a task-specific linear classifier for answer prediction. Similarly as the literature <ref type="bibr" target="#b1">[2]</ref>, we treat VQA as a multi-label classification problem -assigning a soft target score to each answer based on its relevancy to the human answer responses, and then we fine-tune the model by minimizing the cross-entropy loss computed using the predicted scores and the soft target scores. During inference, we simply use Softmax for answer prediction.</p><p>For VQA training, we random sample a set of 2k images from the MS COCO validation set as our validation set, the rest of images in the training and validation are used in the VQA fine-tuning. For the OSCAR+ B model, we fine-tune for 25 epochs with a learning rate of 5e −5 and a batch size of 128. For the OSCAR+ L model, we fine-tune for 25 epochs with a learning rate of 3e −5 and a batch size of 96.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 GQA</head><p>Similarly as VQA, GQA tests the reasoning capability of the model to answer a question. We conduct experiments on the public GQA dataset <ref type="bibr" target="#b12">[13]</ref>. For each question, the model chooses an answer from a shared set of 1, 852 candidates. Our fine-tuning procedure is following Oscar <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref>, which first fine-tunes the model on unbalanced "all-split" for 5 epochs with a learning rate of 5e −5 and a batch size of 128, and then fine-tuned on the "balanced-split" for 2 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Image Captioning</head><p>An image captioning model generates a natural language description for a given image. To enable sentence generation, we fine-tune OSCAR+ using the seq2seq objective. The input samples are processed to triples consisting of image region features, captions, and object tags, in the same way as that during the pre-training. We randomly mask out 15% of the caption tokens and use the corresponding output representations to perform classification to predict the token ids. Similar to previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45]</ref>, the self-attention mask is constrained such that a caption token can only attend to the tokens before its position to simulate a unidirectional generation process. Note that all caption tokens will have full attentions to image regions and object tags but not the other way around.</p><p>During inference, we first encode the image regions, object tags, and a special token [CLS] as input. Then the model starts the generation by feeding in a [MASK] token and selecting a token from the vocabulary based on the likelihood output. Next, the [MASK] token in the previous input sequence is replaced with the selected token and a new [MASK] is appended for the next word prediction. The generation process terminates when the model outputs the [SEP] token. We use beam search (i.e., beam size = 5) <ref type="bibr" target="#b1">[2]</ref> in our experiments and report our results on the COCO image captioning dataset.</p><p>Though the training objective (i.e., seq2seq) for image captioning is different from that used in pretraining (i.e., bidirectional attention-based masked token loss), we directly fine-tune OSCAR+ for image captioning on COCO without additional pre-training on Conceptual Captions <ref type="bibr" target="#b31">[32]</ref>. This is to validate the generalization ability of the OSCAR+ models for generation tasks. We use the same Karpathy split <ref type="bibr" target="#b14">[15]</ref>. For the OSCAR+ B model, we fine-tune with cross-entropy loss for 30 epochs with a batch size of 256 and an initial learning rate of 1e −5 and then with CIDEr optimization <ref type="bibr" target="#b29">[30]</ref> for 10 epochs with a batch size of 128 and initial learning rate of 2e −6 . We compare with several existing methods, including BUTD <ref type="bibr" target="#b1">[2]</ref>, VLP <ref type="bibr" target="#b44">[45]</ref>, AoANet <ref type="bibr" target="#b9">[10]</ref>, OSCAR <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 NoCaps</head><p>Novel Object Captioning [1] extends the image captioning task, is to test models' capability of describing novel objects from the Open Images dataset <ref type="bibr" target="#b16">[17]</ref> which are not seen in the training corpus. Following the restriction guideline of NoCaps, we train OSCAR+ on COCO without the initialization from pre-training, so no additional image-text pairs are used for training except COCO.</p><p>Since NoCaps images are collected from Open Images, we train an object detector using the Open Images training set and apply it to generate the tags. We conduct experiments from BERT model directly without pre-training as required by the task guidelines. For the OSCAR+ B model, we train 30 epochs with a batch size of 256 and learning rate 1e −4 ; further we perform CIDEr optimization with learning rate 5e −6 and batch size 112 for 10 epochs. During inference, we use constrained beam search for decoding. We compare OSCAR+ with OSCAR [21] on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Image-Text Retrieval</head><p>There are two sub-tasks: image retrieval and text retrieval, depending on which modality is used as the retrieved target. Both tasks calculate a similarity score between an image and a sentence, which heavily relies on the cross-modal representations.</p><p>Following Oscar <ref type="bibr" target="#b20">[21]</ref>, we formulate the retrieval as a binary classification problem, where given an aligned image-text pair, we randomly select a different image or a different sentence to form an unaligned pair. The final representation of [CLS] is used as the input to the classifier to predict whether the given pair is aligned or not. In the testing stage, the probability score is used to rank the given image-text pairs of a query.</p><p>Following <ref type="bibr" target="#b18">[19]</ref>, we report the top-K retrieval results on both the 1K and 5K COCO test sets. We adopt the widely used Karpathy split <ref type="bibr" target="#b14">[15]</ref> on the COCO caption dataset <ref type="bibr" target="#b24">[25]</ref> to conduct our experiments. Specifically, the dataset consists of 113, 287 images for training, 5, 000 images for validation, and 5, 000 images for testing. Each image is associated with 5 human-generated captions. For the OSCAR+ B model, we fine-tune with a batch size of 256 for 40 epochs. The initial learning rate is set to 2e −5 and linearly decreases. For the OSCAR+ L model, we fine-tune with a batch size of 128 for 40 epochs. The initial learning rate is set to 1e −5 and linearly decreases. We use the validation set for parameter tuning. We compare with several existing methods, including DVSA <ref type="bibr" target="#b14">[15]</ref>, VSE++ <ref type="bibr" target="#b4">[5]</ref>, DPC <ref type="bibr" target="#b43">[44]</ref>, CAMP <ref type="bibr" target="#b38">[39]</ref>, SCAN <ref type="bibr" target="#b17">[18]</ref>, SCG <ref type="bibr" target="#b32">[33]</ref>, PFAN <ref type="bibr" target="#b37">[38]</ref>, Unicoder-VL <ref type="bibr" target="#b18">[19]</ref>, 12-in-1 <ref type="bibr" target="#b26">[27]</ref>, UNITER <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 NLVR2</head><p>Given a pair of images and a natural language, the goal of NLVR2 <ref type="bibr" target="#b34">[35]</ref> is to determine whether the natural language statement is true about the image pair. For NLVR2 fine-tuning, we first construct two input sequences, each containing the concatenation of the given sentence (the natural language description) and one image, and then two [CLS] outputs from OSCAR+ are concatenated as the joint input for a binary classifier, implemented by an MLP.</p><p>For the OSCAR+ B model, we fine-tune for 20 epochs with learning rate {2e −5 , 3e −5 , 5e −5 } and a batch size of 72. For the OSCAR+ L model, we fine-tune for 20 epochs with learning rate of {2e −5 , 3e −5 } and a batch size of 48. D More on the Effect of the Object-Attribute Vocabulary Size: disentangling the effects of region proposals and model weights In Section 5.2, we demonstrate that the more diverse the visual concepts (object and attribute vocabularies) are, the better the visual region features for VL tasks. The better performance may come from the more diverse proposed regions where the region features are extracted (see the comparison in <ref type="figure">Figure 1</ref>, "region" for short), or from the better model weights that can produce better high-dimensional region representation even for the same region ("model" for short). In this section, we disentangle effects of region proposals and model weights, by performing synthetic experiments in which we use region proposals from one vision model and model weights from another vision model. Our results show that both the region proposals and model weights matter for VL tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Disentangling the effects of region proposals and model weights on R50-C4</head><p>• Grid features perform worse than region features with regions proposed by VG models. By comparing Row "Grid-273" with rows with VG regions, it seems hopeful to close this gap while paying more hardware memory and computational cost in cross-modal models VL. It is three times slower to train the "Grid-273" models than training models with region features. In <ref type="figure" target="#fig_3">Figure 6</ref>, instead of just showing one final number, we provide the mean evaluation curves along training trajectories to demonstrate the ranking, as an even more robust evidence. These results further confirm the conclusions we draw in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Disentangling the effects of region proposals and model weights on the SoTA model</head><p>In <ref type="table" target="#tab_12">Table 18</ref>, we alternate the combination of region proposals and model weights, and evaluate them on VQA. As we can see, the improvement of using boxes from the R101-C4 model <ref type="bibr" target="#b1">[2]</ref> to extract features from our X152-C4 model is much bigger than that of using boxes from our X152-C4 model to extract features from the R101-C4 model <ref type="bibr" target="#b1">[2]</ref>, indicating pre-trained model weights are more important than regions. Inspired by this analysis, we propose the class-agnostic NMS for region selection in the box head of the OD model, which does not sacrifice any VQA performance but greatly improves the model's inference speed. This analysis also suggests that large-scale OD pre-training should improve performance for grid-feature based VL models, as supported by more results in Appendix F.</p><p>In <ref type="table" target="#tab_12">Table 18</ref>, We also report VQA results with COCO groundtruth object regions (GT-Obj, 80 classes) and object-stuff regions (GT-Obj&amp;Stuff, 171 classes). For VQA task, COCO GT boxes are much worse than proposals from VG trained models. This shows the difference between typical OD tasks and OD in VL: OD in VL requires much richer visual semantics to align with the rich semantics in the language modality. This further echoes with our claim that an image understanding module trained with rich semantics is crucial for VL tasks.   <ref type="table" target="#tab_13">Table 19</ref>: C4 vs FPN architecture on VQA. Boxes used to extract features v and tags q used in VL model are the same with those used in OSCAR <ref type="bibr" target="#b20">[21]</ref>. Row "Initial" means using the initialization model without VG training for feature extraction.</p><p>The convolutional head in C4 has a better inductive bias in encoding visual information than the MLP head in FPN. This can be verified by the fact that when vision features from randomly initialized models are used (Row "Initial" in <ref type="table" target="#tab_13">Table 19</ref>), R50-C4 performs much better than R50-FPN, indicating that the initial C4 features encode much more useful visual information than the inital FPN features. The "random" C4 features nearly match the feature from ImageNet pre-trained model (Row "Initial" Column "R50C4"), while "random" FPN features are close to the performance without visual features as input (Row "Initial" Column "no image feature w").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Effect of pooling methods in FPN on VQA performance.</head><p>Different from C4 models that extract region features from a single scale (the end of C4 block), FPN models extract region features from multiple scales adaptively based on the area of the region. Therefore, there is some in-homogeneity in FPN's region features since they may come from different scales. In <ref type="figure">Figure 7</ref>, we show that this is not the cause of FPN's worse performance than C4 on the VQA task. More specifically, we experiment with 4 pooling methods for FPN architecture. (1) adapt: the original FPN's pooling method that extract features adaptively from different scales; (2) max: extract features from all scales and then do a max-pool; (3) avg: extract features from all scales and then do an average-pool; (4) concat: extract features from all scales and then concatenate them together. We also train multiple FPN models on VG with these pooling methods, with or without pre-training on the Objects365 dataset. We experiment on all possible combinations (in total 8 × 4) of 8 vision models and 4 pooling methods on the VQA task. When there is a parameter dimension mis-match, e.g., non-concat FPN models but use concat pooling methods in VQA and vice versa, we specify those parameter randomly with PyTorch's default initialization method. The results in <ref type="figure">Figure 7</ref> shows that (1) there is no obvious difference in different pooling methods, with the default "adapt" and the "concat" methods perform slightly better than "max" and "avg"; (2) (without surprise) the performance is significantly worse when there is a parameter dimension mismatch between vision models and VL task feature extraction methods, i.e., non-concat FPN models but use concat pooling methods in VQA and vice versa. These results show that the pooling method (no matter in vision model training or in VL task feature extraction) is not the root cause of FPN's worse performance than C4 on the VQA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Large-scale object-detection pre-training of C4 and FPN models</head><p>In this paper, we have trained R50-C4, R50-FPN, R152-C4 and R152-FPN models on the merged object detection datasets described in <ref type="table" target="#tab_2">Table 2</ref>. In <ref type="figure">Figure 8</ref>, we report the mAP 50 of checkpoints from these 4 experiments on 4 validation sets: COCO with stuff (top left), Objects365 (top right), OpenImages (bottom left) and Visual Genome (1594 object classes, bottom right). For R50 models, the R50-FPN model is slightly better than C4 on COCO and Objects365 but slightly worse than C4 on Visual Genome. For R152 models, <ref type="figure">Figure 7</ref>: Pooling methods in FPN feature extraction are not the root cause of FPN's worse performance than C4. X-axis: the pooling method when extracting features for VL tasks; Y-axis: the pooling method (vision model) when pre-training the visual feature extraction model. All experiments are using regions from the Bottum-up Top-down model <ref type="bibr" target="#b1">[2]</ref>. Each combination is experimented twice with two random seeds, i.e. seed=42 on the left and seed=88 on the right. The results from two random seeds are consistent.  <ref type="table" target="#tab_2">Table 20</ref>: Ablation study of X152 models on VQA. Vision models in the last three columns are trained with initialization from the ImageNet-5k checkpoint in the first column. All the region features are extracted with boxes proposed by our best X152-C4 model (pre-trained on 4Sets and fine-tuned on VG). By comparing the first column and the last column, we see that our proposed vision pre-training (first on 4 sets and then on VG with attributes) improves performance for both the grid-feature based model and the region-feature based model. Since the X152 backbone is much larger than the R50 backbone in <ref type="figure" target="#fig_2">Figure 5</ref>, the larger model can make better use of the large pre-training datasets and thus have more significant improvements.</p><p>we finally use the R152-C4 model for downstream vision-language tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Grid feature</head><p>In <ref type="table" target="#tab_2">Table 20</ref>, we train grid-feature based and region-feature based X152 models for VQA, with the vision models pre-trained on different vision datasets, i.e., "ImageNet-5k" from <ref type="bibr" target="#b39">[40]</ref>, our 4-dataset merged OD dataset 2 (4Sets), our VG dataset with 1594 object classes and 524 attribute classes (VG with Attr), and first 4Sets and then VG (4Sets→VG). Vision models in the last three cases are trained with initialization from the same ImageNet-5k checkpoint from <ref type="bibr" target="#b39">[40]</ref>. All the region features are extracted with boxes proposed by our best X152-C4 model (pre-trained on 4Sets and fine-tuned on VG). By comparing "ImageNet-5k" and "4Sets→VG", we see that our proposed vision pre-training improves performance for both the grid-feature based model and the region-feature based model. Since the X152 backbone is much larger than the R50 backbone in <ref type="figure" target="#fig_2">Figure 5</ref>, the larger model makes better use of the large pre-training datasets and thus has more significant improvements. It is interesting to see that for grid-feature based models, the "ImageNet-5k" model performs better than the "4Sets" model and the "VG with Attr", while it is not the case for region-feature based models. This may indicate that how the vision model is trained (grid-feature wise or region-feature wise) may have big impact on the downstream VL tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G End-to-end inference efficiency</head><p>We report the end-to-end inference time of different VQA models on a Titan-X GPU and a Xeon E5 CPU in <ref type="table" target="#tab_2">Table 21</ref>. For CPU evaluation, we force that the inference use only one CPU thread. The input image size is 800 × 1333, and we run the inference with batch size 1 (one image-question pair per batch). We can see that (1) vision models dominate the inference time, especially for large models; (2) models based on grid-feature are faster than those based on region feature; (3) with our proposed fast inference trick, region-feature models are greatly sped up and their inference time can be brought to within 3 times of that of grid-feature models on GPU. We find that on CPU with a single thread, our class-agnostic trick does not lead to time saving, because nearly all inference time is taken by the backbone and C4 head and the time from NMS operations is nearly ignorable on CPU.  <ref type="table" target="#tab_2">Table 21</ref>: Time cost of end-to-end inference on VQA. All cross-modal models are BERT-Base. On the SOTA number obtained with X152-C4 region features, the performance keeps the same when changing to the efficient way to extract the feature while the efficiency greatly improves on GPU. The efficient version does not lead to time saving on CPU, because nearly all inference time is taken by the backbone and C4 head and the time from NMS operations is nearly ignorable on CPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Predictions from X152-FPN trained on OpenImages. Test image: COCO test2015 000000028839</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and list their detection results for a more detailed comparison. Detections from X152-FPN trained on Open Images V5. See Figure 2: Surfboard; Surfboard; Surfboard; Surfboard; Man; Human leg; Human leg; Swimwear; Swimwear; Shorts; Shorts; Boy; Human arm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Overall comparison of vocabulary effect on VQA. X-axis: how the R50-C4 model is trained; Yaxis: how the feature is extracted (grid or region features, different kinds of boxes to extract region features). All region features have maximal 50 regions. The top row "Mean" is the average over all rows, showing the overall quality of different vision models. The far-right column "Mean" is the average over all columns, showing the overall quality of different feature extraction methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Left: comparison of object vocab and attribute vocab, average over all types of bounding boxes. Right: comparison of feature extraction methods, average over all types of pre-trained vision models. X-axis is the number of iterations when we take the checkpoint for evaluation. Y-axis is the VQA accuracy on our vqa-dev.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Anderson et al.<ref type="bibr" target="#b1">[2]</ref> 73.<ref type="bibr" target="#b15">16</ref> 73.44 61.58 61.62 40.5 29.7 137.6 22.8 86.58 12.38 54.0 80.8 88.5 70.0 91.1 95.5 78.07 78.36 Ours 75.95 76.12 65.05 64.65 40.9 30.9 140.6 25.1 92.46 13.07 58.1 83.2 90.1 74.6 92.6 96.3 82.05 83.08 ∆ 2.79 ↑ 2.68 ↑ 3.47 ↑ 3.03 ↑ 0.4 ↑ 1.2 ↑ 3.0 ↑ 2.3 ↑ 5.9 ↑ 0.7 ↑ 4.1 ↑ 2.4 ↑ 1.6 ↑ 4.6 ↑ 1.5 ↑ 0.8 ↑ 3.98 ↑ 4.71 ↑</figDesc><table><row><cell>Visual feature</cell><cell>VQA test-dev test-std test-dev test-std GQA</cell><cell>Image Captioning B@4 M C</cell><cell>S</cell><cell>NoCaps C S</cell><cell>Image Retrieval R@1 R@5 R@10</cell><cell>Text Retrieval R@1 R@5 R@10</cell><cell>NLVR2 dev test-P</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Statistics of the Vision pre-training datasets. In sampling, ×k means k copies in one epoch and "CA-2k" means class-aware sampling with at least 2000 instances per class.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effects of different pre-training contrastive losses on downstream tasks (R50-C4 as Vision module and 4-layer Transformer as VL module in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>7 . As shown in Equation 3, L CL3 takes into account two types of training samples x: the {caption, image-tags, image-features} triplets of the image captioning and image tagging data, and the {question, answer, image-features} triplets of the VQA data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>An overall comparison with SoTAs on seven tasks. ∆ indicates the improvement over SoTA. SoTA with subscript S, B, L indicates performance achieved by small models, and models with the model size similar to BERT base and large, respectively. SoTAs: VQA is from ERNIE-VIL</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Evaluation results on VQA. * denotes the No.1 ensemble model of InterBERT Large on the VQA leaderboard.</figDesc><table><row><cell>Method</cell><cell cols="5">LXMERT MMN [3] 12-in-1 OSCAR B NSM [12]</cell><cell>OSCAR+ B w/ VINVL</cell></row><row><cell>Test-dev</cell><cell>60.00</cell><cell>−</cell><cell>−</cell><cell>61.58</cell><cell>−</cell><cell>65.05</cell></row><row><cell>Test-std</cell><cell>60.33</cell><cell>60.83</cell><cell cols="2">60.65 61.62</cell><cell>63.17</cell><cell>64.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>36.2 27.0 113.5 20.3 36.3 27.7 120.1 21.4 VLP [45] 36.5 28.4 117.7 21.3 39.5 29.3 129.3 23.2 AoANet [10] 37.2 28.4 119.8 21.3 38.9 29.2 129.8 22.4 OSCAR B [21] 36.5 30.3 123.7 23.1 40.5 29.7 137.6 22.8 OSCAR L [21] 37.4 30.7 127.8 23.5 41.7 30.6 140.0 24.5 OSCAR+ B w/ VINVL 38.2 30.3 129.3 23.6 40.9 30.9 140.4 25.1 OSCAR+ L w/ VINVL 38.5 30.4 130.8 23.4 41.0 31.1 140.9 25.2</figDesc><table><row><cell>Method</cell><cell>cross-entropy optimization B@4 M C S</cell><cell>CIDEr optimization B@4 M C</cell><cell>S</cell></row><row><cell>BUTD [2]</cell><cell></cell><cell></cell><cell></cell></row></table><note>VQA leaderboard: https://eval.ai/web/challenges/challenge-page/514/leaderboard/138612 Image Captioning Leaderboard: https://competitions.codalab.org/competitions/3221#results</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Image captioning evaluation results (single model) on COCO "Karpathy" test split.</figDesc><table><row><cell>(Note: B@4:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Leaderboard of the state-of-the-art image captioning models on the COCO online testing.</figDesc><table><row><cell>Method</cell><cell cols="10">in-domain CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE near-domain out-of-domain overall in-domain near-domain out-of-domain overall</cell></row><row><cell></cell><cell></cell><cell>Validation Set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Test Set</cell><cell></cell><cell></cell></row><row><cell>UpDown +</cell><cell>79.3 12.4</cell><cell>73.8 11.4</cell><cell>71.7</cell><cell>9.9</cell><cell>74.3 11.2</cell><cell>76.0 11.8</cell><cell>74.2 11.5</cell><cell>66.7</cell><cell>9.7</cell><cell>73.1 11.2</cell></row><row><cell>OSCARB*</cell><cell>83.4 12.0</cell><cell>81.6 12.0</cell><cell cols="2">77.6 10.6</cell><cell>81.1 11.7</cell><cell>81.3 11.9</cell><cell>79.6 11.9</cell><cell cols="2">73.6 10.6</cell><cell>78.8 11.7</cell></row><row><cell>OSCARL*</cell><cell>85.4 11.9</cell><cell>84.0 11.7</cell><cell cols="2">80.3 10.0</cell><cell>83.4 11.4</cell><cell>84.8 12.1</cell><cell>82.1 11.5</cell><cell>73.8</cell><cell>9.7</cell><cell>80.9 11.3</cell></row><row><cell>Human [1]</cell><cell>84.4 14.3</cell><cell>85.0 14.3</cell><cell cols="2">95.7 14.0</cell><cell>87.1 14.2</cell><cell>80.6 15.0</cell><cell>84.6 14.7</cell><cell cols="2">91.6 14.2</cell><cell>85.3 14.6</cell></row><row><cell>VIVO* [9]</cell><cell>92.2 12.9</cell><cell>87.8 12.6</cell><cell cols="2">87.5 11.5</cell><cell>88.3 12.4</cell><cell>89.0 12.9</cell><cell>87.8 12.6</cell><cell cols="2">80.1 11.1</cell><cell>86.6 12.4</cell></row><row><cell>VinVL*</cell><cell>96.8 13.5</cell><cell>90.7 13.1</cell><cell cols="2">87.4 11.6</cell><cell>90.9 12.8</cell><cell>93.8 13.3</cell><cell>89.0 12.8</cell><cell cols="2">66.1 10.9</cell><cell>85.5 12.5</cell></row><row><cell>VinVL+VIVO</cell><cell>103.7 13.7</cell><cell>95.6 13.4</cell><cell cols="2">83.8 11.9</cell><cell>94.3 13.1</cell><cell>98.0 13.6</cell><cell>95.2 13.4</cell><cell cols="2">78.0 11.5</cell><cell>92.5 13.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>NoCaps evaluation results. All the models are trained on COCO without additional image-caption pairs following the restriction of NoCaps. (UpDown + is UpDown+ELMo+CBS, the models with * is +SCST+CBS, VinVL+VIVO is with SCST only.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1K Test Set</cell><cell></cell><cell></cell><cell cols="2">5K Test Set</cell></row><row><cell>Method ↓</cell><cell>BERT</cell><cell></cell><cell cols="2">Text Retrieval</cell><cell cols="3">Image Retrieval</cell><cell>Text Retrieval</cell><cell>Image Retrieval</cell></row><row><cell></cell><cell></cell><cell cols="3">R@1 R@5 R@10</cell><cell cols="3">R@1 R@5 R@10</cell><cell>R@1 R@5 R@10</cell><cell>R@1 R@5 R@10</cell></row><row><cell>Unicoder-VL [19]</cell><cell>B</cell><cell cols="3">84.3 97.3 99.3</cell><cell cols="3">69.7 93.5 97.2</cell><cell>62.3 87.1 92.8</cell><cell>46.7 76.0 85.3</cell></row><row><cell>UNITER [4]</cell><cell>B L</cell><cell>− −</cell><cell>− −</cell><cell>− −</cell><cell>− −</cell><cell>− −</cell><cell>− −</cell><cell>63.3 87.0 93.1 66.6 89.4 94.3</cell><cell>48.4 76.7 85.9 51.7 78.4 86.9</cell></row><row><cell>OSCAR</cell><cell>B L</cell><cell cols="3">88.4 99.1 99.8 89.8 98.8 99.7</cell><cell cols="3">75.7 95.2 98.3 78.2 95.8 98.3</cell><cell>70.0 91.1 95.5 73.5 92.2 96.0</cell><cell>54.0 80.8 88.5 57.5 82.8 89.8</cell></row><row><cell>OSCAR+ w/ VINVL</cell><cell>B L</cell><cell cols="3">89.8 98.8 99.7 90.8 99.0 99.8</cell><cell cols="3">78.2 95.6 98.0 78.8 96.1 98.5</cell><cell>74.6 92.6 96.3 75.4 92.9 96.2</cell><cell>58.1 83.2 90.1 58.8 83.5 90.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Text and Image retrieval evaluation on the COCO 1K and 5K test sets. (B for Base, L for Large)</figDesc><table><row><cell>Method</cell><cell cols="4">MAC VisualBERT LXMERT 12-in-1 base base base</cell><cell>UNITER base large</cell><cell>OSCAR base large</cell><cell>VILLA base large</cell><cell>OSCAR+w/ VINVL base large</cell></row><row><cell>Dev</cell><cell>50.8</cell><cell>67.40</cell><cell>74.90</cell><cell>−</cell><cell cols="3">77.14 78.40 78.07 79.12 78.39 79.76</cell><cell>82.05</cell><cell>82.67</cell></row><row><cell>Test-P</cell><cell>51.4</cell><cell>67.00</cell><cell>74.50</cell><cell>78.87</cell><cell cols="3">77.87 79.50 78.36 80.37 79.47 81.47</cell><cell>83.08</cell><cell>83.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Evaluation results on NLVR2.</figDesc><table><row><cell>vision</cell><cell>vl</cell><cell>no VLP</cell><cell>OSCAR B [21]</cell><cell>OSCAR+ B (ours)</cell></row><row><cell cols="2">R101-C4 [2]</cell><cell>68.52 ±0.11</cell><cell>72.38</cell><cell>72.46±0.05</cell></row><row><cell cols="3">VinVL (ours) 71.34 ±0.17</cell><cell>-</cell><cell>74.90±0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Effects of vision (V) and vision-language (VL) pre-training on VQA.reports the results on 40K images, with 5 reference captions (c5) and 40 reference captions (c40) per image. At the time of submitting this paper, our single model achieves No.1 on the entire leaderboard, outperforming all 263 models, including many ensemble (and anonymous) models. (iv) The Novel Object Captioning (NoCaps) results are shown inTable 9. Without any VLP, i.e. by directly training a BERT-based captioning model on COCO, the model with our new visual features (denoted as VinVL) already surpasses the human performance in CIDEr<ref type="bibr" target="#b12">13</ref> . By adding VIVO<ref type="bibr" target="#b8">[9]</ref> pre-training, our VinVL improves the original VIVO result by 6 CIDEr points and creates a new SoTA. (v) Overall, on all these tasks (VQA in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>and our OSCAR+. Taking the OSCAR B model with R101-C4 features as the baseline, the OSCAR+ B model with our X152-C4 features improves the absolute accuracy from 72.38 to 74.90, in which the OSCAR+ pre-training contributes 5% of the gain (i.e., 72.38 → 72.46) and the vision pre-training (improved visual features) 95% (i.e., 72.46 → 74.90). This demonstrates that vision representations matter significantly in VLP and downstream tasks. Taking the "no VLP" model with R101-C4 features as the baseline, Table 12 shows that the gains of VinVL (71.34−68.52 = 2.82) and VLP (72.46−68.52 = 3.94) are additive (74.90−68.52 ≈ 2.82+3.94). This is intuitive because vision pre-training and VLP improve the Vision model Vision(Img) and VL 35±0.26 67.86±0.31 68.52 ±0.11 69.10±0.06 4Sets→VG 68.3±0.11 68.39±0.16 -71.34 ±0.17</figDesc><table><row><cell>data</cell><cell cols="2">model R50-FPN</cell><cell>R50-C4</cell><cell>R101-C4 [2]</cell><cell>X152-C4</cell></row><row><cell></cell><cell>VG</cell><cell>67.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Ablation of model size and data size on training vision models. Since our four pre-training datasets contain Objects365, it is not surprising that we obtain better results than 42.3 mAP 50 in</figDesc><table><row><cell>Model</cell><cell cols="2">R50-FPN</cell><cell>R50-C4</cell><cell></cell><cell>X152-C4</cell><cell></cell></row><row><cell>Pre-training dataset</cell><cell cols="6">ImageNet 4Sets ImageNet 4Sets ImageNet5k 4Sets</cell></row><row><cell>COCO mAP</cell><cell cols="4">40.2 [40] 44.78 * 38.4 [40] 42.4</cell><cell>42.17</cell><cell>50.51</cell></row><row><cell>VG obj mAP 50</cell><cell>9.6</cell><cell>11.3</cell><cell>9.6</cell><cell>12.1</cell><cell>11.2</cell><cell>13.8</cell></row><row><cell>attr mAP with gt boxes</cell><cell>5.4</cell><cell>5.5</cell><cell>6.3</cell><cell>6.1</cell><cell>6.6</cell><cell>7.1</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc></figDesc><table /><note>Effect of vision pre-training on object detection tasks.model VL(w, q, v) separately. This also indicates that our pre-trained vision model can be utilized in any VL models by directly replacing their vision models, such as R101-C4 [2], with ours.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>BERT B 66.13±0.04 64.25±0.16 66.51±0.11 67.63±0.25 67.86±0.31 68.39±0.16</figDesc><table><row><cell>Dataset name</cell><cell>ImageNet</cell><cell>VG-obj</cell><cell>VG w/o attr</cell><cell>VG [2]</cell><cell>VG</cell><cell>4Sets→VG</cell></row><row><cell>#obj &amp; #attr</cell><cell>1000 &amp; 0</cell><cell>317 &amp; 0</cell><cell>1594 &amp; 0</cell><cell cols="3">1600 &amp; 400 1594 &amp; 524 1848 &amp; 524</cell></row><row><cell>R50-C4 +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 15 :</head><label>15</label><figDesc>Effect of object-attribute vocabulary. We use all grid features (maximal 273) for the ImageNet classification model (first column), and maximal 50 region features for OD models (other columns).perform consistently worse in attribute detection than C4 models, neither do FPN models show any advantage in object detection on VG. This contributes to the inferior performance of FPN, compared to C4, on downstream VL tasks, as discussed in Section 2.1.</figDesc><table><row><cell>How much does the diversity of visual concepts, i.e., object and attribute vocabularies, matter? We</cell></row><row><cell>directly train vision models on different datasets, including (1) standard ImageNet with 1K classes (Ima-</cell></row><row><cell>geNet), (2)</cell></row></table><note>Visual Genome with 317 object classes (VG-obj) that are shared with COCO 80 classes and OpenImagesV5 500 classes, (3) VG with all 1594 object classes (VG w/o attr), (4) VG with 1594 object classes and 524 attribute classes (VG), and (5) the merged OD dataset (4Sets) for pre-training and VG for fine-tuning. For all the OD models (the last four columns in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>±0.94 66.68 ±0.16 68.52 ±0.11 69.05 ±0.06 VinVL (ours) 65.60 ±0.21 68.13 ±0.26 70.25 ±0.05 71.34 ±0.17</figDesc><table><row><cell>model</cell><cell>region</cell><cell>GT-Obj</cell><cell>GT-Obj&amp;Stuff</cell><cell>Anderson et al. [2]</cell><cell>VinVL (ours)</cell></row><row><cell cols="2">Anderson</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">et al. [2]</cell><cell>63.81</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 16 :</head><label>16</label><figDesc>Effect of different region proposals on VQA.In this paper we have presented a new recipe to pre-train an OD model for VL tasks. Compared to the most widely used bottom-up and top-down model<ref type="bibr" target="#b1">[2]</ref>, the new model is bigger, better-designed for VL tasks, and pre-trained on much larger text-image corpora, and thus can generate visual features for a richer collection of visual objects and concepts that are crucial for VL tasks. We validate the new model via a comprehensive empirical study where we feed the visual features to a VL fusion model which is pre-trained on a large-scale</figDesc><table><row><cell>6 Conclusion</cell></row></table><note>paired text-image corpus and then fine-tuned on seven VL tasks. Our results show that the new OD model can substantially uplift the SoTA results on all seven VL tasks across multiple public benchmarks. Our ablation study shows that the improvement is mainly attributed to our design choices regarding diversity of object categories, visual attribute training, training data scale, model size, and model architecture.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc>top): black shorts; young, shirtless, standing, barefoot, surfing, little, playing boy; shirtless, standing, barefoot, walking, wet, surfing, young man; tan, bare, shirtless back; blue, clear, cloudy, hazy, light blue sky; young, shirtless, standing, surfing, barefoot, little boy; brown, short, wet, blond hair; brown, short, wet, blond hair; small, crashing wave; white, wet surfboard; white, crashing, big, rolling wave; wet, tan surfboard; green, blue fin; blue, calm, choppy, wavy, ocean, splashing, foamy, water, rough, sandy, wet ocean; wet, calm, sandy, splashing, wavy water; white, wet surfboard; bare, wet foot; blue, colorful, multi colored, floral shorts; calm, choppy, water, rough, Figure 3: Predictions from R101-C4 trained on VG from [2] (top), X152-C4 pre-trained on 4 OD datasets and finetuned on VG (bottom). Test image: COCO test2015 000000028839 foamy, wavy water; distant, rocky, hazy mountains; standing, shirtless, young, barefoot, wet, surfing, walking, smiling boy; calm ocean; distant, rocky mountain; white, bare, wet surfboard; wet, sandy, calm, tan beach; gray, big rock; blue, calm background; wet, brown, tan, sandy sand; wet shadow; blue, colorful, floral, multi colored swim trunks; yellow, plastic hand. Detections from our pre-trained X152-C4 model pre-trained on four datasets and fine-tuned on VG. There are some repetitive detections, but no obvious wrong detections. See Figure 3 (bottom): blue, green fin; young, barefoot, shirtless, standing, surfing, smiling, little, playing, looking, blond boy; young, barefoot, standing, shirtless, smiling, surfing, blond, playing, looking, little, walking, riding boy; shirtless, barefoot, standing, young, smiling, surfing, walking, wet, playing man; bare, wet foot; black, white surfboard; small, large, white, crashing, big, water, rolling, splashing, rough, foamy wave; bare, wet foot; dark, black, wet, cast shadow; blue, clear, hazy, cloudy, cloudless sky; black, gray, white, raised surfboard; black, wet, short short; brown, short, blond, wet, curly, wavy hair; distant, brown, large, rocky, hazy, big mountain; brown, short, dark, blond, wet hair; blue, white, calm, wavy, choppy, ocean, splashing, water, rough, clear, shallow water; bare, tan, light, beige back; black, blue, wet surfboard; small, dark, water, crashing, rolling, splashing, big wave; wet, white, sandy, tan surfboard; blue, colorful, floral, multi colored, patterned trunk; wet, brown, sandy, tan sand; white, blue, calm, foamy, choppy, splashing, wavy, ocean, rough, water, clear, shallow water; wet, brown, sandy, calm, tan, shallow, smooth, muddy, rough beach; black, white, young board; shirtless, young, standing, barefoot, smiling, surfing, looking, walking, playing boy; blue, calm, choppy, wavy, ocean, clear, rough, splashing, water, foamy, shallow, rippled ocean; yellow, gold bracelet; white, silver, black logo; wet, bare, bent, tan, crossed, hairy, short, skinny, back, muscular, extended, outstretched leg; black, gray, white board; brown, distant, large, rocky, big hill; brown, short, blond, wet, curly head; red, black logo; bare, raised, extended, holding, open, up, bent, outstretched hand; black, wet swim trunks; bare, wet, bent, tan, crossed, skinny, short, back, muscular leg; wet, brown, muddy, sandy, tan, shallow reflection.</figDesc><table><row><cell>B OSCAR+ pre-training</cell></row><row><cell>B.1 Pre-training Corpus</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Small 0 .</head><label>0</label><figDesc>22M Images, 2.5M QAs, 0.7M captions Medium 1.89M Images, 2.5M QAs, 0.7M captions, 1.67M pseudo-captions Large 5.65M Images, 2.5M QAs, 4.68M captions, 1.67M pseudo-captions</figDesc><table><row><cell>Source</cell><cell>VQA (train)</cell><cell>GQA (bal-train)</cell><cell>VG-QA (train)</cell><cell>COCO (train)</cell><cell>Flicker30k OpenImages (train) (od train)</cell><cell>CC (train)</cell><cell>SBU (all)</cell></row><row><cell cols="8">Image/Text 83k/545k 79k/1026k 87k/931k 112k/559k 29k/145k 1.67M/1.67M 3.1M/3.1M 875k/875k</cell></row><row><cell>w, q, v</cell><cell cols="3">Question, Answer, ImageFeatures</cell><cell cols="4">(Generated) Caption, (Generated) ImageTags, ImageFeatures</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 17 :</head><label>17</label><figDesc>Statistics of the pre-training corpus.</figDesc><table><row><cell>B.2 OSCAR+ pre-training objectives</cell></row></table><note>Masked Token Loss: A Loss Mimics Image Captioning. The word tokens of image captions (questions) w and word tokens of object tags (answers) q share the same linguistic semantic space, and the Masked Token Loss (MTL) is applied on tokens of both w and q. We define the discrete token sequence as h [w, q], and apply the Masked Token Loss (MTL) for pre-training. At each iteration, we randomly mask each input token in h with probability 15%, and replace the masked one h i with a special token [MASK]. The goal of training is to predict these masked tokens based on their surrounding tokens h \i and image features v by minimizing the negative log-likelihood:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head></head><label></label><figDesc>±0.94 66.68 ±0.16 68.52 ±0.11 69.05 ±0.06 VinVL (ours) 65.60 ±0.21 68.13 ±0.26 70.25 ±0.05 71.34 ±0.17</figDesc><table><row><cell>model</cell><cell>region</cell><cell>GT-Obj</cell><cell>GT-Obj&amp;Stuff</cell><cell>Anderson et al. [2]</cell><cell>VinVL (ours)</cell></row><row><cell cols="2">Anderson</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">et al. [2]</cell><cell>63.81</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 18 :</head><label>18</label><figDesc>Ablation of region and model on VQA. Two reasons why FPN performs worse than C4 on VL tasks. experimental results confirm the conclusion of<ref type="bibr" target="#b13">[14]</ref> that the FPN model does not provide better region features for VL tasks than the C4 model (Columns "R50C4" vs. "R50FPN" inTable 19). Our analysis reveals two reasons. First of all, all layers involved in feature extraction in the C4 model have been pretrained using ImageNet while the MLP head of FPN does not. It turns out that the VG dataset is still small to train a good visual features for VL tasks and using ImageNet-pre-trained weights is beneficial. This can be verified by two experiments: (1) When the R50-C4 model is trained on VG with its box head randomly initialized (VG-trained -R50C4 w/ box head randomly initialized), the C4 model's performance is the same as FPN; and (2) C4 and FPN achieve the same performance after vision pre-training on 4 datasets (68.3 vs. 68.2). The second reason is due the network architecture (CNN vs. MLP) of the box head in the OD model.±0.13 67.6±0.30 68.0±0.16 68.3±0.11 68.2±0.05 Initial 55.5±0.50 61.8 ±0.47 57.6±0.16 64.8±0.44 66.1±0.23 66.8±0.21</figDesc><table><row><cell>E More on FPN and Comparison of C4 and FPN</cell></row><row><cell>E.1 Our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Figure 8 :</head><label>8</label><figDesc>Checkpoints' mAP 50 on 4 validation sets: COCO with stuff (top left), Objects365 (top right), OpenImages (bottom left) and Visual Genome (1594 object classes, bottom right). For R50 models, the R50-FPN model is slightly better than C4 on COCO and Objects365 but slightly worse than C4 on Visual Genome. For R152 models, the R152-FPN model is consistently worse than the R152-C4 model on all 4 different datasets.the R152-FPN model is consistently worse than the R152-C4 model on all 4 different datasets. Therefore, The other run failed and thus there is no std for this experiment.</figDesc><table><row><cell></cell><cell>ImageNet-5k</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>[40]</cell><cell>4Sets</cell><cell cols="2">VG with Attr 4Sets→VG</cell></row><row><cell>grid feature (273)</cell><cell>68.3±0.29</cell><cell>65.2±2.47</cell><cell>67.5±0.20</cell><cell>69.4 *</cell></row><row><cell>region feature (50)</cell><cell>67.7±0.16</cell><cell>68.5±0.13</cell><cell>69.8±0.23</cell><cell>70.6±0.13</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head></head><label></label><figDesc>059±0.018 0.029±0.002 0.083±0.025 0.030±0.003 0.355±0.022 0.031±0.003 Grid-273 0.056±0.005 0.027±0.002 0.082±0.022 0.034±0.001 0.344±0.036 0.037±0.004 Object 0.373±0.040 0.031±0.005 0.663±0.042 0.034±0.003 0.687±0.064 0.036±0.005 Object-eff 0.165±0.029 0.029±0.002 0.442±0.119 0.036±0.003 0.475±0.049 0.037±0.005 Grid-50 (cpu) 1.943±0.244 0.480±0.042 4.050±0.398 0.469±0.046 17.765±1.693 0.501±0.047 Grid-273 (cpu) 2.032±0.230 1.368±0.056 4.052±0.372 1.283±0.067 17.664±1.713 1.326±0.053 Object (cpu) 11.808±1.322 0.500±0.045 31.863±7.932 0.585±0.044 29.641±3.097 0.565±0.044 Object-eff (cpu) 11.729±1.280 0.510±0.044 31.791±8.027 0.587±0.043 29.687±3.011 0.574±0.036</figDesc><table><row><cell>Model</cell><cell>R50-C4 Vision</cell><cell>VL</cell><cell>R101-C4 [2] Vision VL</cell><cell>X152-C4 Vision</cell><cell>VL</cell></row><row><cell>Grid-50</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the terms region and box interchangeably.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We find in our experiments that using the same training process, the X152-C4 model even produces better object detection result than the X152-FPN model. See Appendix E for details.<ref type="bibr" target="#b2">3</ref> Counting the NMS in the RPN module, there are in total 2 NMS operations in our efficient region feature extractor.<ref type="bibr" target="#b3">4</ref> It includes coordinates of the bounding boxes, and height &amp; width.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We use the same model to extract visual features.<ref type="bibr" target="#b5">6</ref> We use the captioning model released by OSCAR<ref type="bibr" target="#b20">[21]</ref>.<ref type="bibr" target="#b6">7</ref> <ref type="bibr" target="#b5">[6]</ref> uses a deep-learning-based text-image matching model to select the best caption candidate for a given image.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Image Captioning Leaderboard: https://competitions.codalab.org/competitions/3221</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">NoCaps leaderboard: https://eval.ai/web/challenges/challenge-page/355/leaderboard/1011</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">Our use of grid feature follows PixelBert<ref type="bibr" target="#b10">[11]</ref>. See Appendix F for details.<ref type="bibr" target="#b16">17</ref> Using the same training procedure on VG, we trained an R50-C4 model on the OpenImagesV5 dataset (500 classes). Using the region features produced by this model, the VQA performance is 63.55±0.14. The result is slightly worse than that of VG-obj because both VG and VQA images are from the COCO dataset but OpenImages images are not.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Xi Yin for her contributions to this project while she was in Microsoft. We thank Xiyang Dai for his conjecture that C4 arch is better than FPN because C4 arch makes better use of ImageNet initialization weights.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">nocaps: novel object captioning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03230</idno>
		<title level="m">Meta module network for compositional visual reasoning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Vse++: Improved visual-semantic embed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">dings. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Vivo: Surpassing human performance in novel object captioning with visual vocabulary pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13682</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning by abstraction: The neural state machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">GQA: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09506</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">In defense of grid features for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Interbert: Vision-andlanguage interaction for multi-modal pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13198</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">VilBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">12-in-1: Multi-Task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02315</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">X-linear attention networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10971" to="10980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ob-jects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge aware semantic concept expansion for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vl-Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00491</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fcos: A simple and strong anchor-free object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Position focused attention network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09748</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CAMP: Cross-Modal adaptive message passing for text-image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ernie-vil: Knowledge enhanced vision-language representations through scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05535</idno>
		<title level="m">Dual-path convolutional image-text embedding with instance loss</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unified visionlanguage pre-training for image captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">For each model, we also try different ways to extract features: (1) region features from different models&apos; proposed regions (same notations with models) where each image has maximal 50 region features, and (2) grid features where we use all grid features (Grid-273) or randomly sampled 50 grid features (Grid-50) for each image. We present the results of these model-region cross-combination experiments in Figure 5. We also present the mean accuracy over all box types to obtain a robust ranking of different checkpoints and the mean accuracy over all checkpoints to obtain a robust ranking of different box types. We have the following observations: • The richer the object vocabulary is</title>
		<idno>the better for VQA: OI:500 ≈ VG-obj:O317 &lt; ImageNet:O1000 &lt; VG:O1594</idno>
	</analytic>
	<monogr>
		<title level="m">We train vision models v = Vision(Img) on different datasets, i.e., OpenImages with 500 object classes (OI:O500), standard ImageNet with 1K classes (ImageNet:O1000)</title>
		<imprint/>
	</monogr>
	<note>Visual Genome with 317 object classes (VG-obj), Visual Genome with 1594 object classes (VG:O1594), VG with 1594 object classes and 524 attribute classes (VG:O1594A524), pretrain on the merged 4 datasets and finetune on VG:O1594A524 (4Sets→VG:O1594A524)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attribute information is crucial to VL tasks: all features trained with attributes (Columns with VG:O1594A524) are significantly better than those without attributes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">• Even for small vision backbone R50, vision pre-training makes vision features better: Column</title>
		<imprint/>
	</monogr>
	<note>4Sets→VG:O1594A524&quot; are better than all other columns. Notice that the vision pre-training improves both the region features and the grid features</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">• It is crucial to extract features from semantically diverse regions: regions from OI and VG-obj are significantly worse than all other regions, and is even worse than grid features</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
