<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Pursuit of Temporal Accuracy in General Activity Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Pursuit of Temporal Accuracy in General Activity Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting activities in untrimmed videos is an important but challenging task. The performance of existing methods remains unsatisfactory, e.g. they often meet difficulties in locating the beginning and end of a long complex action. In this paper, we propose a generic framework that can accurately detect a wide variety of activities from untrimmed videos. Our first contribution is a novel proposal scheme that can efficiently generate candidates with accurate temporal boundaries. The other contribution is a cascaded classification pipeline that explicitly distinguishes between relevance and completeness of a candidate instance. On two challenging temporal activity detection datasets, THU-MOS14 and ActivityNet, the proposed framework significantly outperforms the existing state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling activities with various temporal structures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detecting human activities is crucial to video understanding. This task has long been an important research topic in computer vision, and is gaining even more attention in recent years, due to the explosive growth of video data. Activity detection aims to answer two questions: <ref type="bibr" target="#b0">(1)</ref> what the activity is and (2) when it starts and ends. Thanks to the advances in deep learning, the past few years witnessed substantial progress in action recognition <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. These methods perform reasonably well in answering the question of "what", namely, they can recognize the class of an action with reasonable accuracy from a well-trimmed video. Nonetheless, recognition methods as such are not capable of answering the other question -"when". This is a major obstacle we face when working with untrimmed videos, i.e. those in which the actual activities only last for a fraction of the entire duration.</p><p>In the real world, however, untrimmed videos dominate. <ref type="bibr">Figure</ref> 1. An example of temporal action detection. The green box denotes an instance of the activity class "LayUp Drill" in the ActivityNet <ref type="bibr" target="#b4">[5]</ref> datasets. The blue box denotes a good detection result. The red boxes demonstrate some cases of bad localization and false detections. Note although the red box in the center has less than 0.4 IOU with the groundtruth instance, it does include a very representative part of the action instance.</p><p>From YouTube to Vimeo, from surveillance systems to personal movies, most videos are untrimmed. This motivates the community to shift its attention to the problem of detecting activities in untrimmed videos. Whereas some attempts <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> have been made to tackle this problem, the performance of existing methods remains far from satisfactory. On ActivityNet 2016 <ref type="bibr" target="#b4">[5]</ref>, the top detection method reports an mAP at 42.5% with 0.5-IOU threshold. As the threshold increases to 0.75 and 0.95, the mAP dramatically reduces to 2.88% and 0.06%. Other methods also see serious performance degradation when the threshold increases. Clearly, these methods, which are considered to represent the state of the art, are lacking in the capability of accurate detection. The difficulties primarily stem from several key challenges that have yet to be solved. First and foremost, it is nontrivial to tell whether a video segment captures an entire action or just a part of it. This issue is particularly prominent for detection methods based on region proposals, due to a key distinction between temporal proposals from spatial proposals: In an image, the appearance of an object often looks quite different from a local part of it. Hence, it is generally not very difficult for a visual detector to tell whether a window corresponds to an entire object or just a local part. However, for a video, one can often easily tell what the action is from just a small segment (or even a single frame), but the entire action may actually last much longer <ref type="bibr" target="#b23">[24]</ref>. The obscured distinction between an action and a part thereof makes it very difficult to accurately locate the starting and ending points.</p><p>Second, the duration of an action can vary significantly, from a second to several minutes. This significant variation in length poses two challenges: (1) Methods relying on sliding windows will meet substantial difficulties adapting to actions of different lengths. To obtain high localization accuracy, a large number of window scales and small sliding steps would be needed, which can lead to dramatically increased computational cost. (2) Conventional action recognition methods mostly operate on densely sampled frames. Hence, processing long actions is very expensive. This issue can limit their capability of analyzing long actions and make it difficult to perform end-to-end learning.</p><p>In this work, we aim to develop a new framework that can detect activities of various lengths from untrimmed videos, and accurately locate their temporal boundaries. This framework adopts the "proposal + classification" paradigm, which has been very successful in object detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>. It is however worth noting that despite its success in spatial detection, extending it to temporal detection is nontrivial, due to the challenges discussed above.</p><p>In tackling these challenges, we develop several innovative techniques. First, we propose a learning-based bottomup scheme to generate temporal proposals, called temporal actionness grouping. In contrast to the methods that rely on sliding windows, this scheme makes no assumption on the activity durations and thus can work with activities with significantly varying lengths. Also, thanks to its bottomup nature, the resultant proposals tend to be more sensitive to temporal boundaries than the sliding windows generated following a regular scheme. Second, we propose a cascaded classification pipeline that treats two kinds of false proposals, namely, background proposals and incomplete proposals, differently. Specifically, it filters out all background proposals in the first stage, and then removes incomplete proposals, i.e. those corresponding to a sub-segment instead of the entire actions, using a completeness filter. We found empirically that this cascaded approach is very effective in distinguishing the complete action proposals from others. Moreover, we adopt the sparse snippet sampling method introduced in <ref type="bibr" target="#b36">[37]</ref>, which can substantially reduce the computational cost for long action proposals.</p><p>We tested the proposed framework on two challenging datasets, namely, THUMOS14 <ref type="bibr" target="#b11">[12]</ref> and ActivityNet <ref type="bibr" target="#b4">[5]</ref>. On both datasets, our method outperforms the state of the art across different IOU levels. The performance gain is especially remarkable under high IOU thresholds, e.g. 0.7 or 0.9. This demonstrated its superior detection accuracy. Also note that the temporal structures of the actions in Ac-tivityNet are very different from those in THUMOS14. The consistent high performance on both datasets also shows the method's strong adaptivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action Recognition. Action recognition has been extensively studied in the past few years <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. Earlier methods are mostly based on hand-crafted visual features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref>. In past several years, deep learning has resulted in great performance gain. Convolutional Neural Networks (CNNs) are first introduced to this task in <ref type="bibr" target="#b13">[14]</ref>. Later, two-stream architectures <ref type="bibr" target="#b25">[26]</ref> and 3D-CNN <ref type="bibr" target="#b31">[32]</ref> are proposed to incorporate both appearance and motion features. There have also been efforts that explore the use of long-range temporal structures <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2]</ref>. Recently, Wang et al. <ref type="bibr" target="#b36">[37]</ref> introduced a segmental architecture that can efficiently handle longer videos via sparse sampling. Most action recognition methods assume that the input videos are well-trimmed, i.e. the action of interest lasts for nearly the entire duration. Hence, they usually ignore the localization issue and can focus on classification.</p><p>Object Detection. Recent object detection methods are good examples of how we can transfer the knowledge we learned in recognition tasks to the problem of detection.</p><p>Mainstream approaches for object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref> usually follow the paradigm of proposal + classification. The proposals are usually generated by bottom-up methods that exploit low-level cues <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42]</ref>. The Faster RCNN framework in <ref type="bibr" target="#b21">[22]</ref> uses a neural network to generate proposals, resulting in improved performance. Compared to methods that rely on sliding windows, these methods that utilize visual cues to generate proposals usually show considerably better performance while requiring less candidates <ref type="bibr" target="#b7">[8]</ref>.</p><p>Temporal Action Detection. Previous works on activity detection mainly use sliding windows as candidates and focus on feature representations and classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40]</ref>. Recent works incorporate deep networks into the detection frameworks and obtain improved performance <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b24">25]</ref>. In <ref type="bibr" target="#b38">[39]</ref>, a Recurrent Neural Network (RNN) is introduced, which takes frame-wise features as inputs and predicts the starting and ending points of the actions. However, in this work, the CNN for feature extraction and the higher-level RNN are trained separately, which may lead to sub-optimal performance -as the CNN features may not be suitable for temporal localization. Empirically, we also found that the predicted temporal boundaries Video Actionness Action Proposals Temporal Actionness Grouping  are not very accurate. In <ref type="bibr" target="#b24">[25]</ref>, a proposal CNN is used to filter out background windows and a localization CNN to improve the localization accuracy. This method relies on C3D <ref type="bibr" target="#b31">[32]</ref>. As C3D needs to be trained on clips of 16 frames with no large intervals in between, this method only handles temporal windows spanning at most 512 frames (about 17 seconds). This issue severely limits its application in real-world scenarios. Our method is distinct from these approaches in two aspects. First,using a novel bottom-up proposal scheme, it can generate more accurate candidates and can handle a wide range of action lengths. Second, the cascaded classifier design distinguishes the tasks of measuring the relevance and evaluating the completeness of a candidate instance, which we found rather important to achieve reasonable temporal localization accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Framework Overview</head><p>The proposed framework, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, comprises two stages: generating temporal proposals and classifying proposed candidates. The former is to produce a set of class-agnostic temporal regions that potentially reflect actions of interest, while the latter is to determine whether each candidate actually corresponds to an action and what class it belongs to. While such a proposal + classification paradigm is widely adopted in image object detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>, its use in action detection, as discussed earlier, still faces several key challenges. These challenges include the obscured distinction between wholes and parts and the sub-stantial variation in action duration. As a key contribution of this work, we propose novel designs for each stage to tackle these challenges.</p><p>Specifically, for temporal proposal generation, it is desirable to have regions that can cover a wide range of durations while accurately matching the ground-truths. Unlike previous methods that rely on sliding windows <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">40]</ref>, we devise a new proposal generation method called Temporal Actionness Grouping (TAG), where a convolutional network is learned to distinguish between action and background, i.e. those snippets that reflect no actions of interest. Given a video, we first sample a sequence of snippets, then use this network to produce actionness scores for them, and finally group them into temporal regions of various granularities in a bottom-up manner. This method can generate proposals with vastly varying lengths. Also, owing to the high sensitivity of the bottom-up procedure to temporal transitions, the boundaries of the derived proposals are often quite accurate.</p><p>With a set of proposed temporal regions, the next stage is to classify them into action classes, while filtering out those that do not actually capture a complete action. As mentioned, distinguishing between complete actions and incomplete ones is an important challenge in action detection. Note that previous methods have also attempted to tackle this issue. For example, the method in <ref type="bibr" target="#b24">[25]</ref> considers incomplete actions as background, which often leads to confusion when training the background/action classifier. We argue that actionness and completeness are essentially different characteristics, and thus design a cascaded classification pipeline with two steps. The first step removes those that belong to the background, while the second step, which we refer to as completeness classification, is dedicated to identifying those candidates that capture only an incomplete part of an action and dropping them from the results.</p><p>In what follows, we will elaborate on the detailed designs of both stages in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Temporal Region Proposals</head><p>The temporal region proposals are generated with a bottom-up procedure, which consists of three steps: extract snippets, evaluate snippet-wise actionness, and finally group them into region proposals. Here, each snippet combines a video frame together with an optical flow field derived therefrom, which conveys not only the scene appearance at a particular time step, but also the motion information at the moment. Given a video, a sequence of snippets will be extracted with a regular interval in between.</p><p>Actionness, as its name suggests, is a class-agnostic measure of the possibility of a snippet residing in any activity instance. Therefore, activity instances are likely to be found in the parts of a video containing snippets with relative higher actionness. To evaluate the actionness, we learn a binary classifier based on the Temporal Segment Network proposed in <ref type="bibr" target="#b36">[37]</ref>. It uses whole videos to train two stream CNNs in order to model the long-range temporal dynamics. In our practice, we modify it to take regions inside videos as input. To train this classifier, we treat all annotated action instances as positive regions, and randomly sample negative regions from the part of videos that have no action annotated with the ratio of 1 : 1.</p><p>With a sequence of snippets extracted from a video, we use the classifier learned as above to evaluate the actionness score for each snippet. The values of the scores range from 0 to 1, and thus can be interpreted as the probability of a snippet being in an action. To generate temporal region proposals, our basic idea is to group consecutive snippets with high actionness scores. As our goal is to develop a generic scheme that works with a wide range of situations, the robustness to noise and the capability of handling substantially varying durations are two desiderata.</p><p>With these objectives in mind, we devise a robust grouping scheme that tolerates occasional outliers, e.g. a small fraction of low-actionness snippets within an action segment should be allowed. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, the scheme first obtains a number of action fragments by thresholding -a fragment here is a consecutive sub-sequence of snippets whose actionness scores are above a certain threshold τ . Then, to generate a region proposal, we pick a fragment as a starting point and expand it recursively by absorbing succeeding fragments. The expansion terminates when the portion of low-actionness snippets goes beyond γ, a positive value which we refer to as the tolerance threshold. Beginning with different fragments, we can obtain a collection of different region proposals. Note that this scheme is controlled by two design parameters: the actionness threshold τ and the tolerance threshold γ. It can be seen that different combination of these thresholds will lead to proposals of different granularities. So we use two sets of evenly distributed values for both τ and γ. The final proposal set is the union of those derived from individual combination of the two values. This multi-threshold design enable us to generate proposals with diverse granularities. More importantly, it removes the need for manual parameter tunning on a specific dataset, which is time costing and can not generalize. Near duplicate proposals collected with this scheme will be pruned using nonmaximal suppression with an IOU threshold 0.95. For a typical video, it would result in a collection that comprises about 30 proposals per minute. While the size of the collection is moderate, the proposals therein, however, can often cover a very wide range of durations, from less than 10 frames to more than 5000 frames.</p><p>We call the proposal generation scheme described above Temporal Actionness Grouping (TAG). Compared to existing methods, it has several advantages: (1) Thanks to the actionness classifier, the generated proposals are mostly focused on action-related contents, which greatly reduce the number of needed proposals. (2) Action fragments are sensitive to temporal transitions. Hence, as a bottom-up method that relies on merging action fragments, it often yields proposals with more accurate temporal boundaries.</p><p>(3) With the multi-threshold design, it can cover a broad range of actions without the need of case-specific parameter tuning. With these properties, the proposed method can achieve high recall with just a moderate number of proposals. This also benefits the training of the classifiers in the next stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Detecting Action Instances</head><p>With a set of candidate temporal regions, the next stage is to identify complete action instances therefrom and classify them to specific action categories. As mentioned, this is accomplished by a cascaded pipeline with two steps: activity classification and completeness filtering. The first step removes those belonging to the background and classifies the remaining ones. The retained subset may still contain incomplete or over-complete instances. The second step will filter out such proposals, using class-specific completeness filters. With task of completeness testing separated from activity classification, we no longer need to entangle two essentially different goals, which, as we empirically found, could severely confuse the classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Activity Classification</head><p>We base the activity classifiers on TSN <ref type="bibr" target="#b36">[37]</ref>. During training, region proposals that overlap with a ground-truth instance with an IOU above 0.7 will be used as positive samples. In selecting negative samples, a different criterion is used. Instead of using IOU, we consider a proposal as a negative sample only when less than 5% of its time span overlaps with any annotated instances. The rationale behind is that incorrectly localized samples, e.g. those only cover small parts of an action, can also have low IOU values. However, if we consider them as negative samples, the activity classifier can be severely confused, as they can still contain parts of action that are very discriminative <ref type="bibr" target="#b23">[24]</ref>. Using the modified criterion above, we can preclude such samples from being fed to the training set, such that the activity classifiers can focus on distinguishing between the actions of interest and the background.</p><p>In the testing, the learned classifiers will be applied to a video at a fixed frame rate, producing the classification scores for each sampled snippet. For each region proposal, the snippet-wise classification scores are aggregated into region-level scores to classify a proposal to its activity class or background. Only the proposals classified as non-background classes will be retained for completeness filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Completeness Filtering</head><p>The second step is to identify those incorrectly localized candidates from the remaining ones, including both incomplete and over-complete action instances. The key question here is what characteristics are good indicators of completeness.</p><p>We observe that it is often quite difficult to tell whether a video segment covers an entire action or not by only look-  ing at the individual snippets inside it. The judgment would become much more obvious if we also look at: 1) the differences between different parts of the segment and 2) the surroundings, e.g. what happens before and after the segment. Inspired by this, we devise a simple feature representation that reuses the activity classification scores, which comprises three parts: (1) A temporal pyramid of two levels. The first level pools the snippet scores within the proposed region. The second level split the segment into two parts and pool the snippet scores inside each part. (2) The average classification scores of two short periods -the ones before and after the proposed region. The combination of these features characterizes not only the temporal structures inside but also the contexts that come before and after. Since the temporal structures of different kinds of activities vary significantly, we train class-specific SVMs on such a representation, one for each class, with hard negative mining.</p><p>Combining both steps, we can obtain the final detection confidence for each proposal, as</p><formula xml:id="formula_0">S Det = P a × exp(S c ),<label>(1)</label></formula><p>where P a is the probability from the activity classifier and S c is the completeness score. This formulation denotes that the final confidence of a detection result is the combination of its class probability and completeness score. Nonmaximal suppression is employed to remove duplicate detections with IOU thresholds of 0.6 on ActivityNet datasets and 0.2 on THUMOS14 following <ref type="bibr" target="#b24">[25]</ref>. Our experiments showed that the completeness filters can effectively remove incorrectly localized proposals. Also, due to the simple feature design, the cost of the completeness filtering is insignificant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>In this section, we evaluate the effectiveness of the proposed framework on standard benchmarks. We first introduce the evaluation datasets and the implementation details of our method, and then explore the effect of components in the proposed framework. Finally, we compare the performance of the proposed framework with other state-of-theart approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>In this work, we conduct experiments on two largescale benchmark datasets: ActivityNet <ref type="bibr" target="#b4">[5]</ref> and THU-MOS14 <ref type="bibr" target="#b11">[12]</ref>. ActivityNet <ref type="bibr" target="#b4">[5]</ref> datasets have two versions, v1.2 and v1.3. The former contains 9682 videos in 100 classes, while the latter, which is a superset of v1.2 and used in the ActivityNet Challenge 2016, contains 19, 994 videos in 200 classes. Videos in each version of the dataset are divided into three disjoint subsets, training, validation, and testing, with a 2 : 1 : 1 split.</p><p>The THUMOS14 <ref type="bibr" target="#b11">[12]</ref> dataset has 1010 videos for validation and 1574 videos for testing. These videos contain annotated action instances that belong to 20 activity classes. This dataset does not provide the training set by itself. Instead, the UCF101 <ref type="bibr" target="#b28">[29]</ref> is appointed as the official training set. As no temporal annotations are provided by this training set, we train the detectors on the validation set and evaluate the performance on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Implementation Details</head><p>We use SGD to learn CNN parameters in our framework, with the batch size of 128 and momentum as 0.9. The network training is conducted with the publicly available TSN toolbox <ref type="bibr" target="#b36">[37]</ref> and Caffe <ref type="bibr" target="#b10">[11]</ref>. We initialize the CNNs with pre-trained models from ImageNet <ref type="bibr" target="#b0">[1]</ref>. The initial learning rates are set to 0.001 for RGB networks and 0.005 for optical flow networks.</p><p>We use an actionness classifier trained on ActivityNet v1.2 on all three datasets for proposal generation. Its RGB CNN is trained for 2500 iterations on ActivityNet v1.2's training set and 18000 iterations for its optical flow CNN. For detectors, the RGB CNN is trained for 9500 iterations and 20000 for optical flow CNN on both versions of Ac-tivityNet. The learning rates are decreased after every 4000 and 9000 iterations, respectively. For THUMOS14, because there are only about 220 videos related to the 20 action classes in the training set, we train RGB CNN for 1000 iterations and optical flow CNN for 6000 iterations. The learning rates are decreased after every 400 and 2500 iterations, respectively. The models and the source codes will be released 1 . 1 https://github.com/yjxiong/action-detection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Evaluation Metrics</head><p>As both datasets come from action recognition challenges, each dataset has its own convention of reporting performance metrics. We follow their conventions, reporting mean average precision (mAP) at different IOU thresholds. To obtain decent mAP values at high IOU criteria, the detected instances must have correct class labels and accurately locate starting and ending time of the complete instances. So these mAP values are also good measures for the accuracy of temporal localization. On both versions of ActivityNet, the IOU thresholds are {0.5, 0.75, 0.95}. The average of mAP values with IOU thresholds [0.5 : 0.05 : 0.95] are also reported on ActivityNet. On THUMOS14, the IOU thresholds are {0.1, 0.2, 0.3, 0.4, 0.5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Exploration Study</head><p>We first conduct experiments to evaluate the effectiveness of individual components in the proposed framework and investigate how they contribute to performance improvement.  <ref type="table">Table 3</ref>. Detection performance of using different action proposals, measured by mAP. (activity classifier is TSN+Inception-v3; completeness filtering is included).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposal Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate Region Proposal Methods</head><p>It is known that in image-based object detection, sparse object candidates are better for detector training and improving detection results. But is it also true for the temporal action detection task?</p><p>To answer this, we compare the performance of different candidate proposing methods, using the average recall with IOU thresholds from 0.5 to 0.95 as the performance metrics. A representative for the dense proposal methods is the sliding window approach. We generate windows in 20 exponential scales starting from 0.3 second long, and slide them through the whole video with a step size of 0.4 times of window length. This setting allow the sliding window approach to generate roughly two times the number of proposal in average compared with TAG.</p><p>In comparison, we also include other state-of-the-art sparse proposal methods, including TAP <ref type="bibr" target="#b3">[4]</ref>, DAP <ref type="bibr" target="#b2">[3]</ref>, and the proposal networks in <ref type="bibr" target="#b24">[25]</ref>. For these compared sparse proposal methods, we also have them to produce twice the average number of proposals compared with TAG. The only exception is TAP <ref type="bibr" target="#b3">[4]</ref> on ActivityNet v1.2 where the publicly available proposal list has on average 90 proposals per video. Results of average recall rates are shown in Table 1. These results show that compared to other methods, TAG can achieve significantly higher recall with considerably less proposals.</p><p>Another desirable property of TAG is that it is based on a class-agnostic actionness classifier. This may enable it to work on unseen activity classes. To investigate this conjecture, we use the actionness classifier trained on ActivityNet v1.2 to generate proposals on THUMOS14 and ActivityNet v1.3. In table 2, it is observed that from overlapped classes with ActivityNet v1.2 to those unseen classes, there is no severe performance drop. This clearly demonstrates the generalization capacity of our proposal scheme.</p><p>Additionally, we evaluate the effectiveness of using sparse proposals for temporal action detection. We compare the detection performance of using TAG and sliding windows for proposals. Here we adjust sliding window configurations to achieve higher AR with much more proposals. The results shown in <ref type="table">Table 3</ref> suggest that using sparse proposals from TAG, we can achieve a comparable mAP on THUMOS14 and a much better mAP on ActivityNet v1.2.</p><p>Activity Classifiers It is known that using deeper CNN architectures will benefit action recognition systems <ref type="bibr" target="#b36">[37]</ref>. To investigate whether this is also true in temporal action detection, we try two architectures, BN-Inception <ref type="bibr" target="#b9">[10]</ref> and a deeper architecture Inception V3 <ref type="bibr" target="#b29">[30]</ref>. Related results are shown in <ref type="table" target="#tab_2">Table 4</ref>, which show that deeper CNN architectures also benefit the task of temporal action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Completeness Filters</head><p>In the proposed framework, the detection of activity instances from candidate proposals is per-Activity Classifiers mAP@0.5 BN-Inception <ref type="bibr" target="#b9">[10]</ref> 40.29 Inception V3 <ref type="bibr" target="#b29">[30]</ref> 41.13  <ref type="table">Table 5</ref>. Ablation study on the design of candidate classification module. "One Stage" refers to a single stage of classifiers. "Cascade + H1" and "Cascade + H2" refer to cascade with different duration based heuristics. "Cascade + Comp." refers to the proposed framework.</p><p>formed in two cascaded stages, namely activity classification and completeness filtering. To investigate the validity of this cascaded design, we perform an ablation study, whose results are shown in <ref type="table">Table 5</ref>.</p><p>The first baseline is a classification module without the cascade design, which has only one set of classifiers. The module is trained with foreground/background IOU thresholds of 0.7 and 0.3 as in <ref type="bibr" target="#b24">[25]</ref>. This basically means that both background and non-complete region proposals are treated as negative samples. The single stage classifiers have to distinguish both kinds of false proposals from complete action instances. This is referred to as "One Stage" in <ref type="table">Table 5</ref>.</p><p>Another baseline is replacing the learning-based completeness filters in the cascade with heuristics. In previous methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref>, duration-based heuristics are widely used to counter the effect of incomplete instances. Here we compare with two representative forms of these heuristics. The first one, "H1" multiplies T α on the classification scores, where T is the relative duration of a segment and α is set to 0.7 to obtain competitive results on ActivityNet v1.2. The second baseline heuristics, "H2", multiplies the detection scores with the frequencies of the proposal durations <ref type="bibr" target="#b24">[25]</ref>. We can observe in <ref type="table">Table 5</ref> that the well tuned heuristics cannot work well on both datasets at the same time. Our learning-based completeness filters perform consistently on different datasets without tunning and produce comparable or superior detection accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Comparison with State of the Arts</head><p>Finally we compare our method with other state-of-theart temporal action detection methods on THUMOS14 <ref type="bibr" target="#b11">[12]</ref>, ActivityNet v1.2 <ref type="bibr" target="#b4">[5]</ref> and ActivityNet v1.3 <ref type="bibr" target="#b4">[5]</ref>. To re- port performance we use the metrics described in Sec. 6.3. Among all the metrics used here, we would like to highlight the average mAP on ActivityNet datasets. It is the average of the mAP values at IOU thresholds from 0.5 to 0.95, which could well reflect the ability to accurately localize the temporal boundaries of action instances. It is also worth noting that the average action duration in THUMOS14 is 4 seconds, while those in ActivityNet datasets can be as long as 50 seconds. Meanwhile the average video duration in THUMOS14 is about twice than those of the ActivityNet datasets (233 v.s. 114). This difference in the statistics actually reflects the different nature of these datasets in terms of granularities and temporal structures. Hence, strong adaptivity is required to perform consistently well on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THUMOS14</head><p>The results on THUMOS14 dataset are shown in <ref type="table" target="#tab_3">Table 6</ref>. We first compare with the challenge results <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>. as well as recent works, including methods using segment-based 3D CNN <ref type="bibr" target="#b24">[25]</ref>, score pyramids <ref type="bibr" target="#b39">[40]</ref>, and recurrent reinforcement learning <ref type="bibr" target="#b38">[39]</ref>. Using the proposed framework, we are able to outperform previous state-of-the-art methods by over 10% in most cases.</p><p>ActivityNet The results of ActivityNet v1.2 and v1.3 are shown in <ref type="table">Table 7</ref> and <ref type="table">Table 8</ref> respectively. We first report results on both validation sets. For references, we list the performance of highest ranking entries in the ActivityNet 2016 challenge. Finally, we submit our results to the test server of ActivityNet v1.3 and report the detection performance on the testing set. The proposed framework, using a single model instead of an ensemble, is able to obtain detection performance very close to the winning submission at the IOU threshold of 0.5. With a higher IOU threshold, 0.75 and 0.95, the detectors are put to test in accurately locating action boundaries. Under such challenging settings, the proposed detector performs dramatically better than all rivals, e.g. with IOU threshold 0.75, we obtain an mAP at 26% (vs 17.8%); with IOU threshold 0.95, we obtain an mAP at 6.66% (vs 2.88%). In terms of average mAP,  <ref type="table">Table 8</ref>. Action detection results on ActivityNet v1.3, measured by mean average precision (mAP) for different IoU thresholds α and the average mAP of IOU thresholds from 0.5 to 0.95. In the table, "Ours-B" refers to the results using the BN-Inception architecture.</p><p>we are able to outperform other state-of-the-arts by around 10%. This clearly demonstrates the superior capability of our method in accurate action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Discussion</head><p>From the experimental results presented above, we have the following observations: 1) Based on class-agnostic actionness, our TAG proposal method excels in generating temporal proposals and can generalize well to unseen activities. The sparse proposals generated by TAG are conducive to the detection performance. 2) The two-stage cascaded design of our classification module is crucial for action detection with high temporal accuracy. It is also a generic design that adapts well to activities with varying temporal structures, such as in THUMOS14 and ActivityNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we proposed a generic framework for the task of temporal action detection. It is built on the proposal + classification paradigm. By introducing the temporal actionness grouping for action proposal and a cascaded design of proposal classifiers, we achieved significant perfor-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>An overview of the proposed framework. This video from the ActivityNet [5] dataset contains five instances of "Triple Jump" class. The proposed action detection framework starts with evaluating the actionness of the snippets of the video. A set of temporal action proposals (in orange color) are generated with temporal actionness grouping (TAG). The proposals are evaluated against the cascaded classifiers to verify their relevance and completeness. Only proposals being complete instances of triple jumping are produced by the framework. Notice how non-complete proposals and background proposals are rejected by the framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>An illustration of the temporal actionness grouping algorithm. We show two concurrent grouping processes with the foreground/background thresholds τ of 0.7 and 0.9. Each snippet is labeled as "1" for foreground or "0" for background. The red open box denotes the proposal undergoing grouping. The gray ones are emitted temporal action proposals. Note there are several "0" snippet grouped because we have not hit the tolerance threshold γ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The proposal classification module. The activity classifiers first remove background proposals and classify the proposals to its activity class. Then the class-aware completeness filters evaluate the remaining proposals using features from the temporal pyramid and surrounding fragments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Triple Jump Triple Jump Triple Jump Triple Jump Triple Jump</head><label></label><figDesc></figDesc><table><row><cell>Detection</cell><cell cols="2">Triple Jump</cell><cell>Triple Jump</cell><cell></cell><cell>Triple Jump</cell><cell></cell><cell></cell><cell cols="2">Triple Jump</cell></row><row><cell>Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Completeness</cell><cell>Complete?</cell><cell>Complete?</cell><cell>Complete?</cell><cell></cell><cell>Complete?</cell><cell></cell><cell>Complete?</cell><cell>Complete?</cell><cell>Complete?</cell></row><row><cell>Filters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Activity Classifiers</cell><cell>Triple Jump?</cell><cell>Triple Jump?</cell><cell>Triple Jump?</cell><cell>Triple Jump?</cell><cell>Triple Jump?</cell><cell>Triple Jump?</cell><cell>Triple Jump?</cell><cell cols="2">Triple Jump? Triple Jump?</cell></row><row><cell>Groundtruth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison between different temporal action proposal methods. "AR" refers to the average recall rates. "-" refers to the case where the result is not available. Study on the ability of the proposal scheme to work with unseen activity classes. We show the average recalls on the two dataset w.r.t. seen and unseen classes for the underlying actionness classifier trained on ActivityNet v1.2</figDesc><table><row><cell></cell><cell></cell><cell cols="4">THUMOS14 ActivityNet v1.2</cell></row><row><cell></cell><cell></cell><cell cols="3"># Prop. AR # Prop.</cell><cell>AR</cell></row><row><cell cols="2">Sliding Windows</cell><cell>204</cell><cell>21.2</cell><cell>100</cell><cell>34.8</cell></row><row><cell cols="2">SCNN-prop [25]</cell><cell>200</cell><cell>20.0</cell><cell>-</cell><cell>-</cell></row><row><cell>TAP [4]</cell><cell></cell><cell>200</cell><cell>23.0</cell><cell>90</cell><cell>14.9</cell></row><row><cell>DAP [3]</cell><cell></cell><cell>200</cell><cell>37.0</cell><cell>100</cell><cell>12.0</cell></row><row><cell>TAG</cell><cell></cell><cell>117</cell><cell>36.9</cell><cell>56</cell><cell>66.7</cell></row><row><cell></cell><cell cols="2">THUMOS14</cell><cell></cell><cell cols="2">ActivityNet v1.3</cell></row><row><cell cols="2">Overlapped</cell><cell>Unseen</cell><cell cols="2">Overlapped</cell><cell>Unseen</cell></row><row><cell cols="6">(10 classes) (10 classes) (100 classes) (100 classes)</cell></row><row><cell>AR</cell><cell>46.6</cell><cell>28.3</cell><cell></cell><cell>68.1</cell><cell>66.4</cell></row><row><cell cols="2">Proposal Method</cell><cell cols="4">mAP@0.5 THUMOS14 ActivityNet v1.2</cell></row><row><cell cols="2">Sliding Windows</cell><cell cols="4">29.82 (787 prop., 62.3AR) (486 prop., 70.9AR) 35.33</cell></row><row><cell>TAG</cell><cell></cell><cell>28.25</cell><cell></cell><cell></cell><cell>41.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Using different CNN architectures for the activity classifiers in the proposed framework. The results are reported on the validation set of ActivityNet v1.2.</figDesc><table><row><cell>Module</cell><cell cols="2">mAP@0.5 THUMOS14 ActivityNet v1.2</cell></row><row><cell>One Stage</cell><cell>13.96</cell><cell>8.97</cell></row><row><cell>Cascade + H1</cell><cell>8.17</cell><cell>42.94</cell></row><row><cell>Cascade + H2</cell><cell>22.13</cell><cell>9.10</cell></row><row><cell>Cascade + Comp.</cell><cell>28.25</cell><cell>41.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Wang et. al. [35] 18.2 17.0 14.0 11.7 8.3 Oneata et. al. [20] 36.6 33.6 27.0 20.8 14.4 Richard et. al. [23] 39.7 35.7 30.0 23.2 15.2 Action detection results on THUMOS14, measured by mean average precision (mAP) for different IoU thresholds α. The upper half of the table show challenge results back in 2014.</figDesc><table><row><cell cols="4">THUMOS14, mAP@α</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>S-CNN [25]</cell><cell cols="5">47.7 43.5 36.3 28.7 19.0</cell></row><row><cell>Yeung et. al. [39]</cell><cell cols="5">48.9 44.0 36.0 26.4 17.1</cell></row><row><cell>Yuan et. al. [40]</cell><cell cols="5">51.4 42.6 33.6 26.1 18.8</cell></row><row><cell>Ours</cell><cell cols="5">64.1 57.7 48.7 39.8 28.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ActivityNet v1.2 (validation), mAP@α Method 0.5 0.75 0.95 Average Ours-B 38.67 22.94 5.23 23.58 Ours 41.13 24.06 5.04 24.88Table 7. Action detection results on ActivityNet v1.2, measured by mean average precision (mAP) for different IoU thresholds α and the average mAP of IOU thresholds from 0.5 to 0.95.. In the table "Ours-B" refer to the results of using BN-Inception architecture instead of Inception V3.</figDesc><table><row><cell cols="4">ActivityNet v1.3 (validation), mAP@α</cell><cell></cell></row><row><cell>Method</cell><cell>0.5</cell><cell>0.75</cell><cell cols="2">0.95 Average</cell></row><row><cell cols="2">Montes et. al. [17] 22.51</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Sigh et. al. [28]</cell><cell>34.47</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang et. al. [38]</cell><cell>43.65</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours-B</cell><cell>37.17</cell><cell>22.13</cell><cell>4.95</cell><cell>22.67</cell></row><row><cell>Ours</cell><cell>39.12</cell><cell>23.48</cell><cell>5.49</cell><cell>23.98</cell></row><row><cell cols="4">ActivityNet v1.3 (testing), mAP@α</cell><cell></cell></row><row><cell>Method</cell><cell>0.5</cell><cell>0.75</cell><cell cols="2">0.95 Average</cell></row><row><cell>Wang et. al. [38]</cell><cell>42.478</cell><cell>2.88</cell><cell>0.06</cell><cell>14.62</cell></row><row><cell>Singh et. al. [27]</cell><cell cols="2">28.667 17.78</cell><cell>2.88</cell><cell>17.68</cell></row><row><cell>Singh et. al. [28]</cell><cell cols="2">36.398 11.05</cell><cell>0.14</cell><cell>17.83</cell></row><row><cell>Ours</cell><cell cols="3">40.689 26.017 6.667</cell><cell>26.05</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>mance improvement over previous state-of-the-art methods. Moreover, we demonstrated that our method is both accurate and generic, being able to localize temporal boundaries precisely and working well on datasets with different temporal structure of activities.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2782" to="2795" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognition using regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">THU-MOS Action Recognition Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bag-of-fragments: Selecting and encoding video fragments for event detection and recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cappallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal activity detection in untrimmed videos with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The lear submission at thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">THUMOS Action Recognition Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action snippets: How many frames does human action recognition require</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for finegrained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Untrimmed video classification for activity detection: submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno>abs/1607.01979</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combining the right features for complex event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2696" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">THUMOS Action Recognition Challenge</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">UTS at activitynet 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AcitivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Realtime action recognition with enhanced motion vector CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
