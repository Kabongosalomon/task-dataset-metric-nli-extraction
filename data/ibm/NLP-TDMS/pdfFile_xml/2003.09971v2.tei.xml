<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Better Variant of Self-Critical Sequence Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
							<email>rluo@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">TTI-Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Better Variant of Self-Critical Sequence Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present a simple yet better variant of Self-Critical Sequence Training. We make a simple change in the choice of baseline function in REINFORCE algorithm. The new baseline can bring better performance with no extra cost, compared to the greedy decoding baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-Critical Sequence Training(SCST), upon its release, has been a popular way to train sequence generation models. While originally proposed for image captioning task, SCST not only has become the new standard for training captioning models <ref type="bibr" target="#b20">(Yao et al., 2017;</ref><ref type="bibr" target="#b4">Dognin et al., 2019;</ref><ref type="bibr">Jiang et al., 2018;</ref><ref type="bibr" target="#b8">Liu et al., 2018b;</ref><ref type="bibr">Ling and Fidler, 2017;</ref><ref type="bibr" target="#b7">Liu et al., 2018a;</ref><ref type="bibr" target="#b2">Chen et al., 2019;</ref><ref type="bibr" target="#b19">Yang et al., 2019;</ref><ref type="bibr" target="#b21">Zhao et al., 2018)</ref>, but also has been applied to many other tasks, like video captioning <ref type="bibr" target="#b17">(Li et al., 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2018;</ref><ref type="bibr">Li and Gong, 2019)</ref>, reading comprehension <ref type="bibr" target="#b6">(Hu et al., 2017)</ref>, summarization <ref type="bibr" target="#b1">(Celikyilmaz et al., 2018;</ref><ref type="bibr" target="#b13">Paulus et al., 2017;</ref><ref type="bibr" target="#b12">Pasunuru and Bansal, 2018)</ref>, image paragraph generation <ref type="bibr" target="#b10">(Melas-Kyriazi et al., 2018)</ref>, speech recognition .</p><p>SCST is used to optimize generated sequences over a non-differentiable objective, usually the evaluation metrics, for example, CIDEr for captioning, ROUGE for summarization. To optimize such objective, SCST adopts REINFORCE with baseline <ref type="bibr" target="#b18">(Williams, 1992)</ref>, where a "Self-Critical" baseline is used; specifically, the score of the greedy decoding output is used as the baseline. This is proved to be better than learned baseline function which is more commonly used in Reinforcement Learning literature.</p><p>In this work, we present a different baseline choice which was first proposed in <ref type="bibr" target="#b11">(Mnih and Rezende, 2016)</ref>, to the best of our knowledge. With more elaboration in Sec. 3, this baseline can be described as a variant of "Self-Critical". This method is simple, but also faster and more effective compared to the greedy decoding baseline used in SCST.</p><p>2 Recap for SCST <ref type="bibr">MIXER (Ranzato et al., 2015)</ref> is the first to use REINFORCE algorithm for sequence generation training. They use a learned function approximator to get the baseline.</p><p>SCST inherits the REINFORCE algorithm from MIXER, but discards the learned baseline function. Instead, SCST uses the reward of the greedy decoding result as the baseline, achieving better captioning performance and lower gradient variance. Many variants of SCST have also been proposed <ref type="bibr" target="#b0">(Anderson et al., 2018;</ref><ref type="bibr" target="#b5">Gao et al., 2019;</ref><ref type="bibr" target="#b6">Hu et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Math formulation</head><p>The goal of SCST, for example in captioning, is to maximize the expected CIDEr score of generated captions. max</p><formula xml:id="formula_0">θ Eĉ ∼p θ (c|I) [R(ĉ)]</formula><p>whereĉ is a sampled caption; I is the image; p θ (c|I) is the captioning model parameterized by θ, and R(·) is the CIDEr score. Since this objective is not non-differentiable with respect to θ, back propagation is not feasible. To optimize it, a policy gradient method, specifically REINFORCE with baseline (Williams, 1992) is used.</p><formula xml:id="formula_1">∇ θ E[R] ≈ (R(ĉ) − b)∇ θ log p θ (ĉ|I)</formula><p>The policy gradient method allows estimating the gradient from individual samples (the right-hand side) and applying gradient ascent. To reduce the variance of the estimation, a baseline b is needed, and b has to be independent ofĉ.</p><p>In SCST, the baseline is set to be the CIDEr score of the greedy decoding caption, denoted as c * . Thus, we have</p><formula xml:id="formula_2">∇ θ E[R] ≈ (R(ĉ) − R(c * ))∇ θ log p θ (ĉ|I)</formula><p>3 The Better SCST The success of SCST comes from better gradient variance reduction introduced by the greedy decoding baseline. In our variant, we use the baseline proposed in <ref type="bibr" target="#b11">(Mnih and Rezende, 2016)</ref> to achieve even better variance reduction.</p><p>Following <ref type="bibr" target="#b11">(Mnih and Rezende, 2016)</ref>, we sample K captions for each image when applying RE-</p><formula xml:id="formula_3">INFORCE:ĉ 1 . . .ĉ K ,ĉ k ∼ p θ (c|I),</formula><p>The baseline for each sampled caption is defined as the average reward of the rest samples. That is, for captionĉ k , its baseline is</p><formula xml:id="formula_4">b k = 1 K − 1 j =k R(ĉ j )<label>(1)</label></formula><p>Since each sample is independently drawn, b k is a valid baseline. The final gradient estimation is</p><formula xml:id="formula_5">∇ θ ≈   R(ĉ k ) − 1 K − 1 j =k R(ĉ j )   ∇ θ log p(ĉ k |I)<label>(2)</label></formula><p>Note that, b k is an estimation of expected reward, which is similar to the learning objective of value functions in other Reinforcement Learning algorithms. The expected reward is usually a good baseline choice in that it can effectively reduce gradient variance. In Sec. 4, we show that our gradient variance is lower than SCST empirically.</p><p>It is still a "Self-Critical" baseline because the critic is still from itself: its other sampling results, instead of the greedy decoding result. Our variant can also been seen as a special case of <ref type="bibr" target="#b5">(Gao et al., 2019)</ref>. While SCST is T-step-maxpro, ours is Tstep-sample, with the naming in <ref type="bibr" target="#b5">(Gao et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>For all models, we first pretrain them using standard cross-entropy loss and then switch to Self-Critical training.</p><p>For a fair comparison, during Self-Critical stage, we always sample 5 captions for each image, same for both SCST and our variant.</p><p>All the experiments are done on COCO captioning dataset <ref type="bibr">(Lin et al., 2014)</ref>. The scores are obtained on Karparthy test split <ref type="bibr">(Karpathy and Fei-Fei, 2015)</ref> with beam search of beam size 5 if not explicitly noted.</p><p>Speed Since no extra greedy decoding is needed, our method is slightly faster than SCST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on different model architectures</head><p>We experiment with four different architectures. FC and Att2in are from <ref type="bibr">SCST(Rennie et al., 2017)</ref>. UpDown is from <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref>. Transformer is adapted from <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> for captioning task. <ref type="table" target="#tab_1">Table 1</ref> shows that our variant is better than SCST on all architectures, especially on Transformer.</p><p>Different training hyperparameters Here we adopt a different training setting ('Long') for Up-Down model. The 'Long' setting (from https:// github.com/yangxuntu/SGAE) uses a larger batch size and a longer training time. <ref type="table" target="#tab_2">Table 2</ref> shows that there is always a gap between our method and SCST which cannot be closed by longer training or a larger batch size. <ref type="table" target="#tab_3">Table 3</ref> shows that our variant is consistently better than SCST with different random seeds. All the models use 'Long' setting with UpDown model. Specifically, we pretrain 5 models using crossentropy loss, and then apply SCST and our method respectively. The same RS * means they share the same pretrained model. <ref type="figure">Figure 1</ref> shows the model performance on the validation set during training, after entering Self-Critical stage. The scores are averaged over the 5 UpDown(Long) models above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training curves</head><p>Is greedy decoding necessary for SCST We also experiment with a variant of SCST, by replacing the greedy decoding output with a sampled output. (This is similar to our method with K = 2.) <ref type="table" target="#tab_5">Table 4</ref> shows that one sample baseline is worse than greedy decoding. This is as expected, because using one sample to estimate the expected reward is too noisy, resulting in larger gradient variance, while the reward of greedy decoding output may     be biased but more stable. It also shows that it is important to use sufficiently large K to have a better estimation of expected reward.</p><p>Variance reduction As stated in Sec. 3, the motivation of using the average reward baseline is for better variance reduction. Here we show it indeed is better in practice.</p><p>The gradient variance is calculated as follows. At the end of each epoch, we take the saved model and run through the training set. We get the gradients from each training batch and calculate the variance for each parameter gradient across batches. To get a single value, we take the average of all the parameters. A mathematic expression of this process is:</p><formula xml:id="formula_6">V = Mean i Var b [grad b θ i ]</formula><p>where i is the index of each parameter; b is the index of each batch; θ is the network parameters; grad b θ i is the gradient of θ i at batch b. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our method is always getting lower variance than SCST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Code release</head><p>Code has been released at https://github. com/ruotianluo/self-critical.pytorch.</p><p>More instructions of using this method are at https://github.com/ruotianluo/ self-critical.pytorch/tree/master/ projects/NewSelfCritical</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a variant of popular SCST, which can work as a drop-in replacement for SCST. This variant reduces the gradient variance when applying REINFORCE by modifying the baseline function. We show that this method is effective on Image Captioning task, and we believe it should benefit other tasks as well.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The gradient variance on training set.(Model: UpDown)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The performance of our method on different model architectures. The numbers are from authors' own implementation.</figDesc><table><row><cell></cell><cell cols="8">Bleu1 Bleu2 Bleu3 Bleu4 ROUGE L METEOR CIDEr SPICE</cell></row><row><cell>UpDown+SCST</cell><cell>79.4</cell><cell>63.3</cell><cell>48.6</cell><cell>36.7</cell><cell>57.6</cell><cell>27.9</cell><cell>122.7</cell><cell>21.5</cell></row><row><cell>UpDown+Ours</cell><cell>80.0</cell><cell>63.9</cell><cell>49.1</cell><cell>37.2</cell><cell>57.8</cell><cell>28.0</cell><cell>123.9</cell><cell>21.5</cell></row><row><cell>UpDown(Long)+SCST</cell><cell>80.3</cell><cell>64.5</cell><cell>49.9</cell><cell>38.0</cell><cell>58.3</cell><cell>28.4</cell><cell>127.2</cell><cell>21.9</cell></row><row><cell>UpDown(Long)+Ours</cell><cell>80.4</cell><cell>64.7</cell><cell>50.1</cell><cell>38.1</cell><cell>58.4</cell><cell>28.5</cell><cell>127.9</cell><cell>22.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance of UpDown model with SCST/Ours under two different hyperparameter settings.</figDesc><table><row><cell></cell><cell cols="8">Bleu1 Bleu2 Bleu3 Bleu4 ROUGE L METEOR CIDEr SPICE</cell></row><row><cell>RS1+SCST</cell><cell>80.3</cell><cell>64.5</cell><cell>49.9</cell><cell>38.0</cell><cell>58.3</cell><cell>28.4</cell><cell>127.2</cell><cell>21.9</cell></row><row><cell>RS1+Ours</cell><cell>80.4</cell><cell>64.7</cell><cell>50.1</cell><cell>38.1</cell><cell>58.4</cell><cell>28.5</cell><cell>127.9</cell><cell>22.0</cell></row><row><cell>RS2+SCST</cell><cell>80.2</cell><cell>64.5</cell><cell>49.9</cell><cell>37.9</cell><cell>58.3</cell><cell>28.3</cell><cell>127.2</cell><cell>21.9</cell></row><row><cell>RS2+Ours</cell><cell>80.2</cell><cell>64.5</cell><cell>50.0</cell><cell>38.1</cell><cell>58.2</cell><cell>28.4</cell><cell>128.0</cell><cell>22.0</cell></row><row><cell>RS3+SCST</cell><cell>80.2</cell><cell>64.5</cell><cell>50.0</cell><cell>38.1</cell><cell>58.3</cell><cell>28.3</cell><cell>127.3</cell><cell>21.8</cell></row><row><cell>RS3+Ours</cell><cell>80.2</cell><cell>64.7</cell><cell>50.2</cell><cell>38.3</cell><cell>58.3</cell><cell>28.4</cell><cell>127.9</cell><cell>22.0</cell></row><row><cell>RS4+SCST</cell><cell>80.2</cell><cell>64.5</cell><cell>49.9</cell><cell>37.9</cell><cell>58.2</cell><cell>28.3</cell><cell>127.0</cell><cell>21.8</cell></row><row><cell>RS4+Ours</cell><cell>80.2</cell><cell>64.5</cell><cell>50.0</cell><cell>38.0</cell><cell>58.3</cell><cell>28.5</cell><cell>127.7</cell><cell>22.0</cell></row><row><cell>RS5+SCST</cell><cell>80.2</cell><cell>64.6</cell><cell>50.2</cell><cell>38.4</cell><cell>58.4</cell><cell>28.5</cell><cell>127.6</cell><cell>21.9</cell></row><row><cell>RS5+Ours</cell><cell>80.2</cell><cell>64.5</cell><cell>49.8</cell><cell>37.9</cell><cell>58.3</cell><cell>28.4</cell><cell>127.8</cell><cell>22.0</cell></row><row><cell>Mean(SCST)</cell><cell>80.2</cell><cell>64.5</cell><cell>50.0</cell><cell>38.0</cell><cell>58.3</cell><cell>28.4</cell><cell>127.3</cell><cell>21.8</cell></row><row><cell>Mean(Ours)</cell><cell>80.2</cell><cell>64.6</cell><cell>50.0</cell><cell>38.1</cell><cell>58.3</cell><cell>28.4</cell><cell>127.9</cell><cell>22.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Within the first 5 block, the models share the same cross-entropy pretrained model (RS stands for random seed). The last block shows the average score of 5 models.</figDesc><table><row><cell></cell><cell>58.0 58.5</cell><cell>SCST Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28.2 28.4 28.6</cell><cell>SCST Ours</cell><cell>122.5 125.0 127.5</cell><cell>SCST Ours</cell><cell>21.50 21.75 22.00</cell><cell>SCST Ours</cell></row><row><cell>ROUGE_L</cell><cell>57.0 57.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>METEOR</cell><cell>27.6 27.8 28.0</cell><cell>CIDEr</cell><cell>115.0 117.5 120.0</cell><cell>SPICE</cell><cell>20.75 21.00 21.25</cell></row><row><cell></cell><cell>56.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>27.4</cell><cell>112.5</cell><cell>20.50</cell></row><row><cell></cell><cell cols="5">40000 60000 80000 100000 120000 140000 160000 Iter</cell><cell></cell><cell cols="2">40000 60000 80000 100000 120000 140000 160000 Iter 27.2</cell><cell>40000 60000 80000 100000 120000 140000 160000 Iter 110.0</cell><cell>Iter 40000 60000 80000 100000 120000 140000 160000 20.25</cell></row><row><cell></cell><cell></cell><cell cols="7">Figure 1: Performance on validation set during training. (With UpDown(Long) + greedy decoding)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1.75 2.00</cell><cell>1e 8</cell><cell></cell><cell></cell><cell></cell><cell>SCST Ours</cell></row><row><cell></cell><cell></cell><cell>Gradient variance</cell><cell>0.50 0.75 1.00 1.25 1.50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.00</cell><cell>350000</cell><cell>400000</cell><cell>450000 Iter</cell><cell cols="2">500000</cell><cell>550000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Replacing the greedy decoding output c * in SCST with a separately drawn sampleĉ . Lijun Li and Boqing Gong. 2019. End-to-end video captioning with multitask reinforcement learning. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 339-348. IEEE. Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. 2018. Jointly localizing and describing events for dense video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7492-7500. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer. Huan Ling and Sanja Fidler. 2017. Teaching machines to describe images via natural language feedback. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 5075-5085. Curran Associates Inc.</figDesc><table><row><cell>Wenhao Jiang, Lin Ma, Yu-Gang Jiang, Wei Liu, and</cell></row><row><cell>Tong Zhang. 2018. Recurrent fusion network for</cell></row><row><cell>image captioning. In Proceedings of the European</cell></row><row><cell>Conference on Computer Vision (ECCV), pages 499-</cell></row><row><cell>515.</cell></row><row><cell>Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-</cell></row><row><cell>semantic alignments for generating image descrip-</cell></row><row><cell>tions. In Proceedings of the IEEE conference</cell></row><row><cell>on computer vision and pattern recognition, pages</cell></row><row><cell>3128-3137.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10357</idno>
		<title level="m">Deep communicating agents for abstractive summarization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving image captioning with conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanpeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liesi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8142" to="8150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Less is more: Picking informative frames for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="358" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial semantic alignment for improved image captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10463" to="10471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-critical n-step training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6300" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Context-aware visual policy network for sequence-level image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05864</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Show, tell and discriminate: Image captioning by self-retrieval with partially labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="338" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminability objective for training descriptive captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6964" to="6974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training for diversity in image paragraph captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="757" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Variational inference for monte carlo objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06725</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06451</idno>
		<title level="m">Multireward reinforced summarization with saliency and entailment</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A reinforced topicaware convolutional sequence-to-sequence model for abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhe</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03616</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boosting image captioning with attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4894" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multi-task learning approach for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1205" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving end-to-end speech recognition with policy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5819" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
