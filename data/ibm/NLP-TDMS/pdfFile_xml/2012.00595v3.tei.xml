<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeFMO: Deblurring and Shape Recovery of Fast Moving Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denys</forename><surname>Rozumnyi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Google Research 4 Visual Recognition Group</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Mixed Reality and AI Zurich Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeFMO: Deblurring and Shape Recovery of Fast Moving Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objects moving at high speed appear significantly blurred when captured with cameras. The blurry appearance is especially ambiguous when the object has complex shape or texture. In such cases, classical methods, or even humans, are unable to recover the object's appearance and motion. We propose a method that, given a single image with its estimated background, outputs the object's appearance and position in a series of sub-frames as if captured by a high-speed camera (i.e. temporal super-resolution). The proposed generative model embeds an image of the blurred object into a latent space representation, disentangles the background, and renders the sharp appearance. Inspired by the image formation model, we design novel self-supervised loss function terms that boost performance and show good generalization capabilities. The proposed DeFMO method is trained on a complex synthetic dataset, yet it performs well on real-world data from several datasets. DeFMO outperforms the state of the art and generates high-quality temporal super-resolution frames.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object blurring is a challenging problem in many image processing and computer vision tasks. The primary sources of image blur are rapid camera motion and object motion combined with long exposure time. Many methods were proposed to address the deblurring task, ranging from image deblurring <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref> to video temporal super-resolution <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. However, they consider only low to medium blur, emerging from global camera blur due to camera motion, defocused camera, or objects moving at moderate speed.</p><p>Only recently, specialized algorithms for the deblurring of fast moving objects (FMOs) have been introduced <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27]</ref>. FMOs are defined as objects that move over a distance larger than their size within the camera exposure time (or within a single time frame in video). FMO detection is important in tracking sports with fast object motion like soccer, tennis, or badminton. It is also beneficial in au- Given an input image I depicting a blurred fast moving object and an estimated background B, DeFMO decomposes the image into a series of deblurred sub-frames with sharp object contours. Examples are from test datasets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27]</ref>, mobile device footage (lighter), and YouTube videos (mic, cap). Ground truth (GT) corresponds either to the high-speed camera frame or a static image. Deblurred images It are sharper than the 'GT', e.g.'pen' or 'key'. tonomous driving to detect impacts with stones, birds, or other wildlife. FMOs are frequently found when capturing falling or thrown objects like pieces of fruit, leaves, flying insects, hailstorm, or rain. Any moving object becomes an FMO in low-light conditions or for long exposure. In other words, one needs to increase the exposure time or the speed of the object to observe an FMO. We consider a setting where the input is an image with an object moving fast and thus appearing blurred. The task is to reconstruct the hypothetical sub-frames that would have been there if this was a short video captured by a high-speed camera for the same time interval. The physical generative model that leads to the input blurred frame is assumed to be a temporal integration of underlying sharp sub-frames, each of which has a much shorter exposure time. To simplify the problem complexity, we assume that background without the object is given, e.g. from previous frames in the video or as a static image captured when there is no object. In practice, a median of several previous frames works well.</p><p>Prior work on FMO deblurring considers only relatively simple, mostly spherical objects <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. This prior work typically assumes that the object in motion has a constant appearance in all sub-frames.</p><p>We propose DeFMO -the first to go beyond these assumptions by handling the time-varying complex appearance of fast moving objects that move over 3D trajectories with 3D rotation. DeFMO is a generative model that reconstructs sharp contours and appearance of FMOs. First, we disentangle the blurred fast moving object from the background into a latent space. Then, a rendering network has the objective to render the sharp object in a series of subframes, capturing the motion in time. The network is trained end-to-end on a synthetic dataset with complex, highly textured objects. Thanks to self-supervised loss function terms inspired by the image formation model with FMOs, our method easily generalizes to real-world data, as shown in <ref type="figure">Fig. 1</ref>. DeFMO can be applied to many fields, such as video temporal super-resolution, data compression, surveillance, astronomy, and microscopy. Overall, the paper makes the following contributions:</p><p>• We present the first fully neural network model for FMO deblurring that bridges the gap between deblurring, 3D modeling, and sub-frame tracking of FMOs.</p><p>• Training only on synthetic data with novel self-supervised losses sets a new state of the art in terms of trajectory and sharp appearance reconstruction of FMOs.</p><p>• We introduce a new synthetic dataset with complex objects, textures, and backgrounds. The dataset and model implementation are made publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Fast moving objects were defined in <ref type="bibr" target="#b25">[26]</ref>, and a proofof-concept method was designed. The blurring and matting (blatting) equation was later introduced in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> as</p><formula xml:id="formula_0">I = H * F + (1 − H * M ) B ,<label>(1)</label></formula><p>where the sharp object given by foreground appearance F and segmentation mask M is blurred and combined with the background B. The blatting equation <ref type="bibr" target="#b0">(1)</ref> assumes that the object appearance F is static within one video frame I. In contrast to our approach, the object must also travel in a 2D plane parallel to the camera plane. Kotera et al. <ref type="bibr" target="#b9">[10]</ref> study the special case that a planar FMO only rotates within a 2D plane parallel to the camera plane. An improved motion blur prior for FMOs was proposed in <ref type="bibr" target="#b32">[33]</ref>. Learned FMO detection is studied in <ref type="bibr" target="#b27">[28]</ref>. Some of the limiting assumptions of TbD were lifted by the TbD-3D <ref type="bibr" target="#b26">[27]</ref> method that assumed a piece-wise constant appearance as</p><formula xml:id="formula_1">I = i Hi * Fi + 1 − i Hi * Mi B ,<label>(2)</label></formula><p>where index i corresponds to one part of the full trajectory H = i H i traveled within one frame I. Sub-frame appearances F i and masks M i account for a potentially rotating or deforming object. Solving (2) simultaneously for H i , F i , M i is in practice computationally infeasible. To address this problem, TbD-3D solves (2) for F i , M i by initially assuming a simpler static appearance model (1) to estimate the trajectory H, then splitting the trajectory into the desired number of pieces, and finally estimating the subframe appearances in a second pass while keeping the trajectory fixed. We claim that solving <ref type="bibr" target="#b1">(2)</ref> in such a fashion is sub-optimal and similar to a chicken-and-egg problem since robust trajectory estimation requires to model the time-varying appearance and vice versa. Hence, TbD-3D strongly depends on the initial trajectory estimation from an external module, such as TbD <ref type="bibr" target="#b10">[11]</ref> or TbD-NC <ref type="bibr" target="#b24">[25]</ref>. In practice, TbD-3D only works when the object has a trivial shape (e.g. a sphere) or appearance (e.g. uniform color).</p><p>Methods, such as <ref type="bibr" target="#b21">[22]</ref> or <ref type="bibr" target="#b6">[7]</ref>, have been proposed to generate a video from a single image, but fast motion is not considered. Blurry video frame interpolation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>, softmax splatting <ref type="bibr" target="#b18">[19]</ref>, or zooming slow-motion <ref type="bibr" target="#b33">[34]</ref> are designed for video frame interpolation to increase the frame rate from several blurred inputs. Still, the considered motion blurs are small compared to what is caused by FMOs. Classical deblurring methods have shown success, such as DeblurGAN-v2 <ref type="bibr" target="#b13">[14]</ref>, deblurring by realistic blurring <ref type="bibr" target="#b34">[35]</ref>, deblurring using deep priors <ref type="bibr" target="#b23">[24]</ref>, and others <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>. These methods assume that anything in the scene can be blurred. In our task, we assume a single object blurred due to its fast motion. Moreover, these methods generate only one deblurred frame, which is not sufficient for FMOs, where the desired result is a set of deblurred high-speed sub-frames. In the experiments, we compare to DeblurGAN-v2 <ref type="bibr" target="#b13">[14]</ref> and Jin et al. <ref type="bibr" target="#b6">[7]</ref>. As these methods are more general, they perform worse than ours on the specific case of blur caused by FMOs, on which we specialize.</p><p>In sum, all existing deblurring methods either lack proper modeling of FMOs or strongly rely on handcrafted priors and impose strong assumptions on the object's shape and appearance. Furthermore, current FMO deblurring  <ref type="figure">Figure 2</ref>. Architecture of DeFMO. The input image and the estimated background are encoded into latent space X. Then, X is augmented with time index channel t and is rendered into the deblurred object with appearance F and mask M . The renderings are generated for many time indices simultaneously with the same rendering network and are averaged in time. We only use the blue part during testing. methods are slow and take seconds per frame. They also rely on other external modules. To address these problems, our method estimates complex shape and appearance of FMOs, all in one network, and runs in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deblurring model</head><p>The input to the method is an RGB image I : D ⊂ R 2 → R 3 containing a blurred FMO and an estimate of the background B : D → R 3 , which does not include the object of interest. In most scenarios, a video stream is available, and B can be estimated as a median of several previous frames, as such an operation will remove all FMOs <ref type="bibr" target="#b10">[11]</ref>.</p><p>The desired output is a sharp rendering of the FMO for all sub-frames at predefined sub-frame time indices t ∈ [0, 1] for which we estimate 4-channel RGBA renderings R t : D → R 4 . These renderings are composed of an RGB part F t : D → R 3 (sharp appearance of the FMO) and an alpha matting mask M t : D → R (segmentation into foreground FMO and background, <ref type="figure">Fig. 2</ref>).</p><p>We encode the input image I and background B into a latent space X ∈ R K . Then, we render a set of sub-frame appearances, which are pushed to be sharp, time-consistent, independent of the background, and which reconstruct the input image with the following image formation model</p><formula xml:id="formula_2">It 0 :t 1 =</formula><p>which is a generalization of the piecewise-constant FMO formation model <ref type="bibr" target="#b1">(2)</ref>. Instead of (1) and (2), we render the object directly at the desired location. We do not disentangle the trajectory blur kernels H t , which are just Dirac deltas in our generalization <ref type="bibr" target="#b2">(3)</ref>. The multiplication of appearance F t by mask M t replaces the constraint F t ≤ M t , e.g. predicting low values in the mask will imply low values in F t M t . We assume that the input image I = I 0:1 . For training, we partition the time duration of the input frame into N equidistant parts to generate the subframes, and evaluate t on the set</p><formula xml:id="formula_3">{ i−1 N −1 } N i=1 . For testing, any t ∈ [0, 1] produces consistent renderings.</formula><p>As a technicality, we render images as they were captured with nearly zero exposure time. Such an image at subframe time index t is captured as</p><formula xml:id="formula_4">I t = lim δ→0 I t−δ:t+δ = F t M t + (1 − M t )B.</formula><p>In practice, we deliberately add time blur for quantitative evaluation to match the highspeed camera frames (see evaluation Sec. 5). Such time blur is generated as temporal super-resolution by l,</p><formula xml:id="formula_5">{I k/l:(k+ )/l } l−1 k=0</formula><p>, where is the exposure fraction. The ability to generate zero-exposure-time images enables the creation of any frame rates with any exposure time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training loss</head><p>The loss function is inspired by the nature of the problem. The main goal is to get sharp (L S ) reconstructions of the object (L F ) that do not contain the background (L L ), move smoothly over time (L T ), and reconstruct the input image (L I ) according to the generalized formation model <ref type="bibr" target="#b2">(3)</ref>. Therefore, the loss is a combination of five parts,</p><formula xml:id="formula_6">L = LF + αI LI + αT LT + αSLS + αLLL .<label>(4)</label></formula><p>Appearance reconstruction loss L F captures the supervised sub-frame reconstruction of the object's appearance and mask. Since the input is a single blurred image, we do not know whether time goes forward or backward. In fact, both of them will generate the same blurred image. These two cases are indistinguishable, as e.g. in determining temporal order of images from 3D structure <ref type="bibr" target="#b29">[30]</ref>. Therefore, we evaluate both directions and calculate the loss only with respect to the time direction that best aligns with the ground truth. More precisely, we define L F as</p><formula xml:id="formula_7">LF {Rt,Rt} 1 0 = min 1 0 LR(Rt,Rt) dt, 1 0 LR(Rt,R1−t) dt ,<label>(5)</label></formula><p>and the rendering loss L R for a pair of the estimated rendering R t and the ground truth renderingR t as</p><formula xml:id="formula_8">LR Rt = (Ft, Mt),Rt = (Ft,Mt) = L1(Mt,Mt,Mt &gt; 0) + L1(Mt,Mt,Mt = 0) + L1(FtMt,FtMt,Mt &gt; 0) ,<label>(6)</label></formula><p>where we combine three terms. The first two terms evaluate the difference between masks on two sets of pixels: where the object is present, and the rest of the image, as given by the ground truth. The last term calculates the difference between the appearances, evaluated on the first set of pixels.</p><p>The intuition of this splitting is that the object usually occupies only a fraction of the input image, and we want the loss to focus more on the object itself. Then, the L 1 loss is</p><formula xml:id="formula_9">L1(M1, M2, O) = 1 |O| p∈D M1(p) − M2(p) 1O(p) ,<label>(7)</label></formula><p>where M (p) denotes the value of the image M at pixel location p. If the occupancy mask O is omitted, the whole image domain D is assumed. The appearance reconstruction loss is the only one that requires ground truth object renderingsR t , i.e. it is the only supervised loss. Image reconstruction loss L I is a direct application of the generalized formation model <ref type="formula">(3)</ref>. We penalize for the difference between the input image and the synthetic image reconstructed from the input background and FMO renderings. The image reconstruction loss is defined as</p><formula xml:id="formula_10">LI ({Ft, Mt} 1 0 ) = L1 I, 1 0 FtMt dt + 1 − 1 0 Mt dt B ,<label>(8)</label></formula><p>where we enforce the underlying physical model of temporal integration. This loss and all the following ones are self-supervised and do not require ground truth. Time-consistency loss L T captures temporal smoothness of the sub-frame renderings, according to the prior knowledge of the problem. We expect that renderings R t will be similar for nearby t. Therefore, the similarity between renderings at two different points in time is defined as the maximum value of normalized cross-correlation over the image domain, which can be efficiently implemented on GPU using convolutions. To account for possible object translation, we apply zero-padding of 10% of the image size to one of the renderings. The loss is defined as</p><formula xml:id="formula_11">LT ({Rt} 1 0 ) = 1 − 1 0 maxncc(Rt, R t+dt ) dt .<label>(9)</label></formula><p>Sharpness loss L S enforces sharp image reconstruction, which is the main task of deblurring. In the FMO setting, the object sharpness is assessed by its mask. Enforcing the mask to be binary is not ideal since we have mixed pixels at object boundaries. However, almost all pixels are expected to be close to zero or one. One mathematical way to express this statement is to minimize the per-pixel binary entropy H 2 averaged over the image domain D,</p><formula xml:id="formula_12">LS({Mt} 1 0 ) = 1 0 1 |D| p∈D H2 Mt(p) dt .<label>(10)</label></formula><p>Latent learning loss L L models the latent space such that blurred images of the same FMO moving along the same trajectory but in front of different backgrounds generate the same latent representation. We achieve this by training in such pairs of images, and compute the loss as</p><formula xml:id="formula_13">LL(X 1 , X 2 ) = 1 K X 1 − X 2 1 ,<label>(11)</label></formula><p>where X 1 and X 2 are latent spaces generated by the first and the second images. All other losses, renderings and computations are done only on the first image, i.e. relating to X 1 . The latent learning loss especially helps in early training stages and stabilizes the training. Joint loss Even perfectly optimized joint loss will not necessarily produce the ground truth renderings because some parts of the loss function are biased. For example, the timeconsistency and sharpness losses are not minimized by the ground truth renderings since the ground truth masks are not binary on boundaries, and the appearance is not static. Whether the ground truth renderings are the global minimizers of the joint loss is an open research question. Some parts of the joint loss can be loosely related to the energy minimization terms in the TbD <ref type="bibr" target="#b10">[11]</ref> and TbD-3D <ref type="bibr" target="#b26">[27]</ref> methods, e.g. appearance reconstruction loss ≈ template-matching term <ref type="bibr" target="#b10">[11]</ref>, image reconstruction loss ≈ data likelihood <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>, time-consistency loss ≈ regularization term enforcing similarity of the object in neighboring time intervals <ref type="bibr" target="#b26">[27]</ref>. Other regularizers, e.g. total variation, are expected to be data-driven and learned by the network from the synthetic training set. In comparison to our work, the previous methods are not neural network models, are handcrafted, computationally slow, and do not handle complex objects well (see Sec. 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and evaluation datasets</head><p>Real-world datasets for evaluation Only a small number of real-world annotated datasets with fast moving object exists. The FMO dataset <ref type="bibr" target="#b25">[26]</ref> contains 16 sports sequences, where FMOs are manually annotated with polygons. It lacks ground truth (GT) sharp appearances, segmentation masks, or even trajectories. The TbD dataset <ref type="bibr" target="#b10">[11]</ref> made a step further and captured 12 high-speed videos at 240 fps in raw format with full exposure. Subsequently, low-speed videos at 30 fps were created by temporal averaging. Ground truth was generated from the high-speed video semi-manually by annotating the object in the first frame, applying the state-of-the-art tracker <ref type="bibr" target="#b15">[16]</ref>, and correcting the mistakes. Thus, ground truth for sharp appearances, masks, <ref type="figure">Figure 3</ref>. FMO reconstruction on a validation image from the synthetic dataset. We compare to the TbD-3D-Oracle <ref type="bibr" target="#b26">[27]</ref> method (with ground truth trajectory). Flying table is synthetically superimposed on the image of a table tennis game. and full trajectory is provided in this dataset. However, the dataset contains only sports videos with mostly spherical objects and almost no appearance changes over time. To address some of these shortcomings, the TbD-3D dataset <ref type="bibr" target="#b26">[27]</ref> was recorded in the same fashion as TbD, but capturing objects that significantly change their appearance within one low-speed video frame. The dataset has only 10 sequences, and the objects are all but one spherical. The recent Falling Objects dataset <ref type="bibr" target="#b9">[10]</ref> is the first to contain objects with nontrivial shapes, e.g. box, pen, marker, cell, key, eraser. Highspeed videos are provided, but no GT trajectories. We augmented the ground truth of <ref type="bibr" target="#b9">[10]</ref> with trajectories by running the tracker <ref type="bibr" target="#b15">[16]</ref> with annotations -we provide a manual box in the first frame, track the object, and correct the mistakes by re-initializing the tracker. We use this augmented dataset for testing and in the ablation study. Synthetic dataset for training is created due to the lack of a large and diverse real-world annotated dataset with FMOs. To create a synthetic image, we use triplets: an object, a 6D trajectory, and a background frame. Then, we render the object along the given 6D trajectory using Blender Cycles <ref type="bibr" target="#b2">[3]</ref> to get a set of sub-frame renderings. Finally, we apply the generalized formation model (3) to generate the low-speed frame showing the blurred FMO.</p><formula xml:id="formula_14">TbD-3D-O Ours GT Inputs (B, I) F 0 F 0.25 F 0.5 F 0.75</formula><p>Objects are sampled from 3D models of the 50 largest classes of the ShapeNet <ref type="bibr" target="#b0">[1]</ref> dataset, each class is represented uniformly. Since most ShapeNet objects are not well-textured, we apply DTD <ref type="bibr" target="#b1">[2]</ref> textures, as in the dataset creation for the Neural Voxel Renderer <ref type="bibr" target="#b22">[23]</ref>. The textures are split into 1600 for training and 200 for validation.</p><p>Trajectories are sampled uniformly as linear with the displacement in the range between 0.5 and 2 object sizes in x, y image directions, and between 0 and 0.2 sizes in z direction towards the camera. 3D rotations are sampled with a maximal rotation change of 30 • in each direction. For rendering, the trajectory is discretized into N = 24 equal parts.</p><p>Backgrounds are sampled from the VOT <ref type="bibr" target="#b12">[13]</ref> sequences for training, and from Sports-1M <ref type="bibr" target="#b7">[8]</ref>  latent space learning, the foreground image is synthesized on a pair of backgrounds. The background B that we use as input to the method is estimated as a median over 5 previous frames. This way, the method gets exposed to complex non-static backgrounds as the VOT dataset has a variety of dynamic scenes but no other FMOs <ref type="bibr" target="#b25">[26]</ref>.</p><p>In total, we generate 50,000 training and 1,000 validation images (example in <ref type="figure">Fig. 3</ref>). The synthetic dataset is more challenging than a real-world dataset (ablation study <ref type="table" target="#tab_1">Table 1</ref>). The reported metrics are better on the test dataset (Falling Objects <ref type="bibr" target="#b11">[12]</ref>) than on the synthetic training dataset, which has more complex objects and textures.</p><p>Training settings Both the encoder and the rendering networks are based on ResNet <ref type="bibr" target="#b3">[4]</ref> with batch norm <ref type="bibr" target="#b4">[5]</ref> and ReLU <ref type="bibr" target="#b17">[18]</ref>. The encoder is ResNet-50 cropped after the fourth down-sampling operator, pre-trained on Im-ageNet <ref type="bibr" target="#b28">[29]</ref>. The latent space is up-scaled by 2 using pixel shuffle <ref type="bibr" target="#b31">[32]</ref> four times, each followed by ResNet bottleneck (i.e. 1024, 256, 64, 16, 4 channels). We used ADAM <ref type="bibr" target="#b8">[9]</ref> optimizer with fixed learning rate 10 −3 . For the input image resolution w × h, the resolution of the latent space is 2048 × w/16 × h/16 = K (due to 4 down-sampling operators). The input to the rendering network is augmented by copying the time index channel t along the first dimension, leading to resolution 2049 × w/16 × h/16. In experiments, we set w = 320 and h = 240. Loss weights are set as α I , α S , α L = 1 and α T = 5. The encoder and the rendering networks have 23.5M and 20.1M parameters, respectively. The model is implemented in PyTorch <ref type="bibr" target="#b20">[21]</ref>, trained in 24 mini-batches on 3 nVidia 16GB GPUs. We trained for 50 epochs, which took approximately four days. For evaluation, the method runs in real-time on a single GPU and works for various resolutions and sub-frame time indices.  <ref type="bibr" target="#b9">[10]</ref>, mostly spherical but significantly textured <ref type="bibr" target="#b26">[27]</ref>, and mostly spherical and uniformly colored objects <ref type="bibr" target="#b10">[11]</ref>. DeFMO is superior to the compared methods by a wide margin on all datasets except for the easiest <ref type="bibr" target="#b10">[11]</ref> since the TbD(-3D) methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref> are specifically designed for such easy cases. TbD-3D-Oracle <ref type="bibr" target="#b26">[27]</ref> is given ground truth trajectories and estimates only sub-frame appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>We evaluated DeFMO on several datasets 2 (Sec. 4) and compared it to the state of the art. Evaluation metrics are PSNR (peak signal-to-noise ratio), SSIM (structural similarity index measure), and TIoU (Intersection over Union averaged along the trajectory) <ref type="bibr" target="#b10">[11]</ref>. We compare to FMO deblurring methods based on energy minimization: TbD <ref type="bibr" target="#b10">[11]</ref>, TbD-3D <ref type="bibr" target="#b26">[27]</ref>, and TbD-3D-Oracle (where we provide the ground truth trajectory). All FMO deblurring methods and ours use the same estimate of the backgrounda median of the previous 5 frames. We also compare to two state-of-the-art generic deblurring and temporal superresolution methods: DeblurGAN-v2 <ref type="bibr" target="#b13">[14]</ref> (generates a single output, compared to the best aligned high-speed frame) and Jin et al. <ref type="bibr" target="#b6">[7]</ref> (generates a mini-video, the same comparison to GT as for DeFMO). We do not compare to the method <ref type="bibr" target="#b9">[10]</ref> since it covers a very special case of a constant planar appearance with 2D rotation parallel to the image plane, and for the general case, it is inferior to TbD-3D <ref type="bibr" target="#b26">[27]</ref>. The authors of <ref type="bibr" target="#b9">[10]</ref> introduced the Falling Objects dataset but evaluated only qualitatively on three chosen frames where the object appearance was constant.</p><p>The high-speed video is available at 8 times higher frame rate than the low-speed video, both at full exposure. Therefore, we generate full exposure ( = 1) temporal super-resolution for quantitative evaluation to match the high-speed frames. As discussed in Sec. 3, we generate {I k/8:(k+1)/8 } 7 k=0 according to <ref type="bibr" target="#b2">(3)</ref>, and discretize each integral by 5 parts. For qualitative results, we reconstruct objects as sharp as possible and visualize zero exposure temporal super-resolution. Since our temporal super-resolution task is done from a single image, the direction of time is am-  biguous. Hence, we compute scores for both directions and report the best one (for all methods). FMOs are retrieved and approximately localized by the bounding box using the FMO detector <ref type="bibr" target="#b25">[26]</ref>. Sub-frame trajectory <ref type="figure" target="#fig_3">(Fig. 4)</ref> is estimated as the center of mass of the generated masks M t . Falling Objects dataset Most previous methods fail on this challenging dataset ( <ref type="table">Table 2</ref>, top block <ref type="bibr" target="#b9">[10]</ref>), mainly because the objects change their appearance too much within one frame <ref type="figure">(Fig. 7)</ref>. DeFMO outperforms all others in all metrics on this dataset. It even outperforms TbD-3D-Oracle, although that method requires GT trajectories as input, which are not available in real-world scenarios. TbD-3D dataset Table 2 (middle block <ref type="bibr" target="#b26">[27]</ref>) shows that DeFMO outperforms all other methods not using the ground  <ref type="figure">Figure 5</ref>. Temporal super-resolution on selected sequences from test datasets. We compare to the TbD-3D-Oracle <ref type="bibr" target="#b26">[27]</ref> with the manually provided ground truth trajectory from a high-speed footage (GT). Ground truth masks are computed as a difference image between the GT sub-frame and the background. The proposed DeFMO method estimates everything just from inputs on the left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input I [14]</head><p>F1 M1 I1 Static <ref type="figure">Figure 6</ref>. FMOs in the wild, captured by a mobile device and reconstructed by DeFMO and DeblurGAN-v2 <ref type="bibr" target="#b13">[14]</ref>. Our results are shown in 3 columns: estimated appearance F1, mask M1, and the composed temporal super-resolution I1. Other methods, such as TbD <ref type="bibr" target="#b10">[11]</ref> and Jin et al. <ref type="bibr" target="#b6">[7]</ref>, do not produce competitive results.</p><p>truth trajectory. DeFMO compares well even against TbD-3D-Oracle (better PSNR, slightly worse SSIM).</p><p>TbD dataset contains objects that are mostly spherical and with constant appearance. In this simplistic setting, DeFMO is slightly worse than TbD(-3D) methods on the SSIM metric ( <ref type="table">Table 2</ref>, bottom block <ref type="bibr" target="#b10">[11]</ref>). On PSNR and TIoU metrics, DeFMO is the best-performing method. Note that TbD(-3D) methods are specifically designed for such simple objects and work well there. Our method is more general and does not have assumptions of spherical or constant objects. However, DeFMO is several orders of magnitude faster but comparable in performance there. One way to improve the performance on such objects is to generate a synthetic training set with objects of constant appearance.</p><p>Ablation study in <ref type="table" target="#tab_1">Table 1</ref> validated that the self-supervised losses (i.e. all except for the supervised appearance reconstruction loss L F ) have a positive impact on the convergence and generalization of the overall model. Neglecting the sharpness loss L S generates more blurry object boundaries, which preserves the reconstruction quality but makes the trajectory less precise. Training with only the appearance reconstruction loss leads to overfitting to the training set. At the other extreme, training in fully self-supervised fashion completely fails ( <ref type="table" target="#tab_1">Table 1</ref>, row 4). We observed that the main problem was in identifying the object of interest and balancing the importance of the background motion. The combination of supervised and self-supervised losses shows the best performance. Discussion None of the objects in the test datasets are present in ShapeNet but are still successfully reconstructed by our method. DeFMO can also reconstruct a deforming object <ref type="figure">(Fig. 5, aerobie)</ref>, even though only rigid bodies were used during training. The method is able to deblur other dynamic objects simultaneously if their motion is similar to the object of interest, e.g. the volleyball in <ref type="figure">Fig. 1</ref>.</p><p>Limitations When the object's appearance is similar to the background color, the problem becomes severely ill-posed. For instance, the black tip of the marker in <ref type="figure">Fig. 7</ref> is not reconstructed, as the object was moving in front of the black background, and both reconstructions with and without the tip correctly lead to almost the same input image. The method is not designed for objects made of transpar-</p><formula xml:id="formula_15">box t 0 t 1 key t 0 t 1 marker t 0 t 1 pen t 0 t 1 B / I [7] [14]</formula><p>TbD <ref type="bibr" target="#b10">[11]</ref> TbD-3D <ref type="bibr" target="#b26">[27]</ref> DeFMO (ours) GT TbD-3D-Oracle <ref type="figure">Figure 7</ref>. Comparison on the Falling Objects dataset <ref type="bibr" target="#b9">[10]</ref> with the state-of-the-art methods: Jin et al. <ref type="bibr" target="#b6">[7]</ref>, DeblurGAN-v2 <ref type="bibr" target="#b13">[14]</ref>, TbD <ref type="bibr" target="#b10">[11]</ref>, TbD-3D <ref type="bibr" target="#b26">[27]</ref> and TbD-3D-Oracle that uses GT trajectories. For each method (except <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> only showing deblurred results), we show from left to right: estimated mask, estimated sharp appearance, and temporal super-resolution frames for t = 0 (top) and t = 1 (bottom). ent materials, e.g. bottle, glass. The two sources of transparency, background-foreground mixing due to fast motion and the transparent material, are too difficult to distinguish.</p><p>Applications of the proposed method include temporal super-resolution ( <ref type="figure">Fig. 5</ref>). It can be used in fields such as astronomy to reconstruct the appearance of fast asteroids or data compression to decrease the frame rate at which a video is stored and then recover it back with DeFMO. Other applications are ball detection in sports and estimation of its speed or full 3D reconstruction of a highly blurred object by applying shape-from-silhouettes <ref type="bibr" target="#b14">[15]</ref>. In combination with a standard tracker, DeFMO can track objects that are FMOs during parts of a video and not blurry in other parts.</p><p>DeFMO also works on sequences recorded in real-life settings, as on a mobile device. We captured videos of handthrown objects with a standard frame rate of 30 fps. Example of their reconstruction is in <ref type="figure">Fig. 6</ref> (shoes) and in <ref type="figure">Fig. 1  (lighter)</ref>. FMO reconstruction of objects from YouTube videos is in <ref type="figure">Fig. 1 (mic, cap)</ref>. More examples and videos are available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a novel generative model for disentangling and deblurring of fast moving objects. Training on a complex synthetic dataset with a carefully designed loss function incorporating prior knowledge of the problem scales well to real-world data. Experimental results show that the proposed model can handle fast moving objects with complex shapes and significant appearance changes within one video frame. DeFMO sets a new state of the art as it outperforms all previous methods on multiple datasets. Temporal super-resolution is among the possible applications.  <ref type="figure">Figure 10</ref>. Temporal super-resolution on selected sequences from test datasets. We compare to the TbD-3D-Oracle <ref type="bibr" target="#b26">[27]</ref> with the manually provided ground truth trajectory from a high-speed footage (GT). Ground truth masks are computed as a difference image between the GT sub-frame and the background. The proposed DeFMO method estimates everything just from inputs on the left. <ref type="bibr" target="#b13">[14]</ref> TbD <ref type="bibr" target="#b10">[11]</ref> TbD-3D <ref type="bibr" target="#b26">[27]</ref> DeFMO (ours) GT TbD-3D-Oracle <ref type="figure">Figure 11</ref>. Comparison on the Falling Objects dataset <ref type="bibr" target="#b9">[10]</ref> with the state-of-the-art methods: Jin et al. <ref type="bibr" target="#b6">[7]</ref>, DeblurGAN-v2 <ref type="bibr" target="#b13">[14]</ref>, TbD <ref type="bibr" target="#b10">[11]</ref>, TbD-3D <ref type="bibr" target="#b26">[27]</ref> and TbD-3D-Oracle that uses GT trajectories. For each method (except <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> only showing deblurred results), we show from left to right: estimated mask, estimated sharp appearance, and temporal super-resolution frames for t = 0 (top) and t = 1 (bottom).</p><formula xml:id="formula_16">cell t 0 t 1 eraser t 0 t 1 B / I [7]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TbD-3D-O</head><p>Ours GT Inputs (B, I) F 0 F 0.25 F 0.5 F 0.75 <ref type="figure">Figure 12</ref>. FMO reconstruction on a validation image from the synthetic dataset. We compare to the TbD-3D-Oracle <ref type="bibr" target="#b26">[27]</ref> method (with ground truth trajectory). Flying mug is synthetically superimposed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>7 I 1 / 7 I 2 / 7 I 3 / 7 I 4 / 7 I 5 / 7 I 6 / 7 I 7 / 7 GTFigure 1 .</head><label>7172737475767771</label><figDesc>Temporal super-resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2 https://github.com/rozumden/fmo-deblurring-benchmark/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Trajectory estimation on sequences from the TbD-3D dataset<ref type="bibr" target="#b26">[27]</ref> (left) and Falling Objects dataset<ref type="bibr" target="#b9">[10]</ref> (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Results on a fast moving car, available on YouTube. I I0 I0.5 I1 GT1 I1.05 I1.1 I1.2 I1.3 I1.4 Standard DeFMO Future prediction Estimated renderings outside of the [0, 1] time range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>F L I L T L L L S PSNR↑ PSNR↑ PSNR↑ SSIM↑ TIoU↑ Ablation study of DeFMO.</figDesc><table><row><cell>for validation. For the</cell></row></table><note>L</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t 1 t 0 FtMt dt + 1 − t 1 t 0 Mt dt B ,(3)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements. This work was supported by a Google Focused</head><p>Research Award, the Czech Science Foundation grant GA18-05360S and Innosuisse grant No. 34475.1 IP-ICT.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Blender -a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blender Online Community</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to extract flawless slow motion from blurry videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiguang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to extract a video sequence from a single motion-blurred image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiguang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Givi</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Restoration of fast moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8577" to="8589" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Intraframe object tracking by deblatting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rozumnyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Motion estimation and deblurring of fast moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="2860" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel performance evaluation methodology for single-target trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukačehovin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2137" to="2155" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The visual hull concept for silhouette-based image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laurentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="162" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="257" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phase-only image based kernel estimation for single image blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bringing alive blurred moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural voxel renderer: Learning an accurate and controllable rendering tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural blind deconvolution using deep priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Dongwei Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Noncausal tracking by deblatting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rozumnyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<editor>Gernot A. Fink, Simone Frintrop, and Xiaoyi Jiang</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="122" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The world of fast moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rozumnyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Novotný</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="4838" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sub-frame appearance and 6d pose estimation of fast moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rozumnyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6777" to="6785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fmodetect: Robust detection and trajectory estimation of fast moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denys</forename><surname>Rozumnyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Sroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arxiv, online</title>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inferring temporal order of images from 3d structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Blurry video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongkuo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Motion blur prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="928" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zooming slow-mo: Fast and accurate one-stage space-time video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="3370" to="3379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deblurring by realistic blurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
