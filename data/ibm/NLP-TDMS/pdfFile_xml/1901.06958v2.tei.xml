<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptation for sEMG-based Gesture Recognition with Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">István</forename><surname>Ketykó</surname></persName>
							<email>istvan.ketyko@nokia-bell-labs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Nokia Bell Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Kovács</surname></persName>
							<email>ferenc.2.kovacs@nokia-bell-labs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Nokia Bell Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztián</forename><surname>Zsolt Varga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nokia Bell Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adaptation for sEMG-based Gesture Recognition with Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/IJCNN.2019.8852018</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-domain adaptation</term>
					<term>recurrent neural network</term>
					<term>muscle-computer interface</term>
					<term>surface electromyography</term>
					<term>EMG</term>
					<term>ges- ture recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surface Electromyography (sEMG/EMG) is to record muscles' electrical activity from a restricted area of the skin by using electrodes. The sEMG-based gesture recognition is extremely sensitive of inter-session and inter-subject variances. We propose a model and a deep-learning-based domain adaptation method to approximate the domain shift for recognition accuracy enhancement. Analysis performed on sparse and High-Density (HD) sEMG public datasets validate that our approach outperforms state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Traditionally, the control of a graphical user interface of a computer or the actions of a robot or drone is being done with hand or arm gestures interacting with a physical controller, like a mouse in case of traditional 2D screens. A touch sensor in case of touch screens can also be thought of as a physical controller. The wearable devices can keep the possibility to build a Human-Computer Interface (HCI), which gives an universal, natural and easy to use interaction with machines. With the advent of wearables there is a opportunity to get rid of the physical controller and interact with the computer without a proxy.</p><p>Sensing hand gestures without a physical proxy can be done by means of wearables or by means of image or video analysis of hand or finger motion <ref type="bibr" target="#b0">[1]</ref>. A wearable-based detection can physically rely on measuring the acceleration and rotations of our bodily parts (arms, hands or fingers) with Inertial Measurement Unit (IMU) sensor(s) or by measuring the myoelectric signals generated by the various muscles of our arms or fingers with EMG sensors. Surface EMG (sEMG) records muscle activity from the surface of the skin which is above the muscle being evaluated. The signal is collected via surface electrodes. Both type of sensors have their own application areas:</p><p>• IMU sensors are typically used for detecting large movements and they are not suitable for recognizing fine gestures such as spread fingers or finger pinching, • EMG devices are typically used for gesture recognition. Usually, both types of sensor data are needed to have good user experience from a HCI point of view.</p><p>In this paper we focus on the main challenges of sEMGbased gesture recognition. In fact, this translates to a time series classification task and several papers provide solutions from classic data science solutions <ref type="bibr" target="#b6">[7]</ref> to deep learning classifications <ref type="bibr" target="#b17">[18]</ref> and it is an active research topic. sEMG signals highly depend on:</p><p>• The subject under test, • Physical conditions of the subject (e.g., skin conductivity), • External/measurement conditions (e.g., sensor placement accuracy).</p><p>If these dependencies are not taken into consideration, like in a scenario when gestures are recognized for one subject in one session without the device being removed from the surface of the skin, the accuracy of state of the art classifiers is above 90%. If any or all these conditions are not met, the accuracy of gesture recognition accuracy degrades to below 50%. In this paper we propose a domain adaptation model that can handle them efficiently. This paper is organized as follows, Section II provides a short summary of the used technologies,then our adaptation model introduced in Section III. Next, we validate our approach using publicly available sEMG data sets: the experimental setup is described in Section IV, and Section V gives the detailed analysis of the experimentation. Finally, we conclude and summarize our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, the used techniques and technologies are introduced:</p><p>• Main properties of sEMG signals and sensors, • sEMG-based gesture recognition techniques, • Recurrent Neural Networks (RNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gestures and sEMG</head><p>The formation of a hand gesture usually adheres to the following pattern. In the onset stage the hand and/or fingers start to execute a motion from a relaxed position until the point they reach their final state because of a physical constraint, and then in the gesture termination stage they go back to a relaxed position. In a sequence of gestures, like in the case of sign languages, the relaxed position is not reached for a long time. Therefore most hand gestures may be considered mixed from the muscle contraction perspective. ©2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. DOI: 10.1109/IJCNN.2019.8852018.</p><p>From the perspective of contraction pattern, hand gestures can determine muscles to be contracted in an isotonic, isometric or mixed pattern. Isotonic contractions involve muscular contractions against resistance in which the length of the muscle changes. Contrary to isotonic contractions, isometric contractions create no change in muscle length but tension and energy are fluctuating. An isometric contraction is typically performed against an immovable object <ref type="bibr" target="#b16">[17]</ref>.</p><p>Furthermore, a sequence of gestures is always a sequence of one or two isotonic contractions followed by exactly one isometric contraction. This is because of the following: during the time interval between relaxed and final states the contraction type is isotonic, because the length of the muscle changes. During the time period where the final state of the gesture is maintained, the contraction type is isometric: tension and energy may fluctuate, but the length of the muscle stays stationary. Finally, in the period when the gesture is terminated and a new gesture follows in the sequence the contraction type is isotonic: the hand/finger is released and/or immediately afterwards contracted again to form the new gesture.</p><p>Muscles generate electric voltage during contraction/detraction. EMG detectors measure this signal through electrodes that are attached to the skin. A digital-analogue conversion is performed with a sampling rate of 100 up to 2000 Hz and the outcome is usually normalized into a range of [-1.0, 1.0]. The typical bandwidth of this signal is 5-450 Hz <ref type="bibr" target="#b24">[25]</ref>. This set of time series (one per each pair of electrodes) represents usually the input for gesture detection algorithms.</p><p>In the number of sensors point of view, two different types of measurement configuration are in use: Usually, these sensors are arranged in a matrix and they cover an area of the skin. If the number of sensors is more than 100, the configuration is called high density EMG. Both configurations have pros and cons. On the one hand, sparse EMG need smaller bandwidth as it has fewer channels and less data to transfer, on the other hand, it is more sensitive to the sensor placement. Otherwise, the dense EMG setups are less sensitive to the sensor placement, but they need more bandwidth and it has wiring issues in case of wearable devices. There are several publicly available EMG datasets, some of them were recorded with parse sensor configuration, while others using dense setup. <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b11">[12]</ref>. electrode skin contact on different subjects has the highest alternation. In sEMG-based gesture recognition there are three cases in terms of the data variability (as shown on <ref type="figure" target="#fig_1">Fig. 1</ref>): 1) Intra-session: in this case the data variabilty comes from differences between the trials/repetitions of the performed gestures by the human subject. 2) Inter-session: in this case there is still the intra-session variability with an additional data variabilty which comes from the differences between the recording sessions. At each recording session the sensor placement can have some shift and/or rotations. 3) Intra-subject: The electromyogram signal is a kind of biological signal which is severely affected by the difference between subjects. In this case the data variabilty comes from the differences of human subjects. The intra-session gesture recognition have been extensively researched. Existing sEMG-based solutions utilizes time domain, frequency domain, and time-frequency domain features. Many researchers focused on presenting new sEMG features based on their domain knowledge or analyzing existing features to propose new feature sets. Traditional machine learning classifiers have been employed to recognize sEMGbased gestures, such as k-Nearest Neighbor (kNN) <ref type="bibr" target="#b19">[20]</ref>, Linear Discriminate Analysis (LDA) <ref type="bibr" target="#b20">[21]</ref>, Hidden Markov Model (HMM) <ref type="bibr" target="#b21">[22]</ref>, and Support Vector Machine (SVM) <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b22">[23]</ref>. The Convolutional Neural Network (CNN) architecture is the most widely used deep learning technique for sEMGbased gesture recognition. <ref type="bibr" target="#b17">[18]</ref> provided a novel CNN model to extract spatial information from the instantaneous sEMG images and achieved state-of-the-art performance. <ref type="bibr" target="#b18">[19]</ref> applies a novel hybrid CNN-RNN architecture with superior results in the intra-session scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. sEMG-based gesture detection</head><p>The inter-session and inter-subject variability causes domain shift in the distributions of the sEMG sensor data. From a machine learning viewpoint, one of the key issues in intersession/subject Muscle-Computer Interfaces (MCIs) is domain adaptation, i.e., developing learning algorithms in which the training data (source domain) used to learn a model have a different distribution compared with the data (target domain) to which the model is applied <ref type="bibr" target="#b17">[18]</ref>. Domain adaptation has gained increasing interest in the context of deep learning. When only a small amount of labeled data is available in the target domain  <ref type="bibr" target="#b25">[26]</ref> during the training phase, fine-tuning pre-trained networks has become the de facto method.</p><p>Another approach is the unsupervised adaptive learning which utilises only unlabellet target data. <ref type="bibr" target="#b19">[20]</ref> compares four concepts which work with SVM and provides state-of-the-art results on the NinaPro dataset. <ref type="bibr" target="#b17">[18]</ref> provides the state-of-theart solution on the CapgMyo dataset. They invented a multisource adaptive batch normalization technique which works with CNN architecture. The drawback of this solution, that in case of multiple sources (i.e., multiple subjects), constraints and considerations are needed per source at pre-training time of that model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Recurrent Neural Networks</head><p>With the increase of computational capabilities in the recent years, neural networks have become more popular due to their ability to tackle complex data science problems. A typical neural network has an input layer, one or many hidden layers and an output layer. Each hidden layer has a set of nodes that take in weighted inputs from the previous layer and provide an output through an activation function to the next layer. Recurrent neural networks (RNNs) are a family of neural networks in which there are feedback loops in the system. Feedback loops allow processing the previous output with the current input, thus making the network stateful, being influenced by or remembering the earlier inputs in each step (see <ref type="figure" target="#fig_2">Fig. 2</ref>). A hidden layer that has feedback loops is also called a recurrent layer. The mathematical representation of a simple recurrent layer can be seen in Eq. <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_0">h t = σ h (w h x t + u h h t−1 + b n ) y t = σ y (w y h t + b y )<label>(1)</label></formula><p>However, regular RNNs suffer from the vanishing gradient problem which means that the gradient of the loss function decays exponentially with time, making it difficult to learn long-term temporal dependencies in the input data. <ref type="bibr" target="#b26">[27]</ref> Long Short Term Memory (LSTM) networks had been proposed to solve this problem. They are a special type of RNN that attempt to solve the vanishing gradient problem <ref type="bibr" target="#b27">[28]</ref>. In this paper we will present a solution that is utilizing LSTM cells.</p><p>LSTM units contain a set of gates that are used to control the stages when information enters the memory (input gate: i t ), when it's output (output gate: o t ) and when it's forgotten (forget gate: f t ) as seen in Eq. (2). This architecture allows the neural network to learn longer-term dependencies and they are </p><formula xml:id="formula_1">f t = σ(W f · [h t−1 , x t ] + b f ) i t = σ(W i · [h t−1 , x t ] + b i ) C t = tanh(W C · [h t−1 , x t ] + b C ) C t = f t * C t−1 + i t * C t o t = σ(W o · [h t−1 , x t ] + b o ) h t = o t * tanh(C t )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. 2-STAGE DOMAIN ADAPTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture</head><p>We propose a model which consists of two components as can be seen at <ref type="figure" target="#fig_4">Fig. 5</ref> and we name it as 2-Stage RNN (2SRNN):</p><p>1) The domain adaptation layer: which is a single fullyconnected layer without a non-linear activation function. The input vector x ∈ R f is the same dimension as the output vector x ∈ R f where f is the number of input . .</p><formula xml:id="formula_2">x f    =    m 11 . . . m 1f . . . m ij . . . m f 1 . . . m f f       x 1 .</formula><p>. .</p><formula xml:id="formula_3">x f    +    b 1 . . . b f   </formula><p>2) The sequence classifier: which is a deep stacked RNN with many-to-one setup followed by a G-way fullyconnected layer and a softmax classifier. G is the number of gestures to be recognized. The linear transformation for domain adaptation with the same M and b is applied to the input of the RNN at each timestamp t:</p><formula xml:id="formula_4">x t = Mx t + b.</formula><p>The transformation of the input values (to solve the domain shift) is approximated with perceptron learning. Our assumption is that this transformation is a linear one. Apparently, a linear transformation yields the highest gain. Also, it could be a more complex (polinomial or non-linear) one. There is still gain as long as the domain adaptation layer is smaller (in size and complexity) than the sequence classifier component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4 visualises our method with two consecutive stages:</head><p>1) Pre-training stage: In the first stage, the weights of the domain adaptation layer are frozen and the sequence classifier is trained from scratch on the source dataset. The domain adaptation layer's initial weights could be several combinations of real numbers but we chose M to be the identity matrix and b to be a vector of zeros to represent the identity transformation. We apply supervised learning. The optimization is a gradient descent with backpropagation. The loss function is the categorical cross entropy:</p><formula xml:id="formula_5"> entropy = − g∈G I g ln p g<label>(3)</label></formula><p>where G is the number of gestures and I is the indicator function whether class label g is the correct classification for the given observation and p is the predicted probability that the observation is of class g.</p><p>2) Domain adaptation stage: In the second stage, the weights of the sequence classifier are frozen in their pretrained state and the domain adaptation layer's weights are trained on the target dataset. In this stage, the same supervised learning is applied. The loss (Eq. <ref type="formula" target="#formula_5">(3)</ref>) is backpropagated to the domain adaptation layer during the process. The advantages of this architecture:</p><p>1) The training during the second stage is very fast because there is only a shallow network to tackle with. 2) Training a linear layer ensures the convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>We approximate the inter-session and inter-subject shift in the values of the sEMG electrodes with a linear transformation of the input. The transformation could be polinomial or nonlinear also but that could result in either a larger domain adaptation network or a non-convex optimization manifold. We let the model discover the coefficients of this linear transformation on the target data during the domain adaptation process.</p><p>We have tested the approach on HD sEMG and sparse sEMG datasets:</p><p>1) CapgMyo dataset <ref type="bibr" target="#b17">[18]</ref>: includes HD-sEMG data for 128 channels acquired from 23 intact subjects. The sampling rate is 1 KHz. It consists of 3 sub-databases: a) DB-a: 8 isometric and isotonic hand gestures were obtained from 18 of the 23 subjects. b) DB-b: 8 isometric and isotonic hand gestures from 10 of the 23 subjects in two recording sessions on different days. c) DB-c: 12 basic movements of the fingers were obtained from 10 of the 23 subjects. We downloaded the pre-processed version from http: //zju-capg.org/myo/data to use the same data as <ref type="bibr" target="#b17">[18]</ref> to be able to compare our results with theirs. In that version, the power-line interference was removed from the sEMG signals by using a band-stop filter (4555 Hz, second-order Butterworth). Only the static part of the movements was kept in it (for each trial, the middle one-second window, 1000 frames of data). They used the middle one second data to ensure that no transition movements are included in it. We rescaled the data to have zero mean and unit variance, then we rectified it and applied smoothing.</p><p>2) NinaPro dataset <ref type="bibr" target="#b22">[23]</ref>: a) DB1: The NinaPro sub-database 1 (DB-1) is for the development of hand prostheses, and contains sparse multi-channel sEMG recordings. It consists of a total of 52 gestures performed by 27 intact subjects. i) Gesture numbers 112: 12 basic movements of the fingers (flexions and extensions). These are equivalent to gestures in CapgMyo DB-c. ii) Gesture numbers 1320: 8 isometric, isotonic hand configurations (hand postures). These are equivalent to gestures in CapgMyo DB-a and DB-b. The data is recorded at a sampling rate of 100 Hz, using 10 sparsely located electrodes placed on subjects upper forearms. The sEMG signals were rectified and smoothed by the acquisition device. We downloaded the re-organized version from http://zju-capg.org/myo/data/ ninapro-db1.zip to use the same data as <ref type="bibr" target="#b17">[18]</ref> for fair comparison. For each trial, we used the middle 1.5second window, 180 frames of data to get the static part of the movements. We used the middle 1.5-second data with the aim that no transition movements are included in it. We decompose the sEMG signals into small sequences using the sliding window strategy with overlapped windowing scheme. The sequence length must be shorter than 300ms <ref type="bibr" target="#b18">[19]</ref> to satisfy real-time usage constraints. To compare our proposed method with previous works, we follow the segmentation strategy in previous studies.</p><p>We use Keras with Tensorflow backend. The domain adaptation layer has the M ∈ R f ×f where f is 128 in case of the CapgMyo dataset and 10 in case of the NinaPro. It is implemented with the TimeDistributed Keras wrapper. For the sequence classifier we use a 2-stack RNN with LSTM cells. Each LSTM cell has a dropout with the probability of 0.5 and 512 hidden units. The RNN is followed by a G-way fullyconnected layer with 512 units (dropout with a probability of 0.5) and a softmax classifier. We use the Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with the learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>The outcomes of our investigations are compared with results from <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b6">[7]</ref>, and <ref type="bibr" target="#b18">[19]</ref>.</p><p>Based on the classification run on the test dataset, taken from the same database as the training dataset in a manner detailed in the subsequent, the classification accuracy is calculated for each database as given below:</p><p>Classification Accuracy = Correct classifications Total classifications * 100% (4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Intra-session validation</head><p>We first look at intra-session validation to benchmark our sequence classifier against the state-of-the-art in the least challenging scenario, on top of distinct datasets, without performing any further optimization.</p><p>In case of CapgMyo dataset we used the same evaluation procedure that was used in the previous study <ref type="bibr" target="#b17">[18]</ref>. For each subject, a classifier was trained by using 50% of the data (E.g., trials 1, 3, 5, 7 and 9 for that subject) and tested by using the remaining half. This procedure was performed on each subdatabase. For DB-b, the second session of each subject was used for the evaluation.</p><p>In previous works on NinaPro DB-1 <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b17">[18]</ref>, the training set consisted of approximately 2/3 of the gesture trials of each subject and the remaining trials constitute the test set.</p><p>We chose 150-ms sequence length for the RNN in all the cases for fair comparison. <ref type="table">Table I</ref> shows our average intrasession recognition accuracy together with the state-of-theart. Columns noted with DB-a, DB-b, DB-c belong to the CapgMyo dataset and columns noted with DB-1 12 gestures </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inter-session validation</head><p>We evaluated inter-session recognition for CapgMyo DBb, in which the model was trained using data recorded from the first session and evaluated using data recorded from the second session. In each case, we ran our domain adaptation for 100 epochs using the following 3 scenarios: 1) Scenario 1: domain adaptation is not applied, 2) Scenario 2: domain adaptation performed on the complete set of target data (all data of the target session); this scenario has only been considered for the purpose of comparability with alternative approaches, 3) Scenario 3: domain adaptation performed on 50% of the trials of the target session, while the validation set is the remaining 50%. Our adaptation scheme enhanced inter-session recognition with 29 percentage points (accuracy of 83.8% compared to 54.6%) which is a 53% improvement (shown in <ref type="table">Table II</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inter-subject validation</head><p>In this experiment, we evaluated inter-subject recognition of 8 gestures using the second recording session of CapgMyo DB-b and the recognition of 12 gestures using CapgMyo DBc and the sub-set of 12 gestures from the NinaPro DB-1. We performed a leave-one-out cross-validation, in which each of the subjects was used in turn as the test subject and a classifier was trained using the data of the remaining subjects, using the following 3 scenarios:   ) Scenario 1: domain adaptation is not applied, 2) Scenario 2: domain adaptation performed on the complete set of target data (all data of the target subject); this scenario has only been considered for the purpose of comparability with alternative approaches, 3) Scenario 3: domain adaptation performed on 50% of the trials of the target subject, while the validation set is the remaining 50%, with the following 2 variants: a) CapgMyo DB-b, DB-c: 50%-50% of the target subject data (5 of the 10 trials are used for domain adaptation and another 5 is for its validation). b) NinaPro DB-1 12 gestures: 50%-50% of the target subject data (5 of the 10 trials are used for domain adaptation and another 5 is for its validation). In case of the CapgMyo DB-b and DB-c we ran our domain adaptation for 100 epochs, and in case of the Ninapro DB-1 for 400 epochs. The sequence length of our RNN was 150 ms in case of the CapgMyo DB-b and DB-c for comparison reasons with <ref type="bibr" target="#b17">[18]</ref>, and 400 ms in case of the Ninapro DB-1 for comparison reasons with <ref type="bibr" target="#b6">[7]</ref>. <ref type="table">Table III</ref> shows the classification accuracies of the various methods. Our adaptation scheme enhanced inter-subject recognition with a 71% improvement on DB-b, 145% improvement on DB-c and 86% improvement on DB-1 12 gestures (shown in <ref type="table">Table III</ref>).</p><p>We summarise the domain adaptation improvement results in <ref type="table" target="#tab_2">Table IV</ref>. As indicated there, the performance of 2SRNN is superior in all cases: the improvement obtained from our domain adaptation in the inter-session and inter-subject cases exceeds those obtained through alternative domain adaptation approaches.</p><p>It is natural to ask how much data is required to obtain a stable recognition accuracy and how our solutions relates to the common supervised fine-tuning method in deep learning. <ref type="figure" target="#fig_5">Fig. 6</ref> visualises a comparison of the inter-subject domain adaptation scenario (on the CapgMyo DB-b) based on our 2-stage RNN method and an adaptation based on supervised fine-tuning in one concrete scenario. In this experiment we limited the available data to 20%, 40%, 60%, 80% and 100% of the total 5 trials used for domain adaptation (the remaining 5 trials are kept for validation). The mean classification accuracy is plotted as a function of the available target data for domain adaptation. <ref type="figure" target="#fig_5">Fig. 6</ref> shows how the accuracy of the two method increases with the amount of available target data and our 2SRNN remains persistently superior to the fine tuning method (by 20%). In each case we ran the domain adaptations for 5 epochs only since it is expected to get improvements quickly for better human-computer interactions. On our server (with 2 Nvidia Titan V GPUs) these 5 training epochs took approximately 7.5 seconds for our 2SRNN and 27.7 seconds for supervised fine-tuning, respectively. Therefore a 20% improvement in accuracy is complemented with a decrease in execution time by almost a factor of 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>For real Human-Computer Interactions the sEMG-based gesture detection must overcome the inter-session and intersubject domain shifts. We proposed a 2-stage domain adaptation solution which has superior performance over the wellknown supervised fine-tuning applied in deep learning and the state-of-the-art unsupervised adaptation methods. Empirical results show that the linear transformation of the input features is a good approximation for handling the domain shifts. It is fast and light weight and applicable (besides the RNN) to any machine learning approaches which are trainable with backpropagation. Combinations of this method with generative unsupervised models can be the next step of further improvements for usable HCI solutions.</p><p>The code is available at https://github.com/ketyi/2SRNN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>sparse EMG: Only a couple of sensors are attached to the skin. Typically, 8-10 sensors are used in this configuration. • dense EMG: Tens of sensors are attached to the skin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>During a (recording/testing) session, there are several repetitions/trials of the same gesture set by the human subjects. Meanwhile, the sEMG electrode sensors expected to remain in the same placement. Multiple sessions naturally differ by sensor placement because of their shift on the skin and rotation around the arm. Apparently, the sensor placement and Domain shift in case of different scenarios</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>A simple RNN in compact and unrolled representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>LSTM cell architecture<ref type="bibr" target="#b25">[26]</ref> Our two-stage domain adaptation method widely used to analyze time-series data.<ref type="bibr" target="#b28">[29]</ref> InFig. 3yellow rectangles represent a neural network layer, circles are pointwise operations and arrows denote the flow of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Our neural network architecture (2SRNN) features. The trainable weights form a square matrix M ∈ R f ×f plus there is a bias vector b ∈ R f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Recognition accuracy comparison of supervised approaches</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>'-' notes that the authors of that method did not focus on the scenario.</figDesc><table><row><cell></cell><cell></cell><cell>CapgMyo</cell><cell></cell><cell></cell><cell>NinaPro</cell></row><row><cell></cell><cell cols="5">DB-a DB-b DB-c DB-1 12 gestures DB-1 8 gestures</cell></row><row><cell cols="4">Du [18] 99.5% 98.6% 99.2%</cell><cell>84%</cell><cell>83%</cell></row><row><cell cols="2">Hu [19] 99.7%</cell><cell>-a</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Atzori [23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90%</cell><cell>-</cell></row><row><cell cols="4">2SRNN 97.1% 97.1% 96.8%</cell><cell>84.7%</cell><cell>90.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE I</cell><cell></cell><cell></cell></row><row><cell cols="6">INTRA-SESSION RECOGNITION ACCURACY RESULTS</cell></row><row><cell></cell><cell></cell><cell cols="4">Scenario 1 Scenario 2 Scenario 3</cell></row><row><cell></cell><cell>Du [18]</cell><cell>47.9%</cell><cell>-</cell><cell></cell><cell>63.3%</cell></row><row><cell></cell><cell>2SRNN</cell><cell>54.6%</cell><cell cols="2">85.8%</cell><cell>83.8%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell></row><row><cell cols="6">INTER-SESSION RECOGNITION ACCURACY RESULTS</cell></row><row><cell></cell><cell></cell><cell cols="3">ON CAPGMYO DB-B</cell><cell></cell></row><row><cell cols="6">and DB-1 8 gestures belong to the NinaPro dataset. As can be</cell></row><row><cell cols="6">seen from Table I the accuracy achieved by our model is at</cell></row><row><cell cols="6">most 2.4 percentage points worse than other methods for the</cell></row><row><cell cols="6">CapgMyo database and with 0.7 up to 7.7 percentage points</cell></row><row><cell cols="6">better for the NinaPro dataset. This accuracy has been achieved</cell></row><row><cell cols="6">by keeping the training duration to constant 100 epochs and</cell></row><row><cell cols="6">without any hyper-parameter tuning. This outcome indicates</cell></row><row><cell cols="6">that that our model is at least comparable in the intra-session</cell></row><row><cell cols="4">case with other approaches.</cell><cell></cell><cell></cell></row></table><note>a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>6% 34.8% 35.1% 96.8% 91.9% 65.7% 89.9% 85.4% 65.2%</figDesc><table><row><cell></cell><cell cols="2">Scenario 1</cell><cell cols="2">Scenario 2</cell><cell></cell><cell></cell><cell cols="2">Scenario 3</cell></row><row><cell cols="9">DB-b DB-c DB-1 DB-b DB-c DB-1 DB-b DB-c DB-1</cell></row><row><cell cols="3">Du [18] 39.0% 26.3% -</cell><cell>-</cell><cell>-</cell><cell cols="4">-55.3% 35.1% -</cell></row><row><cell>Atzori [23] -</cell><cell>-</cell><cell>25%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Patricia [7] -</cell><cell>-</cell><cell>30%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55%</cell></row><row><cell cols="4">2SRNN 52.TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">INTER-SUBJECT RECOGNITION ACCURACY RESULTS</cell><cell></cell></row><row><cell></cell><cell cols="8">Inter-session improvement Inter-subject improvement</cell></row><row><cell></cell><cell></cell><cell>DB-b</cell><cell></cell><cell cols="2">DB-b DB-c</cell><cell></cell><cell>DB-1</cell><cell></cell></row><row><cell>Du [18]</cell><cell></cell><cell>32%</cell><cell></cell><cell cols="2">42% 33%</cell><cell></cell><cell>-</cell><cell></cell></row><row><cell>Patricia [7]</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>83%</cell><cell></cell></row><row><cell>2SRNN</cell><cell></cell><cell>53%</cell><cell></cell><cell cols="3">71% 145%</cell><cell>86%</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell>DOMAIN ADAPTATION IMPROVEMENT COMPARISONS</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey on 3D Hand Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1659" to="1673" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Real-time Human Motion Recognition System Using Topic Model and SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE EMBS International Conference on Biomedical &amp; Health Informatics (BHI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review on applications of activity recognition systems with regard to performance and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Machot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mayr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Journal of Distributed Sensor Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual Domain Adaptation: An Overview of Recent Advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Learning to Speed-Up Control of Prosthetic Hands: a Few Things Everybody Should Know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gregori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gijsberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Rehabilitation Robotics (ICORR), QEII Centre</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1130" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Canonical Correlation Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Intl Conference on Machine Learning</title>
		<meeting><address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-Source Adaptive Learning for Fast Control of Prosthetics Hand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<ptr target="http://archive.ics.uci.edu/ml/datasets/emg+dataset+in+lower+limb" />
		<title level="m">EMG dataset in lower limb</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>retrieved on 27.12</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward Improved Control of Prosthetic Fingers Using Surface Electromyogram (EMG) Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Khushaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takruri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dissanayake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1073110738</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<idno>8a21105e-4eca-4531-b021-a62509711ee0).html</idno>
		<ptr target="https://tutcris.tut.fi/portal/en/datasets/mimetic-interfaces-facial-surface-emg-dataset-2015" />
		<title level="m">Mimetic Interfaces: Facial Surface EMG Dataset</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving EMG based classification of basic hand movements using EMD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sapsanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tzes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society 13 (EMBC 13)</title>
		<imprint>
			<date type="published" when="2013-07" />
			<biblScope unit="page" from="5754" to="5757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<idno>27.12</idno>
		<ptr target="http://zju-capg.org/myo/data" />
		<title level="m">High Density Surface Electromyography Database for Gesture Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparison of six electromyography acquisition setups on hand movement classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pizzolato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">186132</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Design of Hand Gestures for Human-Computer Interaction: Lessons from Sign Language Interpreters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rempel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Camilleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Hum Comput Stud</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">728735</biblScope>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hand Gestures: Perspectives and-Preliminary Implications for AdultsWith Acquired Dysarthria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dagenais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Speech-Language Pathology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">107115</biblScope>
			<date type="published" when="2000-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Cote-Allard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drouin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Campeau-Lecours</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Glette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gosselin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07756v4[cs.LG]19</idno>
		<title level="m">Deep Learning for Electromyographic Hand Gesture Signal Classification Using Transfer Learning</title>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Review of Classification Techniques of EMG Signals during Isotonic and Isometric Contractions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nazmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zamzuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors (Basel)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1304</biblScope>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Surface EMG-Based Inter-Session Gesture Recognition Enhanced by Deep Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<idno type="DOI">10.3390/s17030458</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">458</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A novel attention-based hybrid CNN-RNN architecture for sEMG-based gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0206049</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">206049</biblScope>
			<date type="published" when="2018-10-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">EMG-based hand gesture recognition for realtime biosignal interfacing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mastnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Andr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent User Interfaces</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">3039</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Study on interaction between temporal and spatial information in classification of EMG signals in myoelectric prostheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Caterina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lakany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petropoulakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Soraghan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sign Language Recognition System using SEMG and Hidden Markov Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Swee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anuar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mra</forename><surname>Kadir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Mathematical Methods, Computational Techniques and Intelligent Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5053</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Electromyography data for non-invasive naturally-controlled robotic hand prostheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gijsberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G M</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elsig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Giatsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bassetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2014.53</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140053</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">ICLR</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The ABC of EMG: a practical introduction to kinesiological electromyography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Konrad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">977162214</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<idno>11.01</idno>
		<ptr target="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" />
		<title level="m">Understanding LSTM Networks (retr</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
