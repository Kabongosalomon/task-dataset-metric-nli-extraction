<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geodesic Distance Histogram Feature for Video Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Ping</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Geodesic Distance Histogram Feature for Video Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a geodesic-distance-based feature that encodes global information for improved video segmentation algorithms. The feature is a joint histogram of intensity and geodesic distances, where the geodesic distances are computed as the shortest paths between superpixels via their boundaries. We also incorporate adaptive voting weights and spatial pyramid configurations to include spatial information into the geodesic histogram feature and show that this further improves results. The feature is generic and can be used as part of various algorithms. In experiments, we test the geodesic histogram feature by incorporating it into two existing video segmentation frameworks. This leads to significantly better performance in 3D video segmentation benchmarks on two datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video segmentation is an important pre-processing step for many high-level video applications such as action recognition <ref type="bibr" target="#b0">[1]</ref>, scene understanding <ref type="bibr" target="#b1">[2]</ref>, or 3D reconstruction <ref type="bibr" target="#b2">[3]</ref>. A more compact representation not only reduces the subsequent processing space and time requirements, but also provides sets of visual segments that contain meaningful cues for higher-level computer vision tasks. However, generating supervoxels from videos is a significantly more difficult task than superpixel segmentation from images, due to the heavy computational cost and the extra temporal dimension. Specifically, well delineated spatio-temporal video segments can be used for tracking bounded regions, foreground moving objects, or semantic understanding. For example, locating the movement of hands is helpful for gesture or action recognition, and separating foreground/background can pin-point the region-of-interest for detecting moving objects. Therefore, these spatio-temporal segments should be temporally consistent in order to be beneficial for these computer vision tasks.</p><p>For video segmentations that are initialized from superpixels, the main goal is to consider the connections between neighboring superpixels and to decide which ones belong to the same spatio-temporal cluster. The connections are usually represented as a spatio-temporal graph, where the nodes are the superpixels and the edges connect superpixels that are adjacent to each other. The edges are weighted based on the similarity distances between pairs of superpixels. Previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> proposed a variety of features corresponding to a wide range <ref type="figure">Fig. 1</ref>: The segmentation results on video "monkey" from Segtrack v2 dataset <ref type="bibr" target="#b5">[6]</ref>. Top row: original frames with superimposed ground-truth (green). Second row: segmentation results of the PGP algorithm ( <ref type="bibr" target="#b6">[7]</ref>) using their four predefined features. Third row: result of PGP with our feature integrated. Fourth row: segmentation result of spectral clustering with the6 features proposed in <ref type="bibr" target="#b7">[8]</ref>. Bottom row: segmentation result of spectral clustering with our feature integrated. Our results show better temporal consistency and less over-segmentation. of low and mid-level image cues from superpixels. For example, the within-frame similarities were computed from boundary magnitude, color, texture, and shape, and the temporal connections were defined by the direction of optical flow or motion trajectories. Importantly, the aforementioned features that were used for video segmentation encode only local information, extracted from within each superpixel. One would expect improved performance when combining local and global features, if the appropriate global features per superpixel were extracted.</p><p>The geodesic distance has been shown to be effective for image segmentation problems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> but its applications in the video domain have been limited <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. In this work, we propose a complete methodology for the use of geodesic distance histogram features in the video segmentation problem. The histogram feature describes the superpixel-of-interest by the distribution of the geodesic distances from it, to all other superpixels in the same frame. The representation compactly encodes global similarity relations between segments. Thus, we want to use per-frame geodesic distance information to associate superpixels both within and across frames. However, the nature of this global representation, poses several challenges that need to be addressed, in order to successfully use geodesic distance histograms for video segmentation:</p><p>-The feature needs to be robust across frames in order to perform useful superpixel association. That means if a superpixel has a unique representation in one frame, its representation in the next frame should be also unique, in order to facilitate matching. -For relatively small segments, their similar relationship to global context can dwarf distinctive neighborhood information, which might make them hard to differentiate. -The feature does not encode any spatial relationships between segments.</p><p>Such relationships often offer constrains that allow otherwise similar segments to be distinguished from each other.</p><p>In this paper, we address these issues in order to derive a geodesic histogram feature that is appropriate for video segmentation tasks. In essence, we introduce the necessary local information in the global representation, in order to disambiguate associations across frames. For a given superpixel, we first extract the soft boundary map of the frame where it belongs, then we compute geodesic distances from the superpixel-of-interest to all other superpixels in the same frame using the boundary scores. If we were performing per frame segmentation, a 1D histogram of these scores would suffice <ref type="bibr" target="#b9">[10]</ref>. However, due to motion, this 1D histogram is not robust across frames. As observed previously <ref type="bibr" target="#b12">[13]</ref>, a 2D joint histogram of intensity and geodesic distance is much more robust. To encode more spatial information into the feature, we compute multiple geodesic histograms in a spatial pyramid <ref type="bibr" target="#b13">[14]</ref>. Finally, we weigh the bins with respect to their spatial distance from the superpixel-of-interest, in order to favor potentially discriminative neighborhood information. We show in experiments that when we add our complete geodesic histogram feature into existing frameworks, the resulting segmentations are greatly improved, especially in 3D segmentation accuracy and temporal consistency. The feature is also fast to compute, without increasing significantly processing time for the existing frameworks. The geodesic histogram features are added into two state-of-the-art video segmentation frameworks that are based on superpixel clustering, and tested on two popular datasets using standard 3D segmentation benchmarks.</p><p>The rest of paper is organized as follows: Section 2 discusses related work. Section 3 discusses the motivation, computation, and analysis of the proposed geodesic histogram features. Implementation details are described in Section 3.4. Section 4 presents the experimental results. Section 5 concludes the paper and discusses other possible applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many video segmentation works propose diverse features to capture various kinds of information in order to estimate the similarity between the components of the video. Appearance can be represented by features based on color <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>, texture <ref type="bibr" target="#b15">[16]</ref>, and soft boundaries <ref type="bibr" target="#b16">[17]</ref>. Motion related features have also been utilized often, including short-term motion features based on optical flow <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and long-term motion features based on trajectories <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Superpixel shape is used to compute the similarities among superpixels across frames <ref type="bibr" target="#b14">[15]</ref>. Some works discuss the choice of features to use <ref type="bibr" target="#b7">[8]</ref> as well as the method to incorporate various kinds of features into affinity matrices <ref type="bibr" target="#b3">[4]</ref>.</p><p>Geodesic distances provide appearance-based similarity estimates. Geodesic distances have been applied widely on segmentation related problems on images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref>. A feature based on geodesic distance for matching images of deformed objects has been introduced in <ref type="bibr" target="#b12">[13]</ref>. The authors showed that the geodesic distance could be invariant to object deformations, by encoding pixels as color histograms on the surrounding pixels that have the same geodesic distances. The geodesic distance is also used to propose object segments on images <ref type="bibr" target="#b8">[9]</ref>, which is based on the correlation between the object boundary and the change in the geodesic distance transform. Several video segmentation methods have employed geodesic distance for various purposes. The salient object segmentation framework uses a geodesic distance in each frame to estimate the objectness of superpixels <ref type="bibr" target="#b10">[11]</ref> on a per frame basis. Further work further proposes a spatio-temporal geodesic distance <ref type="bibr" target="#b9">[10]</ref> that extends image segmentation to video segmentation. However, the proposed spatio-temporal distance has to be constrained to be temporally non-decreasing to preserve the metric property, thus limiting the robustness of the method.</p><p>In this paper, we propose a feature based on geodesic distance to estimate the similarity between the superpixels in the video. We consider the frame-wise distribution of the geodesic distances, i.e., the histogram of geodesic distances from each superpixel to all other superpixels in the same frame. This representation compactly encodes the relative similarity distances between the segment containing the superpixel-of-interest to all the other segments on the frame. This global information therefore serves as a complement to the set of to the set of appearance, motion, and shape-based features which only encode information from the inner region of the superpixel-of-interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Geodesic Distance Histogram Feature</head><p>Given a frame of the video, let X be the set of superpixels:</p><formula xml:id="formula_0">X = {x 1 , . . . , x n }.</formula><p>The frame is then represented by a non-negative, undirected graph G = (X, E), where each value in E is associated with a pair of neighboring superpixels in X, and the edge weight is computed as the boundary strength between the two superpixels. The geodesic distance between any two superpixels x i , x j ∈ X is defined as the weight of the shortest path between the two superpixels in G.</p><p>Given a superpixel x i on a frame, the geodesic distance between x i and all other superpixels in the same frame is computed and pooled into a geodesic distance histogram. This histogram contains the global information of the frame with respect to x i in terms of geodesic distance distribution, and can be used for computing pair-wise superpixel similarity both within and across frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">1D Geodesic Distance Histogram.</head><p>The simplest approach is to use an 1D histogram to describe the distribution of the geodesic distances, where a bin of the histogram represents the number of superpixels with a particular geodesic distance. This is similar to the concept of critical level sets <ref type="bibr" target="#b8">[9]</ref>, where each critical level defines a group of superpixels having their geodesic distances less than a certain threshold. Each bin of the histogram is then associated with a region in the image. In order to keep our feature relatively constant across frames, the value of each bin should stay approximately the same. This means that the regions associated with each bin also remain relatively stable. Considering the superpixel (in red) shown in <ref type="figure" target="#fig_0">Fig. 2</ref>(a), two regions corresponding to the first two bins of the histogram are visualized in <ref type="figure" target="#fig_0">Fig. 2(b)</ref>. The first bin collects the votes of all superpixels with the lowest geodesic distance interval, forming the region indicated by the leftmost arrow. However, the region corresponding to the second bin is the combination of superpixels from different semantic regions. The value of the second bin is therefore not robust since these regions could potentially move in different ways, and end up voting for different bins in subsequent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">2D Intensity-Geodesic Distance Histogram.</head><p>We incorporate the intensity feature as an additional cue to complement the geodesic distance, on order to constrain bins to correspond to individual regions instead of disparate groups of regions. Thus the histogram becomes a 2D table where each cell is voted for by the superpixels that have a particular pair of The figure shows that the 2D histogram is more robust than the 1D histogram for across-frame matching: there are multiple superpixels located in multiple regions that have similar 1D histograms with the superpixel-of-interest, while only the superpixels located within the upper-body region have the most similar 2D histograms.</p><p>geodesic distance and intensity. The joint distribution of intensity-geodesic distance was originally proposed in <ref type="bibr" target="#b12">[13]</ref>, where the joint distribution was expected to be stable and informative under a wide range of deformations. <ref type="figure" target="#fig_0">Fig. 2</ref>(c) visualizes the intensity-geodesic distance histogram of a superpixelof-interest (shown in red in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>). Notice that the second bin of the 1D histogram equals to the sum of all cells in the second row of the 2D histogram, and the region from the second bin in the 1D histogram is now separated into multiple smaller regions corresponding to these cells. This is a desired effect given that each of the cells in the 2D histogram contains superpixels from the same semantic region as the 1D case. We also visualized the cell with the highest value in <ref type="figure" target="#fig_0">Fig. 2(c)</ref>, which corresponds to the superpixels within the entire grass field. Such a region is likely to be stable across frames and remain connected. This implies that as long as the intermediate boundaries remain the same, these regions would still contribute to the same cells in the histogram.</p><p>To compute the similarity distance between two histograms, we can use the χ 2 distance or the Earth Mover's Distance. Following <ref type="bibr" target="#b12">[13]</ref>, the χ 2 distance between two 2D histograms H p and H q with size M × N is defined by:</p><formula xml:id="formula_1">χ 2 (H p , H q ) = 1 2 K k=1 M m=1 [H p (k, m) − H q (k, m)] 2 H p (k, m) + H q (k, m)<label>(1)</label></formula><p>The Earth Mover's Distance (EMD) is computed as the sum of the 1D EMDs at each intensity bin of the 2D histogram. <ref type="figure" target="#fig_1">Fig. 3</ref> visualizes the similarity values computed based on 1D and 2D feature histograms from the superpixel-of-interest in <ref type="figure" target="#fig_0">Fig. 2(a)</ref> on a later video frame. In the color scheme, higher similarity is represented by the warmer color. The figure shows that the 1D histogram is less robust than the 2D histogram: there are multiple regions having similar 1D histograms with the superpixel-of-interest, and the superpixel with the highest 1D histogram similarity is in the background. In contrast, the superpixel with the highest similarity using the 2D histogram falls within the same upper-body region, a desirable result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spatial Information</head><p>Pooling methods such as histograms discard spatial information, such as image distance relationships or local neighborhood patterns. We encode spatial cues in two ways: 1) by embedding spatial distances into the voting weight of each superpixel, and 2) by adopting a commonly used spatial pyramid scheme <ref type="bibr" target="#b13">[14]</ref>.</p><p>Spatial distance voting weight For a given superpixel x, its histogram feature is constructed by its intensity and geodesic distances to all other pixels in the same frame. To take the spatial location of these other superpixels into account, the geodesic distances are weighted by the spatial distance of those superpixels to x. In particular, the weighting of superpixel y to the histogram bins of superpixel x in frame f is defined by:</p><formula xml:id="formula_2">weight y = |y| |f | × exp(−µ × L 2 (x, y))<label>(2)</label></formula><p>where |·| is the area and L 2 (·) is the Euclidean distance between two superpixels' center locations. The area component normalizes the influence of superpixels of different sizes. The exponential ensures that nearby superpixels contribute more to the geodesic histogram of x. This is especially helpful for superpixels that belong to smaller segments, for which most other superpixels have large geodesic distances, that would dominate the histogram. Hence two small regions that are locally different would have very similar histograms. The parameter µ of the exponential controls the trade-off between global and local information.</p><p>Spatial pyramid histogram Inspired by the popularity of spatial pyramids <ref type="bibr" target="#b13">[14]</ref>, we incorporated the pyramid scheme into the construction of our feature histogram to encode more spatial information into the features. We implemented two scales of the spatial pyramid: 1x1 and 2x2 grids over a given frame. A histogram is extracted from each cell of the grid. Histograms from the same scale are concatenated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Our features are constructed from the intensity and boundary probability maps. For more robust boundary extraction, we also experiment with two different boundary map methods: spatial edge maps using structured forests <ref type="bibr" target="#b24">[25]</ref>, and motion boundary maps using the method proposed in <ref type="bibr" target="#b25">[26]</ref>.</p><p>Given the combined edge map and the superpixel graph, the geodesic distance feature for each superpixel is computed using Dijkstra's algorithm in O(|X||E|log|X|), with the cost of a path being the accumulated boundary scores between one superpixel to another.</p><p>We empirically set the intensity dimension of the feature histogram at 13 bins, and the geodesic dimension at 9 bins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe our experiments using the geodesic histogram features for video segmentation. We incorporated our features into two existing frameworks that are based on different clustering algorithms: spectral clustering <ref type="bibr" target="#b7">[8]</ref> and parametric graph partitioning <ref type="bibr" target="#b6">[7]</ref>. Spectral clustering performs dimensionality reduction on an affinity matrix based on eigenvalues, while parametric graph partitioning directly performs the clustering on the superpixel graph by modeling L p affinity matrices probabilistically. Also, the method in <ref type="bibr" target="#b7">[8]</ref> generates coarse-to-fine hierarchical segmentation results, while <ref type="bibr" target="#b6">[7]</ref> only outputs a single level of segmentation.</p><p>The experiments were conducted on the Segtrack V2 <ref type="bibr" target="#b5">[6]</ref> and Chen's Xiph.org <ref type="bibr" target="#b23">[24]</ref> datasets, covering a wide range of scenarios for evaluating video segmentation algorithms. We evaluate our segmentation results using the metrics proposed in <ref type="bibr" target="#b26">[27]</ref>, including 3D Accuracy (AC), 3D Under-segmentation Error (UE), 3D Boundary Recall (BR), and 3D Boundary Precision (BP). All experiments were conducted with the exact same set of initial superpixels and other parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Video Segmentation Using Spectral Clustering</head><p>We first evaluate the performance of the framework by adding our feature to spectral clustering <ref type="bibr" target="#b7">[8]</ref>. We use the same 6 features as <ref type="bibr" target="#b7">[8]</ref>: short term temporal, long term temporal, spatio temporal appearance, spatio temporal motion, across boundary appearance, and across boundary motion. The affinity matrix was computed by combining the 6 affinity matrices computed from each feature. We combined the original computed affinity matrix with the geodesic histogram features in order to preserve the algorithm settings and superpixel configurations. The similarity distances based on our features were computed using the χ 2 distance. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the evaluation results of spectral clustering with and without our feature on Segtrack v2 and Chen Xiph.org datasets. We tested four settings of our feature: (i) 2D histogram using only spatial edge maps to compute geodesic distances and without spatial distance voting weight (2D -0), (ii) 2D histogram using spatial edge maps and spatial distance voting weight with µ = 0.02 (2D -0.02 ), (iii) 2D histogram using both spatial edge and motion boundary maps with µ = 0.02 (2D + 0.02) and, (iv) 2D histograms with spatial pyramid (2D + 0.02 sp). Compared to the baseline, our feature significantly improved segmentation performance. The improvement was most significant in 3D accuracy: increased by 5% for Segtrack v2 and 10% for Chen Xiph.org. For Segtrack v2 dataset, our feature was able to improve the segmentation results on all four metrics. For Chen Xiph.org dataset, the feature gave a strong boost to 3D accuracy and 3D boundary precision. For all settings tested, we noticed that motion boundary maps did not affect performance much. Given that motion boundary map generation requires optic flow computation, which can be time consuming, its omission might result in faster implementations. The spatial distance voting weights had a strong impact on the results and clearly improved segmentation.   <ref type="bibr" target="#b7">[8]</ref> on the Segtrack v2 dataset, using four metrics: 3D Accuracy, 3D Under Segmentation Error, 3D Boundary Recall, and 3D Boundary Precision. For the 3D under-segmentation metric, the lower the error the better. For all the other metrics, the higher the score the better. -: using only spatial boundary edge. +: spatial boundary edge and motion boundary edge combined. 0: using spatial voting weight with µ = 0. 0.02: µ = 0.02. sp: with spatial pyramid. These plots show that the addition of our features result in major improvements on 3D Accuracy, and minor but consistent improvements on the three remaining metrics.</p><p>In addition to these improvements, <ref type="figure" target="#fig_6">Fig. 6</ref> shows that the average temporal length of supervoxels consistently increased for all parameter settings of our   <ref type="bibr" target="#b7">[8]</ref> on the Chen Xiph.org dataset, using four metrics: 3D Accuracy, 3D Under Segmentation Error, 3D Boundary Recall, and 3D Boundary Precision. For the 3D under-segmentation metric, the lower the error the better. For all the other metrics, the higher the score the better. -: using only spatial boundary edge. +: spatial boundary edge and motion boundary edge combined. 0: using spatial voting weight with µ = 0. 0.02: µ = 0.02. sp: with spatial pyramid. These plots show that the addition of our features result in major improvements on 3D Accuracy, and minor but consistent improvements on the three remaining metrics. feature by 10% for Segtrack v2 dataset and 5% for Chen Xiph.org dataset, showing that the segmentation results acquired better temporal consistency. Having both longer supervoxels and improved segmentation metrics indicate that our feature provides additional information for more reliable temporal consistency. This is significant, since connecting more corresponding superpixels temporally is a crucial and challenging part of the video segmentation task.</p><p>An interesting qualitative example is shown in <ref type="figure">Fig. 7</ref>, showing the segmentation results for video "soldier" with only two clusters. The second row visualizes the two clusters generated by <ref type="bibr" target="#b7">[8]</ref> using the 6 predefined features with only local information, only capturing the lower leg of the moving soldier. In contrast, the segmentation results improved with the addition of our geodesic feature. The global information that is encoded by our feature seems to have provided better information to the spectral clustering algorithm to segment the main object out of the background. Another qualitative example is shown in the 4th and 5th : Average temporal length of supervoxels generated by spectral clustering (SC) <ref type="bibr" target="#b7">[8]</ref> on the Segtrack v2 and Chen Xiph.org datasets. The results show significant improvements on the temporal consistency with the addition of our feature on Segtrack v2 dataset, and minor but consistent improvement on the Chen Xiph.org dataset. <ref type="figure">Fig. 7</ref>: The figure shows the segmentation results for the video "soldier" from the Segtrack v2 dataset using spectral clustering <ref type="bibr" target="#b7">[8]</ref> with and without our feature. We set the number of output clusters at 2 for this example. The top row shows the original frames with the ground truth highlighted in green. The second row shows the results of spectral clustering with 6 features, as originally proposed in <ref type="bibr" target="#b7">[8]</ref>. The third row shows the results of the algorithm when using the 6 original features plus our feature (2D histogram with spatial information). All other settings were set to be exactly the same. row of <ref type="figure">Fig. 1</ref>. The segment of the baseline shown in the 4th row shows some under-segmentation over the main moving object. This issue however, is less pronounced with our feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Video Segmentation Using Parametric Graph Partitioning.</head><p>Parametric Graph Partitioning (PGP) <ref type="bibr" target="#b6">[7]</ref> is a recent graph-based unsupervised method that generates a single level of video segmentation. The method models edge weights by a mixture of Weibull distributions, and requires that an L p -norm based similarity distance to be utilized. Therefore, we conduct experiments in this section using Earth Mover's Distance as in <ref type="bibr" target="#b6">[7]</ref>. The baseline is the setting <ref type="figure">Fig. 8</ref>: The segmentation results of PGP on video "garden" from the Chen Xiph.org dataset with and without our feature. The top row shows the original frames. The second row shows the segmentation results of PGP using the 4 features proposed in <ref type="bibr" target="#b6">[7]</ref>. The bottom row is the segmentation results of PGP using the 4 features plus our feature (2D histogram with spatial information). originally proposed in <ref type="bibr" target="#b6">[7]</ref> which uses four feature types: intensity, the hue of the HSV color space, the AB component of LAB color space, and gradient orientation. We did not use the motion feature since it did not contribute significantly toward PGP performance as suggested in the original paper.</p><p>Tables 1 and 2 report the quantitative evaluation of PGP with and without our feature on the two datasets. We evaluated the 1D histogram feature on the Chen Xiph.org dataset, shown in <ref type="table" target="#tab_1">Table 2</ref>. While PGP with the 1D feature outperforms the baseline in general, the benchmarks of 3 out of 8 videos decreased. On the other hand, the 2D feature significantly improved the segmentation performance of PGP. For the Segtrack v2 dataset, quantitative results in <ref type="table" target="#tab_0">Table  1</ref> show clear improvements of our feature for PGP, as well as the additional benefits from the spatial pyramid configuration.</p><p>Two example cases of PGP are shown in <ref type="figure">Fig. 8</ref>, and the 2nd and 3rd row of <ref type="figure">Fig. 1</ref>. For the over-segmented scenario in <ref type="figure">Fig.1</ref>, the water was unfavorably divided into many spurious segments by the PGP baseline. Adding our feature did not only help merging the background into one segment, but also enhanced temporal consistency and boundary awareness. Given the under-segmented baseline result on the lower part of the tree shown in <ref type="figure">Fig.8</ref>, our feature helped to segment the entire tree and also reduced over-segmentation in other parts of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature Extraction Running Time</head><p>All experiments were conducted on an Intel Core i7 CPU with 3.5 Ghz, and 16 Gb of memory. When adding our feature into the framework of <ref type="bibr" target="#b7">[8]</ref>, the average additional running time was increased by 67 seconds on a 85-frame video using the default parameter settings, which is a just small fraction of the total running time of several hours. The additional running time increase for the PGP framework was on average 48 seconds, with 300 initial superpixels per frame. These results show that the computational cost of our feature is low, and adds very little overhead to existing frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduced a novel feature for video segmentation based on geodesic distance histograms. The histogram is computed as a spatially-organized distribution of accumulated boundary costs between superpixels, which is a representation that includes more global information than conventional features. We validated the efficacy of our feature by adding it into two recent frameworks for video segmentation using spectral clustering and parametric graph partitioning, and showed that the proposed feature improved the performance of both frameworks in 3D video segmentation benchmarks, as well as the temporal consistency of the resulting supervoxels. We believe that the encoded global information can be further applied to other video related tasks such as moving object tracking, object proposals, and foreground background segmentation.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The figure shows an example of 1D (geodesic distances) and 2D (intensitygeodesic distances) histogram features. (a): frame 1 of video "soccer" from Chen's Xiph.org dataset [24], with soft boundary scores highlighted, and a superpixel-ofinterest marked in red. (b) and (c): the 1D and 2D histograms of the superpixelof-interest, and the frame regions (green) that correspond to the selected bins and cells of the 1D and 2D histograms, respectively. (b) shows that the bins of the 1D histogram contain mixed information, while the cells in (c) contain regions that are more semantically homogeneous.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>(a)"soccer"-fr49 (b)1D across-frame (c)2D across-frame The figures visualize the similarities between the superpixel-of-interest in Fig. 2(a) on a later frame (frame 49) to all other superpixels. Warmer color represents higher similarity. (a): original frame. (b): the similarity map based on 1D geodesic distance histograms. (c): the similarity map based on 2D intensity-geodesic distance histograms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Performance of spectral clustering (SC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Performance of spectral clustering (SC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6: Average temporal length of supervoxels generated by spectral clustering (SC) [8] on the Segtrack v2 and Chen Xiph.org datasets. The results show significant improvements on the temporal consistency with the addition of our feature on Segtrack v2 dataset, and minor but consistent improvement on the Chen Xiph.org dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluation on the Chen Xiph.org dataset. Best values are shown in bold. The table shows the evaluation results of the segmentation generated from the method proposed in<ref type="bibr" target="#b6">[7]</ref> with and without our feature in two configurations: 1D geodesic distance histogram and 2D intensity-geodesic distance histogram. All videos are initialized with 300 superpixels. 70.58 70.98 6.<ref type="bibr" target="#b21">22</ref> 10.31 5.75 80.22 81.64 82.46 37.64 38.60 38.98 Container fa 88.68 86.69 89.05 3.66 7.54 3.45 71.24 70.38 70.74 8.68 16.28 8.55 Garden fa 81.69 83.72 85.46 1.80 1.68 1.47 72.46 77.48 79.91 12.83 12.73 12.41 Ice fa 86.71 87.54 77.83 26.70 42.58 58.59 83.29 80.82 67.47 30.99 29.54 44.48 Paris fa 40.46 51.37 61.44 13.50 12.99 13.15 47.17 53.73 56.68 4.22 4.70 4.73 Soccer fa 85.79 83.95 87.04 4.84 5.46 2.74 31.37 30.47 43.35 5.51 5.20 5.49 Salesman fa 83.39 72.54 84.69 40.48 54.33 12.41 73.01 72.76 79.88 22.41 19.93 13.47 Stefan fa 83.56 81.57 90.14 6.76 19.80 4.87 80.66 74.62 83.30 10.98 15.16 11.04 Mean 77.62 77.25 80.83 12.99 19.34 12.80 67.43 67.74 70.47 16.66 17.77 17.40</figDesc><table><row><cell>Metrics</cell><cell cols="2">3D ACC</cell><cell></cell><cell></cell><cell>UE 3D</cell><cell></cell><cell></cell><cell>BR 3D</cell><cell></cell><cell></cell><cell>BP 3D</cell><cell></cell></row><row><cell>Methods</cell><cell>[7]</cell><cell>1D</cell><cell>2D</cell><cell>[7]</cell><cell>1D</cell><cell>2D</cell><cell>[7]</cell><cell>1D</cell><cell>2D</cell><cell>[7]</cell><cell>1D</cell><cell>2D</cell></row><row><cell>Bus fa</cell><cell>70.72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation on the SegTrack v2 dataset. Best values are shown in bold. The table shows the evaluation results of the segmentation generated from the algorithm proposed in<ref type="bibr" target="#b6">[7]</ref>, and two of our feature configurations: basic 2D histogram (2D) and 2D histogram with spatial information (2Dsp). The algorithms are all initialized with 300 superpixels per frame. .paradise 96.77 96.81 96.79 2.74 3.62 3.90 93.12 94.47 94.80 6.83 6.98 6.71 Birdfall 58.61 67.54 62.22 24.42 11.15 10.52 77.99 90.92 92.36 0.61 0.45 0.47 Bmx-1 94.60 94.50 94.56 5.49 6.44 5.40 98.31 98.32 98.58 4.05 4.66 4.23 Bmx-2 78.00 78.39 81.40 11.43 13.33 12.48 94.00 91.49 95.01 3.72 4.17 3.92 Cheetah-1 73.26 75.76 76.35 30.62 6.59 5.46 92.19 97.54 98.62 1.65 1.09 1.10 cheetah-2 63.84 73.68 69.38 34.64 6.95 8.73 97.85 98.54 98.66 2.19 1.38 1.38 Drift-1 93.85 93.20 93.34 3.77 3.29 3.42 92.70 94.54 94.53 1.22 1.20 1.22 Drift-2 92.43 92.41 92.06 3.31 2.98 2.96 90.52 92.53 92.13 0.94 0.93 0.94 Frog 56.92 64.72 86.67 16.32 14.01 11.60 59.28 76.14 83.26 10.42 3.84 2.25 Girl 87.71 89.18 89.18 10.76 10.18 10.27 90.18 94.59 94.68 5.46 5.39 5.32 Hum.bird-1 65.07 73.32 73.27 9.41 9.16 9.20 88.50 88.48 87.10 3.14 3.26 3.76 Hum.bird-2 77.71 84.95 85.52 6.35 7.04 9.06 94.64 94.26 94.58 5.00 5.18 6.09 Monkey 86.86 89.06 89.62 13.66 3.84 3.73 93.07 98.32 98.37 2.79 1.59 1.62 M.dog-1 88.09 88.70 88.97 9.50 9.30 9.38 95.74 97.44 98.50 1.40 1.42 1.42 M.dog-2 62.57 65.60 64.79 5.82 5.36 5.15 86.80 91.13 90.56 0.91 0.95 0.94 Parachute 92.54 92.31 92.31 19.54 18.29 5.65 95.24 95.71 97.34 1.27 1.13 0.76 Penguin-1 95.72 23.36 93.45 3.38 3.56 3.56 49.25 44.57 44.53 0.89 0.83 0.66 Penguin-2 95.51 95.77 95.79 3.39 3.28 3.28 73.19 71.41 74.78 1.38 1.39 1.17 Penguin-3 96.49 96.79 96.48 3.87 3.87 3.83 67.89 68.13 74.44 1.28 1.32 1.16 Penguin-4 95.72 94.50 94.74 3.87 3.95 3.92 73.54 73.82 73.44 1.16 1.21 0.96 Penguin-5 93.27 92.25 91.63 8.38 8.21 8.22 74.01 72.87 71.14 1.03 1.05 0.82 Penguin-6 92.37 92.64 93.09 3.73 4.02 4.03 62.33 63.52 59.50 1.02 1.08 0.81 Soldier 89.81 90.19 90.19 4.71 4.11 4.40 92.38 93.29 93.48 1.87 1.86 1.89 Worm 92.21 92.71 92.75 10.31 15.18 14.95 89.28 92.72 93.48 1.01 1.19 1.17 Average 84.16 83.26 86.86 10.39 7.40 6.80 84.25 86.45 87.24 2.55 2.23 2.12</figDesc><table><row><cell>Metrics</cell><cell></cell><cell>3D ACC</cell><cell></cell><cell>UE3D</cell><cell></cell><cell>BR3D</cell><cell></cell><cell>BP3D</cell></row><row><cell>Methods</cell><cell>[7]</cell><cell>2D 2Dsp</cell><cell>[7]</cell><cell>2D 2Dsp</cell><cell>[7]</cell><cell>2D 2Dsp</cell><cell>[7]</cell><cell>2D 2Dsp</cell></row><row><cell>B.o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. Partially supported by the Vietnam Education Foundation, NSF IIS-1161876, FRA DTFR5315C00011, the Stony Brook SensonCAT, the SubSample project from the DIGITEO Institute, France, and a gift from Adobe Corporation</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Motion Words for Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Taralova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="725" to="740" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Coarse-to-fine semantic video segmentation using supervoxel trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="1865" to="1872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint Semantic Segmentation and 3D Reconstruction from Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="703" to="718" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classifier based graph construction for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="951" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graphbased video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient video segmentation using parametric graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video Segmentation with Superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2012: 11th Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Daejeon, Korea; Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="760" to="774" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers, Part I</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geodesic Object Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="725" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A geodesic framework for fast interactive image and video segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3395" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geodesic graph cut for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3161" to="3168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deformation invariant image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1466" to="1473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting nonlocal spatiotemporal structure for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="741" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spectral graph reduction for efficient image and streaming video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spatio-temporal clustering of probabilistic region trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nobori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<editor>Metaxas, D.N., Quan, L., Sanfeliu, A., Gool, L.J.V.</editor>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="1738" to="1745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Track to the future: Spatiotemporal video segmentation with long-range motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical video representation with trajectory binary partition tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Palou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Salembier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision: Part V. ECCV&apos;10</title>
		<meeting>the 11th European Conference on Computer Vision: Part V. ECCV&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Propagating multi-class pixel labels throughout video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing Workshop (WNYIPW)</title>
		<meeting><address><addrLine>Western New York.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="14" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to detect motion boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evaluation of super-voxel methods for early video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1202" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
