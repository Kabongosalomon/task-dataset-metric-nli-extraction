<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Iris Segmentation and Localization Using Deep Multi-task Learning Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiyong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Ran He, Senior Member, IEEE</roleName><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">Joint Iris Segmentation and Localization Using Deep Multi-task Learning Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Iris segmentation</term>
					<term>iris localization</term>
					<term>attention mechanism</term>
					<term>multi-task learning</term>
					<term>iris recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Iris segmentation and localization in noncooperative environment is challenging due to illumination variations, long distances, moving subjects and limited user cooperation, etc. Traditional methods often suffer from poor performance when confronted with iris images captured in these conditions. Recent studies have shown that deep learning methods could achieve impressive performance on iris segmentation task [1]- <ref type="bibr" target="#b4">[5]</ref>. In addition, as iris is defined as an annular region between pupil and sclera, geometric constraints could be imposed to help locating the iris more accurately and improve the segmentation results. In this paper, we propose a deep multi-task learning framework, named as IrisParseNet, to exploit the inherent correlations between pupil, iris and sclera to boost up the performance of iris segmentation and localization in a unified model. In particular, IrisParseNet firstly applies a Fully Convolutional Encoder-Decoder Attention Network to simultaneously estimate pupil center, iris segmentation mask and iris inner/outer boundary. Then, an effective post-processing method is adopted for iris inner/outer circle localization. To train and evaluate the proposed method, we manually label three challenging iris datasets, namely CASIA-Iris-Distance, UBIRIS.v2, and MICHE-I, which cover various types of noises. Extensive experiments are conducted on these newly annotated datasets, and results show that our method outperforms stateof-the-art methods on various benchmarks. All the ground-truth annotations, annotation codes and evaluation protocols are publicly available at https://github.com/xiamenwcy/IrisParseNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I Ris recognition has been considered as one of the most stable, accurate and reliable biometric identification technologies <ref type="bibr" target="#b5">[6]</ref>, hence it is widely applied in various biometric applications including intelligent unlocking, border crossing control, security and crime screening, etc. A complete iris recognition system often consists of four sub-processes: iris image acquisition, iris preprocessing, feature extraction and matching. Both Iris segmentation and iris inner/outer circle localization (iris localization) are part of the iris preprocessing step <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr">The</ref>   The first column (a) shows iris images from three datasets (as described in Sec. IV-A) collected in different environments. The second column (b) illustrates the ground truths of pupil center, iris inner/outer boundary and iris segmentation mask, highlighted in yellow, red and aqua, respectively. The third column (c) shows the predicted pupil center (marked as red) and iris inner/outer boundary (highlighted in a color bar where the hotter color indicates the higher probability of a pixel belonging to the actual iris boundary). By utilizing the inherent correlation of pupil center, iris mask (highlighted in aqua in the column (d)) and iris inner/outer boundary, we further eliminate the noise of detected iris boundaries. As shown in the fourth column (d), with the help of refined iris boundaries and pupil center, we could extract coarse iris contours (highlighted in red) as the fitting points, then locate iris inner/outer circle (highlighted in blue) with the least-squares circle fitting algorithm <ref type="bibr" target="#b8">[9]</ref>. Best viewed in color.</p><p>As shown in <ref type="figure">Fig. 1 (a)</ref> and (b), iris refers to an annular region between pupil and sclera. Iris boundaries are approximately defined by two circles, i.e. an inner circle that divides pupil and iris (also called pupillary boundary), and an outer circle that separates iris and sclera (also called limbic boundary). Iris segmentation aims to isolate valid iris texture region from other components, such as pupil, sclera, eyelashes, eyelids, reflections, and occlusions in an eye image to obtain a binary mask, where valid iris pixels are classified as foreground and other pixels are regarded as background. Iris localization refers to estimating the parameters (center and radius) of iris inner and outer circular boundaries. After obtaining parameters of the iris region, normalization is carried out to get normalized image and mask, then followed by feature extraction and match operations to produce the final recognition result. As the beginning of iris recognition flow, accurate segmentation and localization has a great impact on subsequent processes <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Therefore, a segmentation and localization algorithm with high performance is the key to the success of the entire iris recognition system.</p><p>Earlier iris recognition systems require user cooperation and highly controlled imaging conditions, which restricts the applications of iris recognition technology. Hence, it is necessary to develop less constrained iris recognition systems. However, images captured in less constrained scenarios (e.g. long distances, moving subjects, using mobile devices, and limited user cooperation) are often of poor quality and introduce various kinds of noise, such as partial occlusions due to eyelids or glasses and blur caused by motion and defocus, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Over the past decades, a number of methods have been proposed for iris segmentation and localization, such as Hough transform <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, Active Contours <ref type="bibr" target="#b12">[13]</ref>, and GrowCut <ref type="bibr" target="#b13">[14]</ref>. However, these methods could not work well when dealing with degraded images. Compared with these traditional approaches, deep learning models, especially Convolutional Neural Networks (CNNs), have shown incomparable advantages in tasks such as image classification <ref type="bibr" target="#b14">[15]</ref> and object detection <ref type="bibr" target="#b15">[16]</ref>. To be specific, hierachical semantic representations of the input image could be automatically learned in an end-to-end manner without requiring extra human efforts. Since the rapid development of deep learning, a large amount of studies using CNNs have been proposed for iris segmentation <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>, iris bounding box detection <ref type="bibr" target="#b3">[4]</ref>, and pupil center detection <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. However, to the best of our knowledge, little research attention has been devoted to locating iris inner and outer boundaries based on deep learning technology. In addition, the geometric structure of iris, i.e. the pupil center is inside the inner boundary of the iris and the iris mask is located in between the inner and outer boundaries of the iris, could serve as priori constraints in designing iris segmentation and localization algorithms.</p><p>Based on these observations, we propose a deep multi-task learning framework for simultaneous pupil center detection, iris segmentation and iris inner/outer boundary detection, followed by an effective post-processing operation for iris localization, as shown in <ref type="figure">Fig. 1</ref> (c) and (d). Compared with single objective learning, joint learning of multi-modal eye structures makes the network learn more discriminative and essential features.</p><p>To train and evaluate the proposed model, we collect three challenging public iris datasets: CASIA-Iris-Distance <ref type="bibr" target="#b19">[20]</ref>, UBIRIS.v2 <ref type="bibr" target="#b20">[21]</ref> and MICHE-I <ref type="bibr" target="#b21">[22]</ref>. All these datasets contain segmentation annotations provided by other literatures. We also manually label pupil center and iris inner/outer boundary as additional ground truths for each iris image. These datasets contain various categories of noises such as blur, off-axis, occlusions and specular reflections, which could evaluate the robustness of the proposed method. To promote the research on iris preprocessing, we have made our manually annotated labels freely available to the community.</p><p>Main contributions of this paper are summarized as follows: 1) This paper introduces a novel multi-task framework which consists of two parts: the first part is a Fully Convolutional Encoder-Decoder Network equipped with attention modules which could learn more discriminative features for producing multiple probability maps. By optimizing focal loss <ref type="bibr" target="#b22">[23]</ref> and balanced sigmoid crossentropy loss <ref type="bibr" target="#b23">[24]</ref>, the model could alleviate the classimbalanced problem and converge quickly. The second part is an effective post-processing method including edge denoising, Viterbi-based coarse contours detection <ref type="bibr" target="#b24">[25]</ref> and least-squares circle fitting <ref type="bibr" target="#b8">[9]</ref> for iris localization. 2) We select three representative iris datasets and label the pupil center as well as inner/outer boundary for each iris image. Furthermore, we build comprehensive evaluation protocols for evaluating the performance of iris segmentation and localization algorithms.</p><p>3) The proposed method achieves state-of-the-art results on various iris benchmarks. Moreover, it has strong robustness and generalization ability, providing a good foundation for subsequent iris recognition processes. The paper is organized as follows. In Section II, we briefly review related work on iris segmentation and iris localization. Technical details of the proposed method are elaborated in Section III. Section IV introduces three databases and the annotation method that we adopt. Section V describes the evaluation protocols and analyzes experimental results. Finally, we conclude the paper and discuss future work in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>This section provides an overview of literatures on iris segmentation, semantic edge detection and iris localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Iris Segmentation</head><p>Over the past decades, a number of methods are proposed for iris segmentation. In general, these segmentation methods could be classified into two main categories: boundarybased methods and pixel-based methods <ref type="bibr" target="#b0">[1]</ref>. Boundary-based methods mainly locate pupillary, limbic and eyelid boundaries to isolate iris texture regions. On the contrary, pixel-based methods directly distinguish iris pixels from non-iris pixels according to the pixel-level appearance information.</p><p>For boundary-based methods, Daugman's integrodifferential operator <ref type="bibr" target="#b25">[26]</ref> and Wilde's circular Hough transforms <ref type="bibr" target="#b6">[7]</ref> are the two most well-known algorithms. The most critical and fundamental assumption these two methods made is that pupillary and limbic boundaries are circular contours. The integro-differential operator searches for the largest difference of intensity over the parameter space which normally corresponds to pupil and iris boundaries, while Hough transforms find optimal curve parameters by a voting procedure in a binary edge image. Although these methods have achieved good segmentation performance in iris images captured in controlled environments, they are time consuming and not suitable for degraded iris images. To overcome these problems, many noise removal <ref type="bibr" target="#b11">[12]</ref>, coarse iris location <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> and multiple models selection <ref type="bibr" target="#b28">[29]</ref> methods have been proposed to improve the robustness and efficiency of bounding-based iris segmentation methods. Besides, since the pupil and iris boundaries are not strictly circular, some works attempted to use geodesic active contours <ref type="bibr" target="#b29">[30]</ref> or elliptic contours <ref type="bibr" target="#b12">[13]</ref> to replace the circular assumption.</p><p>On the other hand, pixel-based methods exploit low-level visual information of individual pixel, such as intensity and color, to classify the pixels of interest from the background of the image. The most promising method in this category use commonly known pixel-level techniques, such as Graph Cut <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, to pre-process the image and traditional classification methods, such as SVMs <ref type="bibr" target="#b30">[31]</ref>, to classify the iris pixels from non-iris pixels.</p><p>Current boundary-based and pixel-based methods are designed mainly based on prior knowledge and require much preand post-processing effort. Deep learning models, especially Convolutional Neural Networks (CNNs), provide a powerful end-to-end solution to effectively solve these problems.</p><p>Semantic segmentation could be considered as a pixelwise image classification task, i.e. each pixel in the image is assigned an object class. In 2005, Long <ref type="bibr" target="#b31">[32]</ref> et. al. firstly proposed Fully Convolutional Network (FCN) for semantic segmentation. Afterwards, a number of semantic segmentation methods based on FCN have been proposed, such as DeepLab series <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b34">[35]</ref>, U-Net <ref type="bibr" target="#b35">[36]</ref>, and PSPNet <ref type="bibr" target="#b36">[37]</ref> to improve the performance of semantic segmentation. FCN-based methods take the whole image as input and produce a probability density map through a series of convolutional layers without involving fully connected layers. The whole model is endto-end, which does not require any manual processing, and could achieve state-of-the-art performances of the time. Iris segmentation could be regarded as a special binary semantic segmentation problem. Hence, many FCN-based segmentation methods could be directly applied on iris images, such as [1]- <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Inspired by the success of U-Net on binary semantic segmentation task <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref>, in this paper, we propose a Fully Convolutional Encoder-Decoder Attention Network for iris segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantic Edge Detection &amp; Iris Localization</head><p>Edge detection is a classical challenge in computer vision. Previous to the rapid development of deep learning, wellknown Sobel detctor and Canny detector <ref type="bibr" target="#b40">[41]</ref> etc are widely adopted. However, traditional methods are difficult to deal with semantic edges, i.e. edges which we are interseted in. Therefore, a lot of deep learning based methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b41">[42]</ref> are proposed to solve the semantic edge detection problem. Most of these methods adopt Fully Convolutional Networks (FCNs) and directly concatenate the features of different stages to extract semantic edges. In this paper, we mainly concentrate on iris inner/outer boundary detection using deep learning models.</p><p>Classical iris localization methods usually involve Daugman's integral differential operator <ref type="bibr" target="#b25">[26]</ref>, Wildes's circular Hough Transform <ref type="bibr" target="#b6">[7]</ref> and their variants, as described in Sec. II-A. The main idea of these methods is directly searching for the optimal parameters of inner and outer circular boundaries of iris in the parameter space. These methods are efficient but only suitable for iris images without severe distortions and noises. Different from these methods, edge detection based iris localization methods have demonstrated their superiorities on non-ideal iris images. In <ref type="bibr" target="#b27">[28]</ref>, the author adopted coarse-to-fine strategy to localize inner and outer boundaries of iris. Inner boundary is coarsely detected using an iterative search method by exploiting dynamic thresholds and multiple local cues, and outer boundary is first approximated in polar space using adaptive filters, then refined in the cartesian space. As a result, these two boundaries are robust against noises and distortions in iris images, which facilitates the subsequent circle fitting process. In <ref type="bibr" target="#b24">[25]</ref>, the Viterbi algorithm is applied on gradient maps of iris images to find coarse low-resolution contours which means selecting the least number of noisy gradients points as possible, then followed by least-squares circle fitting <ref type="bibr" target="#b8">[9]</ref> for iris localization. Experiment results indicate that the method is accurate and robust, and does not require refined parameter adaptation to various degradations encountered. In this paper, we adopt the method proposed in <ref type="bibr" target="#b24">[25]</ref> as the main body of our post-processing step, and use real iris boundaries extracted by deep learning models in replace of gradient maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TECHNICAL DETAILS</head><p>In this section, we firstly introduce the whole pipeline of our method. After that, we elaborate on the proposed multi-task network framework based on Fully Convolutional Network and attention mechanism, followed by an effective post-processing approach. Finally we describe our training objectives of the proposed model.   The pipeline of the proposed method is illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. IrisParseNet predicts probability maps of pupil center, iris segmentation mask and iris inner/outer boundary. Then, we further utilize the prior geometry relations of these elements to exclude mispredicted results, remove outliers and get the range of iris inner/outer circular boundary(i.e. circle center, minimum/maximum radius). Subsequently, Viterbi algorithm <ref type="bibr" target="#b24">[25]</ref> is used to extract coarse iris inner/outer contour. Finally iris inner/outer circle is localized by fitting on these coarse iris contours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pipeline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-task Network Framework</head><p>Recently, Fully Convolutional Networks (FCNs) have been widely applied in many tasks such as semantic segmentation <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b35">[36]</ref>, edge detection <ref type="bibr" target="#b41">[42]</ref> and salient object detection <ref type="bibr" target="#b42">[43]</ref>. FCNs are built only with locally connected layers, such as convolution, pooling and upsampling layers, and no dense layers such as fully connected layer are used. Hence, FCNs could take images of arbitrary size as input and produce corresponding-sized output, which is desired in spatially dense prediction tasks.</p><p>Accordingly, we propose a multi-task Fully Convolutional Encoder-Decoder Attention Network framework, shown in <ref type="figure">Fig. 4</ref>, which contains an Encoder path and a Decoder path. The Encoder path encodes feature maps of CNN models by convolution, ReLu, etc., to capture semantic information. The Decoder path decodes the feature maps to recover spatial information lost in the pooling layers by concatenation with feature maps of the Encoder path.</p><p>The Encoder path adopts VGG-16 <ref type="bibr" target="#b43">[44]</ref> as the encoding network. We remove the fully convolutional layers and the remaining network is used to learn hierarchical features. The whole encoding network could be divided into 5 stages and every stage is composed of a serial of convolutional layers, batch normalization layers, ReLU layers, and max-pooling layers which gradually reduce the size of feature maps. In lower stages, the feature maps contain more low-level spatial information such as edges but lack semantic information due to small receptive fields. In higher stages, bigger receptive fields extract more semantic information and embed it in the feature maps. In fact, many similar networks, such as ResNet <ref type="bibr" target="#b44">[45]</ref> and DenseNet <ref type="bibr" target="#b45">[46]</ref>, could also be used as the encoding network.</p><p>As described in <ref type="bibr" target="#b36">[37]</ref>, the size of receptive fields could roughly indicate how much the context information is taken into consideration. For dense prediction task, we need to consider both the local spatial features and global, non-local semantic features. Encouraged by the high performance of DeepLab <ref type="bibr" target="#b34">[35]</ref> and PSPNet <ref type="bibr" target="#b36">[37]</ref> on semantic segmentation task, we directly adopt atrous spatial pyramid pooling (ASPP) and Pyramid Pooling Module (PSP) for effectively extracting multi-scale receptive fields to reflect multi-scale context information, respectively.</p><p>In order to further focus on the most important information and suppress distracting noise, we apply attention mechanism to ASPP and PSP. Attention mechanism allows us to adjust the weights of different channels in feature maps and also re-estimates the spatial distribution of feature map according to the context [47]- <ref type="bibr" target="#b51">[51]</ref>. Hence, more discriminative features could be learned. Different from <ref type="bibr" target="#b49">[50]</ref>, we do not apply channel and spatial attention module sequentially, instead, 3D attention maps that integrating cross-channel and spatial information are directly computed.</p><p>After the attention module, we gradually up-sample the feature maps to recover the spatial information. Before upsampling, we need to fuse feature maps from two different layers: the Encoder layer at the same stage and the Decoder layer in the previous stage. The Decoder layer encodes rich context semantic information while the Encoder layer contains the detailed spatial information. The Decoder layer in the previous stage firstly applies two sequential convolutional layers with kernel size of 3×3, batch normalization layers and ReLU layers to further refine features and reduce the number of output channels to half of the number of channels of the Encoder layer at the same stage. Then we fuse the two features by element-wise concatenation.</p><p>After fusing the feature maps of the final stage, we apply a sequence of 3 × 3 convolutional layer, each followed by a batch normalization layer and a ReLu layer to summarize the final semantic feature. Then, a 1×1 convolutional layer with 4 filters and a per-pixel sigmoid function are adopted to generate probability maps of pupil center, iris segmentation mask, iris inner boundary and iris outer boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) ASPP Attention Module:</head><p>Atrous Spatial Pyramid Pooling (ASPP) is first proposed in DeepLab V2 <ref type="bibr" target="#b33">[34]</ref> which is inspired by the success of spatial pyramid pooling in image classification. In ASPP, dilated convolution (or atrous convolution) with different dilation rates is adopted to extract multi-scale contextual information while keeping the spatial resolution of feature maps unchanged. The original ASPP in DeepLab V2 contains four parallel dilated convolutions with increasing dilation rate, such as 6,12,18,24, on top of the last feature map of the model. In DeepLab V3 <ref type="bibr" target="#b34">[35]</ref>, ASPP is improved in three aspects: (1) batch normalization layer is included for scale adjustment; (2) 1×1 convolution is adopted to replace the degenerated dilated convolution with a higher dilation rate, such as 24; and (3) global average pooling is connected to the last feature maps of the model to capture the global contextual information. We will incorporate the improved ASPP with attention module to effectively extract important and discriminative features. The detailed structure of ASPP Attention module is illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>Given an intermediate feature map F as input, a pooling layer with kernel size 3 × 3 and stride 1 is used to get the same sized feature map P as the new input map. Then, five parallel modules are used, including one 1 × 1 convolution with 256 filters (as in Eq. (1)), three dilated convolution with 256 filters and dilation rate set to 6,12,18, respectively (as in Eq. (2)-Eq. (4)), and one global average pooling layer followed by one 1 × 1 convolution with 256 filters and a upsampling layer, mapping the feature map back to the desired dimension (as in Eq. <ref type="formula">(5)</ref>). It is worth noting that all the convolutional layers are followed by a batch normalization layer and a ReLu layer sequentially. These five modules could be mathematically  We extract multi-scale context features using multiple parallel filters with different dilation rates along with global average pooling. Afterwards, visual attention map is computed through one single convolution followed by a sigmoid function. Subsequently, the critical regions of input feature map are highlighted by element-wise dot production with obtained attention map. Finally, we concatenate the pooled input feature map before and after attention to get refined features. described as follows:</p><formula xml:id="formula_0">D 1 (P ) = ReLu(BN (Conv 1×1 (P ))) (1) D 2 (P ) = ReLu(BN (Conv 6 3×3 (P ))) (2) D 3 (P ) = ReLu(BN (Conv 12 3×3 (P ))) (3) D 4 (P ) = ReLu(BN (Conv 18 3×3 (P ))) (4) G(P ) = U p(ReLu(BN (Conv 1×1 (AvgP ool(P ))))) (5)</formula><p>The above feature maps are fused as:</p><formula xml:id="formula_1">H = D 1 (P ) ⊕ D 2 (P ) ⊕ D 3 (P ) ⊕ D 4 (P ) ⊕ G(P )<label>(6)</label></formula><p>where ⊕ represents channel-wise concatenation. Then, we apply one single 3 × 3 convolution to refine the fused feature maps and reduce the number of output channel to 512 to match with the input feature map F . The final 3D attention map M (F ) is produced by applying a per-pixel sigmoid operation to refined feature maps. As a result, values of attention map M (F ) are bounded in [0,1], where the bigger value indicates the higher importance.</p><p>To focus on the more discriminative features of input feature map, the final fusion operation is defined as:</p><formula xml:id="formula_2">F = P ⊕ (P ⊗ (M (F )))<label>(7)</label></formula><p>where ⊗ represents element-wise dot product operation.  <ref type="figure">Fig. 6</ref>. An illustration of PSP Attention Module. We extract both local and global context information by concatenating the input feature map with several sub-region representations of different scales. Then, an attention processing similar to the ASPP Attention module is applied to fused feature maps to get refined features.</p><p>2) PSP Attention Module: The Pyramid Pooling Module is proposed in PSPNet <ref type="bibr" target="#b36">[37]</ref> for semantic segmentation. The module fuses multiple features under different pyramid scales which could be controlled by varying bin sizes of pooling. By setting bin sizes to 1 × 1, 2 × 2, 3 × 3 and 6 × 6, an input feature map could be pooled to four different scales. To be concrete, the first pooling operation is actually global average pooling which captures the global contextual information, whereas the other three pooling operations divide the feature maps into different sub-regions and form multi-scale pooled representation for different localizations. Then, a 1 × 1 convolution (and batch normalization, ReLu) is applied to the global and local context representations to reduce the number of output channels to a quarter of the input feature map F . To further fuse with original input feature map, we must ensure that the pooled feature maps should have the same resolution as the input feature map. Hence, we upsample the pooled maps to be of the same size as the input feature map via bilinear interpolation. Finally, upsampled feature maps are concatenated with the original input feature map as the final pyramid pooling features H. After that, an attention processing similar to ASPP Attention module is applied. The detailed structure of PSP Attention module is illustrated in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Post-Processing</head><p>Probability maps of pupil center, iris segmentation mask and iris inner/outer boundary could be obtained by forwarding the iris image though the network. Then, we get coarse iris inner/outer contour by using Viterbi algorithm <ref type="bibr" target="#b24">[25]</ref> and further fit iris inner/outer circle by using least-squares circle fitting algorithm <ref type="bibr" target="#b8">[9]</ref>. Before searching the coarse contours, we remove the noise from predicted probability maps and get the range of iris inner/outer circular boundary by a serial of robust image processing operations.</p><p>1) Edge Denoising &amp; Boundary Range Estimation: Different from thin contours produced by traditional edge detection methods such as Canny detector <ref type="bibr" target="#b40">[41]</ref>, etc., deep learning based edge detector always produces thick, noisy and blurred edges which are not well aligned to actual image boundaries <ref type="bibr" target="#b23">[24]</ref>. To eliminate noisy edges, we utilize the prior geometric constraint of pupil center, iris segmentation mask and iris inner/outer boundary and adopt threshold segmentation, connected-component analysis and nearest neighbor search to do the job. To be specific, we locate the pupil center in the first place, as shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. Among the four outputs of the network, iris segmentation mask is the most accurate and max-area iris mask connected subregion has the highest confidence. For pupil center localization, the point with the highest score in the probability map of pupil center could be considered as a good initialization. However, there may be more than one candidate center point with high confidence score for some noisy iris images and the highest score could even be achieved by a noisy pixel. Therefore, we present a more robust alternative for pupil center localization. Considering the real pupil center point is adjacent to iris mask, the pupil center is located by searching the nearest pupil center subregion from the circumcircle center of max-area iris mask subregion. Before searching, the probability map of iris mask is segmented using global threshold (200-255) to get iris mask regions with higher confidence. In addition, the probability map of pupil center is segmented by using lower threshold (150-255) to get more candidate regions. After that, we compute connected components of pupil center and iris mask, and then perform nearest neighbor search. Once the nearest connected component of pupil center is found, we consider its geometric center as the estimated pupil center. Since iris center is approximately close to pupil center in most of the cases except for serious deformation, we simply initialize iris center using the coordinates of the pupil center. Afterwards, the range of iris inner/outer circular boundary is estimated. Although the majority of noisy edges are removed via applying threshold segmentation, some edges with highintensity still exist. According to the geometric relationship between iris mask and boundaries, regions where iris boundary is impossible to be located in are further eliminated. More specifically, an enclosing circle close to actual iris outer boundary is generated by taking estimated pupil center as its origin and the maximum distance between the origin and maxarea iris mask as radius. Then, for the iris outer boundary, noisy edges completely falling into the inside and outside of the enclosing circle are excluded. For iris inner boundary, those noisy edges completely falling into the outside of the enclosing circle are also excluded. Finally, we compute the minimum and maximum distances between the pupil/iris center and the refined iris inner/outer boundary. The detailed process is illustrated in <ref type="figure" target="#fig_6">Fig. 8</ref>.</p><p>2) Iris Inner/Outer Circle Localization: We modify the original Viterbi algorithm <ref type="bibr" target="#b24">[25]</ref> by replacing radial gradient maps with refined probability maps of iris inner and outer boundaries, as well as adopting the estimated range of iris inner/outer circular boundary to output coarse iris inner and outer contours. Then, least-squares circle fitting algorithm <ref type="bibr" target="#b8">[9]</ref> is applied on coarse contours to estimate the parameters of iris inner/outer circular boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training Objectives</head><p>We optimize all the outputs of IrisParseNet in an end-to-end manner simultaneously. More formally, given an input image X = {x j , j = 1, ..., |X|} of arbitrary size, we are interested in obtaining probability maps of pupil center, iris segmentation mask, iris inner boundary and iris outer boundary, each of the same size as X.</p><p>1) Pupil Center Detection: We denote P = {p j , j = 1, ..., |X|} as the predicted probability map of pupil center, in which p j ∈ [0, 1] indicates the probability of pixel x j being the pupil center, and index j samples every possible spatial location in the input image X.</p><p>The ground truth of pupil center, denoted byP = {p j , j = 1, ..., |X|}, is a binary image, where pixel valuep j being 1 suggests that the pixel p j belongs to the pupil region, otherwise is part of the background. Due to shortcomings of deep learning models for dense prediction task, the labeled ground truth of pupil center is not a single pixel but a set of pixels located in the neighborhood of the actual pupil center, see Sec. IV-B.</p><p>Due to the extreme imbalance of the number of positive and negative samples in the result of pupil center detection (most of the pixels are background), we use focal loss <ref type="bibr" target="#b22">[23]</ref> as the objective function to alleviate this problem. Focal loss introduces two hyper parameters, i.e. α and γ, to be tuned for better performance:</p><formula xml:id="formula_3">L pupil = l(P,P ) = j − α(1 −p j ) γ log(p j ) ,<label>(8)</label></formula><formula xml:id="formula_4">wherep j = p j ifp j = 1 1 − p j otherwise.<label>(9)</label></formula><p>2) Iris Segmentation: Since iris segmentation can be seen as a binary semantic segmentation task, we simply adopt a standard binary cross-entropy loss to supervise the training process. Let S = {s j , j = 1, ..., |X|} denote the predicted probability map of iris segmentation mask, where s j represents the probability of pixel x j locating in the iris area. The corresponding binary ground truth of iris segmentation mask is denoted asS = {s j , j = 1, ..., |X|}, wheres j is set to 1 if pixel s j is part of the iris region, otherwises j equals to 0. The cross-entropy loss for iris segmentation can be formulated as:</p><formula xml:id="formula_5">L seg = l(S,S) = j −s j log(s j ) − (1 −s j ) log(1 − s j ) ,<label>(10)</label></formula><p>3) Iris Inner/Outer Boundary Detection: Inspired by CASENet <ref type="bibr" target="#b41">[42]</ref>, we define iris inner/outer boundary detection as a two-class edge detection problem. To address the problem of positive/negative imbalancing in edge detection, we use the class-balanced cross-entropy loss function which is firstly introduced in HED <ref type="bibr" target="#b23">[24]</ref>. Suppose the probability maps of iris inner/outer boundary are denoted as {E 1 , E 2 }, in which E k = {e k j , j = 1, ..., |X|, k = 1, 2} and e k j represents the probability of pixel x j belonging to iris inner boundary (k = 1) or iris outer boundary (k = 2). We also manually label the inner and outer boundaries for each iris image, and the ground-truth boundaries are denoted as {Ē 1 ,Ē 2 }, whereĒ k = {ē k j , j = 1, ..., |X|, k = 1, 2} is a binary image indicating the distribution of iris boundaries. The classbalanced cross-entropy loss is formulated as:</p><formula xml:id="formula_6">L edge = l(E 1 , E 2 ;Ē 1 ,Ē 2 ) = k j − βē k j log(e k j ) − (1 − β)(1 −ē k j ) log(1 − e k j ) ,<label>(11)</label></formula><p>where β is the percentage of non-edge pixels in the iris image.</p><p>The overall loss function can be expressed as follow:</p><formula xml:id="formula_7">L h(X|W ), G = λ 1 L pupil + λ 2 L seg + λ 3 L edge = λ 1 l(P,P ) + λ 2 l(S,S) + λ 3 l(E 1 , E 2 ;Ē 1 ,Ē 2 )<label>(12)</label></formula><p>where {P, S, E 1 , E 2 } = h(X|W ) is the prediction from IrisParseNet, G = {P ,S,Ē 1 ,Ē 2 } is the corresponding ground truth. h(X|W ) is the model hypothesis taking image X as input, parameterized by W . We can obtain the optimal parameters by minimizing the overall loss function as follow:</p><formula xml:id="formula_8">(W ) * = argmin L.<label>(13)</label></formula><p>The hyper-parameters α, γ, λ 1 , λ 2 and λ 3 are set to 0.95, 2, 10, 1, 1 in our experiments, respectively .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASETS AND ANNOTATION METHODS</head><p>In this section, we present detailed descriptions of three challenging and popular datasets: CASIA-Iris-Distance <ref type="bibr" target="#b19">[20]</ref>, UBIRIS.v2 <ref type="bibr" target="#b20">[21]</ref> and MICHE-I <ref type="bibr" target="#b21">[22]</ref> and our annotation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>1) CASIA-Iris-Distance (CASIA) contains 2576 images from 142 subjects with resolution of 2352×1728 pixels. The images are captured by self-developed cameras and sample iris images are shown in the first column of <ref type="figure" target="#fig_7">Fig. 9 (a)</ref>. In this dataset, iris images are captured from a distance of more than 3 metres under near infrared illumination (NIR) and meanwhile the subject is moving. A subset which was manually labeled by the author <ref type="bibr" target="#b0">[1]</ref> is selected. This subset includes 400 iris images from the first 40 subjects and to speed up processing, images are resized to 640×480 pixels. We follow the same settings as in <ref type="bibr" target="#b0">[1]</ref> to select first 300 images from the first 30 subjects for training, and the last 100 images from the last 10 subjects are left for testing in the experiments. 2) UBIRIS.v2 (UBIRIS) consists of 11102 images from 261 subjects which are acquired under visible light illumination (VIS). Images in this dataset are captured on-the-move and at-a-distance with Canon EOS 5D camera and involve realistic noises, such as illumination variance, motion/defocus blur and occlusion of glasses and eyelids. In NICE. I competition, a subset of 1000 UBIRIS.v2 images was used. All images were resized to 400×300 pixels and their segmentation ground truths were manually annotated. According to the protocol of NICE.I competition, 500 images are selected for training and another disjoint testing set of 500 images are used for testing. However, the testing set provided by the organizers of the NICE.I competition has only 445 images. The first column of <ref type="figure" target="#fig_7">Fig. 9 (b)</ref> shows some examples of images in UBIRIS.v2. 3) MICHE-I (MICHE) dataset was created to evaluate and develop algorithms for colour iris images captured by mobile devices. Images in MICHE-I were captured by three mobile devices including iPhone5 (abbreviated IP5, 1262 images), Samsung Galaxy S4 (abbreviated GS4, 1297 images), and Samsung Galaxy Tab2 (abbreviated GT2, 632 images) in uncontrolled conditions with visible light illumination (VIS) and without the assistance of any operator <ref type="bibr" target="#b52">[52]</ref>. Following by <ref type="bibr" target="#b53">[53]</ref>, 140 images are selected for training and another 429 images are used for testing. Besides, we also use the manually labeled segmentation ground truths provided by <ref type="bibr" target="#b53">[53]</ref>. To speed up processing and preserve the aspect ratio, the width of all iris images is resized to 400 and height is resized to maintain the same proportions as the original image. Finally, the size of resized image is approximately 400 × 400. The first column of <ref type="figure" target="#fig_7">Fig. 9 (c)</ref> shows some examples of iris images in MICHE-I. The images from these adopted datasets were acquired under different types of less-constrained environments, thus various kinds of noises are taken into consideration. In addition, the imaging light source contains near infrared light and visible light. In summary, these datasets are representative in a variety of iris recognition applications, so it is convicing and reasonable to evaluate the performance of the proposed method using these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Annotation Methods</head><p>Training the proposed model requires ground truths of iris segmentation, iris inner/outer boundary and pupil center. Since the ground truth of iris segmentation has already been provided by other literatures, we only need to obtain annotations of the other three objects. In the whole labeling process, we use the interactive development environment (HDevelop) provided by the machine vision software, i.e. MVTec Halcon <ref type="bibr" target="#b54">[54]</ref>, which significantly facilitates our annotation work.</p><p>We firstly load iris images in a sequence, and then locate the iris inner and outer boundaries by positioning two ellipses close to explicit iris inner and outer boundaries as the ground truth. After that, the center of iris inner elliptical boundary is regarded as the pupil center.</p><p>The initially labeled ground-truth boundaries are too thin, with the width equals to one pixel, but the predicted boundaries from deep models are rather thick. The same problem also occurs in pupil center detection. To tackle this inconsistency, inspired by <ref type="bibr" target="#b55">[55]</ref>, ground-truth images of training set are dilated using morphologic dilation operator with a circular structuring element of radius 3.</p><p>Some examples of manually labeled ground truths can be seen in the second column of <ref type="figure" target="#fig_7">Fig. 9 (a), (b)</ref>, (c), which are sampled from CASIA-Iris-Distance, UBIRIS.v2 and MICHE-I iris datasets, respectively. We sought to accurately locate the iris inner and outer boundaries as well as eliminate all noise presentd to separate the actual iris pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND ANALYSIS</head><p>In this section, extensive experiments are conducted on three manually annotated datasets mentioned as in Sec. IV to evaluate the proposed model. The implementation details and data augmentation methods are firstly demonstrated, and then the evaluation protocols are described. Subsequently, the comparisons of our approach with state-of-the-art iris segmentation and localization methods are presented. Finally, we analyze the contribution of each individual module of the proposed model by ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We implement the proposed architecture based on the publicly available caffe <ref type="bibr" target="#b56">[56]</ref> framework and the whole network is initialized using the VGG-16 model <ref type="bibr" target="#b57">[57]</ref> pretrained on ImageNet. We train the network using mini-batch stochastic gradient descent (SGD) <ref type="bibr" target="#b14">[15]</ref> with batch size of 4, momentum of 0.9 and weight decay of 0.0005. Inspired by <ref type="bibr" target="#b32">[33]</ref>, we use the "poly" learning rate policy where the learning rate is multiplied by (1 − iter max iter ) power with power set to 0.9, initial learning rate set to 1e −3 and maximal iteration of 30000. All experiments are conducted on a NVIDIA TITAN Xp GPU with 12GB memory and an Intel(R) Core(TM) i7-6700 CPU.</p><p>Data augmentation is a simple yet effective way to enrich training data. During training, we augment training data with random combination of different geometric transformations (scaling, translation, flip, rotation, cropping) and image variations (blur) on-the-fly. Detailed augmentation operations are: (1) shuffle images (and gt maps) when reaching the end of an epoch; (2) randomly resize images (and gt maps) to 7 scales (0.5, 0.75, 1, 1.25, 1.5, 1.75, 2.0); (3) randomly blur images (mean filter, gaussian blur, median blur, bilateral filter, box blur); (4) randomly translate images (and gt maps) in x and y axis by a uniform factor between -30 and 30; (5) randomly left or right flip images (and gt maps); (6) randomly rotate images (and gt maps) by a uniform factor between -60 and 60; and (7) random crop images (and gt maps) to a fixed size (321 × 321) at last. For testing, we drop all augmentation operations and directly apply the model on the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Protocols</head><p>To quantitatively evaluate the proposed method, we introduce several evaluation protocols for iris segmentation, iris inner/outer circle localization and iris recognition. The details are described as follows:</p><p>1) Iris segmentation: The NICE. I competition <ref type="bibr" target="#b58">[58]</ref> provides two metrics to evaluate the accuracy of iris segmentation. The first measurement is the average segmentation error rate, which could be formulated as follows:</p><formula xml:id="formula_9">E1 = 1 n × c × r c r G(c , r ) ⊗ M (c , r ) (14)</formula><p>where n is the number of test images of r rows and c columns. In addition, G and M are the ground truth mask and the predicted iris mask, respectively, and c , r are the column and row coordinates of pixels in G and M. The operator ⊗ represents the XOR operation to evaluate the inconsistent pixels between G and M . The second error measure aims to compensate the disproportion between the apriori probabilities of "iris" and "non-iris" pixels in the images. To be specific, it averages the false positives (fp) and false negatives (fn) rates as follows:</p><formula xml:id="formula_10">E2 = 1 2 × n i (f p + f n)<label>(15)</label></formula><p>where n is the number of testing images. We also report the following F-Measure (F1) (the harmonic mean of precision and recall) <ref type="bibr" target="#b59">[59]</ref> and mean Intersection over Union (mIOU) to provide a comprehensive analysis of the propose method.</p><p>The values of E1 and E2 are bounded in [0, 1], where the smaller value indicates the better result. Values of F1 and mIOU also fall in the same interval, but the greater value suggests the higher performance in these cases. 2) Iris inner/outer circle localization: Inspired by <ref type="bibr" target="#b60">[60]</ref>, we compute the Hausdorff distance between detected iris inner/outer circle (denoted as D) and labeled iris inner/outer boundary (denoted as G) to measure the shape similarity, which could be defined as:</p><formula xml:id="formula_11">H(G, D) = max{sup x∈G inf y∈D x−y , sup y∈D inf x∈G x−y }<label>(16)</label></formula><p>Smaller Hausdorff distances correspond to higher shape similarity between detected circles and ground truths, suggesting higher detection accuracy. We report the mean Hausdorff distance (mHdis) for iris inner circle and outer circle to evaluate the performance of localization. The average value of the two mean Hausdorff distances demonstrates the overall accuracy of iris localization, thus we include it in the evaluation protocol.</p><p>Besides, inspired by <ref type="bibr" target="#b61">[61]</ref>, we also report the detection rate with respect to an error threshold given by the Hausdorff distance between detected iris inner/outer circle and ground truths.</p><p>3) Iris recognition: To verify that our iris segmentation and localization framework is able to improve the performance of iris recognition, we conduct iris recognition experiments with all components but iris segmentation and localization methods fixed. We use the equal error rate (EER) and Daugman's decidability index (DI) <ref type="bibr" target="#b25">[26]</ref> to quantitatively evaluate the performance of iris recognition. Higher DI values correspond to better discriminative ability of iris recognition systems, meanwhile the iris recognition system with the lowest EER is considered the most accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Method Comparison 1) Benchmarks:</head><p>We select four representative iris segmentation and localization approaches, including both traditional methods and deep learning based methods, as the benchmark. In particular, T. Tan et. al. <ref type="bibr" target="#b62">[62]</ref> proposed an efficient and robust segmentation method to deal with noisy iris images and it could be roughly divided into four processes: clustering based coarse iris localization, pupillary and limbic boundary localization based on a novel integrodifferential constellation, eyelid localization and eyelash/shadow detection. The method was ranked the first place in NICE.I competition <ref type="bibr" target="#b58">[58]</ref>. Since there is no source code available, we only report the result presented in the paper.</p><p>RTV-L 1 <ref type="bibr" target="#b11">[12]</ref> proposed a novel total-variation based segmentation framework which used l 1 norm regularization to robustly suppress noisy texture pixels to obtain clear iris images. Then, an improved circular Hough transform was used to detect iris and pupil circles on noise-free iris images. Finally, the authors developed a series of robust post-processing operations to locate iris boundaries more accurately. We apply the method on above mentioned three datasets using the source code provided by the authors * .</p><p>Haindl and Krupička <ref type="bibr" target="#b26">[27]</ref> proposed an unsupervised segmentation method for colored eye images obtained through mobile devices. The method was ranked first in the Mobile Iris Challenge Evaluation (MICHE)-I <ref type="bibr" target="#b63">[63]</ref> and also outperformed the NICE.I competition winning algorithm, namely T. Tan et. al. <ref type="bibr" target="#b62">[62]</ref>, with average segmentation error rate E1 of 1.24% on UBIRIS.v2 dataset. We directly use the executable program † provided by the authors to test on UBIRIS.v2 and MICHE-I datasets except CASIA-Iris-Distance, as images in CASIA-Iris-Distance are not captured under visible lights.</p><p>Besides, MFCNs <ref type="bibr" target="#b0">[1]</ref> was the first method that applied fully convolutional network for iris segmentation and achieved better results than previous state-of-the-art methods on CASIA-Iris-Distance and UBIRIS.v2 datasets. We reproduce the method and apply it to our labeled three datasets.</p><p>Note that except for RTV-L 1 , other baseline methods only provide the comparison of iris segmentation mask due to lack of the outputs of iris inner and outer circles.  <ref type="figure" target="#fig_9">Fig. 10, Fig. 11</ref> provide summaries of the performance comparison of the proposed method with baseline approaches on iris segmentation and iris inner/outer circle localization under the proposed evaluation protocols. We also report the storage space of the model and runtime in order to further evaluate the practicability of the proposed method.</p><p>As can be seen from Tab. I, IrisParseNet outperforms other approaches on the task of iris segmentation. Especially, IrisParseNet achieves average segmentation error rates of 0.40%, 0.84%, 0.81% on CASIA-Iris-Distance, UBIRIS.v2 and MICHE-I, respectively. Hence, our method ranks first according to the NICE. I competition protocol(E1). Besides, IrisParseNet (including ASPP-type and PSP-type) also achieves better results in terms of mean value (greater than 91%) and standard deviation (less than 10%) on F1 metric than other approaches, demonstrating that our approach is highly accurate and robust. The same superiority is also observed on E2 and mIOU (approximately 85%) metrics. The parameters of RTV-L 1 are optimized for each dataset, which makes RTV-L 1 consistently achieve the good segmentation results on three iris datasets. It is worth noting that the performance of Haindl and Krupička <ref type="bibr" target="#b26">[27]</ref> is not promising, which is inconsistent with the description in their paper. Although we directly use the execute program provided by the authors when conducting the experiments, we are not able to achieve average segmentation error rates of 1.24% as described in the original paper for UBIRIS.v2, instead a much higher error rate (3.24%) is obtained.</p><p>From Tab. II, we could see that for the task of iris inner/outer circle localization, IrisParseNet consistently outperforms RTV-L 1 on all three datasets under mean Hausdorff distance. Besides, It could be seen from <ref type="figure" target="#fig_9">Fig. 10</ref> and <ref type="figure" target="#fig_10">Fig. 11</ref> that our method performs comparably to or better than RTV-L 1 across the majority of threshold range on all three datasets.</p><p>In terms of two types of attention module, IrisParseNet (ASPP) achieves better results on the task of iris segmentation, but IrisParseNet (PSP) shows higher performance on the task of iris inner/outer circle localization.</p><p>As for the runtime, the proposed method takes approximate 0.3s, 0.1s, 0.1s for the forward propagation of the network, and 0.4s, 0.4s, 0.4s for post-processing on CASIA-Iris-Distance, UBIRIS.v2 and MICHE-I, respectively. Compared with traditional approaches, IrisParseNet is more time-efficient (In GPU time), as the overall runtime is less than 0.7s. Closer observation would reveal that the post-processing step is the most time-comsuming operation, and the runtime of the framework is directly proportional to resolution of input images.</p><p>Although our method achieves good segmentation and localization performance, it consumes relative large storage space (approximately 100MB), that limits its application on mobile platforms. To solve this problem, methods such as parameter pruning and sharing, low-rank factorization, knowledge distillation <ref type="bibr" target="#b64">[64]</ref>, etc., could be adopted to compress the model and further accelerate the training process.</p><p>In summary, the proposed IrisParseNet framework demonstrates noticeable superiority over other methods in accuracy, robustness and usability for the task of iris preprocessing.   <ref type="bibr" target="#b0">1</ref> The overall runtime is the sum of the runtime of iris segmentation and iris inner/outer circle localization.  3) Evaluation of Iris Recognition: To perform iris recognition (more accurately, iris verification) experiments, we use the full set iris images of CASIA-Iris-Distance, UBIRIS.v2 and MICHE-I datasets. To speed up processing, for CASIA-Iris-Distance and MICHE-I datasets, we use classical Viola-Jones eye detector <ref type="bibr" target="#b65">[65]</ref> provided by OpenCV to extract the eye region in images, and all eye regions are resized to 400 × 400. Iris images in UBIRIS.v2 are already scaled to 400 × 300. We use single eye in iris recognition experiments and detailed settings of the experiments are provided in Tab. III.</p><p>The proposed IrisParseNet framework is firstly applied for iris segmentation and localization, then Daugman's rubber sheet normalization method <ref type="bibr" target="#b7">[8]</ref> is used to produce normalized iris image and iris mask for feature extraction and matching. We adopt the 1-D log Gabor filter to extract iris codes and compute Hamming Distance of iris codes to verify whether two iris are from the same class ‡ . The same normalization, feature extraction and matching processes are also adopted in experiments with RTV-L 1 <ref type="bibr" target="#b11">[12]</ref> and Haindl and Krupička <ref type="bibr" target="#b26">[27]</ref>.</p><p>Evaluation results of iris recognition are shown in Tab. IV. From Tab. IV, we could see that experiments using the proposed IrisParseNet framework achieve lower EER and higher DI than those using other methods, especially for CASIA-Iris-Distance, UBIRIS.v2, MICHE-I:iPhone5 and MICHE-I:SamsungGalaxyS4. Experiment results illustrate that our IrisParseNet method greatly improves the performance of iris recognition.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We further explore the contribution of each individual module of the proposed model by conducting ablation study.</p><p>1) Effectiveness of Attention Mechanism: To verify the effectiveness of the attention module, we replace it with two sequential convolutional layers with 256 filters and 512 filters (along with batch normalization layer and ReLU layer). Experiment results are shown in Tab. V, <ref type="figure" target="#fig_1">Fig. 12, and Fig. 13</ref>. From Tab. V, we could see that compared with the original IrisParseNet framework, its variants without attention module suffer from significant performance drop on the task of iris segmentation for all datasets. As for the task of iris localization, removing attention module would result in a significant performance decrease on UBIRIS.v2 and MICHE-I, as shown in <ref type="figure" target="#fig_1">Fig. 12 and Fig. 13</ref>. Hausdorff Distance 2) Effectiveness of Joint Segmentation and Localization: To evaluate the contribution of joint segmentation and localization, we compare three IrisParseNet framework variants:</p><p>original IrisParseNet (ASPP), IrisParseNet only with localization part or segmentation part, as shown in Tab. VI, <ref type="figure" target="#fig_4">Fig. 14  and Fig. 15</ref>, respectively. Experiment results show that joint learning of iris segmentation and localization helps to improve the performance on both iris segmentation and iris localization tasks. Hausdorff Distance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we propose a novel deep multi-task learning framework for joint iris segmentation and localization. In this framework, a Fully Convolutional Encoder-Decoder Attention Network and an effective post-processing operation which exploit the priori geometric constraints of pupil, iris and sclera, are proposed to improve the performance of iris segmentation and localization. Meanwhile, we have collected manual labels of three challenging iris datasets and established comprehensive evaluation protocols, which are publicly available. The proposed method is compared with state-of-the-art methods on the three annotated iris datasets, and shows a leading performance. As for future work, we would explore improving the efficiency of the post-processing step or integrate it into the iris segmentation and localization system to form an endto-end model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. The first column (a) shows iris images from three datasets (as described in Sec. IV-A) collected in different environments. The second column (b) illustrates the ground truths of pupil center, iris inner/outer boundary and iris segmentation mask, highlighted in yellow, red and aqua, respectively. The third column (c) shows the predicted pupil center (marked as red) and iris inner/outer boundary (highlighted in a color bar where the hotter color indicates the higher probability of a pixel belonging to the actual iris boundary). By utilizing the inherent correlation of pupil center, iris mask (highlighted in aqua in the column (d)) and iris inner/outer boundary, we further eliminate the noise of detected iris boundaries. As shown in the fourth column (d), with the help of refined iris boundaries and pupil center, we could extract coarse iris contours (highlighted in red) as the fitting points, then locate iris inner/outer circle (highlighted in blue) with the least-squares circle fitting algorithm [9]. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Examples of degraded iris images with different types of noises. (a) gaze deviation; (b) rotation images; (c) absence of iris; (d) defocus blur; (e) specular reflections; (f) iris occlusions due to glasses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>IrisParseNet</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The pipeline of proposed method: network output and post-processing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>An illustration of ASPP Attention Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Overview of pupil center localization. (a) and (c): threshold segmentation and connected-component analysis; (b): get the circumcircle of max-area mask subregion; (d): nearest neighbor search; (e): get actual pupil center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Edge denoising &amp; boundary range estimation. (a) and (b): threshold segmentation; (c) and (d): generate target region; (e) and (f): edge denoising; (h) and (i): boundary range estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Example images and corresponding ground truths (including iris center(chartreuse), iris inner boundary(magenta), iris outer boundary(red), iris segmentation mask(aqua)) of three iris datesets. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>*</head><label></label><figDesc>The implementation is made available via https://www4.comp.polyu.edu. hk/ ∼ csajaykr/tvmiris.htm † The executable program is made available via http://biplab.unisa.it/ MICHE/MICHE-II/PRL Haindl Krupicka.zip 2) Evaluation of Iris Segmentation and Localization: Tab. I and Tab. II,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Performance comparison of iris inner circle localization against RTV-L 1<ref type="bibr" target="#b11">[12]</ref> on the labeled three iris datasets. Success rate is thresholded on the Hausdorff distance error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>) IrisParseNet(PSP) (a) CASIA-Iris-Distance (b) UBIRIS.v2 (c) MICHE-I Performance comparison of iris outer circle localization against RTV-L 1 [12] on the labeled three iris datasets. Success rate is thresholded on the Hausdorff distance error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Performance comparison of iris outer circle localization with/without attention module on the labeled three iris datasets. Success rate is thresholded on the Hausdorff distance error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Performance comparison of iris inner circle localization with/without joint learning on the labeled three iris datasets. Success rate is thresholded on the Hausdorff distance error. Performance comparison of iris outer circle localization with/without joint learning on the labeled three iris datasets. Success rate is thresholded on the Hausdorff distance error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>supplemental material is made available via https://drive.google. com/open?id=1Fo3Nmv ha5-d5jC2vcbAtbjMyJ aa fL Caiyong Wang is with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China, and also with the Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China. E-mail: wangcaiyong2017@ia.ac.cn. Yuhao Zhu, Yunfan Liu, Ran He and Zhenan Sun(Corresponding author) are with the Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China. E-mail: {yuhao.zhu, yunfan.liu}@cripac.ia.ac.cn, {rhe, znsun}@nlpr.ia.ac.cn.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Concate Element-wise Dot Product ASPP Attention Block Max-pooling Conv + Batch Norm + ReLu Attention Module Conv + Sigmoid Upsampling Conv + Batch Norm +ReLu+Upsample Conv 3x3 S Sigmoid Concate Element-wise Dot Product Conv 1x1 Conv 1x1 Conv 1x1 Conv 1x1 UpsampleGlobal Average Pooling Concate Conv 3x3 S Sigmoid Pooling 3x3, Stride=1 Concate Element-wise Dot Product ASPP Attention Module Max-pooling Conv + Batch Norm + ReLu Attention Block Conv + Sigmoid Upsampling Conv + Batch Norm +ReLu+Upsample Conv 3x3 S Sigmoid Concate Element-wise Dot Product Conv 1x1 Conv 1x1 Conv 1x1 Conv 1x1</head><label></label><figDesc>Overview of Multi-task Attention Network Architecture. Best viewed in color.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pupil Center</cell></row><row><cell>64 Fig. 4. 64</cell><cell>128 128</cell><cell>256 256</cell><cell cols="2">512 512</cell><cell>512 512</cell><cell>1024 1024</cell><cell>128 64 128 64 512 256 256 512 256128 128 256 512 256 256 512 256128 128 256</cell><cell>64 32 64 32 64 128 64 128</cell><cell>32 64 32 4 32 64 32 4</cell><cell>Mask Inner Boundary Outer Boundary</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSP Attention Block</cell></row><row><cell cols="2">(a) Atrous Spatial</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pyramid Pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1x1 Conv (a) Atrous Spatial</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pyramid Pooling 3x3 Conv rate = 6 1x1 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pooling 3x3, Stride=1</cell><cell>3x3 Conv rate = 12 rate = 6 3x3 Conv</cell><cell></cell><cell>Concate</cell><cell>Conv 3x3</cell><cell></cell><cell>Sigmoid S</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>rate = 12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>rate = 18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>rate = 18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(b) Global Average Pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Upsample PSP Attention Block</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Global Average Pooling Concate Conv 3x3 S Sigmoid Pooling 3x3, Stride=1 Concate Element-wise Dot Product ASPP Attention Block Max-pooling Conv + Batch Norm + ReLu Attention Block Conv + Sigmoid Upsampling Conv + Batch Norm +ReLu+Upsample Conv 3x3 S Sigmoid Concate Element-wise Dot Product Conv 1x1 Conv 1x1 Conv 1x1 Conv 1x1 Upsample</head><label></label><figDesc>The above design makes fused feature maps focus only on the most important parts of an input signal. At the same time, the original input is also concatenated to the fused ones to keep other valuable information in the original input signal.</figDesc><table><row><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>512</cell><cell>1024</cell><cell>512 256 256 512 256128 128 256</cell><cell>128 64</cell><cell>64 128</cell><cell>64 32</cell><cell>32 64 32 4</cell></row><row><cell cols="2">(a) Atrous Spatial</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pyramid Pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1x1 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>rate = 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>rate = 12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>rate = 18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>PSP Attention Module</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF DIFFERENT APPROACHES ON THE TASK OF IRIS SEGMENTATION USING THE PROPOSED PROTOCOLS.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>E1 (%)</cell><cell>E2 (%)</cell><cell>F1 µ(%) σ(%)</cell><cell>mIOU (%)</cell><cell>Average Runtime(s)</cell></row><row><cell cols="5">T. Tan et. al. [62] UBIRIS 1.31 N/A N/A N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell></cell><cell cols="5">CASIA 0.68 0.44 87.55 4.58 78.11</cell><cell>2.46</cell></row><row><cell>RTV-L 1 [12]</cell><cell cols="5">UBIRIS 1.21 0.83 85.97 8.72 74.01</cell><cell>1.07</cell></row><row><cell></cell><cell cols="5">MICHE 2.27 1.13 77.10 14.71 64.21</cell><cell>1.58</cell></row><row><cell>Haindl and</cell><cell cols="5">UBIRIS 3.24 1.62 77.03 20.67 65.08</cell><cell>14.33</cell></row><row><cell>Krupička [27]</cell><cell cols="5">MICHE 5.08 2.54 62.19 25.28 49.79</cell><cell>21.94</cell></row><row><cell></cell><cell cols="5">CASIA 0.50 0.25 93.14 2.97 87.30</cell><cell>0.47 †</cell></row><row><cell>MFCNs [1]</cell><cell cols="5">UBIRIS 0.92 0.46 90.78 4.70 81.92</cell><cell>0.32 †</cell></row><row><cell></cell><cell cols="5">MICHE 0.96 0.48 88.70 8.98 80.63</cell><cell>0.38 †</cell></row><row><cell>IrisParseNet (ASPP)</cell><cell cols="5">CASIA 0.40 0.20 94.30 3.70 89.40 UBIRIS 0.84 0.42 91.82 4.26 85.39 MICHE 0.82 0.41 91.33 8.04 84.79</cell><cell>0.25 † 0.11 † 0.13 †</cell></row><row><cell>IrisParseNet (PSP)</cell><cell cols="5">CASIA 0.41 0.21 94.20 3.16 89.19 UBIRIS 0.85 0.42 91.63 4.06 85.07 MICHE 0.81 0.41 91.50 8.01 85.07</cell><cell>0.30 † 0.11 † 0.13 †</cell></row><row><cell>† GPU time.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF DIFFERENT APPROACHES ON THE TASK OF IRIS INNER/OUTER CIRCLE LOCALIZATION USING THE PROPOSED PROTOCOLS.</figDesc><table><row><cell cols="2">Method Dataset</cell><cell>mHdis of Iris Inner Circle</cell><cell>mHdis of Iris Outer Circle</cell><cell>Overall mHdis</cell><cell>Average Runtime (s)</cell><cell>Overall Runtime (s) 1</cell></row><row><cell></cell><cell>CASIA</cell><cell>4.24</cell><cell>7.74</cell><cell>6.08</cell><cell>N/A</cell><cell>2.46</cell></row><row><cell>RTV-L 1 [12]</cell><cell>UBIRIS</cell><cell>8.48</cell><cell>11.72</cell><cell>10.10</cell><cell>N/A</cell><cell>1.07</cell></row><row><cell></cell><cell cols="2">MICHE 11.96</cell><cell>15.49</cell><cell>13.73</cell><cell>N/A</cell><cell>1.58</cell></row><row><cell>IrisParseNet (ASPP)</cell><cell>CASIA UBIRIS MICHE</cell><cell>4.13 6.06 5.67</cell><cell>7.80 6.48 7.33</cell><cell>5.96 6.27 6.50</cell><cell>0.42 † 0.37 † 0.41 †</cell><cell>0.67 ‡ 0.49 ‡ 0.54 ‡</cell></row><row><cell>IrisParseNet (PSP)</cell><cell>CASIA UBIRIS MICHE</cell><cell>4.04 5.99 5.41</cell><cell>7.24 6.61 7.60</cell><cell>5.64 6.30 6.50</cell><cell>0.38 † 0.32 † 0.38 †</cell><cell>0.68 ‡ 0.43 ‡ 0.51 ‡</cell></row><row><cell>† GPU time.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>‡ GPU time + CPU time.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III DETAILED</head><label>III</label><figDesc>SETTINGS OF IRIS RECOGNITION EXPERIMENT.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MICHE</cell><cell></cell></row><row><cell>Dataset</cell><cell>CASIA</cell><cell>UBIRIS</cell><cell cols="3">IP5 GS4 GT2</cell></row><row><cell>No. of subjects</cell><cell>119</cell><cell>259</cell><cell>75</cell><cell>75</cell><cell>75</cell></row><row><cell>No. of classes</cell><cell>238</cell><cell>518</cell><cell cols="2">150 150</cell><cell>150</cell></row><row><cell>No. of images</cell><cell>2280</cell><cell>11100</cell><cell cols="2">995 764</cell><cell>438</cell></row><row><cell>Resolution</cell><cell cols="2">400 × 400 400 × 300</cell><cell></cell><cell cols="2">400 × 400</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF DIFFERENT APPROACHES ON THE TASK OF IRIS RECOGNITION USING THE PROPOSED PROTOCOLS.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>EER</cell><cell>DI</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF IRISPARSENET WITH/WITHOUT ATTENTION MODULE.Fig. 12. Performance comparison of iris inner circle localization with/without attention module on the labeled three iris datasets. Success rate is thresholded on the Hausdorff distance error.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Dataset</cell><cell></cell><cell cols="3">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">E1 (%)</cell><cell cols="2">E2 (%)</cell><cell cols="3">mean F1 (%)</cell><cell>mIOU (%)</cell><cell></cell><cell cols="2">Overall mHdis of Iris Localization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ASPP-type</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">0.40 0.20 94.30 89.40</cell><cell></cell><cell></cell><cell>5.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell></cell><cell cols="3">0.9</cell><cell></cell><cell cols="3">CASIA</cell><cell></cell><cell cols="3">PSP-type</cell><cell cols="2">0.9</cell><cell></cell><cell></cell><cell cols="8">0.41 0.21 94.20 89.19</cell><cell cols="2">0.9</cell><cell>5.64</cell></row><row><cell></cell><cell></cell><cell cols="3">0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">without Attention 0.43 0.21 94.10 89.00 0.8</cell><cell cols="2">0.8</cell><cell>5.76</cell></row><row><cell></cell><cell></cell><cell cols="3">0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.7</cell></row><row><cell cols="2">Detection Rate</cell><cell cols="3">0.3 0.4 0.5 0.6</cell><cell></cell><cell cols="3">UBIRIS</cell><cell cols="16">ASPP-type PSP-type without Attention 0.94 0.47 90.87 83.49 0.84 0.42 91.82 85.39 0.85 0.42 91.63 85.07 0.3 0.4 0.5 0.6 Detection Rate Detection Rate</cell><cell cols="2">0.3 0.4 0.5 0.6</cell><cell>6.27 6.30 7.34</cell></row><row><cell></cell><cell></cell><cell cols="3">0.2 0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">ASPP-type IrisParseNet(ASPP) IrisParseNet(PSP)</cell><cell cols="2">0.2 0.1</cell><cell></cell><cell></cell><cell cols="8">0.82 0.41 91.33 84.79 IrisParseNet(ASPP) IrisParseNet(PSP)</cell><cell cols="2">0.2 0.1</cell><cell>IrisParseNet(ASPP) 6.50 IrisParseNet(PSP)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell cols="3">MICHE 2 4 6</cell><cell cols="4">PSP-type 10 12 14 15 IrisParseNet(without Attention) 8</cell><cell></cell><cell>0</cell><cell cols="2">0</cell><cell cols="8">0.81 0.41 91.50 85.07 2 4 6 8 10 12 14 15 IrisParseNet(without Attention)</cell><cell></cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6 6.50 8 IrisParseNet(without Attention) 10 12 14 15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Hausdorff Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Hausdorff Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hausdorff Distance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">without Attention 0.87 0.44 90.46 83.12</cell><cell></cell><cell></cell><cell>8.07</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.8</cell></row><row><cell cols="3">Detection Rate</cell><cell cols="3">0.4 0.5 0.6 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Detection Rate</cell><cell cols="2">0.4 0.5 0.6 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Detection Rate</cell><cell cols="2">0.4 0.5 0.6 0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.2</cell></row><row><cell></cell><cell></cell><cell cols="4">0.1 1</cell><cell></cell><cell></cell><cell cols="3">IrisParseNet(ASPP) IrisParseNet(PSP)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.1 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">IrisParseNet(ASPP) IrisParseNet(PSP)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.1</cell><cell>1</cell><cell>IrisParseNet(ASPP) IrisParseNet(PSP)</cell></row><row><cell></cell><cell cols="3">0.9</cell><cell cols="2">0</cell><cell></cell><cell></cell><cell cols="5">IrisParseNet(without Attention)</cell><cell></cell><cell cols="2">0 0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">IrisParseNet(without Attention)</cell><cell></cell><cell cols="2">0 0.9</cell><cell>IrisParseNet(without Attention)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14 15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14 15</cell><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14 15</cell></row><row><cell></cell><cell cols="3">0.8</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Hausdorff Distance</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.8</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Hausdorff Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell>Hausdorff Distance</cell></row><row><cell>Detection Rate</cell><cell cols="3">0.7 0.4 0.5 0.6</cell><cell></cell><cell></cell><cell cols="6">(a) CASIA-Iris-Distance</cell><cell></cell><cell>Detection Rate</cell><cell cols="2">0.7 0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) UBIRIS.v2</cell><cell></cell><cell cols="3">Detection Rate</cell><cell>0.7 0.4 0.5 0.6</cell><cell>(c) MICHE-I</cell></row><row><cell></cell><cell cols="3">0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell></row><row><cell></cell><cell cols="3">0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">IrisParseNet(ASPP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">IrisParseNet(ASPP)</cell><cell></cell><cell></cell><cell></cell><cell>IrisParseNet(ASPP)</cell></row><row><cell></cell><cell cols="3">0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">IrisParseNet(PSP)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IrisParseNet(PSP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell>IrisParseNet(PSP)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">IrisParseNet(without Attention)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">IrisParseNet(without Attention)</cell><cell></cell><cell></cell><cell>IrisParseNet(without Attention)</cell></row><row><cell></cell><cell></cell><cell cols="2">0</cell><cell>0</cell><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14 15</cell><cell></cell><cell cols="2">0</cell><cell cols="2">0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14 15</cell><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14 15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Hausdorff Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Hausdorff Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hausdorff Distance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14 15</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF IRISPARSENET WITH/WITHOUT JOINT TRAINING.</figDesc><table><row><cell></cell><cell cols="3">Dataset</cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell>E1 (%)</cell><cell>E2 (%)</cell><cell>mean F1 (%)</cell><cell>mIOU (%)</cell><cell>Overall mHdis of Iris Localization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ASPP-type</cell><cell>0.40 0.20 94.30 89.40</cell><cell>5.96</cell></row><row><cell></cell><cell cols="8">CASIA only Localization N/A N/A N/A</cell><cell>N/A</cell><cell>11.91</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">only Segmentation 0.41 0.20 94.08 89.18</cell><cell>N/A</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ASPP-type</cell><cell>0.84 0.42 91.82 85.39</cell><cell>6.27</cell></row><row><cell></cell><cell cols="8">UBIRIS only Localization N/A N/A N/A</cell><cell>N/A</cell><cell>7.39</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">only Segmentation 0.85 0.42 91.70 83.37</cell><cell>N/A</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ASPP-type</cell><cell>0.82 0.41 91.33 84.79</cell><cell>6.50</cell></row><row><cell></cell><cell cols="8">MICHE only Localization N/A N/A N/A</cell><cell>N/A</cell><cell>10.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">only Segmentation 0.82 0.41 91.32 84.72</cell><cell>N/A</cell></row><row><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14 15</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CASIA RTV-L 1 <ref type="bibr" target="#b11">[12]</ref> 0.2708 <ref type="bibr" target="#b0">1</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accurate iris segmentation in non-cooperative environments using fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Conference on Biometrics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation for cnn based iris segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jalilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jalilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jalilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Biometrics Special Interest Group</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An end to end deep neural network for iris segmentation in unconstrained scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bazrafkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thavalengal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corcoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="79" to="95" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A benchmark for iris location and a deep learning detector evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bezerra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A Z</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weingaertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<idno>abs/1803.01250</idno>
		<ptr target="http://arxiv.org/abs/1803.01250" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Irisdensenet: Robust iris segmentation using densely connected fully convolutional networks in the images by visible light and near-infrared light camera sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arsalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Naqvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Owais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Park</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Handbook of biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iris recognition: an emerging biometric technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1348" to="1363" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How iris recognition works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The essential guide to image processing</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="715" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Least squares fitting of circles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chernov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lesort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="252" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Experimental analysis regarding the influence of iris segmentation on the recognition rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hofbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iet Biometrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="200" to="211" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iris recognition: Analysis of the error rates regarding the accuracy of the segmentation stage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Proença</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image &amp; Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="202" to="206" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An accurate iris segmentation framework under relaxed imaging constraints using total variation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ajay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iris segmentation using geodesic active contours and grabcut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Symposium on Image and Video Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="48" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated segmentation of iris images acquired in an unconstrained environment using hog-svm and growcut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zainal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Suandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="60" to="70" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cnn-based pupil center detection for wearable gaze estimation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chinsatit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saitoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Computational Intelligence and Soft Computing</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deconvolutional neural network for pupil detection in real-world environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vera-Olmos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Malpica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Work-Conference on the Interplay Between Natural and Artificial Computation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to find eye region landmarks for remote gaze estimation in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<idno>abs/1805.04771</idno>
		<ptr target="http://arxiv.org/abs/1805.04771" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">I</forename><surname>Test</surname></persName>
		</author>
		<ptr target="http://www.idealtest.org/dbDetailForUser.do?id=4" />
	</analytic>
	<monogr>
		<title level="m">Casia.v4 database</title>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The ubiris.v2: A database of visible wavelength iris images captured on-themove and at-a-distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Proenca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Filipe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1529" to="1535" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Firme: Face and iris recognition for mobile engagement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Marsico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Riccio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1161" to="1172" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The viterbi algorithm at different resolutions for enhanced iris segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garcia-Salicetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dorizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Conference on Biometrics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="310" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High confidence visual recognition of persons by a test of statistical independence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1148" to="1161" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised detection of non-iris occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haindl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krupička</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="60" to="65" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Irisseg: A fast and robust iris segmentation framework for non-ideal iris images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Conference on Biometrics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving colour iris segmentation using a model selection technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirlantzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Howells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="24" to="32" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Iris segmentation using geodesic active contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving iris segmentation performance via borders recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rongnian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shaojie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computation Technology and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="580" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.7062" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">--</forename><surname>Deeplab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.05587" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Linknet: Exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno>abs/1707.03718</idno>
		<ptr target="http://arxiv.org/abs/1707.03718" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ternausnet: U-net with VGG11 encoder pre-trained on imagenet for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shvets</surname></persName>
		</author>
		<idno>abs/1801.05746</idno>
		<ptr target="http://arxiv.org/abs/1801.05746" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic instrument segmentation in robot-assisted surgery using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iglovikov</surname></persName>
		</author>
		<idno>abs/1803.01207</idno>
		<ptr target="http://arxiv.org/abs/1803.01207" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Casenet: Deep category-aware semantic edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5300" to="5309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno>abs/1804.09337</idno>
		<ptr target="http://arxiv.org/abs/1804.09337" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno>abs/1807.06521</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<ptr target="http://arxiv.org/abs/1807.06521" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bam: Bottleneck attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno>abs/1807.06514</idno>
		<ptr target="http://arxiv.org/abs/1807.06514" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Results from miche iimobile iris challenge evaluation ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Marsico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Proença</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improving colour iris segmentation using a model selection technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirlantzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Howells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="32" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Gmbh</surname></persName>
		</author>
		<title level="m">halcon/hdevelop reference manual</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>11st ed.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning relaxed deep supervision for better edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The nice. i: noisy iris challenge evaluation-part i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Proença</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Biometrics: Theory, Applications, and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A ground truth for iris segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hofbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="527" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gland segmentation in colon histology images: The glas challenge contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="489" to="502" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Evaluation of state-of-the-art pupil detection algorithms on remote eye images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rosenstiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1716" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficient and robust segmentation of noisy iris images for non-cooperative iris recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="230" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Mobile iris challenge evaluation (miche)-i, biometric iris dataset and protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Marsico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Riccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="17" to="23" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A survey of model compression and acceleration for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1710.09282</idno>
		<ptr target="http://arxiv.org/abs/1710.09282" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="I" to="I" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
