<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Occlusion-aware Face Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Occlusion-aware Face Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Occluded face detection is a challenging detection task due to the large appearance variations incurred by various real-world occlusions. This paper introduces an Adversarial Occlusion-aware Face Detector (AOFD) by simultaneously detecting occluded faces and segmenting occluded areas. Specifically, we employ an adversarial training strategy to generate occlusion-like face features that are difficult for a face detector to recognize. Occlusion is predicted simultaneously while detecting occluded faces and the occluded area is utilized as an auxiliary instead of being regarded as a hindrance. Moreover, the supervisory signals from the segmentation branch will reversely affect the features, helping extract more informative features. Consequently, AOFD is able to find the faces with few exposed facial landmarks with very high confidences and keeps high detection accuracy even for masked faces. Extensive experiments demonstrate that AOFD not only significantly outperforms state-of-the-art methods on the MAFA occluded face detection dataset, but also achieves competitive detection accuracy on benchmark dataset for general face detection such as FDDB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face detection has been well studied in recent years. From the pioneering work of Viola-Jones face detector <ref type="bibr" target="#b26">[27]</ref> to recent state-of-the-art CNN-based methods, the performance of face detectors has been improved remarkably. For example, the average precision has been boosted to over 98% <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref> in the unconstrained FDDB dataset.</p><p>Although face detection algorithms have obtained quite good results under general scenarios, detecting faces in specific scenarios is still worth studying. For instance, one of the remaining challenges is partially occluded face detection. Facial occlusions occur frequently, e.g. facial accessories including sunglasses, masks and scarfs. Occluded * corresponding author faces are only partially visible, and occluded regions have arbitrary appearances that may diverse from normal face regions. Hence occluded faces have significant intra-class variation, leading to difficulties in learning discriminative features for detection. A standard paradigm to address this problem is to enlarge the training dataset of occluded faces, but it can't solve this problem in essence. Moreover, the lack of large-scale occluded face datasets makes it harder to handle this obstacle.</p><p>In this paper, we propose a framework for occluded face detection, aiming at formulating a new strategy to tackle the problem of limited occluded face training data, and exploiting the power of CNN representations for the faces with occlusions as far as possible. Firstly, motivated by the remarkable success achieved by adversarial learning in recent years, a deep adversarial network is proposed in our approach to generate face samples with occlusions from the mask generator. A compact constraint is adopted to reinforce the realness of generated masks. Secondly, we introduce an occlusion-aware model by predicting the occlusion segments at the same time with detecting faces.</p><p>In all, the generator aims to make the model focus more on the exposed areas, while the segmentation branch is to extract more informative features of the occluded area. Intuitively, jointly solving these two tasks can be reciprocal.</p><p>To sum up, we make contributions in the following aspects:</p><p>• A novel adversarial framework is proposed to alleviate the lack of occluded training face images by generating occluded or masked face features. We employ a compact constraint to get more realistic occlusions.</p><p>• Mask prediction is conducted simultaneously while detecting occluded faces. The occluded area will NOT be regarded as a hindrance but an auxiliary of face detection.</p><p>• Experimental evaluations on the MAFA dataset demonstrate that the proposed AOFD can significantly improve the face detection accuracy under heavily occlusions. Besides, AOFD can also achieve competitive performance on the unconstrained face detection benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We first briefly survey face detection algorithms, followed by a review of the state-of-the-art occluded face detection researches.</p><p>Face detection can be considered as a special task of object detection. Successful general face detection algorithms often show great performance on face recognition. The Viola-Jones <ref type="bibr" target="#b26">[27]</ref> detector can be recognized as a milestone in the field of face detection. They innovatively adopted AdaBoost to train cascade classifier with Haar-like features, which first makes it possible to apply face detection in realtime applications. Following their work, lots of boostingbased models were proposed <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref>, focusing on designing more sophisticated hand-crafted features or improving the boosting strategy. More Recently, CNN features <ref type="bibr" target="#b29">[30]</ref> were utilized in this boosting framework. Another famous category of face detectors is DPM-based. Deformable part models <ref type="bibr" target="#b3">[4]</ref> were proposed for object detection at first, which acquired impressive accuracy in complex environment. Inspired by this model, many extensions of DPM were developed to face detection <ref type="bibr" target="#b5">[6]</ref> by modeling potential deformations among facial parts. However, DPM models suffered from the high computational complexity, making it difficult to be applied in real-world applications such as digital cameras, phones or other mobile devices.</p><p>Generally speaking, boosting-based methods and DPMbased methods design features and optimize classifiers separately. The pipeline of these methods is divided into two stages, which is not an end-to-end architecture. Recently, benefitting from the prosperity of social network and big data, numerous deep learning based object detection algorithms have been proposed <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1]</ref>. CNN-based detectors therefore have become the mainstream in face detection gradually <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>. CNN-based face detectors directly learn robust face representations from data and optimize classifiers in an end-to-end style. For example, <ref type="bibr" target="#b33">[34]</ref> developed a deep cascaded multi-task framework that predict face and landmark location in a coarse-to-fine manner, and <ref type="bibr" target="#b32">[33]</ref> further improved the performance of cascade models by optimizing feature selection algorithms.</p><p>Although many efforts have been made in face detection, the performance of occluded face detection is still far from satisfactory, and there are few works on occluded face detection as far as we know. <ref type="bibr" target="#b30">[31]</ref> explicitly inferred faceness score through local part responses via an attributeaware model. But additional face-specific attribute annotations needed in this method were very difficult to collect. <ref type="bibr" target="#b21">[22]</ref> introduced a specific grid loss layer into CNNs that minimized the error rates on each sub-block of the feature map independently, thus every sub-part is discriminative on its own. <ref type="bibr" target="#b18">[19]</ref> introduced a partial face detection approach based on detection of facial segments. They mainly focused on detecting incomplete faces that captured by the front camera of smart phones. Recently, <ref type="bibr" target="#b4">[5]</ref> combined pretrained CNN features with local linear embedding (LLE-CNN) to get similarity-based descriptors for partially visible faces. They built a dataset for masked face detection specifically, named the MAFA that contains 35K occluded faces. <ref type="bibr" target="#b27">[28]</ref> applied anchor-level attention on Feature Pyramid Networks <ref type="bibr" target="#b15">[16]</ref>.</p><p>As mentioned above, our work is also related to adversarial learning. Generative Adversarial Network (GAN) <ref type="bibr" target="#b7">[8]</ref> has shown great performance in numerous computer vision applications including image style transfer <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b10">11]</ref>, image generation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10]</ref> and so on. Adversarial learning provides a simple yet efficient way to train powerful models via the min-max two-player game between the generator and the discriminator. Most of the previous work focused on promoting generators. Recently, researchers began to pay attention to increase the capacity of discriminator by adversarial learning. <ref type="bibr" target="#b28">[29]</ref> used adversarial learning in generating hard examples for object detection. <ref type="bibr" target="#b13">[14]</ref> employed Perceptual GAN to enhance the representations for small objects. Inspired by these applications, we develop an adversarial occlusion-aware model, which can synthesize occlusionlike face features for boosting occluded face detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we propose an AOFD method to tackle one of the most common and vicious problems in face detection-occlusion problem. We first analyze the occluded face detection problem (Sec. 3.1) and summarizes the overall architecture of AOFD (Sec. 3.2), and then introduce the mask generation and segmentation method in AOFD in Sec. 3.3 and Sec. 3.4, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Analysis</head><p>In real-world situations, we can generally classify face occlusion problems into three categories: facial landmark occlusion, occluded by faces and occluded by objects. Facial landmark occlusion includes conditions like wearing glasses and gauze masks. Occluded by faces is a complicated situation because a detector easily mis-recognize several faces into one or only detect a part of the faces. The segmentation method is proposed in order to mitigate this problem. When occluded by an object, usually more than half of a face will be directly masked. An original masking strategy is used to mimic these in-the-wild situations.</p><p>We also visualized features of occluded faces in <ref type="figure" target="#fig_2">Figure  3</ref> (a), finding that occluded areas rarely respond. For some heavily occluded faces, useful information in feature maps is too scarce for a detector to identify. To tackle this problem, we may need to enhance representation ability of exposed area. Meanwhile, recognition of occluded area can also bespeak that "there is a face" on the condition that sufficient context information is provided. For the most complex problem where a face is occluded by another face, the context area should cover at least the nearby faces and a larger receptive field is required so that the integrity of the background information can be ensured. This idea is enlightened by human vision, that is, human need a large context to define a small or incomplete object. Besides, as the quality of features directly determines the results, segmentation is better conducted on image features than on RoIs in order to extract more informative feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overall Architecture</head><p>In order to detect faces with heavy occlusion, Adversarial Occlusion-aware Face Detector (AOFD) is designed with the view of (1) effectively utilizing the exposed facial areas, and (2) transferring the interference of the occlusions into beneficial information. For the first problem, we find that undetected faces are typically those with their characteristic part of face occluded, such as eyes and mouth. One feasible way is to mask the distinctive part of face in train-ing set, forcing the detector to learn what possibly a face looks like even if there is less exposed area. To this end, a mask generator is designed in an adversarial way to generate a mask for each positive sample. It will generate different masks with faces of different poses. A masking strategy is applied for a better utilization of the mask generator as well. More details are illustrated in Sec. 3.3.</p><p>For the latter problem, We believe that finding common occlusions is helpful to detect incomplete faces behind them. Thus, an occlusion segmentation branch is introduced to segment occluded areas including hair, glasses, scarves, hands and other objects. This is not an easy task due to few training samples. Therefore, we labeled 374 training samples downloaded from internet for occlusion segmentation and came up with an original training strategy. This dataset is denoted as SFS (small dataset for segmentation). More details are listed in Sec. 4.</p><p>As is demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref>, a mask generator is added after a region of interest (RoI) pooling layer, followed by a classification branch and a bounding box regression branch. Finally, a segmentation branch is in responsible to segment the occluded area inside each bounding box. The final result combining classification, bounding box regression and occlusion segmentation will be output in the end. The overall loss of our architecture takes the following multi-task form:</p><formula xml:id="formula_0">L = αL c + βL b + µL s<label>(1)</label></formula><p>where L c denotes a binary softmax loss for classification, L b denotes a smooth L1 loss for bounding box regression. We apply a binary softmax loss for segmentation branch, which is L s . During training, the coefficients α, β and µ are set 1, 1, and 1e −5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mask Generator</head><p>Mask generator: Since human face is very structural, facial features tend to appear in similar locations. However, with different poses, expressions and occlusions, distinguished facial area varies significantly. Our aim is to find The compact constraint has made the generated masks more accurate and efficient. A quarter of the minimum values is selected as the mask and is marked black. The masks are operated directly on the 7 by 7 RoIs. We map these masks to their corresponding receptive fields in the original image space for a better visualization. this distinguished area and to generate a customized mask. We visualize some mask examples in <ref type="figure" target="#fig_2">Figure 3</ref> (b). As we have observed, occluded area in features rarely respond in real images. To simulate this characteristic, masks are directly operated on RoIs. Therefore, the generator, which contains four convolutional layers with a straight mapping, is designed simply as it can be regarded as a binary prediction problem. Besides, the peculiarity of our mask generator, to distinguish from <ref type="bibr" target="#b28">[29]</ref>, is the original generating procedure and the mask forms. Since face structures are inherently different from those of objects, they need to be learned in a more subtle and flexible way, or no plausible mask can be obtained.</p><p>Masking strategy: The generated mask is a one-channel heat map where 0 represents masked area and 1 otherwise. During training, each pixel value will be squeezed to zero or one. We select a quarter of the minimum values as the mask when training the generator and one-third of the minimum values when training the overall model.</p><p>Heavily occluded samples after masking will become an extremely hard source for training, making the model difficult to converge. To this end, three types of masks are proposed and jointly training with the original features. The first type is to use mask generator, which corresponds to facial landmark occlusion. The second type is to mask half of the features, whether left, right, top or bottom, and the third type is randomly dropping half of the pixels. This masking strategy embodies in-the-wild occlusion types analyzed in Sec. 3.1.</p><p>Loss function: When training the mask generator, we employ an adversarial training method. We aim to increase classification loss as much as possible. Since a masked area is limited and a distinguished facial area is comparatively salient in feature maps, the model can easily con-verge. However, we find it not enough because the occluded area is sometimes strip-like or sporadic, while it is supposed to be more compact in real situations. Recall that the areas with longer or irregular edges will have a larger value for each pixel using a kernel of an edge detector. A kernel to make the occluded area sleeker and more circular is designed as a compact constraint for generated masks. The loss function is:</p><formula xml:id="formula_1">L g = γL com − ηL c<label>(2)</label></formula><p>where L g denotes the loss for generator, L com denotes a compact loss, and γ and η are coefficients. γ is set 1e −6 and η is set 1 in order to balance the derivatives. The compact loss is computed with a convolutional layer in a way as follows:</p><formula xml:id="formula_2">L com = ((1 − mask) * kernel)<label>(3)</label></formula><p>where * denotes a convolutional operation, mask is the first type of mask generated by the mask generator and the last item is the designed kernel, which is</p><formula xml:id="formula_3">  − 1 8 − 1 8 − 1 8 − 1 8 1 − 1 8 − 1 8 − 1 8 − 1 8  </formula><p>In this way, strip-like or sporadic areas will get very high penalty and more reasonable masks can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Segmentation</head><p>Design: Previous works on segmentation have proved that CNNs is capable of comprehending the semantic information of a picture and elaborately conduct a pixel-wise classification. When combining detection with segmentation, it is usually designed in RoI level to achieve higher accuracy.</p><p>Considering segmenting each RoI, one problem in occluded face situation is that the overlap of two bounding boxes will have different meanings. For example, if one face is occluded by another face, part of the front face should be regarded as an occlusion for the back face, while there shouldn't be any occluded area for the front face. Since our destination is to utilize the effective information contained in the occluded area to confirm if there is a back face and then make the exposed area more distinguished, ample context information is required. Moreover, it is the features that really matter in the bounding box classification and regression branch. With reasons above, segmentation is conducted in image level to directly affect the image feature maps. Therefore, the detector is able to find faces with more informative features embodying image-level signals like the appearance of an occlusion or a person. We call this method as an occlusion-aware method.</p><p>The segmentation branch is designed in a fully convolutional way <ref type="figure" target="#fig_3">(Figure 4)</ref>. In order to obviate noise, it follows a bounding box regression branch and only areas inside bounding boxes are maintained. Bounding boxes are enlarged in scale with a factor of 1.3 before dropping the noise. Because it is only an auxiliary task for face detection, we didn't compare it with other segmentation methods. The results <ref type="figure" target="#fig_0">(Figure 1</ref>, Sec.4) should have shown the effectiveness. Although the final results have proved the feasibility of this method, the edges of segmentation seem to be a bit rough. This is caused by the limited size of the SFS training set. Nevertheless, we have verified the possibility to train the model with very limited training samples.</p><p>Loss function: We choose softmax loss instead of L1 or L2 loss used in some segmentation and image generation tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref> because it helps stabilize the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we qualitatively evaluate the proposed method with state-of-the-art methods. We first introduce detailed information during training (Sec. 4.1), and then test AOFD on several comparative benchmarks (Sec. 4.2). A series of ablative studies are conducted to verify the effectiveness of our method (Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Details</head><p>There are two stages in the training procedure. Firstly, based on Faster RCNN <ref type="bibr" target="#b22">[23]</ref>, we train the mask generator with the loss in equation <ref type="bibr" target="#b1">(2)</ref>. Secondly, the detector and the segmentation branch are trained jointly with the parameters in the mask generator fixed.</p><p>In the second stage, due to the limitation of training data for segmentation, an unordinary training strategy is needed. We first train on SFS for 10k iterations, causing overfitting in segmentation. Then the model is trained on the combination of WIDER FACE training set and SFS for 50k iterations with loss weights for segmentation set 1e −7 and finally the model is tuned only on SFS for 3 epochs. Derivatives from WIDER FACE training set will be zero for the segmentation branch during training. Because there are far more training images from the WIDER FACE training set than SFS, the segmentation branch can only be trained every several iterations while the features are changing all the time when training on the combined dataset. In this way, the overall loss can get rid of local minima and the overfitting problem can be solved. The basic learning rate is 0.001. AOFD runs 5 FPS on a TITAN X GPU, which is similar to the original Faster RCNN.</p><p>Experiment settings: AOFD is based on a Faster RCNN with a VGG16 backbone <ref type="bibr" target="#b25">[26]</ref>. For anchors of PRN, we use three aspect ratios (1.7, 1 and 1.3) and four scales (64 2 , 128 2 , 256 2 and 512 2 ). Batch size is set 1. An RoI is treated as foreground if its intersection over union (IoU) with any ground truth bounding box is higher than 0.5. To balance the number of foreground and background training samples, the ratio of foreground RoIs to background RoIs is set 1:3. During training, the short side of an input image is resized to either 512 or 1024 on condition that long side is no longer than 1024.</p><p>Small dataset for segmentation (SFS): SFS contains 376 images with 1138 labeled faces downloading from the Internet. There is at least one occluded face in each image and over 80% of the faces are occluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on benchmarks</head><p>Our model is trained on the WIDER FACE <ref type="bibr" target="#b31">[32]</ref> training set and evaluated on the FDDB and MAFA <ref type="bibr" target="#b4">[5]</ref> databases. Although the MAFA database dose not release its training set, we still obtain state-of-the-art results on the MAFA testing set without fine tuning the model to adjust the variance between different annotation protocols.</p><p>FDDB (Face Detection Data Set and Benchmark) is an unconstrained dataset for face detection. It has 2, 845 images with 5, 171 faces. The detection results of different methods are shown in <ref type="figure">Figure 6</ref>. <ref type="bibr" target="#b17">[18]</ref> and several other methods obtain higher continuous score because they have transformed the rectangle bounding boxes into ellipse ones. The fact that we didn't carry out extra training in the FDDB training set may lead to the increase of localization errors because of the difference of annotation criterion. However, in the comparison with state-of-the-art methods, we observe that AOFD outperforms all the other methods in terms of discrete score, demonstrating its strong ability to detect nearly all large faces even if faces with short side less than around 15 pixels are mostly neglected due to the anchor setting.</p><p>Furthermore, our AOFD also obtains a higher recall rate at 1000 FPs on FDDB than other Faster RCNN methods with similar settings by a large margin ( <ref type="figure">Figure 5</ref> and <ref type="figure">Figure 6)</ref>. The superior performance also reveals that applying masking strategy and training with a segmentation task are valuable attempts to enhance the model's capacity.</p><p>MAFA is designed for the evaluation of masked face detection, which contains 35806 face annotations with a minimum size of 32×32. Since the MAFA testing set uses squares to label faces, the rectangle bounding boxes in our results are transformed into squares to match the annotation.</p><p>There are three types of annotations in the MAFA dataset: masked, unmasked and ignored. Blurry or deformed faces or those with side length less than 32 pixels As shown in <ref type="table">Table 1</ref>, the average precision achieves the highest 91.9% (threshold 0.5) if we only evaluate on the faces with 'masked' and 'unmasked' labels. The result outperforms LLE-CNNs <ref type="bibr" target="#b4">[5]</ref> by a large margin and is also better than the state-of-the-art Face Attention Network (FAN) <ref type="bibr" target="#b27">[28]</ref>. Since AOFD is proposed to address occlusion problem, we also evaluate our model on faces labeled as 'masked' only. AOFD achieves 83.5% and has 7% im-  provement over the state-of-the-art result obtained by FAN. <ref type="figure">Figure 6</ref>(c) further shows the PR (Precision-Recall) curves of the three experimental settings. If we only count the faces annotated as 'masked' (the orange curve in <ref type="figure">Figure 6(c)</ref>), precision witnesses a sharp drop at the beginning. This is caused by unmasked and unlabeled face detections which are regarded as FPs when evaluating masked faces. More results on MAFA are presented in <ref type="figure" target="#fig_5">Figure 7</ref>.</p><p>Furthermore, we have studied the main obstacle of our model to achieve a higher AP. The minimum IoU threshold for a true positive proposal is modified from 0.5 to 0.45, from which we observe that a slight decrease of IoU threshold can boost AP from 91.9% to 93.8%. This explains that the precision of bounding boxes can still be further improved</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Analysis</head><p>To better understand the function of each part of our model, we ablate each component to observe AOFD's performance. In this way, the mask generator and segmentation branch are removed one after the another. We delve into the optimal area of the mask as well and find that the mask area is crucial for the functioning of mask generator. Besides, the efficiency of the compact constraint and the comparison with online hard example mining (OHEM) <ref type="bibr" target="#b23">[24]</ref> are also dis- cussed in this section. Mask facilitates detecting: State-of-the-art detectors are able to detect some of occluded faces, but with lower confidence. As shown in <ref type="figure" target="#fig_6">Figure 8</ref>, AOFD can increase the confidence of occluded faces by a large margin. Without the mask generator, AOFD pays less attention to exposed area or face structure, and the recall rate at 1000 false positives on FDDB drops by 1.3%( <ref type="table">Table 2</ref>). The sharp decline (3.2%) of average precision on the MAFA testing set in <ref type="figure">Figure 5</ref>(a) reveals the value of the mask generator as well. It is also observed that AOFD's results would drop by around 1% with only random and square-like occlusions. Since faces have unique structure characteristics such as facial symmetry, generating adaptive occlusions is essential in order to fool the detector.</p><p>Segmentation increases recall: With the segmentation branch, the result in <ref type="table">Table 2</ref> witnesses an increase of 0.75%. This improvement is relatively slight because there are not many heavily occluded faces in the FDDB testing set. The drop of average precision from 79.9% to 77.4% ( <ref type="figure">Figure  5(b)</ref>) will be more convincing to confirm the effectiveness of the segmentation branch.</p><p>Mask area is crucial: We find that the mask would vitiate the detector if a mask area is too large. Nevertheless, it would be of no use if it is too small. <ref type="figure">Figure 5</ref> gives a brief overview of our experiments, from which we find occluding one-third of features is an ideal area for a mask.</p><p>Compact constraint matters: We propose a compact constraint L c to help generate more practical masks. As is mentioned in Sec. 3.3, the generated masks are discrete or sporadic and are not plausible, e.g., two pixels occlusion on the mouth, three pixels occlusion on the eyes and others on the corners <ref type="figure" target="#fig_2">(Figure 3 (b)</ref>). In our initial experiments, the average precision is 0.785 when masking 1/3 of RoIs without the compact constraint, which is similar to having 1/6 masking area in <ref type="figure">Figure 5</ref>(c). However, masks become harder and more reasonable with L c , which can account for the increase of performance.</p><p>Comparing with OHEM: We compare online hard example mining <ref type="bibr" target="#b23">[24]</ref> with our methods in <ref type="table" target="#tab_2">Table 3</ref>. We can see that the performance of training a Faster RCNN with OHEM is generally worse than a single AOFD without OHEM. But the combination of these two methods leads to a better performance. Although a harder training procedure means a more robust detector under this condition, the measurement of hard level needs to be carefully handled. For example, the decrease incurred by too large masking area demonstrated in <ref type="figure">Figure 5</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper has proposed a face detection model named AOFD to address the long-standing issue of face occlusions. A novel masking strategy has been integrated into AOFD to increase training complexity, and can plastically mimic different situations of face occlusions. The multitask training method with a segmentation branch provides a feasible solution and verifies the possibility to train an auxiliary task with very limited training data. The superior performance on both general face detection and masked face detection benchmarks demonstrates the effectiveness of AOFD.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The proposed AOFD is able to detect various heavilyoccluded faces, while the occlusion-aware segmentation branch predicts masks of the occluded area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of the proposed method. Mask generator are operated directly on RoIs where one type of masks are selected for each proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a) Heat map of the features from conv5 3 extracted from Faster RCNN. Occluded areas have less information in the features (the black areas). (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The image segmentation branch in AOFD. Only areas inside bounding boxes are maintained so as to reduce noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>PR curves of ablative studies on the whole MAFA testing set without OHEM. Ignored-0.919 masked only-0.835 all three types-0.813 (c) PR curves for our experiments on the MAFA testing set Results on FDDB ((a)(b)) and MAFA testing set ((c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results of AOFD on the test set of the MAFA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Not only is AOFD able to detect occluded faces with higher confidences, but also capable of increasing the average precision. The purple bounding boxes are false detections by Faster RCNN and the blue ones are new true positives detected by AOFD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparing AOFD with OHEM on the whole MAFA and FDDB testing set.</figDesc><table><row><cell>Methods</cell><cell cols="2">AP on MAFA Recall on FDDB</cell></row><row><cell>single OHEM</cell><cell>75.9%</cell><cell>96.54%</cell></row><row><cell>single AOFD</cell><cell>79.9%</cell><cell>97.12%</cell></row><row><cell>AOFD with OHEM</cell><cell>81.3%</cell><cell>97.88%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06792</idno>
		<title level="m">Gm-net: Learning features with more efficiency</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view face detection using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Farfade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="643" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting masked faces in the wild with lle-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Occlusion coherence: Localizing occluded faces with a hierarchical deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2385" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face recognition by discriminant analysis with gabor tensor representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Biometrics</title>
		<editor>S.-W. Lee and S. Z. Li</editor>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="87" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual generative adversarial networks for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning surf cascade for fast and accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3468" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online Determination of Track Loss Using Template Inverse Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eighth International Workshop on Visual Surveillance -VS2008</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Recurrent scale approximation for object detection in cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09531</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Partial face detection for continuous authentication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mahbub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Barbello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICIP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2991" to="2995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssh: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grid loss: Detecting occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Poier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="386" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Face attention network: An effective face detector for the occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07246v1</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Submodular asymmetric feature selection in cascade object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1387" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Group cost-sensitive boosting for multiresolution pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3676" to="3682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
