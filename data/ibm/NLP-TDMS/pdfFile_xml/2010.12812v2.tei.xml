<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Frustratingly Easy Approach for Entity and Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
							<email>zzhong@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
							<email>danqic@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Frustratingly Easy Approach for Entity and Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both entity and relation encoders at inference time, achieving an 8-16× speedup with a slight reduction in accuracy. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting entities and their relations from unstructured text is a fundamental problem in information extraction. This problem can be decomposed into two subtasks: named entity recognition <ref type="bibr" target="#b24">(Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b23">Ratinov and Roth, 2009</ref>) and relation extraction <ref type="bibr" target="#b31">(Zelenko et al., 2002;</ref><ref type="bibr" target="#b3">Bunescu and Mooney, 2005)</ref>. Early work employed a pipelined approach, training one model to extract entities <ref type="bibr" target="#b7">(Florian et al., 2004</ref><ref type="bibr" target="#b8">(Florian et al., , 2006</ref>, and another model to classify relations between them <ref type="bibr" target="#b36">(Zhou et al., 2005;</ref><ref type="bibr" target="#b11">Kambhatla, 2004;</ref><ref type="bibr" target="#b4">Chan and Roth, 2011)</ref>. More recently, however, end-toend evaluations have been dominated by systems that model these two tasks jointly <ref type="bibr" target="#b15">(Li and Ji, 2014;</ref><ref type="bibr" target="#b20">Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b12">Katiyar and Cardie, 2017;</ref><ref type="bibr" target="#b32">Zhang et al., 2017a;</ref><ref type="bibr" target="#b18">Luan et al., 2018</ref><ref type="bibr" target="#b17">Lin et al., 2020;</ref><ref type="bibr" target="#b29">Wang and Lu, 2020)</ref>. There has been a long held belief that joint models can better capture the interactions between entities and relations and help mitigate error propagation issues.</p><p>In this work, we re-examine this problem and present a simple approach which learns two encoders built on top of deep pre-trained language models <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b2">Beltagy et al., 2019;</ref><ref type="bibr" target="#b13">Lan et al., 2020)</ref>. The two models -which we refer them as to the entity model and relation model throughout the paper -are trained independently and the relation model only relies on the entity model to provide input features. Our entity model builds on span-level representations and our relation model builds on contextual representations specific to a given pair of spans. Despite its simplicity, we find this pipelined approach to be extremely effective: using the same pre-trained encoders, our model outperforms all previous joint models on three standard benchmarks: ACE04, ACE05 and SciERC, advancing the previous state-of-the-art by 1.7%-2.8% absolute in relation F1.</p><p>To better understand the effectiveness of this approach, we carry out a series of careful analyses. We observe that, (1) the contextual representations for the entity and relation models essentially capture distinct information, so sharing their representations hurts performance; (2) it is crucial to fuse the entity information (both boundary and type) at the input layer of the relation model; (3) leveraging cross-sentence information is useful in both tasks. Hence, we expect that this simple model will serve as a very strong baseline in end-to-end relation extraction and make us rethink the value of joint modeling of entities and relations.</p><p>Finally, one possible shortcoming of our approach is that we need to run our relation model  <ref type="figure">Figure 1</ref>: An example from the SciERC dataset <ref type="bibr" target="#b18">(Luan et al., 2018)</ref>. Given an input sentence MORPA is a fully implemented parser for a text-to-speech system, an end-to-end relation extraction system is expected to extract that MORPA and PARSER are entities of type METHOD, TEXT-TO-SPEECH is a TASK, as well as MORPA is a hyponym of PARSER and MORPA is used for once for every pair of entities. To alleviate this issue, we present a novel and efficient alternative by approximating and batching the computations for different groups of entity pairs at inference time. This approximation achieves an 8-16× speedup with only a slight reduction in accuracy (e.g., 1.0% F1 drop on ACE05), which makes our model fast and accurate to use in practice. Our final system is called PURE (the Princeton University Relation Extraction system) and we make our code and models publicly available for the research community. We summarize our contributions as follows:</p><p>• We present a simple and effective approach for end-to-end relation extraction, which learns two independent encoders for entity recognition and relation extraction. Our model establishes the new state-of-the-art on three standard benchmarks and surpasses all previous joint models. • We conduct careful analyses to understand why our approach performs so well and how different factors impact the final performance. We conclude that it is more effective to learn distinct contextual representations for entities and relations than to learn them jointly. • To speed up the inference time of our model, we also propose a novel efficient approximation, which achieves a large runtime improvement with only a small accuracy drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditionally, extracting relations between entities in text has been studied as two separate tasks: named entity recognition and relation extraction. In the last several years, there has been a surge of interest in developing models for joint extraction of entities and relations <ref type="bibr" target="#b15">(Li and Ji, 2014;</ref><ref type="bibr" target="#b21">Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b20">Miwa and Bansal, 2016)</ref>. We group existing joint models into two categories: structured prediction and multi-task learning:</p><p>Structured prediction Structured prediction approaches cast the two tasks into one unified framework, although it can be formulated in various ways. <ref type="bibr" target="#b15">Li and Ji (2014)</ref> propose an action-based system which identifies new entities as well as links to previous entities, <ref type="bibr" target="#b32">Zhang et al. (2017a)</ref>; <ref type="bibr" target="#b29">Wang and Lu (2020)</ref> adopt a table-filling approach proposed in <ref type="bibr" target="#b21">(Miwa and Sasaki, 2014)</ref>; <ref type="bibr" target="#b12">Katiyar and Cardie (2017)</ref> and <ref type="bibr" target="#b35">Zheng et al. (2017)</ref> employ sequence tagging-based approaches;  and <ref type="bibr" target="#b9">Fu et al. (2019)</ref> propose graph-based approaches to jointly predict entity and relation types; and,  convert the task into a multi-turn question answering problem. All of these approaches need to tackle a global optimization problem and perform joint decoding at inference time, using beam search or reinforcement learning.</p><p>Multi-task learning This family of models essentially builds two separate models for entity recognition and relation extraction and optimizes them together through parameter sharing. <ref type="bibr" target="#b20">Miwa and Bansal (2016)</ref> propose to use a sequence tagging model for entity prediction and a tree-based LSTM model for relation extraction. The two models share one LSTM layer for contextualized word representations and they find sharing parameters improves performance (slightly) for both models. The approach of <ref type="bibr" target="#b1">Bekoulis et al. (2018)</ref> is similar except that they model relation classification as a multi-label head selection problem. Note that these approaches still perform pipelined decoding: entities are first extracted and the relation model is applied on the predicted entities.</p><p>The closest work to ours is DYGIE and DY-GIE++ , which builds on recent span-based models for coreference resolution <ref type="bibr" target="#b14">(Lee et al., 2017)</ref> and semantic role labeling . The key idea of their approaches is to learn shared span representations between the two tasks and update span representations through dynamic graph propagation layers. A more recent work <ref type="bibr" target="#b17">Lin et al. (2020)</ref> further extends DYGIE++ by incorporating global features based on cross-substask and cross-instance constraints. 2 Our approach is much simpler and we will detail the differences in Section 3.2 and explain why our model performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first formally define the problem of end-to-end relation extraction in Section 3.1 and then detail our approach in Section 3.2. Finally, we present our approximation solution in Section 3.3, which considerably improves the efficiency of our approach during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>The input of the problem is a sentence X consisting of n tokens x 1 , x 2 , . . . , x n . Let S = {s 1 , s 2 , . . . , s m } be all the possible spans in X of up to length L and START(i) and END(i) denote start and end indices of s i . Optionally, we can incorporate cross-sentence context to build better contextual representations (Section 3.2). The problem can be decomposed into two sub-tasks:</p><p>Named entity recognition Let E denote a set of pre-defined entity types. The named entity recognition task is, for each span s i ∈ S, to predict an entity type y e (s i ) ∈ E or y e (s i ) = representing span s i is not an entity. The output of the task is Y e = {(s i , e) : s i ∈ S, e ∈ E}.</p><p>Relation extraction Let R denote a set of predefined relation types. The task is, for every pair of spans s i ∈ S, s j ∈ S, to predict a relation type y r (s i , s j ) ∈ R, or there is no relation between them: y r (s i , s j ) = . The output of the task is</p><formula xml:id="formula_0">Y r = {(s i , s j , r) : s i , s j ∈ S, r ∈ R}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Approach</head><p>As shown in <ref type="figure">Figure 1</ref>, our approach consists of an entity model and a relation model. The entity model first takes the input sentence and predicts an entity type (or ) for each single span. We then process every pair of candidate entities independently in the relation model by inserting extra marker tokens to highlight the subject and object and their types. We will detail each component below, and finally summarize the differences between our approach and DYGIE++ .</p><p>Entity model Our entity model is a standard span-based model following prior work <ref type="bibr" target="#b14">(Lee et al., 2017;</ref><ref type="bibr" target="#b18">Luan et al., 2018</ref>. We first use a pre-trained language model (e.g., BERT) to obtain contextualized representations x t for each input token x t . Given a span s i ∈ S, the span representation h e (s i ) is defined as:</p><formula xml:id="formula_1">h e (s i ) = [x START(i) ; x END(i) ; φ(s i )],</formula><p>where φ(s i ) ∈ R d F represents the learned embeddings of span width features. The span representation h e (s i ) is then fed into a feedforward network to predict the probability distribution of the entity type e ∈ E ∪ { }: P e (e | s i ).</p><p>Relation model The relation model aims to take a pair of spans s i , s j (a subject and an object) as input and predicts a relation type or . Previous approaches <ref type="bibr" target="#b18">(Luan et al., 2018</ref> re-use the span representations h e (s i ), h e (s j ) to predict the relationship between s i and s j . We hypothesize that these representations only capture contextual information around each individual entity and might fail to capture the dependencies between the pair of spans. We also argue that sharing the contextual representations between different pairs of spans may be suboptimal. For instance, the words is a in <ref type="figure">Figure 1</ref> are crucial in understanding the relationship between MORPA and PARSER but not for MORPA and TEXT-TO-SPEECH.</p><p>Our relation model instead processes each pair of spans independently and inserts typed markers at the input layer to highlight the subject and object and their types. Specifically, given an input sentence X and a pair of subject-object spans s i , s j , where s i , s j have a type of e i , e j ∈ E ∪ { } respectively. We define text markers as S:e i , /S:e i , O:e j , and /O:e j , and insert them into the input sentence before and after the subject and object spans <ref type="figure">(Figure 1 (b)</ref>). 3 Let X denote this modified sequence with text markers inserted:</p><formula xml:id="formula_2">X = . . . S:e i , x START(i) , . . . , x END(i) , /S:e i , . . . O:e j , x START(j) , . . . , x END(j) , /O:e j , . . . .</formula><p>We apply a second pre-trained encoder on X and denote the output representations by x t . We concatenate the output representations of two start positions and obtain the span-pair representation:</p><formula xml:id="formula_3">h r (s i , s j ) = [ x START(i) ; x START(j) ],</formula><p>where START(i) and START(j) are the indices of S:e i and O:e j in X. Finally, the representation h r (s i , s j ) will be fed into a feedforward network to predict the probability distribution of the relation type r ∈ R ∪ { }: P r (r|s i , s j ).</p><p>This idea of using additional markers to highlight the subject and object is not entirely new as it has been studied recently in relation classification <ref type="bibr" target="#b34">(Zhang et al., 2019;</ref><ref type="bibr" target="#b25">Soares et al., 2019;</ref><ref type="bibr" target="#b22">Peters et al., 2019)</ref>. However, most relation classification tasks (e.g., TACRED <ref type="bibr" target="#b33">(Zhang et al., 2017b)</ref>) only focus on a given pair of subject and object in an input sentence and its effectiveness has not been evaluated in the end-to-end setting in which we need to classify the relationships between multiple entity mentions. We observed a large improvement in our experiments (Section 5.1) and this strengthens our hypothesis that modeling the relationship between different entity pairs in one sentence require different contextual representations. Furthermore, <ref type="bibr" target="#b34">Zhang et al. (2019);</ref><ref type="bibr" target="#b25">Soares et al. (2019)</ref> only consider untyped markers (e.g., S , /S ) and previous end-toend models (e.g., ) only inject the entity type information into the relation model through auxiliary losses. We find that injecting type information at the input layer is very helpful in distinguishing entity types -for example, whether "Disney" refers to a person or an organizationbefore trying to understand the relations.</p><p>Cross-sentence context Cross-sentence information can be used to help predict entity types and relations, especially for pronominal mentions. ;  employ a propagation mechanism to incorporate crosssentence context.  also add a 3-sentence context window which is shown to improve performance. We also evaluate the importance of leveraging cross-sentence context in our approach. As we expect that pre-trained language models to be able to capture long-range dependencies, we simply incorporate cross-sentence context by extending the sentence to a fixed window size W for both the entity and relation model. Specifically, given an input sentence with n words, we augment the input with (W − n)/2 words from the left context and right context respectively.</p><p>Training &amp; inference For both entity model and relation model, we fine-tune the two pre-trained language models using task-specific losses. We use cross-entropy loss for both models:</p><formula xml:id="formula_4">L e = − s i ∈S log P e (e * i |s i ) L r = − s i ,s j ∈S G ,s i =s j log P r (r * i,j | s i , s j ),</formula><p>where e * i represents the gold entity type of s i and r * i,j represents the gold relation type of span pair s i , s j in the training data. For training the relation model, we only consider the gold entities S G ⊂ S in the training set and use the gold entity labels as the input of the relation model. We considered training on predicted entities as well as all spans S (with pruning), but none of them led to meaningful improvements compared to this simple pipelined training (see more discussion in Section 5.3). During inference, we first predict the entities by taking y e (s i ) = arg max e∈E∪{ } P e (e|s i ). Denote S pred = {s i : y e (s i ) = }, we enumerate all the spans s i , s j ∈ S pred and use y e (s i ), y e (s j ) to construct the input for the relation model P r (r | s i , s j ).</p><p>Differences from DYGIE++ Our approach differs from DYGIE++  in the following ways: (1) We use separate encoders for the entity and relation models, without any multi-task learning. The predicted entity types are used directly to construct the input for the relation model. <ref type="formula">(2)</ref> The contextual repre-sentations in the relation model are specific to each pair of spans by using the text markers.</p><p>(3) We only incorporate cross-sentence information by extending the input with additional context (as they did) and we do not employ any graph propagation layers and beam search. 4 As a result, our model is much simpler. As we will show in the experiments (Section 4), it also achieves large gains in all the benchmarks, using the same pre-trained encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient Batch Computations</head><p>One possible shortcoming of our approach is that we need to run our relation model once for every pair of entities. To alleviate this issue, we propose a novel and efficient alternative to our relation model. The key problem is that we would like to re-use computations for different pairs of spans in the same sentence. This is impossible in our original model because we must insert the entity markers for each pair of spans independently. To this end, we propose an approximation model by making two major changes to the original relation model. First, instead of directly inserting entity markers into the original sentence, we tie the position embeddings of the markers with the start and end tokens of the corresponding span: P( S:e i ), P( /S:e i ) := P(x START(i) ), P(x END(i) ) P( O:e j ), P( /O:e j ) := P(x START(j) ), P(x END(j) ), where P(·) denotes the position id of a token. As the example shown in <ref type="figure">Figure 1</ref>, if we want to classify the relationship between MORPA and PARSER, the first entity marker S: METHOD will share the position embedding with the token MOR. By doing this, the position embeddings of the original tokens will not be changed.</p><p>Second, we add a constraint to the attention layers. We enforce the text tokens to only attend to text tokens and not attend to the marker tokens while an entity marker token can attend to all the text tokens and all the 4 marker tokens associated with the same span pair. These two modifications allow us to re-use the computations of all text tokens, because the representations of text tokens are independent of the entity marker tokens. Thus, we can batch multiple pairs of spans from the same sentence in one run of the relation model. In practice, we add all marker tokens to the end of the sentence to form an input that batches a set of span pairs <ref type="figure">(Figure 1(c)</ref>). This leads to a large speedup at inference time and only a small drop in performance (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Datasets We evaluate our approach on three popular end-to-end relation extraction datasets: ACE05 5 , ACE04 6 , and SciERC <ref type="bibr" target="#b18">(Luan et al., 2018)</ref>. <ref type="table" target="#tab_4">Table 2</ref> shows the data statistics of each dataset. The ACE05 and ACE04 datasets are collected from a variety of domains, such as newswire and online forums. The SciERC dataset is collected from 500 AI paper abstracts and defines scientific terms and relations specially for scientific knowledge graph construction. We follow previous work and use the same preprocessing procedure and splits for all datasets. See Appendix A for more details.</p><p>Evaluation metrics We follow the standard evaluation protocol and use micro F1 measure as the evaluation metric. For named entity recognition, a predicted entity is considered as a correct prediction if its span boundaries and the predicted entity type are both correct. For relation extraction, we adopt two evaluation metrics: (1) boundaries evaluation (Rel): a predicted relation is considered as a correct prediction if the boundaries of two spans are correct and the predicted relation type is correct;</p><p>(2) strict evaluation (Rel+): in addition to what is required in the boundaries evaluation, predicted entity types also must be correct. More discussion of the evaluation settings can be found in <ref type="bibr" target="#b1">Bekoulis et al. (2018)</ref>; <ref type="bibr" target="#b27">Taillé et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We use bert-baseuncased <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> and albert-xxlarge-v1 <ref type="bibr" target="#b13">(Lan et al., 2020)</ref> as the base encoders for ACE04 and ACE05, for a fair comparison with previous work and an investigation of small vs large pre-trained models. <ref type="bibr">7</ref> We also use scibert-scivocabuncased <ref type="bibr" target="#b2">(Beltagy et al., 2019)</ref> as the base encoder for SciERC, as this in-domain pre-trained model is shown to be more effective than BERT . We use a context window size of W = 300 for the entity model and W = 100 for 5 catalog.ldc.upenn.edu/LDC2006T06 6 catalog.ldc.upenn.edu/LDC2005T09 7 As detailed in <ref type="table">Table 1</ref>, some previous work used BERTlarge models. We are not able to do a comprehensive study of all the pre-trained models and our BERT-base results are generally higher than most published results using larger models.  <ref type="table">Table 1</ref>: Test F1 scores on ACE04, ACE05, and SciERC. We evaluate our approach in two settings: single-sentence and cross-sentence depending on whether cross-sentence context is used or not. ♣ : These models leverage crosssentence information. † : These models are trained with additional data (e.g., coreference). The encoders used in different models: L = LSTM, L+E = LSTM + ELMo, Bb = BERT-base, Bl = BERT-large, SciB = SciBERT (size as BERT-base), ALB = ALBERT-xxlarge-v1. Rel denotes the boundaries evaluation (the entity boundaries must be correct) and Rel+ denotes the strict evaluation (both the entity boundaries and types must be correct).  the relation model in our default setting using crosssentence context 8 and the effect of different context sizes is provided in Section 5.4. We consider spans up to L = 8 words. For all the experiments, we report the averaged F1 scores of 5 runs. More implementation details can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Table 1 compares our approach PURE to all the previous results. We report the F1 scores in both single-sentence and cross-sentence settings. As is shown, our single-sentence models achieve strong performance and incorporating cross-sentence con-text further improves the results considerably. Our BERT-base (or SciBERT) models achieve similar or better results compared to all the previous work including models built on top of larger pre-trained LMs, and our results are further improved by using a larger encoder ALBERT.</p><p>For entity recognition, our best model achieves an absolute F1 improvement of +1.4%, +1.7%, +1.4% on ACE05, ACE04, and SciERC respectively. This shows that cross-sentence information is useful for the entity model and pre-trained Transformer encoders are able to capture long-range dependencies from a large context. For relation extraction, our approach outperforms the best previous methods by an absolute F1 of +1.8%, +2.8%, +1.7% on ACE05, ACE04, and SciERC respectively. We also obtained a 4.3% higher relation F1 on ACE05 compared to DYGIE++  using the same BERT-base pre-trained model. Compared to the previous best approaches using either global features <ref type="bibr" target="#b17">(Lin et al., 2020)</ref> or complex neural models (e.g., MT-RNNs) <ref type="bibr" target="#b29">(Wang and Lu, 2020)</ref>, our approach is much simpler and achieves large improvements on all the datasets. Such improvements demonstrate the effectiveness  of learning representations for entities and relations of different entity pairs, as well as early fusion of entity information in the relation model. We also noticed that compared to the previous state-of-theart model <ref type="bibr" target="#b29">(Wang and Lu, 2020)</ref> based on ALBERT, our model achieves a similar entity F1 (89.5 vs 89.7) but a substantially better relation F1 (67.6 vs 69.0) without using context. This clearly demonstrates the superiority of our relation model. Finally, we also compare our model to a joint model (similar to DYGIE++) of different data sizes to test the generality of our results. As shown in Appendix C, our findings are robust to data sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Batch Computations and Speedup</head><p>In Section 3.3, we proposed an efficient approximation solution for the relation model, which enables us to re-use the computations of text tokens and batch multiple span pairs in one input sentence. We evaluate this approximation model on ACE05 and SciERC. <ref type="table" target="#tab_6">Table 3</ref> shows the relation F1 scores and the inference speed of the full relation model and the approximation model. On both datasets, our approximation model significantly improves the efficiency of the inference process. 9 For example, we obtain a 11.9× speedup on ACE05 and a 8.7× speedup on SciERC in the single-sentence setting. By re-using a large part of computations, we are able to make predictions on the full ACE05 test set (2k sentences) in less than 10 seconds on a single GPU. On the other hand, this approximation only leads to a small performance drop and the relaion F1 measure decreases by only 1.0% and 1.2% on ACE05 and SciERC respectively in the single-sentence setting. Considering the accuracy and efficiency of this approximation model, we expect it to be very effective to use in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Despite its simple design and training paradigm, we have shown that our approach outperforms all previous joint models. In this section, we aim to take a deeper look and understand what contributes to its final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Importance of Typed Text Markers</head><p>Our key observation is that it is crucial to build different contextual representations for different pairs of spans and an early fusion of entity type information can further improve performance. To validate this, we experiment the following variants on both ACE05 and SciERC:</p><p>TEXT: We use the span representations defined in the entity model (Section 3.2) and concatenate the hidden representations for the subject and the object, as well as their element-wise multiplication:</p><p>[h e (s i ), h e (s j ), h e (s i ) h e (s j )]. This is similar to the relation model in <ref type="bibr" target="#b18">Luan et al. (2018</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TEXTETYPE:</head><p>We concatenate the span-pair representations from TEXT with entity type embeddings ψ(e i ), ψ(e j ) ∈ R d E (d E = 150).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARKERS:</head><p>We use untyped entity types ( S , /S , O , /O ) at the input layer and concatenate the representations of two spans' starting points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARKERSETYPE:</head><p>We concatenate the span-pair representations from MARKERS with entity type embeddings ψ(e i ), ψ(e j ) ∈ R d E (d E = 150).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARKERSELOSS:</head><p>We also consider a variant which uses untyped markers but add another FFNN to predict the entity types of subject and object through auxiliary losses. This is similar to how the entity information is used in multi-task learning .</p><p>TYPEDMARKERS: This is our final model described in Section 3.2 with typed entity markers. <ref type="table">Table 4</ref> summarizes the results of all the variants using either gold entities or predicted entities from the entity model. As is shown, different input representations make a clear difference and the variants of using marker tokens are significantly  <ref type="table">Table 4</ref>: Relation F1 (boundaries) on the development set of ACE05 and SciERC with different input features. e2e: the entities are predicted by our entity model; gold: the gold entities are given. The results are obtained using BERT-base with single-sentence context for ACE05 and SciBERT with cross-sentence context for SciERC. For both ACE05 and SciERC, we use the same entity models with cross-sentence context to compute the e2e scores of using different input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared encoder? Enity F1 Relation F1</head><p>88.8 64.8 87.7 64.4 better than standard text representations and this suggests the importance of learning different representations with respect to different pairs of spans. Compared to TEXT, TYPEDMARKERS improved the F1 scores dramatically by +5.0% and +7.4% absolute when gold entities are given. With the predicted entities, the improvement is reduced as expected while it remains large enough. Finally, entity type is useful in improving the relation performance and an early fusion of entity information is particularly effective (TYPEDMARKERS vs MARK-ERSETYPE and MARKERSELOSS). We also find that MARKERSETYPE to perform even better than MARKERSELOSS which suggests that using entity types directly as features is better than using them to provide training signals through auxiliary losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Modeling Entity-Relation Interactions</head><p>One main argument for joint models is that modeling the interactions between the two tasks can contribute to each other. In this section, we aim to validate if it is the case in our approach. We first study whether sharing the two representation encoders can improve performance or not. We train the entity and relation models together by jointly  optimizing L e + L r <ref type="table" target="#tab_8">(Table 5</ref>). We find that simply sharing the encoders hurts both the entity and relation F1. We think this is because the two tasks have different input formats and require different features for predicting entity types and relations, thus using separate encoders indeed learns better task-specific features. We also explore whether the relation information can improve the entity performance. To do so, we add an auxiliary loss to our entity model, which concatenates the two span representations as well as their element-wise multiplication (see the TEXT variant in Section 5.1) and predicts the relation type between the two spans (r ∈ R or ). Through joint training with this auxiliary relation loss, we observe a negligible improvement (&lt; 0.1%) on averaged entity F1 over 5 runs on the ACE05 development set. To summarize, (1) entity information is clearly important in predicting relations (Section 5.1). However, we don't find that relation information to improve our entity model substantially 10 ; (2) simply sharing the encoders does not provide benefits to our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mitigating Error Propagation</head><p>A well-known drawback of pipeline training is the error propagation issue. In our final model, we use gold entities (and their types) to train the relation model and the predicted entities during inference and this may lead to a discrepancy between training and testing. In the following, we describe several attempts we made to address this issue.</p><p>We first study whether using predicted entities -instead of gold entities -during training can mitigate this issue. We adopt a 10-way jackknifing method, which is a standard technique in many NLP tasks such as dependency parsing <ref type="bibr" target="#b0">(Agić and Schluter, 2017)</ref>. Specifically, we divide the data into 10 folds and predict the entities in the k-th fold using an entity model trained on the remainder. As shown in <ref type="table" target="#tab_10">Table 6</ref>, we find that jackknifing strategy hurts the final relation performance surprisingly. We hypothesize that it is because it introduced additional noise during training. Second, we consider using more pairs of spans for the relation model at both training and testing time. The main reason is that in the current pipeline approach, if a gold entity is missed out by the entity model during inference, the relation model will not be able to predict any relations associated with that entity. Following the beam search strategy used in the previous work , we consider using λn (λ = 0.4 and n is the sentence length) 11 top spans scored by the entity model. We explored several different strategies for encoding the top-scoring spans for the relation model: (1) typed markers: the same as our main model except that we now have markers e.g., S: , /S: as input tokens;</p><p>(2) untyped markers: in this case, the relation model is unaware of a span is an entity or not; (3) untyped markers trained with an auxiliary entity loss (e ∈ E or ). As <ref type="table" target="#tab_10">Table 6</ref> shows, none of these changes led to significant improvements and using untyped markers is espe-cially worse because the relation model struggles to identify whether a span is an entity or not.</p><p>In sum, we do not find any of these attempts improved performance significantly and our simple pipelined training turns out to be a surprisingly effective strategy. We do not argue that this error propagation issue does not exist or cannot be solved, while we will need to explore better solutions to address this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of Cross-sentence Context</head><p>In <ref type="table">Table 1</ref>, we demonstrated the improvements from using cross-sentence context on both the entity and relation performance. We explore the effect of different context sizes W in <ref type="figure" target="#fig_0">Figure 2</ref>. We find that using cross-sentence context clearly improves both entity and relation F1. However, we find the relation performance doesn not further increase from W = 100 to W = 300. In our final models, we use W = 300 for the entity model and W = 100 for the relation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a simple and effective approach for end-to-end relation extraction. Our model learns two encoders for entity recognition and relation extraction independently and our experiments show that it outperforms previous stateof-the-art on three standard benchmarks considerably. We conduct extensive analyses to undertand the superior performance of our approach and validate the importance of learning distinct contextual representations for entities and relations and using entity information as input features for the relation model. We also propose an efficient approximation, obtaining a large speedup at inference time with a small reduction in accuracy. We hope that this simple model will serve as a very strong baseline and make us rethink the value of joint training in end-to-end relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>We use ACE04, ACE05, and SciERC datasets in our experiments. <ref type="table" target="#tab_4">Table 2</ref> shows the data statistics of each dataset.</p><p>The ACE04 and ACE05 datasets are collected from a variety of domains, such as newswire and online forums. We follow 's preprocessing steps 12 and split ACE04 into 5 folds and ACE05 into train, development, and test sets.</p><p>The SciERC dataset is collected from 12 AI conference/workshop proceedings in four AI communities <ref type="bibr" target="#b18">(Luan et al., 2018)</ref>. SciERC includes annotations for scientific entities, their relations, and coreference clusters. We ignore the coreference annotations in our experiments. We use the processed dataset which is downloaded from the project website 13 of <ref type="bibr" target="#b18">Luan et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We implement our models based on Hugging-Face's Transformers library <ref type="bibr" target="#b30">(Wolf et al., 2019)</ref>. For the entity model, we follow  and set the width embedding size as d F = 150 and use a 2-layer FFNN with 150 hidden units and ReLU activations to predict the probability distribution of entity types: P e (e | s i ) = softmax(W e FFNN(h e (s i )).</p><p>For the relation model, we use a linear classifier on top of the span pair representation to predict the probability distribution of relation types: P r (r|s i , s j ) = softmax(W r h r (s i , s j )). For our approximation model (Section 4.3), we batch candidate pairs by adding 4 markers for each pair to the end of the sentence, until the total number of tokens exceeds 250. We train our models with Adam optimizer of a linear scheduler with a warmup ratio of 0.1. For all the experiments, we train the entity model for 100 epochs, and a learning rate of 1e-5 for weights in pre-trained LMs, 5e-4 for others and a batch size of 16. We train the relation model for 10 epochs with a learning rate of 2e-5 and a batch size of 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Performance with Varying Data Sizes</head><p>We compare our pipeline model to a joint model with 10%, 25%, 50%, 100% of training data on <ref type="bibr">12</ref> We use the script provided by   the ACE05 dataset. Here, our goal is to understand whether our finding still holds when the training data is smaller (and hence it is expected to have more errors in entity predictions).</p><p>Our baseline of joint model is our reimplementation of DYGIE++ , without using propagation layers (the encoders are shared for the entity and relation model and no input marker is used; the top scoring 0.4n entities are considered in beam pruning). As shown in <ref type="table" target="#tab_12">Table 7</ref>, we find that our model achieves even larger gains in relation F1 over the joint model, when the number of training examples is reduced. This further highlights the importance of explicitly encoding entity boundaries and type features in data-scarce scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Effect of different context window sizes, measured on the ACE05 development set with the BERTbase model. We use the same entity model (an entity model with W = 300) to report the relation F1 scores (boundaries).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2010.12812v2 [cs.CL] 23 Mar 2021</figDesc><table><row><cell cols="2">(a) Entity model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell>Task</cell></row><row><cell></cell><cell>mor</cell><cell>##pa</cell><cell cols="7">is a fully implemented par</cell><cell>#ser</cell><cell>for a text -to -speech system</cell></row><row><cell cols="2">(b) Relation model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">HYPONYM-OF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;S:Md&gt;</cell><cell>mor ##pa</cell><cell cols="2">&lt;/S:Md&gt;</cell><cell cols="5">is a fully implemented</cell><cell cols="2">&lt;O:Md&gt;</cell><cell>par</cell><cell>#ser</cell><cell>&lt;/O:Md&gt;</cell><cell>for a text -to -speech system</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">USED-FOR</cell></row><row><cell cols="6">mor ##pa is a fully implemented</cell><cell>&lt;S:Md&gt;</cell><cell>par</cell><cell cols="2">#ser</cell><cell>&lt;/S:Md&gt;</cell><cell>for a</cell><cell>⟨O:Tk&gt;</cell><cell>text -to -speech</cell><cell>&lt;/O:Tk&gt;</cell><cell>system</cell></row><row><cell cols="7">(c) Relation model with batch computations</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HYPONYM-OF</cell><cell>USED-FOR</cell></row><row><cell cols="5">mor ##pa is a fully implemented par</cell><cell>#ser</cell><cell cols="5">for a text -to -speech system</cell><cell>[S:Md] [/S:Md]</cell><cell>[O:Md] [/O:Md] [S:Md] [/S:Md]</cell><cell>[O:Tk] [/O:Tk]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The statistics of the datasets. We use ACE04, ACE05, and SciERC for evaluating end-to-end relation extraction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Approx. (single) 65.7 384.7 47.0 301.1</figDesc><table><row><cell>Model</cell><cell cols="2">ACE05 Rel Speed</cell><cell cols="2">SciERC Rel Speed</cell></row><row><cell></cell><cell cols="4">(F1) (sent/s) (F1) (sent/s)</cell></row><row><cell>Full (single)</cell><cell>66.7</cell><cell>32.1</cell><cell>48.2</cell><cell>34.6</cell></row><row><cell>Full (cross)</cell><cell>67.7</cell><cell>14.7</cell><cell>50.1</cell><cell>19.9</cell></row><row><cell cols="5">Approx. (cross) 66.5 237.6 48.8 194.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>We compare our full relation model and the approximation model in both accuracy and speed. The accuracy is measured as the relation F1 (boundaries) on the test set. These results are obtained using BERTbase for ACE05 and SciBERT for SciERC in both single-sentence and cross-sentence settings. The speed is measured on a single NVIDIA GeForce 2080 Ti GPU with a batch size of 32.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Relation F1 (boundaries) scores when entity and relation encoders are shared and not shared on the ACE05 development set. This result is obtained from BERT-base models with cross-sentence context.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: We compare relation F1 (boundaries) with</cell></row><row><cell>different training strategies on the development sets of</cell></row><row><cell>ACE05 and SciERC. This result is from training BERT-</cell></row><row><cell>base and SciBERT models with cross-sentence context.</cell></row><row><cell>typed: typed markers, untyped: untyped markers, un-</cell></row><row><cell>typed + eloss: untyped markers with auxiliary entity</cell></row><row><cell>loss. See text for more details.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>: https://github.com/luanyi/DyGIE/tree/ master/preprocessing. 13 http://nlp.cs.washington.edu/sciIE/ Training data Ours Joint Ent Rel Ent Rel 10% 82.0 46.9 81.5 37.0 25% 84.9 57.6 84.6 49.0 50% 85.5 61.9 86.2 57.7 100% 87.2 63.4 87.4 61.0</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>F1 scores on ACE05 development set when only a subset of training samples (10%, 25%, 50%, or 100%) are provided.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code and models are publicly available at https: //github.com/princeton-nlp/PURE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is an orthogonal contribution to ours and we will explore it for future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our final model indeed only considers ei, ej = . We have explored strategies using spans which are predicted as for the relation model but didn't find improvement. See Section 5.3 for more discussion.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">They also incorporated coreferences and event prediction in their framework. We focus on entity and relation extraction in this paper and we leave these extensions to future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We use a context window size W = 100 for the ALBERT entity models to reduce GPU memory usage.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Note that we only applied this batch computation trick at inference time, because we observed that training with batch computation leads to a slightly (and consistently) worse result. We hypothesize that this is due to the impact of increased batch sizes. We still modified the position embedding and attention masks during training (without batching the instances though).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10"><ref type="bibr" target="#b20">Miwa and Bansal (2016)</ref> observed a slight improvement on entity F1 by sharing the parameters (80.8 → 81.8 F1) on the ACE05 development data. observed that their relation propagation layers improved the entity F1 slightly on SciERC but it hurts performance on ACE05.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">This pruning strategy achieves a recall of 96.7% of gold relations on the development set of ACE05.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Yi Luan for the help with the datasets and evaluation. We thank Howard Chen, Ameet Deshpande, Dan Friedman, Karthik Narasimhan, and the anonymous reviewers for their helpful comments and feedback. This work is supported in part by a Graduate Fellowship at Princeton University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How (not) to train a dependency parser: The curious case of jackknifing part-of-speech taggers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeljko</forename><surname>Agić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="679" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2830" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3606" to="3611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spanlevel model for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpit</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5308" to="5314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A statistical model for multilingual entity detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nicolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factorizing complex models: A case study in mention detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
	<note>Nanda Kambhatla, and Imed Zitouni</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GraphRel: Modeling text as relational graphs for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Hsuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jointly predicting predicates and arguments in neural semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="364" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Going out on a limb: Joint extraction of entity mentions and relations without dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="917" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ALBERT: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1340" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A joint neural model for information extraction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Tjong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint type inference on entities and relations via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1361" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Let&apos;s stop incorrect comparisons in end-to-end relation extraction! In</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Taillé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Guigue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5788" to="5793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two are better than one: Joint entity and relation extraction with tablesequence encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction with global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
