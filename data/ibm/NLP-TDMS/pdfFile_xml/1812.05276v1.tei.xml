<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IPOD: Intensive Point-based Object Detector for Point Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Tencent</roleName><forename type="first">Youtu</forename><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IPOD: Intensive Point-based Object Detector for Point Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel 3D object detection framework, named IPOD, based on raw point cloud. It seeds object proposal for each point, which is the basic element. This paradigm provides us with high recall and high fidelity of information, leading to a suitable way to process point cloud data. We design an end-to-end trainable architecture, where features of all points within a proposal are extracted from the backbone network and achieve a proposal feature for final bounding inference. These features with both context information and precise point cloud coordinates yield improved performance. We conduct experiments on KITTI dataset, evaluating our performance in terms of 3D object detection, Bird's Eye View (BEV) detection and 2D object detection. Our method accomplishes new state-of-the-art , showing great advantage on the hard set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Great breakthrough has been made in 2D image recognition tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9]</ref> with the development of Convolutional Neural Networks (CNNs). Meanwhile, 3D scene understanding with point cloud also becomes an important topic, since it can benefit many applications, such as autonomous driving <ref type="bibr" target="#b12">[13]</ref> and augmented reality <ref type="bibr" target="#b26">[27]</ref>. In this work, we focus on one of the most important 3D scene recognition tasks -that is, 3D object detection based on point cloud, which predicts the 3D bounding box and class label for each object in the scene.</p><p>Challenges Different from RGB images, LiDAR point cloud is with its unique properties. On the one hand, they provide more spatial and structural information including precise depth and relative location. On the other hand, they are sparse, unordered, and even not uniformly distributed, bringing huge challenge to 3D recognition tasks.</p><p>To deploy CNNs, most existing methods convert 3D point clouds to images by projection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref> or voxelize cloud with a fixed grid <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. With the compact representation, CNN is applied. Nevertheless, these hand-crafted representations may not be always optimal regarding the detection performance. Along another line, F-PointNet <ref type="bibr" target="#b28">[29]</ref> crops point cloud in a frustum determined by 2D object detection results. Then a PointNet <ref type="bibr" target="#b29">[30]</ref> is applied on each frustum to produce 3D results. The performance of this pipeline heavily relies on the image detection results. Moreover, it is easily influenced by large occlusion and clutter objects, which is the general weakness of 2D object detectors.</p><p>Our Contribution To address aforementioned drawbacks, we propose a new paradigm based on raw point cloud for 3D object detection. We take each point in the cloud as the element and seed them with object proposals. The raw point cloud, without any approximation, is taken as input to keep sufficient information. This design is general and fundamental for point cloud data, and is able to handle occlusion and clutter scenes.</p><p>We note it is nontrivial to come up with such a solution due to the well-known challenges of heavily redundant proposals and ambiguity on assigning corresponding groundtruth labels. Our novelty is on a proposal generation module to output proposals based on each point and effective selection of representative object proposals with corresponding ground-truth labels to ease network training. Accordingly, the new structure extracts both context and local information for each proposal, which is then fed to a tiny PointNet to infer final results. We evaluate our model on 2D detection, Bird's Eye View (BEV) detection, and 3D detection tasks on KITTI benchmark <ref type="bibr" target="#b0">[1]</ref>. Experiments show that our model outperforms state-of-the-art LIDAR based 3D object detection frameworks especially for difficult examples. Our experiments also surprisingly achieve extremely high recall without the common projection operations. Our primary contribution is manifold.</p><p>• We propose a new proposal generation paradigm for point cloud based object detector. It is a natural and general design, which does not need image detection while yielding much higher recall compared with widely used voxel and projection-based methods. <ref type="figure">Figure 1</ref>. Illustration of our framework. It consists of three different parts. The first is a subsampling network to filter out most background points. The second part is for point-based proposal generation. The third component is the network architecture, which is composed of backbone network, proposal feature generation module and a box prediction network. It classifies and regresses generated proposals.</p><p>• A network structure with input of raw point cloud is proposed to produce features with both context and local information.</p><p>• Experiments on KITTI datasets show that our framework better handles many hard cases with highly occluded and crowded objects, and achieves new stateof-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Semantic Segmentation There have been several approaches to tackle semantic segmentation on point cloud. In <ref type="bibr" target="#b32">[33]</ref>, a projection function converts LIDAR points to a UV map, which is then classified by 2D semantic segmentation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b2">3]</ref> in pixel level. In <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, a multi-view based function produces the segmentation mask. The method fuses information from different views. Other solutions, such as <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref>, segment the point cloud from raw LI-DAR data. They directly generate features on each point while keeping original structural information. Specifically, a max-pooling method gathers the global feature; it is then concatenated with local feature for processing.</p><p>3D Object Detection There are roughly three different lines for 3D object detection. They are voxel-grid based, multi-view based and PointNet based methods. V oxel-grid Method: There are several LIDAR-data based 3D object detection frameworks using voxel-grid representation. In <ref type="bibr" target="#b31">[32]</ref>, each non-empty voxel is encoded with 6 statistical quantities by the points within this voxel. A binary encoding is used in <ref type="bibr" target="#b19">[20]</ref> for each voxel grid. They utilized hand-crafted representation. VoxelNet <ref type="bibr" target="#b34">[35]</ref> instead stacks many VFE layers to generate machine-learned representation for each voxel.</p><p>M ulti-view Method: MV3D <ref type="bibr" target="#b3">[4]</ref> projected LIDAR point cloud to BEV and trained a Region Proposal Network (RPN) to generate positive proposals. Afterwards, it merged features from BEV, image view and front view in order to generate refined 3D bounding boxes. AVOD <ref type="bibr" target="#b18">[19]</ref> improved MV3D by fusing image and BEV features like <ref type="bibr" target="#b22">[23]</ref>. Unlike MV3D, which only merges features in the refinement phase, it also merges features from multiple views in the RPN phase to generate more accurate positive proposals. However, these methods still have the limitation when detecting small objects such as pedestrians and cyclists. They do not handle several cases that have multiple objects in depth direction.</p><p>P ointN et Method: F-PointNet <ref type="bibr" target="#b28">[29]</ref> is the first method of utilizing raw point cloud to predict 3D objects. Initially, a 2D object detection module <ref type="bibr" target="#b10">[11]</ref> is applied to generate frustum proposals. Then it crops points and passes them into an instance segmentation module. At last, it regresses 3D bounding boxes by the positive points output from the segmentation module. Final performance heavily relies on the detection results from the 2D object detector. In contrast, our design is general and effective to utilize the strong representation power of point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Framework</head><p>Our method aims to regress the 3D object bounding box from the easy-to-get point-based object proposals, which is a natural design for point cloud based object detection. To make it feasible, we design new strategies to reduce redundancy and ambiguity existing introduced by trivially seeding proposals for each point. After generating proposals, we extract features for final inference. Our framework is illustrated in <ref type="figure">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Point-based Proposal Generation</head><p>The point cloud gathered from LiDAR is unordered, making it nontrivial to utilize the powerful CNN to achieve good performance. Existing methods mainly project point cloud to different views or divide them into voxels, transforming them into a compact structure. We instead choose a more general strategy to seed object proposals based on each point independently, which is the elementary component in the point cloud. Then we process raw point cloud directly. As a result, precise localization information and a high recall are maintained, i.e., achieving a 96.0% recall on KITTI <ref type="bibr" target="#b11">[12]</ref> dataset.</p><p>Challenges Albeit elegant, point-based frameworks inevitably face many challenges. For example, the amount of points is prohibitively huge and high redundancy exists between different proposals. They cost much computation during training and inference. Also, ambiguity of regression results and assigning ground-truth labels for nearby proposals need to be resolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting Positive Points</head><p>The first step is to filter out background points in our framework. We use a 2D semantic segmentation network named subsampling network to predict the foreground pixels and then project them into point cloud as a mask with the given camera matrix to gather positive points. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the positive points within a bounding box are clustered. We generate proposals at the center of each point with multiple scales, angles and shift, which is illustrated in <ref type="figure">Figure 3</ref>. These proposals can cover most of the points within a car.</p><p>Reducing Proposal Redundancy After background point removal, around 60K proposals are left; but many of them are redundant. We conduct non-maximum suppression (NMS) to remove the redundancy. The score for each proposal is the sum of semantic segmentation scores of interior points, making it possible to select proposals with a relatively large number of points. The intersection-over-union (IoU) value is calculated based on the projection of each proposal to the BEV. With these operations, we reduce the number of effective proposals to around 500 while keeping a high recall.</p><p>Reduction of Ambiguity There are cases that two different proposals contain the same set of points, as shown in <ref type="figure">Figure 4</ref>(a). Since the feature for each proposal is produced using the interior points, these proposals thus possess the same feature representation, leading to the same classification or regression prediction and yet different bounding box regression results. To eliminate this contradiction, we align these two proposals by replacing their sizes and centers with pre-defined class-specific anchor size and the center of the set of interior points. As illustrated in <ref type="figure">Figure 4</ref>(a), the two different proposals A and B are aligned with these steps to proposal C. Another ambiguity lies on assigning target labels to proposals during training. It is not appropriate to assign positive and negative labels considering only IoU values between proposals and ground-truth boxes, as what is performed in 2D object detector. As shown by <ref type="figure">Figure 4(b)</ref>, proposal A contains all points within a ground-truth box and overlaps with this box heavily. It is surely a good positive proposal. Contrarily, proposal B is with a low IoU value and yet contains most of the ground-truth points. With the criterion in 2D detector only considering box IoU, proposal B is negative. Note in our point-based settings, interior points are with much more importance. It is unreasonable to consider the bounding box IoU.</p><p>Our solution is to design a new criterion named PointsIoU to assign target labels. PointsIoU is defined as the quotient between the number of points in the intersection area of both boxes and the number of points in the union area of both boxes. According to PointsIoU, both proposals in <ref type="figure">Figure 4</ref>(b) are now positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>Accurate object detection requires the network to be able to produce correct class label with precise localization information for each instance. As a result, our network needs to be aware of context information to help classification and utilize fine-grained location in raw point cloud. Our network takes entire point cloud as input and produces the feature representation for each proposal. As shown in <ref type="figure">Figure  1</ref>, our network consists of a backbone, a proposal feature generation module and a box prediction network.</p><p>Backbone Network The backbone network based on PointNet++ <ref type="bibr" target="#b30">[31]</ref> takes entire point cloud, with each point parameterized by coordinate and reflectance value, i.e., ([x, y, z, r]). The network is composed of a number of set abstraction (SA) levels and feature propagation (FP) layers, effectively gathering local features from neighboring points and enlarging the receptive field for each point. For N × 4 input points, the network outputs the feature map with size N × C where each row represents one point. Computation is shared by all different proposals, greatly reducing computation. Since features are generated from the raw points, no voxel projection is needed. The detail of our backbone network is shown in <ref type="figure" target="#fig_4">Figure 6(a)</ref>.</p><p>Proposal Feature Generation The feature of each proposal has two parts, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The first is cropped from the extracted feature map. Specifically, for each proposal, we randomly select M = 512 points. Then we take corresponding feature vector with size M × C and denoted it as F 1 . With the SA and FP operations in Point-Net++, these features well capture context information.</p><p>Besides this high-level abstraction, point location is also with great importance. Our second part is the proposal feature F 2 , the canonized coordinates of the M selected points. Compared with the original coordinates, the canonized ones are more robust to the geometric transformation. We utilize T-Net, which is one type of supervised Spatial Transformation Network (STN) <ref type="bibr" target="#b15">[16]</ref>, to calculate residuals from proposal center to real object center, denoted as ∆C ctr . The input to T-Net is the concatenation of part 1 of proposal feature F 1 and the M points' corresponding XY Z coordinates, normalized by subtracting the center coordinates of these M points. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, part 2 of proposal feature is the canonized coordinates of these M points, calculated by normalized coordinates subtracting the center shift predicted by T-Net. The final proposal feature is the concatenation of F 1 and F 2 .</p><p>Bounding-Box Prediction Network In this module, for each proposal, we use a small PointNet++ to predict its class, size ratio, center residual as well as orientation. Detailed structure of our prediction network is illustrated in <ref type="figure" target="#fig_4">Figure 6</ref>(b). We utilize 3 SA modules with MLP layers for feature extraction. Then average pooling is used to produce the global feature. Two branches for regression and classification is applied. For size ratio, we directly regress the ratio of ground-truth size and the proposal's, parametrized by (t l , t h , t w ). We further predict the shift (t x , t y , t z ) from refined center by T-Net to ground-truth center. As a result, final center prediction is calculated by C pro +∆C ctr +∆C * ctr , where C pro , ∆C ctr and ∆C * ctr denote the center of proposal, prediction of T-Net and shift from bounding-box prediction network, respectively. For heading angle, we use a hybrid of classification and regression formulation following <ref type="bibr" target="#b28">[29]</ref>. Specifically, we pre-define N a equally split angle bins and classify the proposal's angle into different bins. Residual is regressed with respect to the bin value. N a is set to 12 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions</head><p>We use a multi-task loss to train our network. The loss function is defined as Eq. (1), where L cls is the labeled classification loss. L loc denotes location regression loss, L ang and L cor are angle and corner losses respectively. s i and u i are the predicted semantic score and ground-truth label for proposal i, respectively. N cls and  N pos are the number of proposals and positive samples.</p><formula xml:id="formula_0">L total = 1 N cls i L cls (s i , u i ) + λ 1 N pos i [u i ≥ 1](L loc + L ang + L cor ),<label>(1)</label></formula><p>The Iverson bracket indicator function [u i ≥ 1] reaches 1 when u i ≥ 1 and 0 otherwise. Note that we only determine if each proposal is positive or negative. L cls is simply the softmax cross-entropy loss. We parameterize a proposal p by its center (p x , p y , p z ), size (p l , p h , p w ), angle p θ and its assigned ground truth box g as (g x , g y , g z ), (g l , g h , g w ) and g θ . Location regression loss is composed of T-Net center estimation loss, center residual prediction loss and size residual prediction loss, expressed as</p><formula xml:id="formula_1">L loc = L dis (t ctr , v ctr ) + L dis (t * ctr , v * ctr )+ L dis (t * size , v * size ),<label>(2)</label></formula><p>where L dis is the smooth-l 1 loss. t ctr and t * ctr are predicted center residuals by T-Net and regression network respectively, while v ctr and v * ctr are targets for them. t * size is the predicted size ratio and v * size is the size ratio of groundtruth. The target of our network is defined as</p><formula xml:id="formula_2">   v ctr = g k − p k , k ∈ (x, y, z) v * ctr = g k − p k − t k , k ∈ (x, y, z) v * size = (g k − p k )/p k , k ∈ (l, h, w)<label>(3)</label></formula><p>Angle loss includes orientation classification loss and residual prediction loss as L angle = L cls (t a−cls , v a−cls )+L dis (t a−res , v a−res ), <ref type="bibr" target="#b3">(4)</ref> where t a−cls and t a−res are predicted angle class and residual while v a−cls and v a−res are their targets.</p><p>Corner loss is the distance between the predicted 8 corners and assigned ground truth, expressed as</p><formula xml:id="formula_3">L corner = 8 k=1 P vk − P tk ,<label>(5)</label></formula><p>where P vk and P tk are the location of ground-truth and prediction for point k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on the widely employed KITTI Object Detection Benchmark <ref type="bibr" target="#b0">[1]</ref>. There are 7,481 training images / point clouds and 7,518 test images / point clouds in three categories of Car, Pedestrian and Cyclist. We use the most widely used average precision (AP) metric to compare different methods. During evaluation, we follow the official KITTI evaluation protocol -that is, the IoU threshold is 0.7 for class Car and 0.5 for Pedestrian and Cyclist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Following <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b18">19]</ref>, we train two networks, one for car and the other for both pedestrian and cyclist.</p><p>Positive Points Selection For semantic segmentation, which is used for foreground point selection, we use the Deeplab-v3 <ref type="bibr" target="#b2">[3]</ref> based on X-ception <ref type="bibr" target="#b4">[5]</ref> network trained on Cityscapes <ref type="bibr" target="#b5">[6]</ref> due to the lack of semantic segmentation annotation in KITTI dataset. Pixels labeled as car, rider, person or bicycle are regarded as foreground. There is no class cyclist in Cityscapes. We thus combine bicycle and rider segmentation masks for cyclist. After getting positive masks, we project these pixels back to the 3D points and acquire foreground points. To align network input, we randomly choose N points per point cloud from foreground. If positive point number is less than N , we pad to N points from the negative points. During training and testing, we set N = 10K for the car detection model and N = 5K for the pedestrian and cyclist model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anchor Sizes</head><p>We define the anchor size (l a = 3.9, h a = 1.6, w a = 1.6) for car detection model, and sizes (l a = 1.6, h a = 1.6, w a = 0.8) and (l a = 0.8, h a = 1.6, w a = 0.8) for the other model. For each size, we use 2 different angles, and 3 shifts, yielding k = 6 proposals centered at the location of each point, as illustrated in <ref type="figure">Figure 3</ref>. Training Parameters During training, we use ADAM <ref type="bibr" target="#b17">[18]</ref> optimizer with an initial learning rate of 0.001 for the first 90 epochs and then decay the learning rate by 0.1 in every 10 epochs. We train 120 epochs in total. Each batch consists of 8 point clouds evenly distributed on 4 GPU cards. For each input point cloud, we sample 64 proposals, with a ratio of 1:3 for positives and negatives. Our implementation is based on Tensorflow <ref type="bibr" target="#b1">[2]</ref>. During training the car model, a proposal is considered positive if its PointsIoU with a certain ground-truth box is higher than 0.55 and negative if its PointsIoU is less than 0.55 with all ground-truth boxes. The positive and negative PointsIoU thresholds are 0.5 and 0.5 for the pedestrian and cyclist model.</p><p>Data Augmentation Data augmentation is important to prevent overfitting. We have used four point cloud augmentation methods following <ref type="bibr" target="#b34">[35]</ref>. First, all instance objects with their interior points are randomly transformed respectively. For each bounding box, we randomly rotate it by a uniform distribution ∆θ 1 ∈ [−π/3, +π/3] and randomly add a translation (∆x, ∆y, ∆z) to its XY Z value as well as its interior points. Second, each point cloud is flipped along the x-axis in camera coordinate with probability 0.5. Third, we randomly rotate each point cloud around y-axis (up orientation) by a uniformly distributed random variable ∆θ 2 ∈ [−π/4, +π/4]. Finally, we apply a global scaling to point cloud with a random variable drawn from uniform distribution [0.9, 1.1]. During training, we randomly use one of these four augmentations for each point cloud.</p><p>Post-process After predicting the bounding box for each proposal, we use a NMS layer with IoU threshold 0.01 to remove overlapped detection. The input scores to NMS are classification probabilities, and the predicted bounding boxes are projected to BEV before performing NMS.</p><p>(a) (c) (b) <ref type="figure">Figure 7</ref>. Illustration of failure cases in Cyclist detection. In this case, we can only get points labeled as rider, which leads to the detection result tending to be Pedestrian rather than Cyclist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on KITTI Test Set</head><p>For evaluation on the test set, we train our model on our own split train/val set at a ratio of 4:1. The performance of our method and its comparison with previous state-of-thearts is listed in <ref type="table" target="#tab_1">Table 1</ref>. Compared to F-PointNet, our model greatly improves the detection accuracy on the hard set by 6.75%, 2.52%, and 4.14% on 2D, BEV and 3D respectively, which indicates that our model can overcome the limitation of 2D detector on cluttered objects.</p><p>Compared to multi-view based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>, ours performs better in Pedestrian prediction by a large margin of 6.12%, 1.87%, and 1.51% on the easy, moderate and hard levels separately. So our strategy is good at detecting dense objects, such as a crowd of people. Compared to VoxelNet <ref type="bibr" target="#b34">[35]</ref>, our model shows superior performance in all classes. We present several detection results in <ref type="figure" target="#fig_5">Figure 8</ref>, where many difficult cases are decently dealt with.</p><p>We note that one current problem, due to the dataset limitation, is the absence of class cyclist in Cityscapes dataset. So it is hard to select foreground points of cyclists, leading to relatively weak performance on this class. Several imperfect semantic masks are shown in <ref type="figure">Figure 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>For ablation studies, we follow VoxelNet <ref type="bibr" target="#b34">[35]</ref> to split the official training set into a train set of 3,717 images and a val set of 3,769 images. Images in train/val set belonging to different video clips. All ablation studies are conducted on the car class due to the relatively larger amount of data to make system run stably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Subsampling Network</head><p>Subsampling network in our model is to eliminate complicated background points so as to reduce computation complexity. We conduct experiments on using 2D segmentation and 3D segmentation as point selection masks. Here, we choose 2D segmentation rather than 2D detection since segmentation results are with much higher quality and more useful on cluttered scenes. As listed in <ref type="table">Table 2</ref>, 2D semantic segmentation is the best choice empirically for our model.</p><p>However, if we use 3D semantic segmentation module instead, either sampling points or taking one part of point cloud as input each time by sliding window is needed, due to the GPU memory limitation. Our observation is that results with 3D segmentation are even with lower quality than those from 2D segmentation. We also compare with the baseline solution without masking foreground points, denoted as 'No Mask'. Still, this baseline performs notably  <ref type="table">Table 2</ref>. 3D object detection AP on KITTI val set. "Random Choice" means randomly sampling points inside a point cloud. "Sliding Window" stands for taking one part of point cloud as input each time and ensembling these detection results. "No Mask" means directly using all points in a point cloud as positive points. "3D" means directly using 3D segmentation module as the subsampling method. "2D" means using 2D image segmentation as the subsampling method.  worse than our full design, manifesting the importance of subsampling network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of PointsIoU Sampling and Context Feature</head><p>PointsIoU sampling is used in the training phase as the criterion to assign positive and negative proposals. As shown in <ref type="table">Table 3</ref>, it significantly promotes our model's performance by 7.3% in hard cases, compared with trivially using bounding box IoU. It is helpful also in suppressing false positive proposals as well as false negative proposals. PointNet++ backbone can extract the context feature for the whole point cloud, which is beneficial to 3D detection. AP in <ref type="table">Table 3</ref> demonstrates its ability, which increases by around 15% on easy set, 20% on both moderate and hard sets.</p><p>Effects of Proposal Feature As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, our proposal feature is composed of high-level abstraction feature and canonized coordinates. We investigate the effect of using different components in the point pooling feature. As shown in  <ref type="table" target="#tab_5">Table 5</ref>. Effect of using different components of proposal feature. A tick in "high-level" item means using points feature with highlevel abstraction and context information from the backbone network. A tick in "normalized" item means we simply use the coordinates normalized by points center as the second part of proposal feature. Confirmation on the "canonized" item means the second part proposal feature is the canonized XY Z value instead of the normalized XY Z.  <ref type="table">Table 6</ref>. 3D and BEV detection AP on KITTI val set of our model for "Car" compared to other state-of-the-art methods. and canonized coordinates enhance the model's capability greatly and is thus adopted in our final structure.</p><p>Result on Validation Our results on KITTI validation set including "Car", "Pedestrian" and "Cyclist" are shown in <ref type="table">Table 4</ref>, with all components included. Our results on validation set compared to other state-of-the-art methods are listed in <ref type="table">Table 6</ref>. Compared to F-PointNet, our model performs notably better on small objects with only a few points, which brings great benefit to detection on hard set. Moreover, compared to multi-view-based methods like <ref type="bibr" target="#b18">[19]</ref>, our method also yields intriguing 3D object detection performance. It is because our model is more sensitive to structure and appearance details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>We have proposed a new method operating on raw points. We seed each point with proposals, without loss of precious localization information from point cloud data. Then prediction for each proposal is made on the proposal feature with context information captured by large receptive field and point coordinates that keep accurate shape information. Our experiments have shown that our model outperforms state-of-the-art 3D detection methods in hard set by a large margin, especially for those high occlusion or crowded scenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of point-based proposal generation. (a) Semantic segmentation result on the image. (b) Projected segmentation result on point cloud. (c) Point-based proposals on positive points after NMS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Illustration of proposal generation on each point from BEV. We totally generate 6 proposals based on 2 different anchors with angle of 0 or 90 degree. For each fundamental anchor, we use 3 different shifts along horizontal axis at ratios of -0.5, 0 and 0.5. Illustration of paradox situations. (a) Different proposals with the same output. (b) True positive proposal assigned to a negative label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of proposal feature generation module. It combines location information and context feature to generate offsets from the centroid of interior points to the center of target instance object. The predicted residuals are added back to the location information in order to make feature more robust to geometric transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Illustration of two network architectures. (a) Backbone architecture. It takes a raw point cloud (x, y, z, r) as input, and extracts both local and global features for each point by stacking SA layers and FP modules. (b) Bounding-box prediction network. It takes the feature from proposal feature generation module as input and produces classification and regression prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Visualizations of our results on KITTI test set. In each image, a box detected as car, pedestrian or cyclist is in yellow, red or green respectively. The top row in each image is 3D object detection results projected on to the RGB images. The middle is 2D semantic segmentation results of images. The bottom is 3D object detection results in the LIDAR phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance on KITTI test set for both Car, Pedestrian and Cyclists.</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Easy</cell><cell>AP 2D (%) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP BEV (%) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP 3D (%) Moderate</cell><cell>Hard</cell></row><row><cell>MV3D [4]</cell><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>86.02</cell><cell>76.90</cell><cell>68.49</cell><cell>71.09</cell><cell>62.35</cell><cell>55.12</cell></row><row><cell>AVOD [19]</cell><cell></cell><cell>89.73</cell><cell>88.08</cell><cell>80.14</cell><cell>86.80</cell><cell>85.44</cell><cell>77.73</cell><cell>73.59</cell><cell>65.78</cell><cell>58.38</cell></row><row><cell>VoxelNet [35] F-PointNet [29]</cell><cell>Car</cell><cell>N/A 90.78</cell><cell>N/A 90.00</cell><cell>N/A 80.80</cell><cell>89.35 88.70</cell><cell>79.26 84.00</cell><cell>77.39 75.33</cell><cell>77.47 81.20</cell><cell>65.11 70.39</cell><cell>57.73 62.19</cell></row><row><cell>AVOD-FPN [19]</cell><cell></cell><cell>89.99</cell><cell>87.44</cell><cell>80.05</cell><cell>88.53</cell><cell>83.79</cell><cell>77.90</cell><cell>81.94</cell><cell>71.88</cell><cell>66.38</cell></row><row><cell>Ours</cell><cell></cell><cell>90.20</cell><cell>89.30</cell><cell>87.37</cell><cell>86.93</cell><cell>83.98</cell><cell>77.85</cell><cell>79.75</cell><cell>72.57</cell><cell>66.33</cell></row><row><cell>AVOD [19]</cell><cell></cell><cell>51.64</cell><cell>43.49</cell><cell>37.79</cell><cell>42.51</cell><cell>35.24</cell><cell>33.97</cell><cell>38.28</cell><cell>31.51</cell><cell>26.98</cell></row><row><cell>VoxelNet [35]</cell><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>46.13</cell><cell>40.74</cell><cell>38.11</cell><cell>39.48</cell><cell>33.69</cell><cell>31.51</cell></row><row><cell>F-PointNet [29]</cell><cell>Pedestrian</cell><cell>87.81</cell><cell>77.25</cell><cell>74.46</cell><cell>58.09</cell><cell>50.22</cell><cell>47.20</cell><cell>51.21</cell><cell>44.89</cell><cell>40.23</cell></row><row><cell>AVOD-FPN [19]</cell><cell></cell><cell>67.32</cell><cell>58.42</cell><cell>57.44</cell><cell>58.75</cell><cell>51.05</cell><cell>47.54</cell><cell>50.80</cell><cell>42.81</cell><cell>40.88</cell></row><row><cell>Ours</cell><cell></cell><cell>73.28</cell><cell>63.07</cell><cell>56.71</cell><cell>60.83</cell><cell>51.24</cell><cell>45.40</cell><cell>56.92</cell><cell>44.68</cell><cell>42.39</cell></row><row><cell>AVOD [19]</cell><cell></cell><cell>65.72</cell><cell>56.01</cell><cell>48.89</cell><cell>63.66</cell><cell>47.74</cell><cell>46.55</cell><cell>60.11</cell><cell>44.90</cell><cell>38.80</cell></row><row><cell>VoxelNet [35]</cell><cell></cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>66.70</cell><cell>54.76</cell><cell>50.55</cell><cell>61.22</cell><cell>48.36</cell><cell>44.37</cell></row><row><cell>F-PointNet [29]</cell><cell>Cyclist</cell><cell>84.90</cell><cell>72.25</cell><cell>65.14</cell><cell>75.38</cell><cell>61.96</cell><cell>54.68</cell><cell>71.96</cell><cell>56.77</cell><cell>50.39</cell></row><row><cell>AVOD-FPN [19]</cell><cell></cell><cell>68.65</cell><cell>59.32</cell><cell>55.82</cell><cell>68.09</cell><cell>57.48</cell><cell>50.77</cell><cell>64.00</cell><cell>52.18</cell><cell>46.61</cell></row><row><cell>Ours</cell><cell></cell><cell>82.90</cell><cell>65.28</cell><cell>57.63</cell><cell>77.10</cell><cell>58.92</cell><cell>51.01</cell><cell>71.40</cell><cell>53.46</cell><cell>48.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>3D object detection AP on KITTI val set. A tick in the "Points IoU" item means we use Points IoU. Otherwise, we use original bounding box IoU for assigning positive and negative proposals as an alternative. A tick in the "Context Feature" item means we use the PointNet++ backbone to extract context feature for the whole point cloud. 3D object detection AP and BEV detection AP on KITTI val set of our model for cars, pedestrians and cyclists.</figDesc><table><row><cell>Benchmark</cell><cell cols="3">Easy Moderate Hard</cell></row><row><cell>Car (3D Detection)</cell><cell>84.1</cell><cell>76.4</cell><cell>75.3</cell></row><row><cell>Car (BEV)</cell><cell>88.3</cell><cell>86.4</cell><cell>84.6</cell></row><row><cell cols="2">Pedestrian (3D Detection) 69.6</cell><cell>62.3</cell><cell>54.6</cell></row><row><cell>Pedestrian (BEV)</cell><cell>72.4</cell><cell>67.8</cell><cell>59.7</cell></row><row><cell>Cyclist (3D Detection)</cell><cell>81.9</cell><cell>57.1</cell><cell>54.6</cell></row><row><cell>Cyclist (BEV)</cell><cell>84.3</cell><cell>61.8</cell><cell>57.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>, combination of high-level feature</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" />
		<title level="m">&quot;kitti 3d object detection benchmark</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DSSD : Deconvolutional single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I. J. Robotics Res</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multiview random forest of local experts combining RGB and LIDAR data for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Villalonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<editor>IV</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pointsift: A sift-like network module for 3d point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcnn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiple 3d object tracking for augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pedestrian detection combining RGB and dense LIDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nunes</surname></persName>
		</author>
		<editor>ICoR</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems XI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent CRF for real-time roadobject segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
