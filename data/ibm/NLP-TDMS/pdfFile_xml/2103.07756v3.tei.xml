<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 LEARNING WITH FEATURE-DEPENDENT LABEL NOISE: A PROGRESSIVE APPROACH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
							<email>zheng.songzhu@stonybrook.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
							<email>mayank.isi@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
							<email>chao.chen.1@stonybrook.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 LEARNING WITH FEATURE-DEPENDENT LABEL NOISE: A PROGRESSIVE APPROACH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of featuredependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels. * Equal contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Addressing noise in training set labels is an important problem in supervised learning. Incorrect annotation of data is inevitable in large-scale data collection, due to intrinsic ambiguity of data/class and mistakes of human/automatic annotators <ref type="bibr">(Yan et al., 2014;</ref><ref type="bibr" target="#b2">Andreas et al., 2017)</ref>. Developing methods that are resilient to label noise is therefore crucial in real-life applications.</p><p>Classical approaches take a rather simplistic i.i.d. assumption on the label noise, i.e., the label corruption is independent and identically distributed and thus is feature-independent. Methods based on this assumption either explicitly estimate the noise pattern <ref type="bibr" target="#b26">(Reed et al., 2014;</ref><ref type="bibr" target="#b25">Patrini et al., 2017;</ref><ref type="bibr" target="#b10">Dan et al., 2019;</ref> or introduce extra regularizer/loss terms <ref type="bibr" target="#b24">(Natarajan et al., 2013;</ref><ref type="bibr" target="#b34">Van Rooyen et al., 2015;</ref><ref type="bibr" target="#b38">Xiao et al., 2015;</ref><ref type="bibr">Zhang &amp; Sabuncu, 2018;</ref><ref type="bibr" target="#b3">Arazo et al., 2019;</ref><ref type="bibr" target="#b28">Shen &amp; Sanghavi, 2019)</ref>. Some results prove that the commonly used losses are naturally robust against such i.i.d. label noise <ref type="bibr" target="#b22">(Manwani &amp; Sastry, 2013;</ref><ref type="bibr" target="#b13">Ghosh et al., 2015;</ref><ref type="bibr" target="#b12">Gao et al., 2016;</ref><ref type="bibr" target="#b14">Ghosh et al., 2017;</ref><ref type="bibr" target="#b6">Charoenphakdee et al., 2019;</ref><ref type="bibr" target="#b15">Hu et al., 2020)</ref>.</p><p>Although these methods come with theoretical guarantees, they usually do not perform as well as expected in practice due to the unrealistic i.i.d. assumption on noise. This is likely because label noise is heterogeneous and feature-dependent. A cat with an intrinsically ambiguous appearance is more likely to be mislabeled as a dog. An image with poor lighting or severe occlusion can be mislabeled, as important visual clues are imperceptible. Methods that can combat label noise of a much more general form are very much needed to address real-world challenges.</p><p>To adapt to the heterogeneous label noise, state-of-the-arts (SOTAs) often resort to a data-recalibrating strategy. They progressively identify trustworthy data or correct data labels, and then train using these data <ref type="bibr" target="#b31">(Tanaka et al., 2018;</ref><ref type="bibr" target="#b20">Lu et al., 2018;</ref><ref type="bibr" target="#b18">Li et al., 2019)</ref>. The models gradually improve as more clean data are collected or more labels are corrected, eventually converging to models of high accuracy. These data-recalibrating methods best leverage the learning power of deep neural nets and achieve superior performance in practice. However, their underlying mechanism remains a mystery. No methods in this category can provide theoretical insights as to why the model Published as a conference paper at ICLR 2021 can converge to an ideal one. Thus, these methods require careful hyperparameter tuning and are hard to generalize.</p><p>In this paper, we propose a novel and principled method that specifically targets the heterogeneous, feature-dependent label noise. Unlike previous methods, we target a much more general family of noise, called Polynomial Margin Diminishing (PMD) label noise. In this noise family, we allow arbitrary noise level except for data far away from the true decision boundary. This is consistent with the real-world scenario; data near the decision boundary are harder to distinguish and more likely to be mislabeled. Meanwhile, a datum far away from the decision boundary is a typical example of its true class and should have a reasonably bounded noise level.</p><p>Assuming this new PMD noise family, we propose a theoretically-guaranteed data-recalibrating algorithm that gradually corrects labels based on the noisy classifier's confidence. We start from data points with high confidence, and correct the labels of these data using the predictions of the noisy classifier. Next, the model is improved using cleaned labels. We continue alternating the label correction and model improvement until it converges. See <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration. Our main theorem shows that with a theory-informed criterion for label correction at each iteration, the improvement of the label purity is guaranteed. Thus the model is guaranteed to improve with sufficient rate through iterations and eventually becomes consistent with the Bayes optimal classifier.</p><p>Beside the theoretical strength, we also demonstrate the power of our method in practice. Our method outperforms others on CIFAR-10/100 with various synthetic noise patterns. We also evaluate our method against SOTAs on three real-world datasets with unknown noise patterns.</p><p>To the best of our knowledge, our method is the first data-recalibrating method that is theoretically guaranteed to converge to an ideal model. The PMD noise family encompasses a broad spectrum of heterogeneous and feature-dependent noise, and better approximates the real-world scenario. It also provides a novel theoretical setting for the study of label noise.</p><p>Related works. We review works that do not assume an i.i.d. label noise. <ref type="bibr" target="#b23">Menon et al. (2018)</ref> generalized the work of <ref type="bibr" target="#b13">(Ghosh et al., 2015)</ref> and provided an elegant theoretical framework, showing that loss functions fulfilling certain conditions naturally resist instance-dependent noise. The method can achieve even better theoretical properties (i.e., Bayes-consistency) with stronger assumption on the clean posterior probability η. In practice, this method has not been extended to deep neural networks. <ref type="bibr" target="#b9">Cheng et al. (2020)</ref> proposed an active learning method for instance-dependent label noise.  The algorithm iteratively queries clean labels from an oracle on carefully selected data. However, this approach is not applicable to settings where kosher annotations are unavailable. Another contemporary work <ref type="bibr">(Chen et al., 2021)</ref> showed that the noise in real-world dataset is unlikely to be i.i.d., and proposed to fix the noisy labels by averaging the network predictions on each instance over the whole training process. While being effective, their method lacks theoretical guarantees.  showed by regulating the topology of a classifier's decision boundary, one can improve the model's robustness against label noise.</p><p>Data-recalibrating methods use noisy networks' predictions to iteratively select/correct data and improve the models. <ref type="bibr" target="#b31">Tanaka et al. (2018)</ref> introduced a joint training framework which simultaneously enforces the network to be consistent with its own predictions and corrects the noisy labels during training.  identified noisy labels as outliers based on their label consistencies with surrounding data. <ref type="bibr" target="#b20">Lu et al. (2018)</ref> used a curriculum learning strategy where the teacher net is trained on a small kosher dataset to determine if a datum is clean; then the learnt curriculum that gives the weight to each datum is fed into the student net for the training and inference. <ref type="bibr">(Yu et al., 2019;</ref><ref type="bibr" target="#b4">Bo et al., 2018)</ref> trained two synchronized networks; the confidence and consistency of the two networks are utilized to identify clean data. <ref type="bibr" target="#b37">Wu et al. (2020)</ref> selected the clean data by investigating the topological structures of the training data in the learned feature space. For completeness, we also refer to other methods of similar design <ref type="bibr" target="#b19">(Li et al., 2017;</ref><ref type="bibr" target="#b33">Vahdat, 2017;</ref><ref type="bibr" target="#b2">Andreas et al., 2017;</ref><ref type="bibr" target="#b32">Thulasidasan et al., 2019;</ref><ref type="bibr" target="#b3">Arazo et al., 2019;</ref><ref type="bibr" target="#b29">Shu et al., 2019;</ref><ref type="bibr">Yi &amp; Wu, 2019)</ref>.</p><p>As for theoretical guarantees, <ref type="bibr" target="#b27">Ren et al. (2018)</ref> proposed an algorithm that iteratively re-weights each data point by solving an optimization problem. They proved the convergence of the training, but provided no guarantees that the model converges to an ideal one. <ref type="bibr" target="#b1">Amid et al. (2019b)</ref> generalized the work of <ref type="bibr" target="#b0">(Amid et al., 2019a)</ref> and proposed a tempered matching loss. They showed that when the final softmax layer is replaced by the bi-tempered loss, the resulting classifier will be Bayes consistent. Zheng et al. (2020) proved a one-shot guarantee for their data-recalibrating method; but the convergence of the model is not guaranteed. Our method is the first data-recalibrating method which is guaranteed to converge to a well-behaved classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>We start by introducing the family of Poly-Margin Diminishing (PMD) label noise. In Section 2.2, we present our main algorithm. Finally, we prove the correctness of our algorithm in Section 3.</p><p>Notations and preliminaries. Although the noise setting and algorithm naturally generalize to multiclass, for simplicity we focus on binary classification. Let the feature space be X . We assume the data (x, y) is sampled from an underlying distribution D on X × {0, 1}. Define the posterior probability η(x) = P[y = 1 | x]. Let τ 0,1 (x) = P[ y = 1 | y = 0, x] and τ 1,0 (x) = P[ y = 0 | y = 1, x] be the noise functions, where y denotes the corrupted label. For example, if a datum x has true label y = 0, it has τ 0,1 (x) chance to be corrupted to 1. Similarly, it has τ 1,0 (x) chance to be corrupted from 1 to 0. Let η(x) = P[ y = 1 | x] be the noisy posterior probability of y = 1 given feature x.</p><p>Let η * (x) = I {η(x)≥ 1 2 } be the (clean) Bayes optimal classifier, where I A equals 1 if A is true, and 0 otherwise. Finally, let f (x) : X → [0, 1] be the classifier scoring function (the softmax output of a neural network in this paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">POLY-MARGIN DIMINISHING NOISE</head><p>We first introduce the family of noise functions τ this paper will address. We introduce the concept of polynomial margin diminishing noise (PMD noise), which only upper bounds the noise τ in a certain level set of η(x), thus allowing τ to be arbitrarily high outside the restricted domain. This formulation not only covers the feature-independent scenario but also generalizes scenarios proposed by <ref type="bibr" target="#b11">(Du &amp; Cai, 2015;</ref><ref type="bibr" target="#b23">Menon et al., 2018;</ref><ref type="bibr" target="#b9">Cheng et al., 2020)</ref>. Definition 1 (PMD noise). A pair of noise functions τ 0,1 (x) and τ 1,0 (x) are polynomial-margin diminishing (PMD), if there exist constants t 0 ∈ (0, 1 2 ), and c 1 , c 2 &gt; 0 such that:</p><formula xml:id="formula_0">τ 1,0 (x) ≤ c 1 [1 − η(x)] 1+c2 ; ∀η(x) ≥ 1 2 + t 0 , and τ 0,1 (x) ≤ c 1 η(x) 1+c2 ; ∀η(x) ≤ 1 2 − t 0 .</formula><p>(1)</p><p>We abuse notation by referring to t 0 as the "margin" of τ . Note that the PMD condition only requires the upper bound on τ to be polynomial and monotonically decreasing in the region where the Bayes classifier is fairly confident. For the region {x : |η(x) − 1 2 | &lt; t 0 }, we allow both τ 0,1 (x) and τ 1,0 (x) to be arbitrary. <ref type="figure" target="#fig_1">Figure 2</ref>(d) illustrates the upper bound (orange curve) and a sample noise function (blue curve). We also show the corrupted data according to this noise function (black points are the clean data whereas red points are the data with corrupted labels).</p><p>The PMD noise family is much more general than existing noise assumptions. For example, the boundary consistent noise (BCN) <ref type="bibr" target="#b11">(Du &amp; Cai, 2015;</ref><ref type="bibr" target="#b23">Menon et al., 2018)</ref> assumes a noise function that monotonically decreases as the data are moving away from the decision boundary. See <ref type="figure" target="#fig_1">Figure  2</ref>(c) for an illustration. This noise is much more restrictive compared to our PMD noise which (1) only requires a monotonic upper bound, and (2) allows arbitrary noise strength in a wide buffer near the decision boundary. <ref type="figure" target="#fig_1">Figure 2</ref>(b) shows a traditional feature-independent noise pattern <ref type="bibr" target="#b26">(Reed et al., 2014;</ref><ref type="bibr" target="#b25">Patrini et al., 2017)</ref>, which assumes τ 0,1 (x) (resp. τ 1,0 (x)) to be a constant independent of x. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">THE PROGRESSIVE CORRECTION ALGORITHM</head><p>Our algorithm iteratively trains a neural network and corrects labels. We start with a warm-up period, in which we train the neural network (NN) with the original noisy data. This allows us to attain a reasonable network before it starts fitting noise <ref type="bibr">(Zhang et al., 2017)</ref>. After the warm-up period, the classifier can be used for label correction. We only correct a label on which the classifier f has a very high confidence. The intuition is that under the noise assumption, there exists a "pure region" in which the prediction of the noisy classifier f is highly confident and is consistent with the clean Bayes optimal classifier η * . Thus the label correction gives clean labels within this pure region. In particular, we select a high threshold θ. If f predicts a different label thanỹ and its confidence is above the threshold, |f (x) − 1/2| &gt; θ, we flip the labelỹ to the prediction of f . We repeatedly correct labels and improve the network until no label is corrected. Next, we slightly decrease the threshold θ, use the decreased threshold for label correction, and improve the model accordingly. We continue the process until convergence. For convenience in theoretical analysis, in the algorithm, we define a continuous increasing threshold T and let θ = 1/2 − T . Our algorithm is summarized in Algorithm 1. We term our algorithm as PLC (Progressive Label Correction). In Section 3, we will show that this iterative algorithm will converge to be consistent with clean Bayes optimal classifier η * (x) for most of the input instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Progressive Label Correction</head><formula xml:id="formula_1">Input: DatasetS = {(x 1 , y 0 1 ), · · · , (x n , y 0 n )}, initial NN f (x), step size β, initial and end thresholds (T 0 , T end ), warm-up m, total round N Output: f f inal (·) 1: T ← T 0 2: θ ← 1/2 − T 0 3: for t ← 1, · · · , N do 4: Train f (x) onS 5: for all (x i , y t−1 i ) ∈S and |f (x i ) − 1 2 | ≥ θ do 6: y t i ← I {f (xi)≥ 1 2 } 7: end for 8: if t ≥ m then 9: θ ← 1/2 − T 10: if ∀i ∈ [1, · · · , n], y t i = y t−1 i then 11: T ← min(T (1 + β), T end ) 12: end if 13: end if 14:S ← {(x 1 , y t 1 ), · · · , (x n , y t n )} 15: end for</formula><p>Generalizing to the multi-class scenario. In multi-class scenario, denote by f i (x) the classifier's prediction probability of label i. Let h x be the classifier's class prediction, i.e., h x = arg max i f i (x).</p><p>We change the f (x) − 1 2 term to the gap between the highest confidence f hx (x) and the confidence on y, f y (x). If the absolute difference between these two confidences is larger than certain threshold θ, then we correct y to h x . In practice, we find using the difference of logarithms will be more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYSIS</head><p>Our analysis focuses on the asymptotic case and answers the following question: Given infinitely many data with corrupted labels, is it possible to learn a reasonably good classifier? We show that if the noise satisfies the arguably-general PMD condition, the answer is yes. Assuming mild conditions on the hypothesis class of the machine learning model and the distribution D, we prove that Algorithm 1 obtains a nearly clean classifier. This reduces the challenge of noisy label learning from a realizable problem into a sample complexity problem. In this work we only focus on the asymptotic case, and leave the sample complexity for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ASSUMPTIONS</head><p>Our first assumption restricts the model to be able to at least approximate the true Bayes classifier η(x). This condition assumes that given a hypothesis class H with sufficient complexity, the approximation gap between a classifier f (x) in this class and η(x) is determined by the inconsistency between the noisy labels and the Bayes optimal classifier. Definition 2 (Level set (α, )-consistency). Suppose data are sampled as (x,ỹ) ∼ D(x,η(x)) and</p><formula xml:id="formula_2">f (x) = arg min h∈H E (x,ỹ)∼D(x,η(x)) Loss(h(x),ỹ). Given ε &lt; 1 2 , we call H is (α, )-consistent if: |f (x) − η(x)| ≤ αE (z,ỹ)∼D(z,η(z)) 1 {ỹz =η * (z)} (z) η(z) − 1 2 ≥ η(x) − 1 2 + . (2)</formula><p>For two input instances z and x such that η(z) &gt; η(x) (and hence the clean Bayes optimal classifier η * (x) has higher confidence at z than it does at x), the indicator function</p><formula xml:id="formula_3">1 {ỹz =η * (z)} z : η(z) − 1 2 ≥ η(x) − 1 2</formula><p>equals to 1 if the label of the more confident point z is inconsistent with η * (x). This condition says that the approximation error of the classifier at x should be controlled by the risk of η * (·) at points z where η * (·) is more confident than it is at x.</p><p>We next define a regularity condition of data distribution which describes the continuity of the level set density function.</p><p>Definition 3 (Level set bounded distribution). Define the margin t(x) = |η(x) − 1 2 | and G(t) be the cdf of t: G(t) = P x∼D (|η(x) − 1 2 | ≤ t). Let g(t) = G (t) be the density function of t. We say the distribution D is (c * , c * )-bounded if for all 0 ≤ t ≤ 1/2, 0 &lt; c * ≤ g(t) ≤ c * . If D is (c * , c * )-bounded, we define the worst-case density-imbalance ratio of D by D := c * c * .</p><p>The above condition enforces the continuity of level set density function. This is crucial in the analysis since such a continuity allows one to borrow information from its neighborhood region so that a clean neighbor can help correct the corrupted label. To simplify the notation, we will omit D in the subscript when we mention . From now on, we will assume:</p><p>Assumption 1. There exist constants α, , c * , c * such that the hypothesis class H is (α, )-consistent and the unknown distribution D is (c * , c * )-bounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MAIN RESULT AND PROOF SKETCH</head><p>In this section we first state our main result, and then present the supporting claims. Complete proofs can be found in the appendix. Our main result below states that if our starting function is trained correctly, i.e., f (x) = arg min h∈H E (x,ỹ)∼D(x,η(x)) Loss(h(x),ỹ), then Algorithm 1 terminates with most of the final labels matching the Bayes optimal classifier labels. In practice, minimizing true risk is not achievable. Instead, the empirical risk is used to estimate true risk, approaching true risk asymptotically. For a scoring function f , we will denote by y f (x) := I(f (x) ≥ 1/2) the label predicted by f . Theorem 1. Under Assumption 1, for any noise τ which is PMD with margin t 0 , define e 0 = max(t 0 , α+ε 1+2α ). Then for the output of Algorithm 1 with f as above and with the following initializations:</p><formula xml:id="formula_4">(1) T 0 &lt; 1 2 − e 0 , (2) m ≥ α ε log( 2T0 1−2e0 ), (3) N ≥ m + 1 β log( T0 3ε ), (4) T end ≤ 3 and (5) ε α ≤ β ≤ 2ε α , we have: P x∼D [y f f inal (x) = η * (x)] ≥ 1 − 3c * .</formula><p>In the remainder of this section we shall assume that the noise τ is PMD with margin t 0 . To prove our result we first define a "pure" level set. We now state a lemma that forms the foundation of our progressive correction algorithm. We show that given a tiny region where the model is reliable, we can move one step forward by trusting the model. Although the improvement is slight in a single round, it empowers a conservatively recursive step in the Algorithm 1. Lemma 1 (One round purity improvement). Suppose Assumption 1 is satisfied, and assume an f such that there exists a pure (e, f, η)-level set with 3 ≤ e &lt; 1 2 .</p><formula xml:id="formula_5">Letη new (x) = y f (x) if |f (x) − 1/2| ≥ e andη(x) if |f (x) − 1/2| &lt; e, and assume f new = arg min h∈H E (x,ỹ)∼D(x,ηnew(x)) Loss(h(x),ỹ). Let e new = min{e|e &gt; 0, L(e, η) is pure for f new }. Then 1 2 − e new ≥ (1 + ε α )( 1 2 − e).</formula><p>The above lemma states that the cleansed region will be enlarged by at least a constant factor. In the following lemma, we justify the functionality of the first m warm-up rounds. Since the initial neural network can behave badly, the region where we can trust the classifier can be very limited. Before starting the flipping procedure in a relatively larger level set, one first needs to expand the initial tiny region 1 2 − e 0 to a constant T 0 . Lemma 2 (Warm-up rounds). Suppose for a given function f 0 there exists a level set L(e 0 , η) which is pure for f 0 . Given T 0 &lt; 1/2, after running Algorithm 1 for m ≥ α ε log( 2T0 1−2e0 ) rounds, there exists a level set L( 1 2 − T 0 , η) that is pure for f 0 . Next we present our final lemma that combines the previous two lemmata. Lemma 3. Suppose Assumption 1 is satisfied, and for a given function f 0 there exists a level set L(e 0 , η) which is pure for f 0 . If one runs Algorithm 1 starting with f 0 and the initializations: (1)</p><formula xml:id="formula_6">T 0 &lt; 1 2 − e 0 , (2) m ≥ α ε log( 2T0 1−2e0 ), (3) N ≥ m + 1 β log( 1−6ε 2T0 ), (4) T end ≤ 1 2 − 3 and (5) ε α ≤ β ≤ 2ε α , then we have P x∼D [y f f inal (x) = η * (x)] ≥ 1 − 3c * .</formula><p>This lemma states that given an initial model that has a reasonably pure super level set, one can manage to progressively correct a large fraction of corrupted labels by running Algorithm 1 for a sufficient long time with carefully chosen parameters. The limit of Algorithm 1 will depend on the approximation ability of the neural network, which is characterized by parameter ε in Definition 2. To prove Theorem 1 using Lemma 3, it suffices to get a model which has a reliable region. This is provably achievable by training with a family of good scoring functions on PMD noisy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate our method on both synthetic and real-world datasets. We first conduct synthetic experiments on two public datasets CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b16">(Krizhevsky et al., 2009</ref>). To synthesize the label noise, we first approximate the true posterior probability η using the confidence prediction of a clean neural network (trained with the original clean labels). We call these original labels raw labels.</p><p>Then we sample y x ∼ η(x) for each instance x. Instead of using raw labels, we use these sampled labels y x as the clean labels, whose posterior probabilities are exactly η(x); and therefore the neural network is the Bayes optimal classifier η * : X → {1, · · · , C}, where C is the number of classes. Note that in multi-class setting, η(x) has a vector output and η i (x) is the i-th element of this vector.</p><p>Noise generation. We consider a generic family of noise. We consider not only feature-dependent noise, but also hybrid noise that consists of both feature-dependent noise and i.i.d. noise.</p><p>For feature-dependent noise, we use three types of noise functions within the PMD noise family. To make the noise challenging enough, for input x we always corrupt label from the most confident category u x to the second confident category s x , according to η(x). Because s x is the class that confuses η * (x) the most, this noise will hurt the network's performance the most. Note that y x is sampled from η(x), which has quite an extreme confidence. Thus we generally assume y x is u x . For each datum x, we only flip it to s x or keep it as u x . The three noise functions are as follows:</p><formula xml:id="formula_7">Type-I : τ ux,sx = − 1 2 [η ux (x) − η sx (x)] 2 + 1 2 , Type-II : τ ux,sx = 1 − [η ux (x) − η sx (x)] 3 , Type-III : τ ux,sx = 1 − 1 3 [η ux (x) − η sx (x)] 3 + [η ux (x) − η sx (x)] 2 + [η ux (x) − η sx (x)] .</formula><p>Notice that the noise level is determined by the η(x) naturally and we cannot control it directly. To change the noise level, we multiply τ ux,sx by a certain constant factor such that the final proportion of noise matches our requirement. For PMD noise only, we test noise levels 35% and 70%, meaning that 35% and 70% of the data are corrupted due to the noise, respectively.</p><p>For i.i.d. noise we follow the convention and adopt the commonly used uniform noise and asymmetric noise <ref type="bibr" target="#b25">(Patrini et al., 2017)</ref>. We artificially corrupt the labels by constructing the noise transition matrix T , where T ij = P ( y = j|y = i) = τ ij defines the probability that a true label y = i is flipped to j. Then for each sample with label i, we replace its label with the one sampled from the probability distribution given by the i-th row of matrix T . We consider two kinds of i.i.d. noise in this work.</p><p>(1) Uniform noise: the true label i is corrupted uniformly to other classes, i.e., T ij = τ /(C − 1) for i = j, and T ii = 1 − τ , where τ is the constant noise level; (2) Asymmetric noise: the true label i is flipped to j or stays unchanged with probabilities T ij = τ and T ii = 1 − τ , respectively.</p><p>Baselines. We compare our method with several recently proposed approaches.  <ref type="bibr">et al., 2020)</ref>. All these methods are generic and handle the label noise without assuming the noise structures. Finally, we also provide the results by standard method, which simply trains the deep network on noisy datasets in a standard manner.</p><p>During training, we use a batch size of 128 and train the network for 180 epochs to ensure the convergence of all methods. We train the network with SGD optimizer, with initial learning rate 0.01. We randomly repeat the experiments 3 times, and report the mean and standard deviation values. Our code is available at https://github.com/pxiangwu/PLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>. <ref type="table" target="#tab_1">Table 1</ref> lists the performance of different methods under three types of feature-dependent noise at noise levels 35% and 70%. We observe that our method achieves the best performance across different noise settings. Moreover, notice that some of the baseline methods' performances are inferior to the standard approach. Possible reasons are that these methods behave too conservatively in dealing with noise. Thus they only make use of a small subset of the original training set, which is not representative enough to grant the model good discriminative ability.</p><p>In <ref type="table" target="#tab_2">Table 2</ref> we show the results on datasets corrupted with a combination of feature-dependent noise and i.i.d. noise, which ends up to real noise levels ranging from 50% to 70% (in terms of the proportion of corrupted labels). I.i.d. noise is overlayed on the feature-dependent noise. Our method outperforms baselines under these more complicated noise patterns. In contrast, when the noise level is high like the cases where we further apply additional 30% and 60% uniform noise, performances of a few baselines deteriorate and become worse than the standard approach.</p><p>We carry out the ablation studies on hyper-parameters θ 0 (determining the initial confidence threshold for label correction, see Algorithm 1) and β (the step size). In Tables 3 and 4, we show that our method is robust against the choice of θ 0 and β up to a wide range. Notice that to compare against the threshold θ 0 , here we are calculating the absolute difference of log f y (x) and log f hx (x). As mentioned in Section 2.2, this operation gives a good performance in practice.    Results on real-world noisy datasets. To test the effectiveness of the proposed method under realworld label noise, we conduct experiments on the Clothing1M dataset <ref type="bibr" target="#b38">(Xiao et al., 2015)</ref>. This dataset contains 1 million clothing images obtained from online shopping websites with 14 categories. The labels in this dataset are quite noisy with an unknown underlying structure. This dataset provides 50k, 14k and 10k manually verified clean data for training, validation and testing, respectively. Following <ref type="bibr" target="#b31">(Tanaka et al., 2018;</ref><ref type="bibr">Yi &amp; Wu, 2019)</ref>, in our experiment we discard the 50k clean training data and evaluate the classification accuracy on the 10k clean data. Also, following <ref type="bibr">(Yi &amp; Wu, 2019)</ref>, we use a randomly sampled pseudo-balanced subset as the training set, which includes about 260k images. We set the batch size 32, learning rate 0.001, and adopt SGD optimizer and use ResNet-50 with weights pre-trained on ImageNet, as in <ref type="bibr" target="#b31">(Tanaka et al., 2018;</ref><ref type="bibr">Yi &amp; Wu, 2019)</ref>.</p><p>We compare our method with the following baselines. (1) Standard; (2) Forward Correction <ref type="bibr" target="#b25">(Patrini et al., 2017)</ref>; (3) D2L ; (4) JO <ref type="bibr" target="#b31">(Tanaka et al., 2018)</ref>; (5) PENCIL (Yi &amp; Wu, 2019); (6) DY <ref type="bibr" target="#b3">(Arazo et al., 2019</ref>); (7) GCE (Zhang &amp; Sabuncu, 2018); (8) SL ; (9) MLNT <ref type="bibr" target="#b18">(Li et al., 2019)</ref>; (10) <ref type="bibr">LRT (Zheng et al., 2020)</ref>. In <ref type="table" target="#tab_5">Table 5</ref> we observe that our method achieves the best performance, suggesting the applicability of our label correction strategy in real-world scenarios.</p><p>Apart from Clothing1M, we also test our method on another smaller dataset, Food-101N <ref type="bibr" target="#b17">(Lee et al., 2018)</ref>. Food-101N is a dataset for food classification, and consists of 310k training images collected from the web. The estimated label purity is 80%. Following <ref type="bibr" target="#b17">(Lee et al., 2018)</ref>, the classification accuracy is evaluated on the Food-101 <ref type="bibr" target="#b5">(Bossard et al., 2014)</ref> testing set, which contains 25k images with curated annotations. We use ResNet-50 pre-trained on ImageNet. We train the network for 30 epochs with SGD optimizer. The batch size is 32 and the initial learning rate is 0.005, which is divided by 10 every 10 epochs. We also adopt simple data augmentation procedures, including random horizontal flip, and resizing the image with a short edge of 256 and then randomly cropping a 224x224 patch from the resized image. We repeat the experiments with 3 random trials and report the mean value and standard deviation. The results are shown in <ref type="table" target="#tab_6">Table 6</ref>. Our method much improves upon the previous approaches.</p><p>Finally, we test our method on a recently proposed real-world dataset, ANIMAL-10N <ref type="bibr" target="#b30">(Song et al., 2019)</ref>. This dataset contains human-labeled online images for 10 animals with confusing appearance. The estimated label noise rate is 8%. There are 50,000 training and 5,000 testing images. Following <ref type="bibr" target="#b30">(Song et al., 2019)</ref>, we use VGG-19 with batch normalization. The SGD optimizer is employed. Also following <ref type="bibr" target="#b30">(Song et al., 2019)</ref>, we train the network for 100 epochs and use an initial learning rate of 0.1, which is divided by 5 at 50% and 75% of the total number of epochs. We repeat the experiments with 3 random trials and report the mean value and standard deviation. As is shown in <ref type="table" target="#tab_7">Table 7</ref>, our method outperforms the existing baselines.   <ref type="bibr" target="#b17">(Lee et al., 2018)</ref> 83.95 PLC (ours) 85.28 ± 0.04 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose a novel family of feature-dependent label noise that is much more general than the traditional i.i.d. noise pattern. Building upon this noise assumption, we propose the first datarecalibrating method that is theoretically guaranteed to converge to a well-behaved classifier. On the synthetic datasets, we show that our method outperforms various baselines under different featuredependent noise patterns subject to our assumption. Also, we test our method on different real-world noisy datasets and observe superior performances over existing approaches. The proposed noise family offers a new theoretical setting for the study of label noise.  </p><formula xml:id="formula_8">+ P z [ỹ z = η * (z), 1 2 − e ≤ η(z) ≤ 1 2 − e + γ] P z [|η(z) − 1 2 | ≥ |η(x) − 1 2 |] ≤ c * γ c * ( 1 2 −e+γ) + P z [ỹ z = η * (z), 1 2 + e ≤ η(z)] P z [|η(z) − 1 2 | ≥ |η(x) − 1 2 |] + P z [ỹ z = η * (z), η(z) ≤ 1 2 − e] P z [|η(z) − 1 2 | ≥ |η(x) − 1 2 |] =0 = c * γ c * ( 1 2 − e + γ)</formula><p>.</p><p>If ε α ( 1 2 − e) ≤ γ ≤ 2ε α ( 1 2 − e), the impurity in super level set 1 2 + e − γ is at most 2 α . The level set (α, ε)-consistency condition implies |f new (x) − η(x)| ≤ 3 for x s.t. e − γ ≤ |η(x) − 1 2 |. If e ≥ 3 , f new (x) will give the same label as η * (x) and thus (e − γ, η)-level set becomes pure for f . Meanwhile, the choice of γ ensures that 1 2 − e new ≥ (1 + ε α )( 1 2 − e). Lemma 2 (Warm-up rounds). Suppose for a given function f 0 there exists a level set L(e 0 , η) which is pure for f 0 . Given T 0 &lt; 1/2, after running Algorithm 1 for m ≥ α ε log( 2T0 1−2e0 ) rounds, there exists a level set L( 1 2 − T 0 , η) that is pure for f 0 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the algorithm using synthetic data. (a) Gaussian blob with clean label (η * (x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of different noise functions. (a) The original data: Gaussian blob with clean labels (by clean label, we refer to the prediction of the Bayes optimal classifier η * (x), not y). Confident region of η (and thus f ) in this case is the place where η(x) is close to 0 or 1. Blue and green dots correspond to different classes. (b) Uniform label noise: each point has an equal probability to be flipped. Red dots are data with corrupted labels; black dots correspond to data that are not corrupted. (c) BCN noise: the level of noise is decreasing as η * (x) becomes confident. (d) PMD noise: noise level (blue) is only upper bounded by diminishing polynomial function when η(x) is higher or lower than certain threshold. The upper bound is shown in solid orange curve. The dashed orange curve means the noise level near the decision boundary is unbounded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Definition 4 (Pure (e, f, η)-level set). A set L(e, η) := {x||η(x) − 1 2 | ≥ e} is pure for f if y f (x) = η * (x) for all x ∈ L(e, η).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(1) GCE (Zhang &amp; Sabuncu, 2018); (2) Co-teaching+ (Yu et al., 2019); (3) SL (Wang et al., 2019); (4) LRT (Zheng</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>One round purity improvement). Suppose Assumption 1 is satisfied, and assume an f such that there exists a pure (e, f, η)-level set with3 ≤ e &lt; 1 2 . Letη new (x) = y f (x) if |f (x) − 1/2| ≥ e andη(x) if |f (x) − 1/2| &lt; e, and assume f new = arg min h∈H E (x,ỹ)∼D(x,ηnew(x)) Loss(h(x),ỹ). Let e new = min{e|e &gt; 0, L(e, η) is pure for f new }. Then 1 2 − e new ≥ (1 + ε α )( 1 2 − e).Proof: We analyze the case where η(x) &gt; 1 2 . The analysis on the other side can be derived similarly. Due to the fact that there exists a level set L(e, η) pure to f , we have e ≤ |η(x) − 1 2 |, ∀x:E (z,ỹ)∼(D,ηnew(z)) 1 {ỹz =η * (z)} (z) η(z)Now consider x where e − γ ≤ |η(x) − 1 2 |. Since the distribution D is (c * , c * )-bounded, we have:E (z,ỹ)∼(D,ηnew(z)) [1 {ỹz =η * (z)} (z)||η(z) =P z [ỹ z = η * (z)||η(z) ỹ z = η * (z), |η(z) − 1 2 | ≥ |η(x) − 1 2 |] P z [|η(z) − 1 2 | ≥ |η(x) − 1 2 |] ≤ P z [ỹ z = η * (z), 1 2 + e − γ ≤ η(z) ≤ 1 2 + e] P z [|η(z) − 1 2 | ≥ |η(x) − 1 2 |]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) on CIFAR-10 and CIFAR-100 under different feature-dependent noise types and levels. The average accuracy and standard deviation over 3 trials are reported. ± 1.12 45.44 ± 0.64 40.30 ± 1.46 41.11 ± 1.92 44.67 ± 3.89 46.04 ± 2.20 Type-III ( 35% ) 76.89 ± 0.79 78.38 ± 0.67 79.18 ± 0.61 78.81 ± 0.29 81.08 ± 0.35 81.50 ± 0.50 Type-III ( 70% ) 43.32 ± 1.00 41.90 ± 0.86 37.10 ± 0.59 38.49 ± 1.46 44.47 ± 1.23 45.05 ± 1.13 ± 0.29 56.70 ± 0.71 58.37 ± 0.18 55.20 ± 0.33 56.74 ± 0.34 60.01 ± 0.43 Type-I ( 70% ) 39.32 ± 0.43 39.53 ± 0.28 40.01 ± 0.71 40.02 ± 0.85 45.29 ± 0.43 45.92 ± 0.61 Type-II ( 35% ) 57.83 ± 0.25 56.57 ± 0.52 58.11 ± 1.05 56.10 ± 0.73 57.25 ± 0.68 63.68 ± 0.29 Type-II ( 70% ) 39.30 ± 0.32 36.84 ± 0.39 37.75 ± 0.46 38.45 ± 0.45 43.71 ± 0.51 45.03 ± 0.50 Type-III ( 35% ) 56.07 ± 0.79 55.77 ± 0.98 57.51 ± 1.16 56.04 ± 0.74 56.57 ± 0.30 63.68 ± 0.29 Type-III ( 70% ) 40.01 ± 0.18 35.37 ± 2.65 40.53 ± 0.60 39.94 ± 0.84 44.41 ± 0.19 44.45 ± 0.62</figDesc><table><row><cell>Dataset</cell><cell>Noise</cell><cell>Standard</cell><cell>Co-teaching+</cell><cell>GCE</cell><cell>SL</cell><cell>LRT</cell><cell>PLC (ours)</cell></row><row><cell></cell><cell>Type-I ( 35% )</cell><cell cols="6">78.11 ± 0.74 79.97 ± 0.15 80.65 ± 0.39 79.76 ± 0.72 80.98 ± 0.80 82.80 ± 0.27</cell></row><row><cell></cell><cell>Type-I ( 70% )</cell><cell cols="6">41.98 ± 1.96 40.69 ± 1.99 36.52 ± 1.62 36.29 ± 0.66 41.52 ± 4.53 42.74 ± 2.14</cell></row><row><cell cols="8">Type-II ( 35% ) Type-II ( 70% ) 45.57 CIFAR-100 76.65 ± 0.57 77.34 ± 0.44 77.60 ± 0.88 77.92 ± 0.89 80.74 ± 0.25 81.54 ± 0.47 CIFAR-10 Type-I ( 35% ) 57.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) on CIFAR-10 and CIFAR-100 under different hybrid noise types and levels. The average accuracy and standard deviation over 3 trials are reported. ± 0.64 75.43 ± 2.96 76.91 ± 0.56 77.14 ± 0.70 76.96 ± 0.45 78.31 ± 0.41 Type-II + 30% Uniform 74.92 ± 0.63 75.19 ± 0.54 75.69 ± 0.21 75.08 ± 0.47 75.94 ± 0.58 80.08 ± 0.37 Type-II + 60% Uniform 64.02 ± 0.66 59.89 ± 0.63 66.39 ± 0.29 66.76 ± 1.60 58.99 ± 1.43 71.21 ± 1.46 Type-II + 30% Asymmetric 74.28 ± 0.39 73.37 ± 0.83 75.30 ± 0.81 75.43 ± 0.42 77.03 ± 0.62 77.63 ± 0.30 Type-III + 30% Uniform 74.00 ± 0.38 77.31 ± 0.11 77.00 ± 0.12 76.22 ± 0.12 75.66 ± 0.57 80.06 ± 0.47 Type-III + 60% Uniform 63.96 ± 0.69 56.78 ± 1.56 67.53 ± 0.51 67.79 ± 0.54 59.36 ± 0.93 73.48 ± 1.84 Type-III + 30% Asymmetric 75.31 ± 0.34 74.62 ± 1.71 75.70 ± 0.91 76.09 ± 0.10 77.19 ± 0.74 77.54 ± 0.70 ± 0.56 52.33 ± 0.64 52.90 ± 0.53 51.34 ± 0.64 45.66 ± 1.60 60.09 ± 0.15 Type-I + 60% Uniform 35.97 ± 1.12 27.17 ± 1.66 38.62 ± 1.65 37.57 ± 0.43 23.37 ± 0.72 51.68 ± 0.10 Type-I + 30% Asymmetric 45.85 ± 0.93 51.21 ± 0.31 52.69 ± 1.14 50.18 ± 0.97 52.04 ± 0.15 56.40 ± 0.34 Type-II + 30% Uniform 49.32 ± 0.36 51.99 ± 0.75 53.61 ± 0.46 50.58 ± 0.25 43.86 ± 1.31 60.01 ± 0.63 Type-II + 60% Uniform 35.16 ± 0.05 25.91 ± 0.64 39.58 ± 3.13 37.93 ± 0.22 23.05 ± 0.99 49.35 ± 1.53 Type-II + 30% Asymmetric 46.50 ± 0.95 51.07 ± 1.44 51.98 ± 0.37 49.46 ± 0.23 52.11 ± 0.46 61.43 ± 0.33 Type-III + 30% Uniform 48.94 ± 0.61 49.94 ± 0.44 52.07 ± 0.35 50.18 ± 0.54 42.79 ± 1.78 60.14 ± 0.97 Type-III + 60% Uniform 34.67 ± 0.16 22.89 ± 0.75 36.82 ± 0.49 37.65 ± 1.42 22.81 ± 0.72 50.73 ± 2.16 Type-III + 30% Asymmetric 45.70 ± 0.12 49.38 ± 0.86 50.87 ± 1.12 48.15 ± 0.90 50.31 ± 0.39 54.56 ± 1.11</figDesc><table><row><cell>Dataset</cell><cell>Noise</cell><cell>Standard</cell><cell>Co-teaching+</cell><cell>GCE</cell><cell>SL</cell><cell>LRT</cell><cell>PLC (ours)</cell></row><row><cell></cell><cell>Type-I + 30% Uniform</cell><cell cols="6">75.26 ± 0.32 78.72 ± 0.53 78.08 ± 0.66 77.79 ± 0.46 75.97 ± 0.27 79.04 ± 0.50</cell></row><row><cell></cell><cell>Type-I + 60% Uniform</cell><cell cols="6">64.25 ± 0.78 55.49 ± 2.11 67.43 ± 1.43 67.63 ± 1.36 59.22 ± 0.74 72.21 ± 2.92</cell></row><row><cell cols="3">CIFAR-10 75.21 CIFAR-100 Type-I + 30% Asymmetric Type-I + 30% Uniform 48.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The effect of θ 0 on the performance.</figDesc><table><row><cell cols="5">We use CIFAR-10 with 35% feature-dependent</cell></row><row><cell cols="2">noise, and set β = 0.1.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>exp (θ 0 )</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>Type-I Noise</cell><cell cols="4">83.33 83.04 82.66 82.94</cell></row><row><cell cols="5">Type-II Noise 81.84 81.18 81.09 81.24</cell></row><row><cell cols="5">Type-III Noise 81.79 81.75 81.98 82.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The effect of β on the performance.</figDesc><table><row><cell cols="5">We use CIFAR-10 with 35% feature-dependent</cell></row><row><cell cols="3">noise, and set exp (θ 0 ) = 0.3.</cell><cell></cell><cell></cell></row><row><cell>β</cell><cell>0.05</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell>Type-I Noise</cell><cell cols="4">83.58 83.04 83.28 83.31</cell></row><row><cell cols="5">Type-II Noise 80.94 81.18 80.98 80.86</cell></row><row><cell cols="5">Type-III Noise 81.91 81.75 82.13 82.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Test accuracy (%) on Clothing1M.</figDesc><table><row><cell>Method</cell><cell cols="3">Standard Forward D2L</cell><cell>JO</cell><cell>PENCIL</cell><cell>DY</cell><cell>GCE</cell><cell>SL</cell><cell>MLNT LRT PLC (ours)</cell></row><row><cell>Accuracy</cell><cell>68.94</cell><cell>69.84</cell><cell cols="2">69.47 72.23</cell><cell>73.49</cell><cell cols="3">71.00 69.75 71.02 73.47 71.74</cell><cell>74.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="2">: Test accuracy (%) on Food-101N.</cell></row><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Standard</cell><cell>81.67</cell></row><row><cell>CleanNet</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="2">: Test accuracy (%) on ANIMAL-10N.</cell></row><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Standard</cell><cell>79.4 ± 0.14</cell></row><row><cell cols="2">SELFIE (Song et al., 2019) 81.8 ± 0.09</cell></row><row><cell>PLC (ours)</cell><cell>83.4 ± 0.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Yan Yan, Rómer Rosales, Glenn Fung, Ramanathan Subramanian, and Jennifer Dy. Learning from multiple annotators with varying expertise. Machine learning, 95(3):291-327, 2014. Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In CVPR, pp. 7017-7025, 2019. Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In ICML, pp. 7164-7173, 2019. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017. Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NeurIPS, pp. 8778-8788, 2018. Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, and Chao Chen. Error-bounded correction of noisy labels. In ICML, pp. 11447-11457, 2020.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. The authors acknowledge support from US National Science Foundation (NSF) awards CRII-1755791, CCF-1910873, CCF-1855760. This effort was partially supported by the Intelligence Advanced Research Projects Agency (IARPA) under the contract W911NF20C0038. The content of this paper does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof: The proof follows from the fact that each round of label flipping improves the purity by a factor of (1 + ε α ). To obtain an at least T 0 pure region, it suffices to repeat the flipping step for m ≥ α ε log( T0 1−2e0 ) rounds. Lemma 3. Suppose Assumption 1 is satisfied, and for a given function f 0 there exists a level set L(e 0 , η) which is pure for f 0 . If one runs Algorithm 1 starting with f 0 and the initializations: (1)</p><p>Proof: The proof can be done by combining Lemma 1 and Lemma 2. In the first m iterations, by Lemma 2, we can guarantee a level set ( 1 2 − T 0 , η) pure to f . In the rest of the iterations we ensure the level set |η(x) − 1 2 | ≥ 1 2 − T is pure. We increase T by a reasonable factor of β to avoid incurring too many corrupted labels while ensuring enough progress in label purification, i.e., ε</p><p>This condition ensures the correctness of flipping when T ≤ 1 2 − 3ε. The purity cannot be improved once T ≥ 1 2 − 3ε = T end since there is no guarantee that f (x) has consistent label with η(x) when |η(x) − 1 2 | &lt; 3ε and |η(x) − f (x)| ≤ 3ε. By (c * , c * )-bounded assumption on D, its mass of impure 3ε level set region is at most 3c * . Theorem 1. Under assumption 1, for any noise τ which is PMD with margin t 0 , define e 0 = max(t 0 , α+ε 1+2α ). Then for the output of Algorithm 1 with f as above and with the following initializations:</p><p>Proof: The proof is based on Lemma 3 plus a verification of the existence of f 0 for which there exists a pure (e 0 , f, η)-level set. Let:</p><p>In the level set |η(x) − 1 2 | ≥ e 0 , P z [ỹ z = η * (z)| |η(z) − 1 2 | ≥ e 0 ] ≤ 1 2 − e 0 + τ (z). By level set (α, ε)-consistency, it suffices to satisfy α( 1 2 − e o + τ ) + ε ≤ e 0 to ensure that f (x) has the same prediction with η(x) when |η(x) − 1 2 | ≥ e 0 . By polynomial level set diminishing noise, we have τ (x) ≤ 1 2 − e 0 if e 0 &gt; t 0 , and thus by choosing e 0 = max(t 0 , α+ε 1+2α ) one can ensure that initial f 0 has a pure (e 0 , f 0 , η)-level set. The rest of the proof follows from Lemma 3.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two-temperature logistic regression based on the tsallis divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manfred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2388" to="2396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust bi-tempered logistic loss based on bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15013" to="15022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alldrin</forename><surname>Veit Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chechik</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krasin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gupta</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abhinav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie Serge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6575" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Quanming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xingrui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niu</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu Weihua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugiyama</forename><surname>Tsang Ivor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8536" to="8546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On symmetric losses for learning from corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nontawat</forename><surname>Charoenphakdee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A topological regularizer for classifiers via persistent homology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2573" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond classconditional assumption: A primary attempt to combat instance-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning with bounded instance-and label-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1789" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Hendrycks Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mazeika</forename><surname>Kimin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mantas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2712" to="2721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modelling class noise with symmetric and asymmetric distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2589" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07526</idno>
		<title level="m">On the resistance of nearest neighbor to random noisy labels</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Making risk minimization tolerant to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomput</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple and effective regularization methods for training on noisily labeled data with generalization guarantee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingli</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/˜kriz/learning-features-2009-TR" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1928" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhengyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leung</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li-Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Sudanthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3361" to="3370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Noise tolerance under risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from binary labels with instance-dependent noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="1561" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2233" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4331" to="4340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with bad training data via iterative trimmed loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5739" to="5748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Metaweight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Combating label noise in deep learning using abstention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohd-Yusof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6234" to="6243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5596" to="5605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8688" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A topological filter for learning with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">L dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6222" to="6233" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
