<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">(AF) 2 -S3Net: Attentive Feature Fusion with Adaptive Feature Selection for Sparse Semantic Segmentation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah Ark&apos;s Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
									<region>Markham, ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Razani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah Ark&apos;s Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
									<region>Markham, ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Taghavi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah Ark&apos;s Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
									<region>Markham, ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enxu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah Ark&apos;s Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
									<region>Markham, ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah Ark&apos;s Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
									<region>Markham, ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">(AF) 2 -S3Net: Attentive Feature Fusion with Adaptive Feature Selection for Sparse Semantic Segmentation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autonomous robotic systems and self driving cars rely on accurate perception of their surroundings as the safety of the passengers and pedestrians is the top priority. Semantic segmentation is one of the essential components of road scene perception that provides semantic information of the surrounding environment. Recently, several methods have been introduced for 3D LiDAR semantic segmentation. While they can lead to improved performance, they are either afflicted by high computational complexity, therefore are inefficient, or they lack fine details of smaller instances. To alleviate these problems, we propose (AF) 2 -S3Net, an end-to-end encoder-decoder CNN network for 3D LiDAR semantic segmentation. We present a novel multibranch attentive feature fusion module in the encoder and a unique adaptive feature selection module with feature map re-weighting in the decoder. Our (AF) 2 -S3Net fuses the voxel-based learning and point-based learning methods into a unified framework to effectively process the large 3D scene. Our experimental results show that the proposed method outperforms the state-of-the-art approaches on the large-scale SemanticKITTI benchmark, ranking 1 st on the competitive public leaderboard competition upon publication.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding of the surrounding environment has been one of the most fundamental tasks in autonomous robotic systems. With the challenges introduced with recent technologies such as self-driving cars, a detailed and accurate understanding of the road scene has become a main part of any outdoor autonomous robotic system in the past few years. To achieve an acceptable level of road scene understanding, many frameworks benefit from image semantic segmentation, where a specific class is predicted for every pixel in the input image, giving a clear perspective of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SalsaNext</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MinkNet42</head><p>Ground Truth <ref type="figure">Figure 1</ref>: Comparison of our proposed method with Sal-saNext <ref type="bibr" target="#b8">[9]</ref> and MinkNet42 <ref type="bibr" target="#b7">[8]</ref> on SemanticKITTI benchmark <ref type="bibr" target="#b2">[3]</ref>.</p><p>Although image semantic segmentation is an important step in realizing self driving cars, the limitations of a vision sensor such as inability to record data in poor lighting conditions, variable sensor sensitivity, lack of depth information and limited field-of-view (FOV) makes it difficult for vision sensors to be the sole primary source for scene understanding and semantic segmentation. In contrast, Light Detection and Ranging (LiDAR) sensors can record accurate depth information regardless of the lighting conditions with high density and frame rate, making it a reliable source of information for critical tasks such as self driving.</p><p>LiDAR sensor generates point cloud by scanning the environment and calculating time-of-flight for the emitted laser beams. In doing so, LiDARs can collect valuable information, such as range (e.g., in Cartesian coordinates) and intensity (a measure of reflection from the surface of the objects). Recent advancement in LiDAR technology makes it possible to generate high quality, low noise and dense scans from desired environments, making the task of scene understanding a possibility using LiDARs. Although rich in information, LiDAR data often comes in an unstructured format and partially sparse at far ranges. These characteristics make the task of scene understating challenging using LiDAR as primary sensor. Nevertheless, research in scene understanding and in specific, semantic segmentation using LiDARs, has seen an increase in the past few years with the availability of datasets such as semanticKITTI <ref type="bibr" target="#b2">[3]</ref>.</p><p>The unstructured nature and partial sparsity of LiDAR data brings challenges to semantic segmentation. However, a great effort has been put by researchers to address these obstacles and many successful methods have been proposed in the literature (see <ref type="bibr">Section 2)</ref>. From real-time methods which use projection techniques to benefit from the available 2D computer vision techniques, to fully 3D approaches which target higher accuracy, there exist a range of methods to build on. To better process LiDAR point cloud in 3D and to overcome limitations such as non-uniform point densities and loss of granular information in voxelization step, we propose (AF) 2 -S3Net, which is built upon Minkowski Engine <ref type="bibr" target="#b7">[8]</ref> to suit varying levels of sparsity in LiDAR point clouds, achieving state-of-the-art accuracy in semantic segmentation methods on SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>. <ref type="figure">Fig. 1</ref> demonstrates qualitative results of our approach compared to Sal-saNext <ref type="bibr" target="#b8">[9]</ref> and MinkNet42 <ref type="bibr" target="#b7">[8]</ref>. We summarize our contributions as,</p><p>• An end-to-end encoder-decoder 3D sparse CNN that achieves state-of-the-art accuracy in semanticKITTI benchmark <ref type="bibr" target="#b2">[3]</ref>;</p><p>• A multi-branch attentive feature fusion module in the encoder to learn both global contexts and local details;</p><p>• An adaptive feature selection module with feature map re-weighting in the decoder to actively emphasize the contextual information from feature fusion module to improve the generalizability;</p><p>• A comprehensive analysis on semantic segmentation and classification performance of our model as opposed to existing methods on three benchmarks, semanticKITTI <ref type="bibr" target="#b2">[3]</ref>, nuScenes-lidarseg <ref type="bibr" target="#b4">[5]</ref>, and ModelNet40 <ref type="bibr" target="#b33">[33]</ref> through ablation studies, qualitative and quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">2D semantic Segmentation</head><p>SqueezeSeg <ref type="bibr" target="#b31">[31]</ref> is one of the first works on LiDAR semantic segmentation using range-image, where LiDAR point cloud projected on a 2D plane using spherical transformation. SqueezeSeg <ref type="bibr" target="#b31">[31]</ref> network is based on an encoder-decoder using Fully Connected Neural Network (FCNN) and a Conditional Random Fields (CRF) as a Recurrent Neural Network (RNN) layer. In order to reduce number of the parameters in the network, SqueezeSeg incorporates "fireModules" from <ref type="bibr" target="#b14">[14]</ref>. In a subsequent work, SqueezeSegV2 <ref type="bibr" target="#b32">[32]</ref> introduced Context Aggregation Module (CAM), a refined loss function and batch normalization to further improve the model. SqueezeSegV3 <ref type="bibr" target="#b34">[34]</ref> stands on the shoulder of <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b14">14]</ref>, adopting a Spatially-Adaptive Convolution (SAC) to use different filters in different locations in relation to the input image. Inspired by YOLOv3 <ref type="bibr" target="#b25">[25]</ref>, RangeNet++ <ref type="bibr" target="#b21">[21]</ref> uses a DarkNet backbone to process a range-image. In addition to a novel CNN, RangeNet++ <ref type="bibr" target="#b21">[21]</ref> proposes an efficient way of predicting labels for the full point cloud using a fast implementation of K-nearest neighbour (KNN).</p><p>Benefiting from a new 2D projection, PolarNet [37] takes on a different approach using a polar Birds-Eye-View (BEV) instead of the standard 2D grid-based BEV projections. Moreover, PolarNet encapsulates the information regarding each polar gird using PointNet, rather than using hand crafted features, resulting in a data-driven feature extraction, a nearest-neighbor-free method and a balanced grid distribution. Finally, in a more successful attempt, Sal-saNext <ref type="bibr" target="#b8">[9]</ref>, makes a series of improvements to the backbone introduced in SalsaNet [1] such as, a new global contextual block, an improved encoder-decoder and Lovász-Softmax loss <ref type="bibr" target="#b3">[4]</ref> to achieve state-of-the-art results in 2D LiDAR semantic segmentation using range-image input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D semantic Segmentation</head><p>The category of large scale 3D perception methods kicked off by early works such as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b39">39]</ref> in which a voxel representation was adopted to capitalize vanilla 3D convolutions. In attempt to process unstructured point cloud directly, PointNet <ref type="bibr" target="#b22">[22]</ref> proposed a Multi-Layer Perception (MLP) to extract features from input points without any voxelization. PointNet++ <ref type="bibr" target="#b24">[24]</ref> which is an extension to the nominal work Pointnet <ref type="bibr" target="#b22">[22]</ref>, introduced sampling at different scales to extract relevant features, both local and global. Although effective for smaller point clouds, Methods rely on Pointnet <ref type="bibr" target="#b22">[22]</ref> and its variations are slow in processing large-scale data.</p><p>Down-sampling is at the core of the method proposed in RandLA-Net <ref type="bibr" target="#b13">[13]</ref>. As down-sampling removes features randomly, a local feature aggregation module is also introduced to progressively increase the receptive field for each 3D point. The two techniques used jointly to achieve both efficiency and accuracy in large-scale point cloud semantic segmentation. In a different approach, Cylinder3D <ref type="bibr" target="#b38">[38]</ref> uses cylindrical grids to partition the raw point cloud. To extract features, authors in <ref type="bibr" target="#b38">[38]</ref> introduced two new CNN blocks. An asymmetric residual block to ensure features related to cuboid objects are being preserved and Dimension-decomposition based Context Modeling in which multiple low-rank contexts are merged to model a high-ranked tensor suitable for 3D point cloud data.</p><p>Authors in KPConv <ref type="bibr" target="#b28">[28]</ref> introduced a new point convolution without any intermediate steps taken in processing point clouds. In essence, KPConv is a convolution operation which takes points in the neighborhood as input and processes them with spatially located weights. Furthermore, a deformable version of this convolution operator was also introduced that learns local shifts to make them adapt to point cloud geometry. Finally, MinkowskiNet <ref type="bibr" target="#b7">[8]</ref> introduces a novel 4D sparse convolution for spatio-temporal 3D point cloud data along with an open-source library to support auto-differentiation for sparse tensors. Overall, where we consider the accuracy and efficiency, voxel-based methods such as MinkowskiNet <ref type="bibr" target="#b7">[8]</ref> stands above others, achieving state-of-the-art results within all sub-categories of 3D semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Hybrid Methods</head><p>Hybrid methods, where a mixture of voxel-based, projection-based and/or point-wise operations are used to process the point cloud, has been less investigated in the past, but with availability of more memory efficient designs, are becoming more successful in producing competitive results. For example, FusionNet <ref type="bibr" target="#b35">[35]</ref> uses a voxel-based MLP, called voxel-based mini-PointNet which directly aggregates features from all the points in the neighborhood voxels to the target voxel. This allows FusionNet <ref type="bibr" target="#b35">[35]</ref> to search neighborhoods with low complexity, processing large scale point cloud with acceptable performance. In another approach, 3D-MiniNet <ref type="bibr" target="#b1">[2]</ref> proposes a learning-based projection module to extract local and global information from the 3D data and then feeds it to a 2D FCNN in order to generate semantic segmentation predictions. In a slightly different approach, MVLidarNet <ref type="bibr" target="#b6">[7]</ref> benefits form range-image LiDAR semantic segmentation to refine object instances in bird's-eye-view perspective, showcasing the applicability of LiDAR semantic segmentation in real-world applications.</p><p>Finally, SPVNAS <ref type="bibr" target="#b27">[27]</ref> builds upon the Minkowski Engine <ref type="bibr" target="#b7">[8]</ref> and designs a hybrid approach of using 4D sparse convolution and point-wise operations to achieve state-ofthe-art results in LiDAR semantic segmentation. To do this, authors in SPVNAS <ref type="bibr" target="#b27">[27]</ref> use a neural architecture search (NAS) <ref type="bibr" target="#b18">[18]</ref> to efficiently design a NN, based on their novel Sparse Point-Voxel Convolution (SPVConv) operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>The sparsity of outdoor-scene point clouds makes it difficult to extract spatial information compared to indoor-scene point clouds with fixed number of points or based on the dense image-based dataset. Therefore, it is difficult to leverage the indoor-scene or image-based segmentation methods to achieve good performance on a large-scale driving scene covering more than 100m with non-uniform point densities. Majority of the LiDAR segmentation methods attempt to either transform 3D LiDAR point cloud into 2D image using spherical projection (i.e., perspective, bird-eye-view) or directly process the raw point clouds. The former approach abandons valuable 3D geometric structures and suffers from information loss due to projection process. The latter approach requires heavy computations and not feasible to be deployed in constrained systems with limited resources. Recently, sparse 3D convolution became popular due to its success on outdoor LiDAR semantic segmentation task. However, out of a few methods proposed in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">27]</ref>, no advanced feature extractors were proposed to enhance the results similar to computer vision and 2D convolutions.</p><p>To overcome this, we propose (AF) 2 -S3Net for Li-DAR semantic segmentation in which a baseline model of MinkNet42 <ref type="bibr" target="#b7">[8]</ref> is transformed into an end-to-end encoderdecoder with attention blocks and achieves stat-of-the-art results. In this Section we first present the proposed network architecture along with its novel components, namely AF2M and AFSM. Then, the network optimization is introduced followed by the training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem statement</head><p>Lets consider a semantic segmentation task in which a LiDAR point cloud frame is given with a set of unordered points (P, L) = ({p i , l i }) with p i ∈ R din and i = 1, ..., N , where N denotes the number of points in an input point cloud scan. Each point p i contains d in input features, i.e., Cartesian coordinates (x, y, z), intensity of returning laser beam (i), colors (R, G, B), etc. Here, l i ∈ R represents the ground truth labels corresponding to each point p i . However, in object classification task, a single class label L is assigned to an individual scene containing P points.</p><p>Our goal is to learn a function F cls (., Φ) parameterized by Φ that assigns a single class label L for all the points in the point cloud or in other words, F seg (., Φ), that assigns a per point labelĉ i to each point p i . To this end, we propose (AF) 2 -S3Net to minimize the difference between the predicted label(s),L andĉ i , and the ground truth class label(s), L and l i , for the tasks of classification and segmentation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>The block diagram of the proposed method, (AF) 2 -S3Net, is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> : Concatenation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attentive Feature Fusion</head><p>Input 3D Point Cloud Output 3D Point Cloud</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Coordinates Point Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attentive Feature Fusion Adaptive Feature Selection</head><p>:Conv (2x2x2) <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b0">1]</ref> :Conv (2x2x2) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">32]</ref> : Concatenation A sparse tensor can be expressed as P s = [C, F ], where C ∈ R N ×M represents the input coordinate matrix with M coordinates and F ∈ R N ×K denotes its corresponding feature matrix with K feature dimensions. In this work, we consider 3D coordinates of points (x, y, z), as our sparse tensor coordinate C, and per point normal features (n x , n y , n z ) along with intensity of returning laser beam (i) as our sparse tensor feature F . Exploiting normal features helps the model to learn additional directional information, hence, the model performance can be improved by differentiating the fine details of the objects. The detailed description of the network architecture is provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point label</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse Linear Excitation</head><formula xml:id="formula_0">Sparse Global Pooling Squeeze [N, 96] [N, 32] [N, 32] [N, 32] c c F ℎ1 ℎ2 ℎ3 ℎ π � ( * , * , * ) = ′ 1 , ′ 2 , ′ 3 π J J 2 3 1</formula><p>Attentive Feature Fusion (AF2M): To better extract the global contexts, AF2M embodies a hybrid approach, covering small, medium and large kernel sizes, which focuses on point-based, medium-scale voxel-based and large-scale voxel-based features, respectively. The block diagram of AF2M is depicted in <ref type="figure" target="#fig_0">Fig. 2 (top-left)</ref>. Principally, the proposed AF2M fuses the featuresx = [x 1 , x 2 , x 3 ] at the corresponding branches using g(·) which is defined as,</p><formula xml:id="formula_1">g(x 1 , x 2 , x 3 ) αx 1 + βx 2 + γx 3 + ∆<label>(1)</label></formula><p>where α, β and γ are the corresponding coefficients that scale the feature columns for each point in the sparse tensor, and are processed by function f enc (·) as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Moreover, the attention residuals, ∆, is introduced to stabi-lize the attention layers h i (·), ∀i ∈ {1, 2, 3}, by adding the residual damping factor. This damping factor is the output of residual convolution layer π. Further, function π can be formulated as π sigmoid(bn(conv(f enc (·))))</p><p>Finally, the output of AF2M is generated by F(g(·)), where F is used to align the sparse tensor scale space with the next convolution block. As illustrated in <ref type="figure" target="#fig_0">Fig. 2 (top-left)</ref>, for each h i , ∀i ∈ {1, 2, 3}, the corresponding gradient of weight w hi can be computed as:</p><formula xml:id="formula_3">w hi = w hi − ∂J ∂g ∂g ∂f enc ∂f enc ∂h i − ∂J ∂g ∂g ∂π ∂π ∂h i<label>(3)</label></formula><p>where J is the output of F. Considering g(·) is a linear function of concatenated featuresx and ∆, we can rewrite Eq. 3 as follows:</p><formula xml:id="formula_4">w hi = w hi − ∂J ∂gx ∂f enc ∂h i − ∂J ∂g ∂π ∂h i<label>(4)</label></formula><p>where ∂fenc ∂hi = S j (δ ij − S j ) is the Jacobian of softmax function S(x) : R N → R N and maps ith input feature column to jth output feature column, and δ is Kronecker delta function where δ i=j = 1 and δ i =j = 0. As shown in Eq. 5,</p><formula xml:id="formula_5">S j (δ ij − S j ) =    −S 2 1 S 1 (1 − S 2 ) . . . . . . . . . S N (1 − S 1 ) . . . −S 2 N   <label>(5)</label></formula><p>when the softmax output S is close to 0, the term ∂fenc ∂hi approaches to zero which prompts no gradient, and when S is close to 1, the gradient is close to identity matrix. As a result, when S → 1, all values in α or β or γ get very high confidence and the update of w hi becomes:</p><formula xml:id="formula_6">w hi = w hi − ∂J ∂g ∂π ∂h i + ∂J ∂gx I<label>(6)</label></formula><p>and in the case of S → 0, the update gradient will only depends on π. <ref type="figure">Fig. 3</ref> further illustrates the capability of the proposed AF2M and highlights the effect of each branch visually.   <ref type="figure">Figure 3</ref>: Illustration of Attentive Feature Fusion and spatial geometry of point cloud. The three labels αx 1 , βx 2 , and γx 3 represent the branches in AF2M encoder block. The first branch, αx 1 , learns to capture and emphasize the fine details for smaller instances such as person, pole and trafficsign across the driving scenes with varying point densities. The shallower branches, βx 2 and γx 3 , learn different attention-maps that focus on global contexts embodied in larger instances such as vegetation, sidewalk and road surface. (best viewed on display)</p><p>Adaptive Feature Selection module (AFSM): The block diagram of AFSM is shown in <ref type="figure" target="#fig_0">Fig. 2 (top-right)</ref>. In AFSM decoder block, the feature maps from multiple branches in AF2M, x 1 , x 2 , and x 3 , are further processed by residual convolution units. The resulted output, x 1 , x 2 , and x 3 , are concatenated, shown as f dec , and are passed into a shared squeeze re-weighting network <ref type="bibr" target="#b12">[12]</ref> in which different feature maps are voted. This module acts like an adaptive dropout that intentionally filters out several feature maps that are not contributing to the final results. Instead of directly passing through the weighted feature maps as output, we employed a damping factor θ = 0.35, to regularize the weighting effect. It is worth noting that the skip connection connecting the attentive feature fusion module branches to the last decoder block, ensures that the error gradient propagates back to the encoder branches for better learning stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Optimization</head><p>We leveraged a linear combination of geo-aware anisotrophic <ref type="bibr" target="#b17">[17]</ref>, Exponential-log loss <ref type="bibr" target="#b30">[30]</ref> and Lovász loss <ref type="bibr" target="#b3">[4]</ref> to optimize our network. In particular, geo-aware anisotrophic loss is beneficial to recover the fine details in a LiDAR scene. Moreover, Exponential-log loss <ref type="bibr" target="#b30">[30]</ref> loss is used to further improve the segmentation performance by focusing on both small and large structures given a highly unbalanced dataset.</p><p>The geo-aware anisotrophic loss can be computed by,</p><formula xml:id="formula_7">L geo (y,ŷ) = − 1 N i,j,k C c=1 M LGA ψ y ijk,c logŷ ijk,c (7)</formula><p>where y andŷ are the ground truth label and predicted label. Parameter N is the local tensor neighborhood and in our experiment, we empirically set it as 5 (a 10 voxels size cube). Parameter C is the semantic classes, <ref type="bibr" target="#b17">[17]</ref>. We normalized local geometric anisotropy within the sliding window Ψ of the current voxel cell p and its neighbor voxel grid q ψ ∈ Ψ.</p><formula xml:id="formula_8">M LGA = Ψ ψ=1 (c p ⊕ c q ψ ), defined in</formula><p>Therefore, the total loss used to train the proposed network is given by,</p><formula xml:id="formula_9">L tot (y,ŷ) = w 1 L exp (y,ŷ) + w 2 L geo (y,ŷ) + w 3 L lov (y,ŷ)<label>(8)</label></formula><p>where w 1 , w 2 , and w 3 denote the weights of Exponentiallog loss <ref type="bibr" target="#b30">[30]</ref>, geo-aware anisotrophic, and Lovász loss, respectively. They are set as 1, 1.5 and 1.5 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>We base our experimental results on three different dataset, namely, SemanticKITTI, nuScenes and ModelNet40 to show the applicability of the proposed methods in different scenes and domains. As for the Se-manticKITTI and ModelNet40, (AF) 2 -S3Net is compared to the previous state-of-the-art, but due to a recently announce challenge for nuScenes-lidarseg dataset <ref type="bibr" target="#b4">[5]</ref>, we provide our own evaluation results against the baseline model.</p><p>To evaluate the performance of the proposed method and compare with others, we leverage mean Intersection over Union (mIoU) as our evaluation metric. mIoU is the most popular metric for evaluating semantic point cloud segmentation and can be formalized as mIoU = As for the training parameters, we trained our model with SGD optimizer with momentum of 0.9 and learning rate of 0.001, weight decay of 0.0005 for 50 epochs. The experiments are conducted using 8 Nvidia V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Evaluation</head><p>In this Section, we provide quantitative evaluation of (AF) 2 -S3Net on two outdoor large-scale public dataset: SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> and nuScenes-lidarseg dataset <ref type="bibr" target="#b4">[5]</ref> for semantic segmentation task and on ModelNet40 <ref type="bibr" target="#b33">[33]</ref> for classification task. SemanticKITTI dataset: we conduct our experiments on SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> dataset, the largest dataset for autonomous vehicle LiDAR segmentation. This dataset is based on the KITTI dataset introduced in <ref type="bibr" target="#b10">[11]</ref>, containing 41000 total frames which captured in 21 sequences. We list our experiments with all other published works in <ref type="table" target="#tab_2">Table 1</ref>. As shown in <ref type="table" target="#tab_2">Table 1</ref>, our method achieves state-of-the-art performance in SemanticKITTI test set in terms of mean IoU. With our proposed method, (AF) 2 -S3Net, we see a 2.7% improvement from the second best method <ref type="bibr" target="#b27">[27]</ref> and 15.4% improvement from baseline model (MinkNet42 <ref type="bibr" target="#b7">[8]</ref>). Our method dominates greatly in classifying small objects such as bicycle, person and motorcycle, making it a reliable solution to understating complex scenes. It is worth noting that (AF) 2 -S3Net only uses the voxelized data as input, whereas the competing methods like SPVNAS <ref type="bibr" target="#b27">[27]</ref> use both voxelized data and point-wise features. Nuscenes dataset: to prove the generalizability of our proposed method, we trained our network with nuSceneslidarseg dataset <ref type="bibr" target="#b4">[5]</ref>, one of the recently available large-scale datasets that provides point level labels of LiDAR point clouds. It consists of 1000 driving scenes from various locations in Boston and Singapore, providing a rich set of la-beled data to advance self driving car technology. Among these 1000 scenes, 850 of them is reserved for training and validation, and the remaining 150 scenes for testing. The labels are, to some extent, similar to the semanticKITTI dataset <ref type="bibr" target="#b2">[3]</ref>, making it a new challenge to propose methods that can handle both datasets well, given the different sensor setups and environment they record the dataset. In Table 2, we compared our proposed method with MinkNet42 <ref type="bibr" target="#b7">[8]</ref> baseline and the projection based method SalsaNext <ref type="bibr" target="#b8">[9]</ref>. Results in <ref type="table" target="#tab_3">Table 2</ref> shows that our proposed method can handle the small objects in nuScenes dataset and indicates a large margin improvement from the competing methods. Considering the large difference between the two public datasets, we can prove that our work can generalize well.</p><p>ModelNet40: to expand and evaluate the capabilities of the proposed method in different applications, ModelNet40, a 3D object classification dataset <ref type="bibr" target="#b33">[33]</ref> is adopted for evaluation. ModelNet40 contains 12, 311 meshed CAD models from 40 different object categories. From all the samples, 9, 843 models are used for training and 2, 468 models for testing. To evaluate our method against existing stat-of-theart, we compare (AF) 2 -S3Net with techniques in which a single input (e.g., single view, sampled point cloud, voxel) has been used to train and evaluate the models. To make (AF) 2 -S3Net compatible for the task of classification, the decoder part of the network is removed and the output of the encoder is directly reshaped to the number of the classes in ModelNet40 dataset. Moreover, the model is trained only using cross-entropy loss. <ref type="table">Table 3</ref> presents the overall classification accuracy results for our proposed method and previous state-of-the-art. With the introduction of AF2M in our network, we achieved similar performance to the pointbased methods which leverage fine-grain local features.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Evaluation</head><p>In this section, we visualize the attention maps in AF2M by projecting the scaled feature maps back to original point cloud. Moreover, to better present the the improvements that has been made against the baseline model MinkNet42 <ref type="bibr" target="#b7">[8]</ref> and SalsaNext <ref type="bibr" target="#b8">[9]</ref>, we provide the error maps which highlights the superior performance of our method.</p><p>As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, our method is capable of capturing fine details in a scene. To demonstrate this, we train (AF) 2 -S3Net on SemanticKITTI as explained above and visualize a test frame. In <ref type="figure" target="#fig_3">Fig. 5</ref> we highlight the points with top 5% feature norm from each scaled feature maps of αx 1 , βx 2 and γx 3 with cyan, orange and green colors, respectively. It can be observed that our model learns to put its attention on small instances (i.e., person, pole, bicycle, etc.) as well as larger instances (i.e., car, region boundaries, etc.). <ref type="figure" target="#fig_2">Fig. 4</ref> shows some qualitative results on SemanticKITTI (top) and nuScenes (bottom) benchmark. It can be observed that the proposed method surpasses the baseline (MinkNet42 <ref type="bibr" target="#b7">[8]</ref>) and range-based SalsaNext <ref type="bibr" target="#b8">[9]</ref> by a large margin, which failed to capture fine details such as cars and vegetation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>To show the effectiveness of the proposed attention mechanisms, namely, AF2M and AFSM introduced in Section 3, along with other design choices such as loss functions, this section is dedicated to a thorough ablation study starting from our baseline model introduced in <ref type="bibr" target="#b7">[8]</ref>. The baseline is MinkNet42 which is a semantic segmentation residual NN model for 3D sparse data. To start off with a well trained baseline, we use Exponential Logarithmic Loss <ref type="bibr" target="#b30">[30]</ref> to train the model which results in 59.8% mIoU accuracy for the validation set on semanticKITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Main operator Overall Accuracy (%) Vox-Net <ref type="bibr" target="#b20">[20]</ref> voxels 3D Operation 83.00 Mink-ResNet50 <ref type="bibr" target="#b7">[8]</ref> voxels  <ref type="table">Table 3</ref>: Classification accuracy results on ModelNet40 dataset <ref type="bibr" target="#b33">[33]</ref>, for input size 1024 × 3.</p><p>Next, we add our proposed AF2M to the baseline model to help the model extract richer features from the raw data. This addition of AF2M improves the mIoU to 65.1%, an increase of 5.3%. In our second study and to show the effectiveness of the AFSM only, we first reduce the AF2M block to only output {x 1 , x 2 , x 3 } (see <ref type="figure" target="#fig_0">Fig. 2</ref> for reference), and then add the AFSM to the model. Adding AFSM shows an increase of 3.5% in mIoU from the baseline. In the last step of improving the NN model, we combine AF2M and AFSM together as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, which result in mIoU of 68.6% and an increase of 8.8% from the baseline model.</p><p>Finally, in our last two experiments, we study the effect of our loss function by adding Lovász loss and the combination of Lovász and geo-aware anisotrophic loss, resulting in mIoU of 70.2% and 74.2%, respectively. The ablation studies presented, shows a series of adequate steps in the design of (AF) 2 -S3Net, proving the steps taken in the design of the proposed model are effective and can be used separately in other NN models to improve the accuracy.  <ref type="table">Table 4</ref>: Ablation study of the proposed method vs baseline evaluated on SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> validation dataset (seq 08).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Distance-based Evaluation</head><p>In this section, we investigate how segmentation is affected by distance of the points to the ego-vehicle. In order to show the improvements, we follow our ablation study and compare (AF) 2 -S3Net and the baseline (MinkNet42) on the SemanticKITTI validation set (seq 8). <ref type="figure" target="#fig_4">Fig. 6</ref> illustrates the mIoU of (AF) 2 -S3Net as opposed to the baseline and SalsaNext w.r.t. the distance to the ego-vehicle's Li-DAR sensors. The results of all the methods get worse by increasing the distance due to the fact that point clouds generated by LiDAR are relatively sparse, especially at large distances. However, the proposed method can produce better results at all distances, making it an effective method to be deployed on autonomous systems. It is worth noting that, while the baseline methods attempt to alleviate the sparsity problem of point clouds by using sparse convolutions in a residual style network, it lacks the necessary encapsulation of features proposed in Section 3 to robustly predict the semantics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">conclusion</head><p>In this paper, we presented an end-to-end CNN model to address the problem of semantic segmentation and classification of 3D LiDAR point cloud. We proposed (AF) 2 -S3Net, a 3D sparse convolution based network with two novel attention blocks called Attentive Feature Fusion Module (AF2M) and Adaptive Feature Selection Module (AFSM), to effectively learn local and global contexts and emphasize the fine detailed information in a given LiDAR point cloud. Extensive experiments on several benchmarks, SemanticKITTI, nuScenes-lidarseg, and Mod-elNet40 demonstrated the ability to capture the local details and the state-of-the-art performance of our proposed model. Future work will include the extension of our method to end-to-end 3D instance segmentation and object detection on large-scale LiDAR point cloud.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of (AF) 2 -S3Net. The top left block is Attentive Feature Fusion Module (AF2M) that aggregates Local and global context using a weighted combination of mutually exclusive learnable masks, α, β, and γ. The top right block illustrates how Adaptive Feature Selection Module (AFSM) uses shared parameters to learn inter relationship between channels across multi-scale feature maps from AF2M. (best viewed on display) processed by (AF) 2 -S3Net which is built upon 3D sparse convolution operations which suits sparse point clouds and effectively predicts a class label for each point given a Li-DAR scan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure X :</head><label>X</label><figDesc>Illustration of Attentive Feature Fusion and spatial geometry of point cloud. The three labels $\alpha x_{1}$, $\beta x_{2}$, and $\gamma x_{3}$ represent the branches in AF2M encoder block. The first branch, $\alpha x_{1}$, learns to capture and emphasize the fine details for smaller instances such as person, pole and trafficsign across the driving scenes with varying point densities. The shallower branches, $\beta x_{2}$ and $\gamma x_{3}$, learn different attention-maps that focus on global contexts embodied in larger instances such as vegetation, sidewalk and road surface</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Compared to SalsaNext and MinkNet42, our method has a lower error (shown in red) recognizing region surface and smaller objects on nuScenes validation set, thanks to the proposed attention modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Reference image (top), Prediction (bottom-right), attention map (bottom-left) on SemanticKITTI test set. Color codes are: road | side-walk | parking | car | bicyclist | pole | vegetation | terrain | trunk | building | other-structure | other-object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>mIoU vs Distance for (AF) 2 -S3Net vs. baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. (AF) 2 -S3Net consists of a residual network based backbone and two novel modules, namely Attentive Feature Fusion module (AF2M) and Adaptive Feature Selection Module (AFSM). The model takes in a 3D LiDAR point cloud and transforms it into sparse tensors containing coordinates and features corresponding to each point. Then, the input sparse tensor is</figDesc><table><row><cell>[ ,</cell><cell>]</cell><cell cols="2">Conv (3x3x3) [4, 64] Conv (4x4x4) [4, 64] Conv (12x12x12) [4, 32]</cell><cell>Conv (4x4x4) [64, 32] Conv (5x5x5) [64, 32]</cell><cell>Conv (8x8x8) [32, 32]</cell><cell>1 2 3</cell><cell cols="2">[N, 32] [N, 32] [N, 32]</cell><cell>α β γ Broadcast × × ×</cell><cell>�</cell><cell>Conv (2x2x2) [32, 32]</cell><cell>1 2 3</cell><cell>Conv (2x2x2) [32, 32] Conv (2x2x2) [32, 32] Conv (2x2x2) [32, 32]</cell><cell></cell><cell>� �</cell><cell>′ 1 ′ 2 ′ 3</cell><cell>[3, 96]</cell><cell>(1-θ)</cell><cell>× �</cell><cell>θ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>( * , * , * ) =</cell><cell>= 1</cell><cell></cell><cell>, ,</cell><cell>=</cell><cell></cell><cell cols="3">: Concatenation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Adaptive Feature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Selection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv</cell><cell></cell><cell cols="2">Conv</cell><cell>Conv</cell><cell cols="2">TrConv</cell><cell>TrConv</cell><cell>TrConv</cell><cell></cell><cell>TrConv</cell><cell>Conv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(3x3x3)</cell><cell></cell><cell cols="2">(3x3x3)</cell><cell>(3x3x3)</cell><cell cols="2">(3x3x3)</cell><cell>(3x3x3)</cell><cell>(3x3x3)</cell><cell></cell><cell>(3x3x3)</cell><cell>(3x3x3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[32, 64]</cell><cell></cell><cell cols="2">[64, 128]</cell><cell>[128, 256]</cell><cell cols="2">[256, 128]</cell><cell>[128, 64]</cell><cell>[64, 96]</cell><cell></cell><cell>[192, 32]</cell><cell>[32, 20]</cell></row><row><cell></cell><cell></cell><cell>[ ,</cell><cell>]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[ ,</cell><cell>]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>×</cell><cell>: Multiplication</cell><cell>�</cell><cell>: Sum</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>51.3 83.8 30.6 43.0 26.0 19.6 8.5 3.4 0.0 92.6 65.3 77.4 30.1 89.7 63.7 83.4 64.3 67.4 58.6 67.1 RangeNet++ [21] 52.2 91.4 25.7 34.4 25.7 23.0 38.3 38.8 4.8 91.8 65.0 75.2 27.8 87.4 58.6 80.5 55.1 64.6 47.9 55.9 -S3Net [Ours] 69.7 94.5 65.4 86.8 39.2 41.1 80.7 80.4 74.3 91.3 68.8 72.5 53.5 87.9 63.2 70.2 68.5 53.7 61.5 71.0 Segmentation IoU (%) results on the SemanticKITTI [3] test dataset. -S3Net [Ours] 83.0 62.2 60.3 12.6 82.3 80.0 20.1 62.0 59.0 49.0 42.2 67.4 94.2 68.0 64.1 68.6 82.9 82.4</figDesc><table><row><cell>Method</cell><cell>Mean IoU</cell><cell>Car</cell><cell>Bicycle</cell><cell>Motorcycle</cell><cell>Truck</cell><cell>Other-vehicle</cell><cell>Person</cell><cell>Bicyclist</cell><cell>Motorcyclist</cell><cell>Road</cell><cell>Parking</cell><cell>Sidewalk</cell><cell>Other-ground</cell><cell cols="2">Building</cell><cell>Fence</cell><cell>Vegetation</cell><cell>Trunk</cell><cell>Terrain</cell><cell>Pole</cell><cell>Traffic-sign</cell></row><row><cell>S-BKI [10]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LatticeNet [26]</cell><cell cols="21">52.9 92.9 16.6 22.2 26.6 21.4 35.6 43.0 46.0 90.0 59.4 74.1 22.0 88.2 58.8 81.7 63.6 63.1 51.9 48.4</cell></row><row><cell>RandLA-Net [13]</cell><cell cols="21">53.9 94.2 26.0 25.8 40.1 38.9 49.2 48.2 7.2 90.7 60.3 73.7 20.4 86.9 56.3 81.4 61.3 66.8 49.2 47.7</cell></row><row><cell>PolarNet [37]</cell><cell cols="21">54.3 93.8 40.3 30.1 22.9 28.5 43.2 40.2 5.6 90.8 61.7 74.4 21.7 90.0 61.3 84.0 65.5 67.8 51.8 57.5</cell></row><row><cell>MinkNet42 [8]</cell><cell cols="21">54.3 94.3 23.1 26.2 26.1 36.7 43.1 36.4 7.9 91.1 63.8 69.7 29.3 92.7 57.1 83.7 68.4 64.7 57.3 60.1</cell></row><row><cell>3D-MiniNet [2]</cell><cell cols="21">55.8 90.5 42.3 42.1 28.5 29.4 47.8 44.1 14.5 91.6 64.2 74.5 25.4 89.4 60.8 82.8 60.8 66.7 48.0 56.6</cell></row><row><cell cols="22">SqueezeSegV3 [34] 55.9 92.5 38.7 36.5 29.6 33.0 45.6 46.2 20.1 91.7 63.4 74.8 26.4 89.0 59.4 82.0 58.7 65.4 49.6 58.9</cell></row><row><cell>Kpconv [28]</cell><cell cols="21">58.8 96.0 30.2 42.5 33.4 44.3 61.5 61.6 11.8 88.8 61.3 72.7 31.6 90.5 64.2 84.8 69.2 69.1 56.4 47.4</cell></row><row><cell>SalsaNext [9]</cell><cell cols="21">59.5 91.9 48.3 38.6 38.9 31.9 60.2 59.0 19.4 91.7 63.7 75.8 29.1 90.2 64.2 81.8 63.6 66.5 54.3 62.1</cell></row><row><cell>FusionNet [35]</cell><cell cols="21">61.3 95.3 47.5 37.7 41.8 34.5 59.5 56.8 11.9 91.8 68.8 77.1 30.8 92.5 69.4 84.5 69.8 68.5 60.4 66.5</cell></row><row><cell>KPRNet [15]</cell><cell cols="21">63.1 95.5 54.1 47.9 23.6 42.6 65.9 65.0 16.5 93.2 73.9 80.6 30.2 91.7 68.4 85.7 69.8 71.2 58.7 64.1</cell></row><row><cell>SPVNAS [27]</cell><cell cols="21">67.0 97.2 50.6 50.4 56.6 58.0 67.4 67.1 50.3 90.2 67.6 75.4 21.8 91.6 66.9 86.1 73.4 71.0 64.3 67.3</cell></row><row><cell>(AF) 2 Method</cell><cell>FW mIoU</cell><cell>Mean IoU</cell><cell>Barrier</cell><cell>Bicycle</cell><cell>Bus</cell><cell>Car</cell><cell>Construction</cell><cell>vehicle</cell><cell>Motorcycle</cell><cell>Pedestrian</cell><cell>Traffic cone</cell><cell>Trailer</cell><cell>Truck</cell><cell>Driveable</cell><cell>surface</cell><cell>Other</cell><cell>flat ground</cell><cell>Sidewalk</cell><cell>Terrain</cell><cell>Manmade</cell><cell>Vegetation</cell></row><row><cell>SalsaNext [9]</cell><cell cols="15">82.8 58.8 56.6 4.7 77.1 81.0 18.4 47.5 52.8 43.5 38.3 65.7 94.2</cell><cell cols="6">60.0 68.9 70.3 81.2 80.5</cell></row><row><cell>MinkNet42 [8]</cell><cell cols="15">82.7 60.8 63.1 8.3 77.4 77.1 23.0 55.1 55.6 50.0 42.5 62.2 94.0</cell><cell cols="6">67.2 64.1 68.6 83.7 80.8</cell></row><row><cell>(AF) 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Segmentation IoU (%) results on the nuScenes-lidarseg<ref type="bibr" target="#b4">[5]</ref> validation dataset. Frequency-Weighted IoU denotes that each IoU is weighted by the point-level frequency of its class.</figDesc><table><row><cell>Attention Map</cell><cell>Prediction</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">n n c=1 T Pc T Pc+F Pc+F Nc , where T P c is the number of true positive points for class c, F P c is the number of false positives, and F N c is the number of false negatives.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Salsanet: Fast road and vehicle segmentation in lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saimir</forename><surname>Eren Erdal Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selcuk</forename><surname>Baci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cavdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV2020)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d-mininet: Learning a 2d representation from point clouds for fast and efficient 3d lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9296" to="9306" />
		</imprint>
	</monogr>
	<note>Sven Behnke, Cyrill Stachniss, and Jürgen Gall</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The lovász-softmax loss: a tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mvlidarnet: Real-time multi-class scene understanding for autonomous driving using multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Oldja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Smolyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Eden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Pehserl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2288" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Salsanext: Fast semantic segmentation of lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><forename type="middle">Erdal</forename><surname>Aksoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="655" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian spatial kernel smoothing for scalable dense semantic mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessy</forename><forename type="middle">W</forename><surname>Grizzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maani</forename><surname>Ryan M Eustice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghaffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="790" to="797" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kprnet: Improving projection-based lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyvid</forename><surname>Kochanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Fatemeh Karimi Nejadasl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Booij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV Workshop</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ggm-net: Graph geometric moments convolution neural network for point cloud shape classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deren</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="124989" to="124998" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth based semantic scene completion with position importance aware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="219" to="226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latticenet: Fast point cloud segmentation using permutohedral lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Radu Alexandru Rosu</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Schütt</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Quenzel</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behnke</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<idno>2020. 7</idno>
	</analytic>
	<monogr>
		<title level="j">Robotics: Science and Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Haotian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Zhijian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Voxsegnet: Volumetric cnns for semantic part segmentation of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d segmentation with exponential logarithmic loss for highly unbalanced object sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Ken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanveer</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Syeda-Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Squeeze-segv3: Spatially-adaptive convolution for efficient pointcloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep fusionnet for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Inteligence</title>
		<meeting>AAAI Conference on Artificial Inteligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Cylinder3d: An effective 3d framework for driving-scene lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01550</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
