<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Trumble</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Creative Intelligence Lab</orgName>
								<orgName type="institution">Adobe Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>GILBERT ET AL: SEMANTIC ESTIMATION OF 3D BODY SHAPE AND POSE 1 https://www.surrey.ac.uk/people/andrew-gilbert https://www.surrey.ac.uk/people/adrian-hilton</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim to simultaneously estimate the 3D articulated pose and high fidelity volumetric occupancy of human performance, from multiple viewpoint video (MVV) with as few as two views. We use a multi-channel symmetric 3D convolutional encoder-decoder with a dual loss to enforce the learning of a latent embedding that enables inference of skeletal joint positions and a volumetric reconstruction of the performance. The inference is regularised via a prior learned over a dataset of view-ablated multi-view video footage of a wide range of subjects and actions, and show this to generalise well across unseen subjects and actions. We demonstrate improved reconstruction accuracy and lower pose estimation error relative to prior work on two MVV performance capture datasets: Human 3.6M and TotalCapture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human performance capture is used extensively within biomechanics and the creative industries. Commercial approaches are typically constrained to skeletal joint estimation in the presence of subject-worn markers captured from multiple viewpoints by specialised (e.g. infra-red) cameras. In this paper, we present a method for video-based performance capture, able to estimate both 3D skeletal pose and shape (volumetric occupancy) of a subject accurately from multiple-viewpoint video (MVV). Uniquely, we do so without a parametric shape model (e.g. SMPL <ref type="bibr" target="#b21">[21]</ref>), and without the need for worn markers or sensors <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b43">42]</ref>, nor a large camera count <ref type="bibr" target="#b7">[8]</ref>. Our approach considers MVV with as few as two wide baseline cameras, motivated by real-world scenarios that may constrain the on-set deployment of c 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1908.03030v2 [cs.CV] 7 Sep 2020 large numbers of witness camera views due to limitations on camera cost or placement (e.g. security or sports events).</p><p>Our technical contribution is to learn a generative model that accepts a coarse poor quality volumetric proxy formed from a low number of wide baseline camera views of a subject. In a single inference step, we estimate both the skeletal joint positions (pose) and refine a higher fidelity volumetric reconstruction from the rough proxy (occupancy).</p><p>Our architecture is a volumetric encoder-decoder convolutional neural network (CNN) in which the latent bottleneck is partially constrained to estimate the 3D skeletal pose and partially unimpeded to enhance the fidelity of volumetric reconstructions derived from just a few wide-baseline camera viewpoints. A joint loss between both outputs is used within a generative adversarial network to ensure the refinement of the volumetric solution to enable it to be perceptually indistinguishable from real high-fidelity reconstructions restoring fine detail such as hands and legs. Unlike prior work that has explored volumetric encoder-decoder networks for pose <ref type="bibr" target="#b39">[38]</ref> or for content up-scaling <ref type="bibr" target="#b11">[12]</ref>, we leverage use of 2D semantic detections to supplement the background occupancy volumetric proxy. The encoder-decoder network serves to learn a prior for human shape, regularised by a generative adversarial network (GAN) loss that ensures realism in the output high-fidelity volumetric reconstruction output and enabling both the pose estimation and reconstruction to be learnt from a minimal set of camera views. The work by Trumble et al <ref type="bibr" target="#b39">[38]</ref> inspires this work, however with significantly improved performance through the introduction of several notable novelties; the inclusion of semantic labels as well as occupancy probabilities in the voxels that make the PVH. The incorporation of a GAN discriminator on the output volume and the extension of the bottleneck of the encoder-decoder with additional latent features besides the body joint coordinates. We demonstrate SOTA results and several ablation studies in the paper which show the value of these contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is inspired by contemporary super-resolution (SR) algorithms that apply learned priors to enhance visual detail in images, volumetric performance capture or reconstruction and human pose estimation (HPE).</p><p>Super-resolution: Classical image restoration / SR approaches combine multiple data sources (e.g. images <ref type="bibr" target="#b10">[11]</ref>, or self-similar patches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b48">47]</ref>) under regularization e.g. total variation <ref type="bibr" target="#b30">[29]</ref>. Convolutional neural network (CNN) autoencoders have been applied to image <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b46">45]</ref> and video-upscaling <ref type="bibr" target="#b31">[30]</ref>. Volumetric SR has been explored for microscopy <ref type="bibr" target="#b0">[1]</ref>, and for multi-spectral sensing <ref type="bibr" target="#b3">[4]</ref>. Recently SR for volumetric performance capture was explored using encoder-decoder networks <ref type="bibr" target="#b11">[12]</ref>.</p><p>Volumetric Performance Reconstruction: Volumetric performance capture pipelines typically use multiple wide baseline viewpoints <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">32]</ref> arranged around the capture volume. More recently, data driven machine learning approaches <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b47">46]</ref> have demonstrated improve reconstruction from a single camera. Varol et al <ref type="bibr" target="#b41">[40]</ref> use a neural network for direct inference of volumetric body shape from a single image. While Jackson et al <ref type="bibr" target="#b40">[39]</ref> directly regress the volumetric representation of the 3D geometry using a standard, spatial, CNN architecture, and Zheng et al <ref type="bibr" target="#b47">[46]</ref> also uses the parametric representation of the SMPL body model <ref type="bibr" target="#b21">[21]</ref> fusing different scales of image features into the 3D space through volumetric feature transformation, to recover accurate surface geometry.</p><p>Human Performance Estimation: There are two distinct categories of HPE; bottom-up data-driven and top-down fitting a model. In general, top-down 2D pose estimation fits a previously defined articulated limb model to data incorporating kinematics into the optimisation to bias toward possible configurations. The model can be user-defined or learnt through a <ref type="figure">Figure 1</ref>: Network architecture. The input is a low fidelity geometric proxy (V L ) from two wide baseline camera views. This proxy is passed through a decoder-encoder to produce a 3D human pose estimate (joint angles J(V L )) via the latent space and to output, a high-fidelity geometric proxy (V H ) regularised via discriminator (D).</p><p>data defined model such as the SMPL Body Model <ref type="bibr" target="#b21">[21]</ref>. Spatio-temporal tracking of pictorial structures is applied to HPE in <ref type="bibr" target="#b19">[19]</ref>, and <ref type="bibr" target="#b1">[2]</ref> explored the fusion of pictorial structures with Ada-Boost shape classification. Malleson et al <ref type="bibr" target="#b22">[22]</ref> used IMUs with a full kinematic solve to adequately estimate 3D pose both indoor and outdoor. Recently, the SMPL model has been employed by several pose estimation techniques with IMUs <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b43">42]</ref> and 2D images <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b34">33]</ref>.</p><p>Bottom-up pose estimation is driven by image parsing to isolate components, Srinivasan et al <ref type="bibr" target="#b32">[31]</ref> used graph-cuts to parse a subset of salient shapes from an image and group these into a model of a person. Ren et al <ref type="bibr" target="#b27">[27]</ref> recursively splits Canny edge contours into segments, classifying each as a putative body part using cues such as parallelism. Ren <ref type="bibr" target="#b26">[26]</ref> also used Bag of Visual Words for implicit pose estimation as part of a pose similarity system for dance video retrieval. In DeepPose, Toshev <ref type="bibr" target="#b36">[35]</ref> used a cascade of convolutional neural networks to estimate 2D pose in images. Elhayek et al <ref type="bibr" target="#b9">[10]</ref> used MVV with a Convnet to produce 2D pose estimations while Rhodin et al <ref type="bibr" target="#b28">[28]</ref> minimised the edge energy inspired by volume ray casting to deduce the 3D pose. Trumble et al <ref type="bibr" target="#b38">[37]</ref> used a flattened MVV based spherical histogram with a 2D convnet to estimate pose. While Pavlakos et al <ref type="bibr" target="#b24">[24]</ref> used a simple volumetric representation in a 3D convnet for pose estimation and Wei et al <ref type="bibr" target="#b45">[44]</ref> performed related work in aligning pairs of joints to estimate 3D human pose. Since detecting pose for each frame individually leads to incoherent and jittery predictions over a sequence, many approaches exploit temporal information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">23]</ref> often using LSTMs <ref type="bibr" target="#b14">[15]</ref>. Trumble et al. <ref type="bibr" target="#b39">[38]</ref> estimate 3D pose using the latent space of a volumetric encoder-decoder, but do not incorporate semantic information nor GAN constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Joint minimal camera Pose and Volume reconstruction</head><p>We present an overview of our process for simultaneously estimating pose and high fidelity occupancy in <ref type="figure">Figure 1</ref>. First, a pre-processing step <ref type="bibr" target="#b13">[14]</ref> reconstructs a coarse Probabilistic Visual Hull (PVH) proxy using a limited number of cameras (Sec. 3.2). For each voxel, we encode a feature reflecting its occupancy and semantic label (e.g. joints) lifted from 2D. This initial estimate (Sec. 3.1) typically contains phantom limbs and sub-volumes. Next, a 3D convolutional encoder-decoder (Sec. 3.3) and generative adversarial network (GAN) (Sec. 3.3.1), learns a deep representation of body shape and the skeletal pose encoding with a dual loss. The feature representation of the PVH (akin to a low-fidelity image in superresolution pipelines), is deeply encoded via a series of convolution layers, embedding the skeletal joint positions in a latent or hidden layer, concatenating the joint estimates with an additional unconstrained feature representation. This latent space enables non-linear mapping decoding to a high fidelity PVH, while the 3D joint estimations are fed to LSTM layers to enforce the temporal consistency of the 3D joints (Sec. 3.3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual Features</head><p>To estimate the pose, we propose to lift 2D visual features to form a 3D voxel features from two distinct modes created from RGB images of each camera view; a 2D foreground occupancy matte and 2D semantic joint detections. The probabilistic occupancy provides a low fidelity shape-based feature, relatively invariant to appearance and clothing, that complements a semantic contextual 2D joint estimate that provides internal feature description. To compute the matte, the difference between the current frame I and a predefined clean plate P approximates pixel occupancy. A thresholded L2 distance between the two images in the HSV colour domain provides a soft occupancy probability. 2D joint belief labels estimated through the approach of Wei <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b45">44]</ref> generate the 2D semantic joint detections, a multi-stage process that iteratively refines the 2D joint estimates based on both the input image and the previous stageâȂŹs returned pixel-wise belief map. At each stage s and for each joint label j the algorithm returns dense per pixel belief maps m j s , which provides the confidence of a joint centre for any given pixel (x, y).</p><formula xml:id="formula_0">M(x, y) = arg max j m j S (x, y)<label>(1)</label></formula><p>The per joint belief maps are maximised over the confidence of all possible joint labels to produce a single label per pixel image M(x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Volumetric Representation</head><p>To construct our data representation consisting of a volume voxel, we use a multi-channel based probabilistic visual hull (PVH). We assume a capture volume observed by a limited number C of camera views c = [1, ..,C] for which extrinsic parameters {R c ,COP c } (camera orientation and focal point) and intrinsic parameters { f c , o x c , o y c } (focal length, and 2D optical centre) are known. An external process, (e.g. a person tracker) isolates the bounding subvolume X I ∈ V corresponding to, and centred upon, a single subject, and which is decimated</p><formula xml:id="formula_1">into voxels V L i = v i x v i y v i z for i = [1, . . . , |V L |]; each voxel is 5mm 3 in size. Each voxel v i ∈ V L projects to coordinates (x[v i ], y[v i ]) in each camera view c.</formula><p>Then given an 2D image denoted as I c , with Φ = [1, . . . , φ ] feature channels (from 2D occupancy/joints), point (x c , y c ) is the point within I c to which V L i projects in a given view:</p><formula xml:id="formula_2">x[V L i ] = f c v i x v i z + o x c and y[V L i ] = f c v i y v i z + o y c ,<label>(2)</label></formula><formula xml:id="formula_3">v i x v i y v i z = COP c − R −1 c V i L .<label>(3)</label></formula><p>The likelihood of the voxel being part of the performer in a given view c is:</p><formula xml:id="formula_4">p(V L i |c) = I c (x[V L i ], y[V L i ], φ ).<label>(4)</label></formula><p>The overall probably of occupancy for a given voxel p(</p><formula xml:id="formula_5">V L i , φ ) is: p(V L i , φ ) = C ∏ i=1 1/(1 + e −p(V L i |c) ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dual Loss Convolutional Volumetric Network</head><p>We propose to learn a deep representation or output given an input tensor V L where V L ∈ R X×Y ×Z×φ , where each dimension encodes the probability of volume occupancy p(X,Y, Z) derived from a PVH obtained using a low camera count (Eq.5) from channels (φ ); foreground occupancy and semantic 2D joint estimates. We wish to train a deep representation to solve the prediction problem V H = F(V L ) for similarly encoded tensor V H ∈ R W ×H×D×φ derived from a higher fidelity PVH of identical dimension obtained using a higher camera count. Where W, H, D, φ are the width, height, depth and channel of the performance capture volume respectively. Function F is learnt using a CNN, specifically a convolutional Sec. 3.3 consisting of successive three-dimensional (3D) alternate convolutional filtering operations and down-or up-sampling with nonlinear activation layers for a similarly encoded output The goal of F is thus to regress a high fidelity 3D volumetric representation given an initial poor fidelity blocky 3D volume estimate. Learning the end-to-end mapping from blocky volumes generated from a small number of camera viewpoints to both cleaner high fidelity volumes as if made by a greater number of camera viewpoints and accurate 3D joint position estimates, requires estimation of the weights φ in F represented by the convolutional and deconvolutional kernels. Specifically, given a collection of training sample triplets x i , z i , j i , where x i ∈ V L is an instance of a low camera count volume, z i ∈ V H is the high camera count output groundtruth volume and j i ∈ J is a vector of groundtruth joint positions for the given volume. The Mean Squared Error (MSE) is minimised at the output of the bottleneck and decoder across N = W × H × D voxels through the two losses L joint and L PV H .</p><formula xml:id="formula_6">tensor V H , where V H = F(V L ) = D(E(V L ))</formula><formula xml:id="formula_7">L(φ ) = L joint + λ L PV H = 1 N N ∑ i=1 F(x i : φ ) − z i 2 2 + λ E(V L : φ ) − j i 2 2<label>(6)</label></formula><p>Where λ = 10 − 3, ensures both terms are of a similar magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Generative Adversarial Network Model</head><p>The encoder-decoder model described in the section above with the dual volume and joint pose loss can produce consistent results. However, we propose to constrain and improve the reconstruction quality of the decoder output of the 3D occupancy volume and the pose estimation by employing a generative adversarial network (GAN). The encoder model from section 3.3, which we refer to as the Generator G estimates the improved volume, whilst the discriminator maximises the chance of recognising real PVH volumes as real and generated PVH volumes as fake, optimizing the minimax objective::</p><formula xml:id="formula_8">min G max D V (D, G) = E x∼P r [log(D(x))] + E x∼P g [log(1 − D( x))]<label>(7)</label></formula><p>where P r is the (real) data distribution and P g is the (generated) model distribution, defined by x = G(z), z ∼ P(z), where the input z is a sample from a simple noise distribution. Once both objective functions are defined, they are learnt jointly by the alternating gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Skip Connections</head><p>Deeper networks in image restoration tasks can result in finer image details being lost given the compact latent space. Recovery of this detail is an under-determined problem, exasperated by the need to reconstruct the additional dimension in volumetric data. We add skip connections between two corresponding convolutional and deconvolutional layers. Omitting the skip connections the detail of extremities such as lower arm position is poorly estimated by both the volume and 3d joints (see sup. material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Temporal Consistency</head><p>Given the inherent temporal nature of the human pose, we enforce temporal consistency with additional Long Short Term Memory (LSTM) layers. These help to smooth noisy individual joint detections to enable a smoother prediction of the joint estimation. The latent vector from the encoder J(V Lt ) = E(V Lt ) at time t consisting of concatenated joint spatial coordinates passed through a series of gates resulting in an output joint vector J o . The aim is to learn the function that minimises the loss between the input vector and the output vector J o = o t • tanh(c t ) (• denotes the Hadamard product) where o t is the output gate, and c t is the memory cell, a combination of the previous memory c t−1 multiplied by a decay based forget gate, and the input gate. Thus, intuitively the LSTM result is the combination of the previous memory and the new input vector. In this implementation, the model consists of two LSTM layers both with 1024 memory cells, using a look back of T = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>To quantify the performance of our proposed approach, we report Mean Per Joint Position Error, the mean 3D Euclidean distance between ground-truth and estimated joint positions of the 26 joints. We performed quantitative evaluation over two public multi-view video datasets of human actions. 3D human pose is evaluated for Human 3.6M <ref type="bibr" target="#b18">[18]</ref>, and the performance of both the skeleton estimation and volume reconstruction is evaluated on TotalCapture <ref type="bibr" target="#b37">[36]</ref>.</p><p>To train F, we initially, train the encoder for just the skeleton loss, purely as a pose regression task without the decoder or critic networks, due to the large parameter count in the volumetric network. These trained weights initialise the encoder stage to help constrain the latent representation during the full, dual-loss network training. Then given the learnt weights as initialisation for the encoder section, we train the entire encoder/decoder network end-to-end constrained by the dual loss of the skeleton and volume occupancy through the GAN critic network. The encoder-decoder Generator and Discriminator network are trained alternately, with the opposing network weights fixed.</p><p>We train with a batch size of 32 and a sequence length of T = 5 (we experimented with different sequence lengths and found sequence length 3, 4, 5 and 6 generally gave similar results). We augment the data during training with a random rotation around the central vertical axis of the PVH to introduce rotation invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Num SeenSubjects(S1,2,3) UnseenSubjects(S4,5) Mean Cams W2 FS3 A3 W2 FS3 A3 Tri-CPM-LSTM <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TotalCapture Evaluation</head><p>We quantitatively evaluate tracking accuracy on the TotalCapture dataset <ref type="bibr" target="#b37">[36]</ref>. We study the accuracy gain due to our method by ablating the set of camera views available on the TotalCapture dataset. Jointly training the generative adversarial dual loss model using high fidelity PVHs obtained using all (C = 8) views of the dataset and 78-D vector concatenation of the 26 3D Cartesian pose joint coordinates. With the corresponding input low fidelity, PVHs obtained using fewer views (we train for C = 2 and C = 4 random neighbouring views), we follow the train and test strategy of <ref type="bibr" target="#b37">[36]</ref>. The dataset contains five subjects, with four diverse categories of sequences; ROM, Walking, Acting, and Freestyle, with each sequence, repeated three times by each subject. The sequences are long, with around 3000-5000 frames, resulting in 1.9M frames. Within the acting and freestyle sequences, there is a great deal of diversity in the captured content.</p><p>The PVH at C = 8 provides the ideal 3D reconstruction proxy estimation for comparison, while C = {2, 4} input covers at most a narrow 90 • view of the scene. Before refinement, the ablated view PVH data exhibits phantom extremities and lacks fine-grained detail, particularly at C = 2 <ref type="figure" target="#fig_3">(Fig. 4)</ref>. These crude volumes would be unsuitable for pose estimation or reconstruction as they do not reflect the true geometry and would cause poor defined joint estimations and severe visual misalignments when projecting camera texture onto the model. However, our method can estimate the joint positions accurately and also clean up and hallucinate a volume equivalent to one produced by the unabated C = 8 camera viewpoints. Tab. 1 quantifies the pose animation error between previous approaches using in general multiple camera views <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b39">38]</ref> or additional data modalities <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b43">42]</ref> and our proposed approach with only two camera views. We outperform best camera approach <ref type="bibr" target="#b25">[25]</ref> by 8 mm indicating the importance of the GAN loss and semantic 2D joint estimates.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>To understand the influence of the individual components and design decisions, we perform an ablative analysis of tracking accuracy for our individual contributions (Tab. 2). Each part of the process enables an improvement in the accuracy performance, especially the use of temporal information (EncoderLSTM) and dual loss in the approach (AutoEncLSTM). The inclusion of the 2D joint (2DJoint) estimates into the dual-channel PVH further reduces this loss by around 4 mm to 31.1 average joint error. The inclusion of the Discriminator (GAN8cam) to enforce improved 3D occupancy volume result, enables the loss to be further reduced to 21mm per joint using all eight camera views. The greater the number of cameras, the more visually realistic the input PVH is. However, it is possible to remove a large number of these cameras with little or no impact on performance (GAN4cam and GAN2cam).</p><p>Despite greatly degrading the appearance of the input PVH when using only 2 or 4 views as input, as indicated by <ref type="figure" target="#fig_2">Fig. 3</ref>. The figure also illustrates the resulting output PVH, and this can be seen to be of a high-fidelity result invariant to the number of cameras used. In summary, using a low fidelity PVH from only two camera views with phantom and missing voxels, achieves a headline performance of 21.4mm mean per joint error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluating Reconstruction Accuracy</head><p>In addition to the pose estimation, the dual loss model is also able to reconstruct the highfidelity 3D volume for the given low fidelity PVH input. Tab. 3 quantifies the error between the unablated (C = 8) and the reconstructed volumes for C = {2, 4} view PVH data, baselining these against C = {2, 4} PVH prior to enhancement via our learnt model (input).</p><p>To measure the performance, we compute the average per-frame MSE of the probability Method Cams SeenSubs(S1,2,3) UnseenSubs(S4,5) Mean C W2 FS3 A3 W2 FS3 A3  <ref type="table">Table 3</ref>: Quantitative performance of volumetric reconstruction on the TotalCapture dataset using 2-4 cameras before our approach (Input) and after, versus unablated groundtruth using eight cameras (error as MSE ×10 −3 ). Our method reduces reconstruction error to 30% of the baseline (Input) for two views.</p><p>of occupancy across each sequence. Comparing the two and four camera PVH volume before enhancement and our results indicate a reduction in MSE of around three times through our approach when using two cameras views for the input and a halving of MSE for a PVH formed from 4 cameras. View count C = 4 in a 180 • arc around the subject perform slightly better than C = 2 neighbouring views in a 90 • arc. However, the performance decrease is minimal for the significantly increased operational flexibility that a two camera deployment provides. In all cases, MSE is more than halved (up to 34% lower) using our refined PVH for a reduced number of views. Using only two cameras, we can produce an equal volume to that reconstructed from a full 360 • C = 8 setup. We show qualitative results of using only two and four camera viewpoint to construct the volume in <ref type="figure" target="#fig_3">Fig. 4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Human 3.6M evaluation</head><p>We perform a further quantitative and qualitative evaluation on the Human 3.6M <ref type="bibr" target="#b18">[18]</ref> dataset. Human 3.6M is the largest publicly available dataset for human 3D pose estimation and contains 3.6 million images of 7 different professional actors performing 15 everyday activities. Each video is captured using four calibrated cameras arranged in the 360 • arrangement and contains 3D pose ground truth. We follow the standard train and evaluation protocols of the Human3.6M dataset <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b35">34]</ref>. Therefore, we explore (  <ref type="table">Table 4</ref>: Comparison of the proposed 3 methods to baseline methods on Human 3.6M.</p><p>Human3.6Model: A baseline approach, using the specified Human 3.6M training data with the four cameras assuming the semantic 2D joints will compensate in part for the phantom part and ghosting that occurs to the occupancy voxels. Our network improves on Qiu <ref type="bibr" target="#b25">[25]</ref>, and dramatically improves on other prior work. By using the information of temporal context and semantic joint estimations, our network reduces the overall error in estimating 3D joint locations, especially on actions like phone, photo, sit and sitting down on which for previous methods did not perform well due to heavy occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This proposed work generates accurate 3D joint and 3D volume proxy reconstructions, from a minimal set of only two wide baseline cameras, through learning constrained by a dual loss on the joints and a generative adversarial loss on the 3D volume. The dual loss in conjunction with the Discriminator in the GAN framework delivers state of the art performance. Furthermore, we have demonstrated that a trained model with plentiful data (from the TotalCapture dataset) can be used to improve performance on other sets of data (in this case from the Human3.6M dataset) that have a limited set of camera views.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>for the learnt encoder (E) and decoder (D) functions. The encoder yields a latent feature representation via a series of 3D convolutions. Each convolutional layer is followed by batch normalisation and a ReLU in the Generator and convolutional strides for a layer in both the encoder and decoder. The encoder enforcesJ(V L ) = E(V L ) where J(V L ) isa concatenation of the skeletal pose vector corresponding to the input PVH; specifically a 78-D vector concatenation of 26 3D Cartesian joint coordinates in x, y, z to generate the pose estimate and an additional latent embedding of size e (in general e = 200). The decoder inverts this process to output tensor V H matching the input resolution but with higher fidelity. The full network parameters are: n E = [64, 64, 128, 128, 256], n D = [256, 128, 128, 64, 64], k E = [3, 3, 3, 3, 3], k D = [3, 3, 3, 3, 3], k s = [0, 1, 0, 1, 0] where k[i] indicates the kernel size and n[i] is the number of filters at layer i for the encoder (E) and decoder (D) parameters respectively. The location of the two skip connections are indicated by s and link two groups of convolutional layers to their corresponding mirrored up-convolutional layer. The passed convolutional feature maps are averaged to the up-convolutional feature maps element-wise and passed to the next layer after rectification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Representative pose estimations from (Fr)ames of unseen (S)ubjects performing (A)ctions with challenging poses (TotalCapture dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples of input/resultant reconstructions for [2,4,8] cameras on TotalCapture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative visual comparison of the input PVH and 3D Pose estimate on encoder against the resultant Reconstruction and 3D Pose estimation using C = {2} views on the TotalCapture dataset. False colour volume occupancy (PVH) and groundtruth C = 8 PVH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>TCModel: Transfer of the trained 2 → 8 camera views model from the TotalCapture dataset, without any further training, to estimate pose as if 8 cameras were used at acquisition. TCModel+FineTune(H36M): 2 epochs of fine-tuning of the learnt 2 → 8 TCModel on Hu-man3.6M dataset. Our TotalCapture trained model (TotalCaptureModel) improves the baseline training of Human 3.6M (Human3.6Model) alone by 5mm and the combined TotalCapture of finetuned model TotalCapture+FineTune(H36M Model) improves this performance by a further 10mm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our approach on TotalCapture to other human pose estimation approaches, expressed as average per joint error (mm) on previously seen and unseen test subjects. (where W2, FS3, A3 are groups of test sequences of walking, freestyle and acting respectively)</figDesc><table><row><cell>5]</cell><cell>8</cell><cell cols="3">45.7 102.8 71.9</cell><cell cols="3">57.8 142.9 59.6</cell><cell>80.1</cell></row><row><cell>2D Matte-LSTM [37]</cell><cell>8</cell><cell cols="7">94.1 128.9 105.3 109.1 168.5 120.6 121.1</cell></row><row><cell>3D-PVH [36]</cell><cell cols="3">8+13 IMU 30.0 90.6</cell><cell>49.0</cell><cell cols="4">36.0 112.1 109.2 70.0</cell></row><row><cell>AutoEnc [38]</cell><cell>8</cell><cell cols="2">13.4 49.8</cell><cell>24.3</cell><cell cols="2">22.0 71.7</cell><cell>40.7</cell><cell>35.5</cell></row><row><cell>Fusion-RPSM [25]</cell><cell>8</cell><cell cols="2">19 58</cell><cell>21</cell><cell>32</cell><cell>54</cell><cell>33</cell><cell>29</cell></row><row><cell cols="3">IMU 1Cam SMPL [42] 1+13 IMU -</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>26.0</cell></row><row><cell>Proposed DualLoss GAN</cell><cell>2</cell><cell cols="2">9.2 30.3</cell><cell>15.2</cell><cell cols="2">13.3 41.7</cell><cell>25.3</cell><cell>21.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of the Mean per joint error (mm). for the individual components on the TotalCapture dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Input 2 19.1 28.5 23.9 23.4 27.5 25.2 24.6 Input 4 11.4 16.5 12.5 12.0 15.2 14.2 11.6 [12] 2 5.43 10.03 6.70 5.34 10.05 8.71 7.71 Ours 2 5.44 9.94 6.34 5.16 9.86 8.49 7.34 Ours 4 4.85 9.32 5.84 4.83 9.56 8.03 7.02</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multifocus structured illumination microscopy for fast volumetric super-resolution imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abrahamsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Optics Express</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4135" to="4140" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pictoral structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Volumetric super-resolution of multispectral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05745v1</idno>
	</analytic>
	<monogr>
		<title level="m">Corr</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Surface-based Character Animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Representations of the Real World: How to Capture, Model, and Render Visual Reality, chapter 16</title>
		<editor>Marcus Magnor, Oliver Grau, Olga Sorkine-Hornung, and Christian Theobalt</editor>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2015-04" />
			<biblScope unit="page" from="239" to="252" />
		</imprint>
	</monogr>
	<note>ISBN 9781482243819</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High-quality streamable free-viewpoint video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Gillett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Evseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient ConvNet-based marker-less motion capture in general scenes with a low number of cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Edilson De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3810" to="3818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Volumetric performance capture from minimal camera viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="566" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Computer Vision (ICCV)</title>
		<meeting>Intl. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A bayesian approach to image-based visual hull reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="69" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards accurate markerless human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Classner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Black</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A unified spatio-temporal articulated model for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Real-time full-body motion capture from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hilton</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang Keze Wang Mude Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual sentences for pose retrieval over low-resolution crossmedia dance collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recovering human body configurations using pairwise constraints between parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Computer Vision</title>
		<meeting>Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="824" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">General automatic human shape and motion capture using volumetric contour cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="509" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-linear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comp. Vision and Pattern Recognition (CVPR)</title>
		<meeting>Comp. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bottom-up recognition and parsing of the human body</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A free-viewpoint video renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Kilner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Graphics, GPU, and Game Tools</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3d human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lifting from the deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00295</idno>
	</analytic>
	<monogr>
		<title level="m">Convolutional 3d pose estimation from a single image</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep pose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
		<ptr target="http://epubs.surrey.ac.uk/841740/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 28th British Machine Vision Conference</title>
		<meeting>28th British Machine Vision Conference</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep convolutional networks for marker-less human pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collomosse</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Visual Media Production (CVMP 2016)</title>
		<meeting>the 13th European Conference on Visual Media Production (CVMP 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep autoencoder for combined human pose estimation and body model upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV&apos;18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparse inertial poser: Automatic 3d human pose estimation from sparse imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Conference of the European Association for Computer Graphics (Eurographics</title>
		<meeting>the 38th Annual Conference of the European Association for Computer Graphics (Eurographics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep networks for image superresolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Computer Vision (ICCV)</title>
		<meeting>Intl. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Processing Systems (NIPS)</title>
		<meeting>Neural Inf. essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="350" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deephuman: 3d human reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Single image super-resolution using deformable patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comp. Vision and Pattern Recognition (CVPR)</title>
		<meeting>Comp. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2917" to="2924" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
