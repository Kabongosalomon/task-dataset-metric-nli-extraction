<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aerial Imagery Pixel-level Segmentation Aerial Imagery Pixel-level Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ir</forename><forename type="middle">Michael</forename><surname>Heffels</surname></persName>
							<email>m.r.heffels@student.tue.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>dr. ir</roleName><forename type="middle">Joaquin</forename><surname>Vanschoren</surname></persName>
							<email>j.vanschoren@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Aerial Imagery Pixel-level Segmentation Aerial Imagery Pixel-level Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer Vision</term>
					<term>Convolutional Neural Networks</term>
					<term>Remote sensing</term>
					<term>semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aerial imagery can be used for important work on a global scale. Nevertheless, the analysis of this data using neural network architectures lags behind the current state-of-the-art on popular datasets such as PASCAL VOC, CityScapes and Camvid. In this paper we bridge the performance-gap between these popular datasets and aerial imagery data. Little work is done on aerial imagery with state-of-the-art neural network architectures in a multiclass setting. Our experiments concerning data augmentation, normalisation, image size and loss functions give insight into a high performance setup for aerial imagery segmentation datasets. Our work, using the state-of-the-art DeepLabv3+ Xception65 architecture, achieves a mean IOU of 70% on the DroneDeploy validation set. With this result, we clearly outperform the current publicly available state-of-the-art validation set mIOU (65%) performance with 5%. Furthermore, to our knowledge, there is no mIOU benchmark for the test set. Hence, we also propose a new benchmark on the DroneDeploy test set using the best performing DeepLabv3+ Xception65 architecture, with a mIOU score of 52.5%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Aerial imagery is analyzed for a broad number of applications. Our work is inspired by the fact that high-resolution aerial imagery analysis helps to address topics such as <ref type="bibr">deforestation (Green and Sussman, 1990)</ref>, declining biological diversity <ref type="bibr" target="#b24">(Richards and Friess, 2016)</ref>, refugee camp planning and maintenance <ref type="bibr" target="#b17">(Ko, 2018)</ref>, global poverty <ref type="bibr" target="#b26">(Roser and Ortiz-Ospina, 2013)</ref>, urban planning, precision agriculture, and geographic information system (GIS) updating <ref type="bibr">(Cheng and Han, 2016)</ref>.</p><p>In this paper, we strive to bridge the gap between current state-of-the-art image segmentation solutions on non-aerial imagery and the solutions in aerial imagery, by achieving higher mIOU accuracy on more classes. More formally we ask: How can we improve the current state-of-the-art neural network architectures used for aerial imagery, to obtain new state-of-the-art performance for multi-class pixel level semantic segmentation on aerial imagery?</p><p>To properly answer this question, we first present several key concepts for this area in Section 2. Next, we discuss the most important related work done in this area in Section 3. In Section 4 we discuss how we incorporated the techniques found in Section 3 in a model design strategy. In Section 5 we elaborate on the choice of the DroneDeploy dataset, as well as how our experiments and adaptations help shape the final model and how this final model performs in practice on the DroneDeploy dataset. Finally in Section 6 we present our conclusions and recommend several opportunities for future work in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background information</head><p>In 2020, different challenges of semantic segmentation within Computer Vision are being solved with a mean Intersection-Over-Union (mIOU) of up to 90% on popular segmentation datasets, such as CityScapes (mIOU 85.1% on test set <ref type="bibr">(Tao et al., 2020)</ref>), Camvid (mIOU 81.7% <ref type="bibr" target="#b35">(Zhu et al., 2018)</ref>) and PASCAL VOC 2012 (mIOU 90.5% <ref type="bibr">(Zoph et al., 2020)</ref>). However, this performance is not trivial for the same applications on aerial imagery. Moreover, most novel image segmentation neural network architectures and implementations are focused on non-aerial imagery.</p><p>With the increasing number of remote sensing devices with high quality spatial resolution, the amount of aerial imagery data also increases. Spatial resolution is the length of one side of a single pixel. For example, an image with a spatial resolution of 1.5 meters (such as the SPOT-7 (Airbus Defence and Space, 2013)) means that a single pixel represents an area on the ground that is 1.5 meters in length and width. Spatial resolutions between 0.41 -4 m are considered high spatial resolution, whereas spatial resolutions between 30 -1000 m are considered low spatial resolution <ref type="bibr" target="#b6">(Campbell and Wynne, 2011)</ref>.</p><p>Where in the early days spatial resolutions of 120 meters were the norm, current state-ofthe-art imagery satellites have spatial resolutions of &lt; 0.5 meters <ref type="bibr" target="#b34">(Zhou, 2003)</ref>. The general trend is that, in recent years, the world is launching more satellites with ever improving spatial resolutions. Smaller spatial resolution means less physical distance between each pixel, which means that these images could be segmented more accurately.</p><p>Noteworthy is that popular regular image datasets (for example CIFAR, ImageNet) typically have a radiometric resolution of 8-bits with RGB values varying between 0 and 255, whereas modern observation satellites have 16-bit RGB values, which means that these are more precisely measured between 0 and 2 16 = 65536. Radiometric resolution quantifies the ability to discriminate between slight energy differences. This is stored in a number of bits for each band. For earlier launched satellites 8-bit data was common in remote sensed data, newer sensors (like Landsat 8, launched in 2013) have 16-bit data products. These bits represent the number of different intensities of radiation the sensor is able to distinguish and record, as shown in a Figure from an introductory course <ref type="bibr">(Humboldt State University, 2019)</ref> in remote sensing in <ref type="figure" target="#fig_0">Figure 1</ref>. Hence, we can safely say that the use of satellite imagery has great potential, as well as challenges for machine learning. Adopted from Humboldt State University <ref type="formula" target="#formula_0">(2019)</ref>, course introduction to remote sensing</p><p>Traditional aerial imagery analysts are often still manually analysing images and labeling them by hand. In recent years the groundbreaking results of neural networks in computer vision have led to the first neural network developments in the segmentation of images as we will discuss later on in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related work on aerial image segmentation</head><p>This Section discusses fundamental image segmentation techniques on which our baseline experiments are based, different challenges when segmenting aerial images and finally more novel techniques which help obtain pixel-level segmentation on aerial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Segmentation</head><p>Before we discuss the relevant work regarding neural networks and aerial imagery, it is important to note that our proposed solutions focus on some form of segmentation. Namely, semantic or instance segmentation. We show the definition of both types below. These definitions are interpreted from <ref type="bibr" target="#b3">(Arnab et al., 2018)</ref>:</p><p>Definition 1 (Semantic segmentation) the process of assigning a label to every pixel in the image, where multiple objects of the same class are treated as the same entity Definition 2 (Instance segmentation) the process of assigning a label to every pixel in the image, but here multiple objects of the same class are treated as distinct individual objects (or instances).</p><p>In general, instance segmentation is considered more difficult than semantic segmentation. The difference between both variants is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. In short, this means that in order to learn a segmentation mask, we need to classify every pixel in an image. If we color-code each class, this classification at the pixel level will eventually form a segmentation mask such as the masks in <ref type="figure" target="#fig_1">Figure 2</ref>. In this paper, we will focus solely on the semantic segmentation task, since we see no clear added value using instance segmentation on aerial imagery at this time. This scope is also specified in our research question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fundamental image segmentation techniques</head><p>The well known u-net architecture is a crucial baseline for image segmentation developments. It won the ISBI challenge <ref type="bibr" target="#b25">(Ronneberger et al., 2015)</ref> for segmentation of neuronal structures in electron microscopic stacks. Furthermore, the authors show that their solution requires a very small number of training examples: it achieved a mean Intersection-Over-Union (mIOU) of 92% on a 'PhC-U373' cells dataset after training on 35 example 512x512 pixel images. This mean Intersection-Over-Union score, shown in equation 1 is the most used evaluation metric for semantic image segmentation.</p><formula xml:id="formula_0">mIOU = C i=1 A i ∩B i A i ∪B i C<label>(1)</label></formula><p>In equation 1, the intersection (A i ∩ B i ) for each class 1 ≤ i ≤ C, comprises the pixels found in both the prediction mask A i and the ground truth mask B i . The union (A i ∪ B i ) includes all pixels found in either the prediction mask or the ground truth mask. After computing the IOU for each trainable class, the score is averaged out over the number of classes C, with or without taking into account class imbalance. Most of the time the unbalanced version is used, as displayed in equation 1.</p><p>The u-net architecture is shown below in <ref type="figure">Figure 3</ref>. It is based on a publication by <ref type="bibr" target="#b20">(Long et al., 2014)</ref>, which introduces the Fully Convolutional Network (FCN). A FCN replaces the final dense layers by more convolution layers. Hence, a FCN uses filters to learn representations and make decisions based on local spatial input throughout the entire <ref type="figure">Figure 3</ref>: U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations. Adopted from <ref type="bibr" target="#b25">(Ronneberger et al., 2015)</ref> neural network. This makes sense for our use case, since correctly classifying any pixel in an image strongly depends on local information. One other big advantage of doing this is that the size of input is now flexible. U-nets are very effective for tasks where the output size is similar to the input size, where the output needs to be the same high resolution. This makes them very good for creating segmentation masks, amongst other tasks. The work's main contribution is the introduction of shortcut or skip connections, which we will discuss in more detail below.</p><p>The first half of the u-net is just a traditional stack of convolutional and max pooling layers (the encoder in <ref type="figure">Figure 3</ref>). This half takes care of finding "what" is in the image, just like a regular CNN finds characteristics in the image from edges (in earlier layers) to more complex structures (in deeper layers). The second path is the symmetric expanding path (the decoder in <ref type="figure">Figure 3</ref>) which is used to enable precise localisation using transposed convolutions. This prevents the "where" information of objects from getting lost by gradually applying up-sampling. Moreover, at every step of the decoder, u-nets use skip connections denoted by the grey arrows in <ref type="figure">Figure 3</ref>. These skip connections concatenate the output of the transposed convolution layers with the feature maps from the Encoder at the same level. Skip connections are very powerful in the decoding process. A decoder with skip connections allows the network to adjust coarse predictions using lower-level features such as edges and other small patterns. Thus it is an end-to-end fully convolutional network (FCN), for example it only contains convolutional layers and does not contain any dense layers. All further details on u-nets can be found in <ref type="bibr" target="#b25">Ronneberger et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Challenges when segmenting aerial images</head><p>Segmenting datasets with one <ref type="bibr">(Shermeyer et al. (2020)</ref>, Soman (2019)) or a few classes (Sang and Minh, 2018) has been done using u-nets or similar performing architectures, but aerial imagery's biggest potential is the ability to capture a large portion of the earth (with a larger number of classes) at the same time. To illustrate this, we present a partial image taken from the DroneDeploy dataset used for this research in  However, when compared to popular multi-class segmentation datasets such as CityScapes, Camvid and PASCAL VOC, there are two challenges. Namely, smaller image resolution per class and, since all pictures are taken from above in aerial imagery, there is only one image perspective for all classes. Furthermore, there are hardly any high quality labeled aerial imagery datasets with many classes available. Right now, physically smaller classes (for example cars, persons) cause accuracy issues and class imbalance, presumably because small classes give the network a relatively small number of pixels to learn from and to do inference on.</p><p>The challenge of aerial images' resolution gets amplified by the fact that the classes we are trying to predict in an image contain less pixels than the classes in traditional groundlevel images. This is where FCN's and u-nets segmentation masks lack quality, because of excessive downsizing due to consecutive pooling operations. The performance of several FCN-based architectures is often still mentioned for reference in other works, such as the work from Chen et al. (2018a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Novel techniques to achieve pixel-level segmentation on aerial images</head><p>The next sections will elaborate on different more novel techniques which focus on accuracy. The model that we elaborate on the most is DeepLabv3+ (Chen et al., 2018b), because of their state-of-the-art performance on segmentation datasets, as well as the fact that the proposed techniques also seem suitable for aerial imagery challenges. We will explain these techniques in the coming sections.</p><p>Other novel solutions were considered but not deemed ideal, such as YOLOv4 <ref type="bibr">(Bochkovskiy et al., 2020)</ref> which focuses on high-speed object detection and HRNet-OCR <ref type="bibr">(Tao et al., 2020)</ref> which focuses quite specifically on the CityScapes dataset and its shortcomings regarding coarse labeling. Another very interesting work rethinks self-training, a semi-supervised learning concept which <ref type="bibr">Zoph et al. (2020)</ref> use to show state-of-the-art segmentation results on the PASCAL VOC dataset. Since this work only came out in June 2020, we were not able to incorporate it into this work in time, but encourage the reader to experiment with this technique in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Dilated Convolution</head><p>Pooling operations are useful for extracting sharp low-level features such as edges and points. It is also done to reduce variance by letting information spread from any point in the image, and to reduce computational costs. However, when several pooling operations are used downwards in the encoder, a lot of fine-grained information is lost. Moreover, being a fully convolutional network for a segmentation problem, the decoder needs to upsample the features by the same factor using deconvolution which again is a memory and computational expensive operation because of the large number of learnable parameters.</p><p>The original DeepLab paper (Chen et al., 2018a) proposes the usage of dilated convolution which helps processing a larger context image using the same number of parameters. In theory this should be ideal for aerial images and their large coverage, although there is a trade-off between a large field-of-view and accurate localisation. We address this trade-off with Dilated Spatial Pyramidal Pooling, explained in Section 3.4.2.</p><p>Dilated convolution increases the size of the convolution filter by appending zeros to fill the gaps between pixel inputs. When the dilation rate (denoted in <ref type="figure" target="#fig_4">Figure 5</ref> as D) D = 1, it is equivalent to a conventional convolution. When D = 2, a zero is inserted between every pixel input, making the filter cover a larger area. It now has the capacity to grasp the context of a 5x5 convolution filter, while having 3x3 convolution filter computational complexity. The center image in <ref type="figure" target="#fig_4">Figure 5</ref> illustrates this. During up sampling a similar advantage regarding lower computational complexity holds, since bi-linear up sampling does not need any parameters as opposed to deconvolution. However, if we want to capture feature responses at multiple scales, we need to do multiple passes on one image. This negatively affects performance. The next technique will mitigate this issue. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Dilated Spatial Pyramidal Pooling</head><p>The second technique proposed in (Chen et al., 2018a) is called Atrous Spatial Pyramidal Pooling (ASPP), which we will call Dilated SPP to remain consistent. The underlying technique (SPP) is first introduced in SPP-net by <ref type="bibr">He et al. (2014)</ref>. One main contribution of SPP is very applicable to aerial imagery, namely robustly segmenting images from different scales and sizes. The spatial pyramidal pooling layers are able to capture information at multiple scales, by concatenating them to a 1-dimensional vector. In short, it works as follows. The Dilated CNN score map from each filter with different dilation rates is extracted and fused, by taking at each position the maximum response across the different scales. This significantly improves performance at minimal cost, because the feature responses of all Dilated CNN layers for multiple scales are computed from one original image, instead of processing the original image multiple times at different scales. In theory this contribution greatly benefits our goal, because aerial imagery is collected from a vast number of different heights and resolutions.</p><p>Dilated SPP elegantly combines this technique with the same concept as dilated convolutions. A dilated convolution filter runs over the image using multiple dilation rates, after which SPP combines these filters to create one large feature map to ultimately classify the pixel of interest. <ref type="figure" target="#fig_5">Figure 6</ref> shows this process in a 2-dimensional plane. To conclude, DSPP allows us to grasp a much larger context while classifying each pixel, whilst keeping the computational cost relatively low due to the use of dilated convolutions. Combining the techniques discussed in sections 3.2, 3.4.1 and 3.4.2 we now have a computationally efficient convolution technique which is able to grasp a large context whilst classifying each pixel. However, due to the use of pooling layers and down-sampling, we lose locality accuracy. This is a problem, because the segmentation output will be coarse and the boundaries are not concretely defined. Especially with aerial images, where every pixel matters, this makes a difference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Conditional Random Fields</head><p>The third and final contribution of (Chen et al., 2018a) is the introduction of a fully connected Conditional Random Field (CRF). In more traditional applications, short-range CRF's are used to make noisy segmentation maps smoother, "favoring same-label assignments to spatially proximal pixels". Instead, the authors use a fully connected CRF model which uses an energy function shown below in equation 2.</p><formula xml:id="formula_1">E(x) = i θ i (x i ) + ij θ ij (x i , x j )<label>(2)</label></formula><p>The vector x is the label assignment for pixels. The first sum contains the unary potential</p><formula xml:id="formula_2">θ i (x i ) = −logP (x i ), where P (x i )</formula><p>is the label assignment probability at pixel i computed by a Dilated CNN. The second sum contains the pairwise potential θ ij (x i , x j ), which uses two Gaussian kernels in different feature spaces. The first kernel depends on both pixel positions and RGB color, the second kernel only depends on pixel positions. Together, these kernels similarly label pixels with similar color and position. For more details on the pairwise potential we refer the reader to the original Deeplab paper by <ref type="bibr" target="#b7">Chen et al. (2018a)</ref>. The essential benefit of using the fully connected CRF is that accurate (sharper) inference can now be done more efficient, especially on the boundaries of an object.</p><p>Still, better results are possible by using the final adaptation of DeepLabv3+, the modified aligned xception backbone instead of a ResNet backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Modified Aligned Xception</head><p>As discussed in <ref type="bibr">(Chen et al., 2018b)</ref>, the authors of <ref type="bibr">(Dai et al., 2017)</ref> have shown promising results in the task of object detection using their Modified Aligned Xception model. Their main two contributions are deformable convolution and deformable Region of Interest (RoI) pooling, pushing the state-of-art results using the techniques discussed in sections 3.4.1, 3.4.2 and 3.4.3 even further. The main idea of deformable convolution is to make it possible to alter the common square shape of filters. To achieve this, the regular sample matrix in each filter/kernel is augmented with learnable offset data. This makes it possible to create specifically shaped filters, depending on which features the network is learning while predicting each class.</p><p>As visible in <ref type="figure" target="#fig_6">Figure 7</ref>, deformable convolutions allow for more flexible sampling locations while using standard 3x3 sized filter parameters, albeit augmented with offset values. For the scope of this research, we choose to follow the same modifications to Xception as DeepLabv3+ in (Chen et al., 2018b) since that configuration gave them the state-of-the-art results on PASCAL VOC 2012. Having discussed the four techniques above, we will now compare these modern techniques against the current level of aerial image segmentation solutions used in other works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">State-of-the-art of aerial image segmentation</head><p>A recently published journal article <ref type="bibr">(Wurm et al., 2019)</ref> in the International Society for Photogrammetry and Remote Sensing (ISPRS) used FCN-VGG19, without mentioning any of the architectures discussed above, or why they were not considered. Nevertheless, they state that they observed high performance of semantic segmentation for mapped slums in the optical data they used. Specifically, a mean IOU of 87.43% over 4 different classes.</p><p>Another paper (Viet Sang and Duc Minh, 2018) which proposes a deep learning approach to aerial image segmentation uses the somewhat outdated FCN architecture. They use 6 different classes, but only report precision, recall and F1 scores instead of the mean IOU which is a more conservative metric, especially if one of the classes scores relatively low which is the case for this work: clutter scores 34.1% accuracy, but the overall reported score is 91.0%.</p><p>Other recent work ) uses more modern architectures, but focuses on satellite imagery with just one class, either roads or buildings. They report a mean IOU of 65.1% on the DEEPGLOBE road extraction dataset.</p><p>There is one work <ref type="bibr" target="#b19">(Liu et al., 2019)</ref> which uses the DeepLabv3+ architecture on the satellite imagery dataset SpaceNet-6 <ref type="bibr">(Shermeyer et al., 2020)</ref>. However, this is also a single building-class dataset. Interestingly, in this work the authors state that the DeepLabv3+ architecture performs non-optimal on remote sensing images, due to the relatively simple decoder design. Therefore they add two extra upsampling layers and several skip connections between the encoder and the decoder. Using their decoder and the Xception encoder they outperform DeepLabv3+ by improving the mean IOU score from 73.1% to 74.5%.</p><p>Since we discovered this work rather late during this research, we recommend additional experiments with this adapted DeepLabv3+ as a possibility for future work.</p><p>Considering the above mentioned state-of-the-art, we conclude that given a suitable dataset (for example with high enough quality), there is a lot of room for improvement on aerial imagery analysis in general, by using the current state-of-the-art architectures on richer datasets containing more than one class. This is exactly what we this work does, by applying the DeepLabv3+ architecture to the richer DroneDeploy dataset in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model design</head><p>In this section we discuss the model design choices for our research for both u-net and DeepLabv3+. In Section 3 we covered each architectures' main contributions, advantages and disadvantages. We will now focus on their specific implementation in this section. Also we discuss several adaptations we had to make in order to create a compatible and complete implementation for our experiments. For a complete list of these alterations we refer the reader to Appendix B.</p><p>The u-net architecture discussed in Section 3.2 is a well known and often used baseline in many dataset benchmark comparisons, as is the case for our research. More specifically, we use two different deep learning libraries with their own u-net implementation. One from Keras and the other from fastai's U-Net learner <ref type="bibr">(Howard and Gugger, 2020)</ref> which is based on PyTorch. Fastai's u-net implementation uses PixelShuffle and sub-pixel convolution "initialised to convolution NN resize" (ICNR) initialisation <ref type="bibr">(Aitken et al., 2017)</ref> on top of the regular u-net implementation and call this a "DynamicUnet". PixelShuffle and ICNR combined allow for more modelling power at the same computational complexity and produces clean outputs by eliminating checkerboard artifacts. For the scope of this work it is not necessary to elaborate more on these techniques, since these are not used in any of our further experiments except for the fastai u-net implementation which we did not alter.</p><p>The main contributions of the DeepLabV3+ architecture has been thoroughly discussed in sections 3.4.1, 3.4.2 and 3.4.3. The architecture shows state-of-the-art performance on all popular segmentation datasets mentioned in the introduction, CityScapes (mIOU 82.1% on test set), Camvid (mIOU 81.7% <ref type="bibr" target="#b35">(Zhu et al., 2018)</ref>) and PASCAL VOC 2012 (mIOU 89% on test set). Since one of the u-net benchmark implementations is based on PyTorch, we first attempted to use a PyTorch implementation for DeepLabv3+ as well.</p><p>Adapting the PyTorch version's code of DeepLabv3+ to our dataset turned out to be too comprehensive. The code base was built up quite modular in terms of running several experiments with different architectures, but not modular at all in terms of using it on new or custom datasets. For this reason, we turned to the official DeepLabv3+ implementation from Tensorflow which we were able to adapt to work well with our aerial dataset. The link to the original GitHub repository is available in <ref type="bibr">Chen et al. (2018b)</ref>.</p><p>For all architectures (Keras u-net, PyTorch fastai u-net, Tensorflow DeepLabv3+) we added an additional focal loss function which is described in Section 4.4. Furthermore, since the original DeepLabv3+ implementation did not include our aerial DroneDeploy dataset, we made several changes to the data pre-processing code in order to make the DeepLabv3+ pipeline compatible for our work. For details on these changes and more information to recreate our research setup we refer the reader to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generalisation techniques</head><p>Improving the generalisation ability of deep learning models is a difficult challenge on its own. The better a model performs on unseen data, the higher its generalisation ability. Well performing strategies to improve this ability are for example (dropout) regularisation, batch normalisation, transfer learning using pre-trained networks and data augmentation. Regarding data augmentation, there is a slight difference in working with aerial imagery when compared to ground-level imagery. For instance, there is no added value in flipping an object over its y-axis if this variant does not occur in real life. Thus, y-axis flipping does not make sense for ground-level images but it does for aerial imagery. On the other hand, extensive zooming (&gt; 5%) only seems makes sense for ground-level images, since from an aerial point-of-view the classes are seen from roughly the same distance, regardless of the dataset. In order to confirm this, we check this empirically by increasing zooming augmentation to 10% and 15% in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Encoders and backbones</head><p>For both u-net implementations, our experiments on the dataset include many different ResNet encoders. The encoders we use are ResNet-18, ResNet-34, ResNet-50 and ResNet-101. The encoder weights are pre-trained on ImageNet and unfrozen. The decoder is not pre-trained in any way, as is usually the case.</p><p>For the DeepLabv3+ implementation, our experiments on the dataset include two network backbones XCeption65 and ResNetV1-50 Beta. The modified ResNet-50 backbone replaces the first original 7x7 convolution with three 3x3 convolutions. The XCeption65 backbone uses the model initialisation xception65 coco voc trainaug, which is pre-trained on ImageNet, MS-COCO and PASCAL VOC 2012 train aug. The ResNetV1-50 Beta backbone uses the model initialisation resnet v1 50 beta imagenet which is pre-trained on Im-ageNet and unfrozen. For both the u-net and DeepLabv3+ pre-trained ResNet encoders, using ImageNet is solely based on decreasing training time to obtain better results faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image tile input size</head><p>Since aerial images are too large to fit into GPU memory as a whole, we slice the original images into smaller tiles. In order to gain insight in the effect of different tile sizes, we experiment with two different tile sizes on a subset of the aerial data. However, as stated we are bound by the limitations of the working memory of our GPU. Therefore we will perform the experiments with tile sizes 300x300 and 500x500 pixels respectively in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation metrics and loss functions</head><p>Since we are dealing with a pixel level segmentation problem, it might seem logical to think of pixel accuracy as a valid evaluation metric. However, this is not the case, since high pixel accuracy does not give any guarantees regarding segmentation quality. This issue is caused by class imbalance. When the class distribution of an image is extremely imbalanced, for example one class covers 98% of the original image, and the model classifies all pixels as that class, 98% of pixels are classified accurately. For aerial datasets such as the DroneDeploy dataset this problem is more important since the ground (37.7%) and vegetation (10.43%) classes are dominantly present, so this cannot be ignored. Instead, the performance is measured in terms of pixel intersection-over-union averaged across the 6 classes (mIOU), which does take class imbalance into account because we measure the accuracy of each class in the image separately. mIOU is used by the current state-of-the-art models and makes our results easier to compare against current and future work.</p><p>In terms of loss functions, a popular choice in multi-class segmentation challenges is the categorical cross entropy (CCE) loss L CCE , which is depicted in equation 3 below. Here, y ic is the i-th scalar value of class c in the model output, y ic is the corresponding i-th target value for class c, and output size D is the number of scalar values in the model output.</p><p>Being negative (as usual), we ensure that the loss decreases when the modeled and true distributions get closer to each other. The total number of classes is shown with the letter C.</p><formula xml:id="formula_3">L CCE = − C c=1 D i=1 y ic · logŷ ic (3)</formula><p>CCE loss is still widely used in the current state-of-the-art, but given the expected class imbalance issues in our dataset, we also inspect a loss function which deals explicitly with class imbalance. The need for this is also discussed in <ref type="bibr" target="#b20">(Long et al., 2014)</ref> where they discuss weighting their loss "for each output channel in order to counteract a class imbalance present in the dataset". Thus, we also run several experiments with Focal loss, introduced in <ref type="bibr" target="#b18">(Lin et al., 2017)</ref>. Focal loss is designed to address accuracy issues in situations where there is a large ratio (for example 1:1000) or more between the imbalanced classes. In this publication the authors show how they arrive at the proposed Focal loss starting from a binary cross entropy loss. We refer the interested reader to their work and do not repeat these notational steps. The variant the authors used in practice is shown with equation 4 where α = 0.25 on a scale of 0.1 ≤ α ≤ 0.999 and γ = 2 on a scale of 0 ≤ γ ≤ 5.0 worked best in their experiments.</p><formula xml:id="formula_4">F L(p t ) = −α t (1 − p t ) γ log(p t )<label>(4)</label></formula><p>Focal Loss relies on a large ratio between imbalanced classes, because the contribution each class makes to the loss depends on the number of correct classifications. The more correct classifications, the less important this class becomes in terms of the loss. Thus, the loss depends more on problematic classes. With γ = 2, an example classified with p t = 0.9 would have a 100 times lower loss than regular cross entropy. This means that this example has a 100 times lower impact on our optimisation process. In comparison, an example classified with p t = 0.5 only has a 4 times lower loss than regular cross entropy and its contribution is therefore relatively larger.</p><p>In this section we have described several model design choices and the adaptations we made in order to prepare them for our experiments. In the next section we will find out how the models perform in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment design</head><p>Both architectures' implementations have been introduced in Section 4. In this section we look into how these design choices affect their performance in detail by experimenting on an aerial imagery dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Candidate datasets</head><p>Geospatial datasets comes in a large number of different file formats, sizes, and schemes. They often require specific domain knowledge which complicate their use in machine learning. There are a few datasets with high quality annotations, although they are still scarce in 2020. We consciously chose the DroneDeploy dataset, mainly because of its high quality segmentation masks. We will continue by describing the characteristics of this dataset in Section 5.2. Three other datasets were considered and inspected by implementing a few exploratory experiments, namely SpaceNet-6 <ref type="bibr">(Shermeyer et al., 2020)</ref>, Defence Science &amp; Technology Laboratory Satellite Imagery Feature Detection (DSTL Kaggle challenge) and SkyScapes-Dense <ref type="bibr" target="#b21">(Majid Azimi et al., 2019)</ref>. However, these were not selected eventually due to the lack of multiple classes, labeling quality and public availability, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The DroneDeploy dataset</head><p>The full DroneDeploy dataset <ref type="bibr" target="#b23">(Pilkington et al., 2019)</ref> contains 55 RGB images, 9.1 GB in total. The individual image sizes vary from 6.8 MB to 637 MB, all with 0.1 meter per pixel spatial resolution, taken from all over the United States along with elevation maps and segmentation masks. Since these images are too large to process at once, each one is sliced into tiles. The dataset is split into 35/8/12 images for train/validation/test purposes respectively. For purposes of being reproducible we use the same splits as documented in the dataset benchmark available at <ref type="bibr" target="#b23">(Pilkington et al., 2019)</ref>. Depending on the tile resolution, the train and validation files combined (43 in total) contain either 1968 tiles of 500x500 pixels, or 6888 tiles of 300x300 pixels, all non-overlapping. The test images are only sliced during inference.</p><p>Once the tiles are created and split into train and validation datasets, we are free to randomise the tile order. Furthermore, since this is a pixel-level segmentation task, there are no typical "boundary issues", which do occur in other tasks such as object detection if an object is only half present in a sliced tile.</p><p>The segmentation masks are annotated with 6+1 classes -namely Building, Clutter, Vegetation, Water, Ground, Car and 'Ignore' -where the 'Ignore' class refers to mask areas of missing labels or image boundaries. During the slicing process into tiles, all individual tiles containing only 'Ignore' pixels are left out of the rest of training and inference process. During this research we only use the 55 RGB images and their segmentation masks, since we attain to achieve a result which generalizes well, and will therefore hold for many different datasets. Other datasets, such as SpaceNet-6 and DSTL, also have image data in bands specific to satellite data, for example panchromatic, multispectral or SWIR. The distribution of all classes and their corresponding color map is provided below in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>Color code Percentage of total pixels 1: Building Red 5.6% 2: Clutter/debris Purple 2% 3: Vegetation Green 10.43% 4: Water Orange 1.2% 5: Ground White 37.7% 6: Car Blue 0.38% 0: Ignore Magenta 42.7% <ref type="table">Table 1</ref>: Pixel distribution of the 6 annotated classes + 1 Ignore class in the DroneDeploy dataset <ref type="figure" target="#fig_7">Figure 8</ref> shows one full image and its corresponding ground truth segmentation mask. We can see cars, ground, buildings, vegetation, clutter and a large number of ignore pixels. Most of the images are largely labeled like this one. Again, sliced tiles containing ignore pixels only are intentionally left out of the training process. with its corresponding ground truth segmentation mask in the center and overlaid with 50% transparency on the right</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance exploration</head><p>While exploring design choices, we explore different choices in terms of encoders, augmentation, normalisation and loss functions before we go ahead and train a configurated network on the full dataset. The DroneDeploy dataset GitHub repository implementation uses the u-net architecture, based on either Keras or fastai (PyTorch). First we establish a baseline with this implementation and obtain mean F1-scores of 0.7806 and 0.8026 using fastai and Keras u-nets respectively, which is trained for 15 epochs. These results are also displayed in <ref type="table" target="#tab_3">Table 3</ref>. Next, we explore encoders, tile sizes, augmentation, normalisation and loss function options, as discussed in Section 4, by using the dataset-sample DroneDeploy subset. In an iterative process, we measure the precision and recall of each implementation after 15 epochs on the sample dataset and base our decision on the interim results. Precision and recall scores are displayed in <ref type="figure" target="#fig_0">Figures 9 and 10</ref> respectively. Unless specified otherwise, the tile size is always 300x300 pixels. The sample baseline is ran without any alterations, using a pre-trained ResNet18 encoder and categorical cross entropy loss.</p><p>While inspecting the precision and recall scores, we see that adding data augmentation, batch normalisation and deeper encoders generally helps performance. Experimenting with different zoom augmentation levels shows that increasing zoom from 5% to 10% does improve the precision with a marginal 0.0053, while the recall drops with 0.0047. The precision and recall scores both decrease when we increase zoom augmentation further to 15%.</p><p>Using the alternate Focal loss function does not show immediate performance gain. This is not very surprising given the fact that the focal loss performs better when there is a large ratio class imbalance (see Section 4.4) which might not be the case for the sample dataset. To verify the performance of the Focal loss function we will include this variant in the full dataset experiments.</p><p>Somewhat surprisingly, increasing the tile size to 500x500 pixels per tile has a negative effect on performance. This might be the result of overfitting, because the model is trained with more parameters on a smaller amount of tiles due to the tile size. While this might only be the case with the basic u-net architecture, we choose not to pursue this further, because of GPU memory limitations and because there does not seem to be a clear advantage. Lastly, in terms of encoders the ResNet50 architecture seems to hit the "complexity sweet spot" for our task after testing with ResNet18, ResNet34, ResNet50 and ResNet101 encoders. We will use these preliminary results as pointers when training our architectures on the full dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training the network</head><p>For training purposes, the dataset is divided into non-overlapping tiles with tile sizes of 300x300 pixels in which at least 1 class of interest is present. The total dataset contains 6888 image-tiles. 14 image-tiles only contain the "Ignore" class, which we omit during training for this reason. Hence we have 6874 image-tiles at our disposal. Interestingly, there are 1524 image-tiles containing only one relevant class, which occurs for the classes "Building", "Clutter", "Vegetation" and "Water". Apart from "Clutter", which is a rather ambiguous name for a class, this seems to make sense.</p><p>Training is done using a single GPU, on either a GeForce GTX 1080 Ti with 11GB of RAM or a Tesla V100 with 16GB of RAM. The details of both clusters are shown in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results</head><p>In this section we present our results on the DroneDeploy dataset in <ref type="table" target="#tab_3">Table 3</ref>. The DeepLabv3+ models are trained for 40 epochs on the train dataset and evaluated on the validation and test set. The u-net models are trained on the train dataset and tuned on the validation set. Hence, we only report the mIOU scores on the test set for these models. <ref type="figure" target="#fig_0">Figure 13</ref> gives an impression of the fastai Focal u-net model's performance, while <ref type="figure" target="#fig_0">Figure 15</ref> shows the performance of the keras u-net CCE model performance.</p><p>In a very recent, unpublished preprint <ref type="bibr">(Parmar et al., 2020)</ref> the authors show a 65.0 mIOU score on the validation set. In this work there is no score reported on the test set. Our mIOU score using the DeepLabv3+ architecture of 69.9 is achieved on the same validation set, calculated over the first 5 classes. For the sixth class, "car", the DeepLabv3+ implementation consistently reports "not a number". Nevertheless, as the prediction images in <ref type="figure" target="#fig_0">Figures 11 and 12</ref> show, the model seems to perform quite well on cars as well. Nevertheless the prediction is not perfect yet, because the left most car in <ref type="figure" target="#fig_0">Figure 12</ref> suffers from an overhanging tree which causes a-somewhat understandable-misclassification. Hence, with a small reservation for this class, we show the potential to outperform the current publicly available state-of-the-art mIOU with 4.9 percent. Furthermore, there is no other mIOU benchmark for the test set to our knowledge. Hence, we propose a new benchmark on the DroneDeploy test set using the best performing DeepLabv3+ Xception65 architecture, with a mIOU score of 52.5%. <ref type="figure" target="#fig_0">Figure 11</ref>: Showcasing the performance on the "car" class. Zoomed in image tile of the DroneDeploy test set on the left. In the center the DeepLabv3+ Xception65 prediction mask, with ground , cars and vegetation pixels. On the right, the prediction mask overlaid with 50% transparency. <ref type="figure" target="#fig_0">Figure 12</ref>: Showcasing the performance on the "car" class. A second zoomed in image tile of the DroneDeploy test set on the left. In the center the DeepLabv3+ Xception65 prediction mask, with ground , cars and vegetation pixels. On the right, the prediction mask overlaid with 50% transparency.</p><p>All current scores presented in <ref type="table" target="#tab_3">Table 3</ref>   Additionally, we present our results on the DroneDeploy test set in <ref type="table" target="#tab_5">Table 4</ref>. Here however, the models are trained on both the train and the validation set for 40 epochs.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Discussion</head><p>Surprisingly, the models trained on the train and validation set in perform worse on the test set than the models trained on just the train set. Since we used the given dataset split of 35/8/12 for respectively the train, validation and test set, we suspect that the original 55 RGB images were not thoroughly randomised. Specifically, we suspect that the validation set is significantly different than the test set, since the scores decrease when we add the validation set to our training data. Further investigation of these results is left as a recommendation for future work. Also, while inspecting the model performance and comparing several images' ground truth mask with its corresponding prediction mask, we found that the "ground truth" labels can be debatable at times. In <ref type="figure" target="#fig_0">Figure 15</ref> for example, there is a large patch of predicted "vegetation" between the orange labeled "water" pools at the bottom. In the ground truth mask in <ref type="figure" target="#fig_0">Figure 14</ref> this is annotated as "ground", while it clearly shows characteristics of "vegetation" in the original image. The fact that roads and dirt also classify as "ground" strengthens the comment that a green patch, sharing more characteristics with "vegetation", should actually be classified as "vegetation" instead of "ground".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>We conclude this work by presenting our conclusions and recommendations for future work. The complete relevant codebase for this work is available online at https://github.com/ mrheffels/aerial-imagery-segmentation. The other repositories we build upon are all mentioned in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Conclusions</head><p>In this paper, we present the performance of a fundamental segmentation neural network architecture, u-net, including a large number of different experiments while exploring their effect on the model performance. Based on the results of our exploration, we find that data augmentation, batch normalisation and the ResNet50 encoder generally help performance. Zoom augmentation above 5%, tile sizes of 500x500 pixels and the alternate Focal loss, all do not improve performance convincingly.</p><p>Finally, we propose a new performance benchmark for the DroneDeploy validation and test set. We obtain mean IOU scores of 69.9% and 52.5% on the validation and test set respectively, using the DeepLabv3+ Xception65 architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Future work</head><p>For future work, we greatly encourage the interested researcher to extend the results found in this research for use on satellite imagery, for example to train networks more specifically using data in the panchromatic, multispectral or SWIR wavebands. Also, as stated in Section 3.5, a straightforward first extension can be made by altering the DeepLabv3+ architecture as proposed by <ref type="bibr" target="#b19">Liu et al. (2019)</ref>.</p><p>Furthermore, it might be interesting to use this new baseline and extend it by incorporating the available elevation maps of the DroneDeploy dataset. In general, being able to use different types of input data rather than just RGB data can be of great value for aerial image machine learning problems. One can imagine that other input data such as elevations can help a network distinguish the difference between for example cars, ground or buildings a lot easier than when it is just using RGB input data.</p><p>As stated in Section 3.4, another potential extension is self-training, a semi-supervised learning concept which <ref type="bibr">Zoph et al. (2020)</ref> use to show state-of-the-art segmentation results on the PASCAL VOC dataset. This very recent work also shows promising opportunities for aerial imagery, since the number of annotated examples are relatively low, which could be compensated using this technique in future works. Another argument for this extension's potential is the fact that high quality aerial imagery annotation is very time consuming to do manually. A well trained model has the potential to create new high quality annotations on a large scale.</p><p>Lastly, as stated in the discussion, if one does not take into account the given DroneDeploy dataset train/validation/test split, it is possible to reshuffle the complete dataset and experiment with a custom defined dataset split. For this work this was not within scope, to make the scores comparable to other public benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>From left to right, 8 bit, 2 bit and 1 bit radiometric resolutions are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An example of semantic segmentation and instance segmentation from left to right, respectively. (image adopted from Arnab et al. (2018))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>A partial image of the DroneDeploy dataset<ref type="bibr" target="#b23">(Pilkington et al., 2019</ref>) on the left with 5 different classes, together with its corresponding segmentation mask on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Dilated convolution filters with dilation rates D = 1, D = 2, D = 3 respectively which correspond to the number of zeroes between the original pixel inputs. Original image from<ref type="bibr" target="#b4">(Artacho and Savakis, 2019)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Dilated Spatial Pyramid Pooling with filter size 3 and dilation rates D = 6, D = 12, D = 18, D = 24 to create multiple Field-Of-Views shown in different colors. The extracted features from different dilation rates are then concatenated by converting to a 1d vector to create the final result. Original image from<ref type="bibr" target="#b7">(Chen et al., 2018a)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Illustration of the sampling locations in 3x3 standard and deformable convolutions. (a) regular sampling grid (green points) of standard convolution. (b) deformed sampling locations (dark blue points) with augmented offsets (light blue arrows) in deformable convolution. (c)(d) are special cases of (b), showing that the deformable convolution generalizes various transformations for scale, (anisotropic) aspect ratio and rotation. Adopted from(Dai et al., 2017)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Image 551063e3c5 8FCB044F58INSPIRE of the DroneDeploy dataset on the left,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Recall scores recorded using the Weights and Biases package, for all exploratory alterations while training the DroneDeploy sample dataset. S : Sample, A : Augmentation with 5% zoom, A10/15 : Augmentation with 10% or 15% zoom respectively, N : Batch Normalisation, BC : Bigger tile size (500x500). ResXX denotes the used encoder and Focal denotes using the Focal Loss together with the Fast AI u-net implementation. Added final scores after 15 epochs in bottom right for readability in non-digital form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Zoomed in image of the DroneDeploy dataset on the left, fastai Focal ResNet50 u-net baseline's predicted semantic segmentation mask in the center and its ground truth on the right Figure 14: Image 1476907971 CHADGRISMOPENPIPELINE of the DroneDeploy dataset overlaid with 50% transparency ground truth label Figure 15: Image 1476907971 CHADGRISMOPENPIPELINE of the DroneDeploy dataset overlaid with 65% transparency predicted label by keras CCE ResNet50 u-net baseline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Figure 9: Precision scores recorded using the Weights and Biases package, for all exploratory alterations while training the DroneDeploy sample dataset. S : Sample, A : Augmentation with 5% zoom, A10/15 : Augmentation with 10% or 15% zoom respectively, N : Batch Normalisation, BC : Bigger tile size (500x500). ResXX denotes the used encoder and Focal denotes using the Focal Loss together with the Fast AI u-net implementation. Added final scores after 15 epochs in bottom right for readability in non-digital form.</figDesc><table><row><cell>CPU Type</cell><cell>RAM</cell><cell>GPU Type</cell><cell>RAM CUDA cores</cell></row><row><cell cols="4">Intel Xeon Broadwell-EP 2683v4 1024GB GeForce GTX 1080Ti 11GB 3584</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Details of the cluster used for training and experimentation</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>are obtained by training for 40 epochs on the GeForce GTX 1080Ti GPU with the aforementioned split of 35/8 for the training and validation data respectively. All final mIOU scores are measured on the 12 images from the test set.</figDesc><table><row><cell>Method</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>f1</cell><cell>val</cell><cell>test</cell></row><row><cell>DLv3+ (Xception65)</cell><cell cols="7">46.0 79.6 79.6 88.8 55.4 nan -</cell><cell cols="2">69.9 52.5</cell></row><row><cell>DLv3+ (ResNetV1 50 Beta)</cell><cell cols="7">48.6 67.6 85.7 88.4 47.3 nan -</cell><cell cols="2">67.5 48.0</cell></row><row><cell cols="9">Keras u-net (ResNet50) Focal 36.9 17.7 56.1 15.9 73.9 36.1 0.8053 -</cell><cell>39.8</cell></row><row><cell cols="9">Keras u-net (ResNet50) CCE 34.2 17.6 53.5 18.7 75.6 52.8 0.8144 -</cell><cell>42.5</cell></row><row><cell cols="9">fastai u-net (ResNet50) Focal 28.3 8.2 37.0 7.2 71.7 28.8 0.7536 -</cell><cell>30.2</cell></row><row><cell cols="9">fastai u-net (ResNet50) CCE 30.2 8.3 43.8 5.6 66.7 21.5 0.7387 -</cell><cell>29.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of DeepLabv3+ and u-net architecture variants trained on the DroneDeploy dataset. val: mean IOU on validation set trained on train set only. test: mean IOU on test set trained on train+validation set.</figDesc><table><row><cell>Class IOU scores: 1:Building, 2:Clutter, 3:Vegetation, 4:Water, 5:Ground, 6:Car</cell></row><row><cell>f1: f1 mean, reported for benchmark architecture comparison against DroneDe-</cell></row><row><cell>ploy public leaderboard.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of DeepLabv3+ architecture variants trained on the DroneDeploy train and validation images. test: mean IOU on test set.</figDesc><table><row><cell>Class IOU scores: 1:Building, 2:Clutter, 3:Vegetation, 4:Water, 5:Ground,</cell></row><row><cell>6:Car.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: creating environments for the DroneDeploy fastai/keras benchmark and DeepLabv3+ codebase</head><p>It is crucial to take note of your own GPU driver environment. For this work the following environment was available (High Performance Cluster at Eindhoven University of Technology):</p><p>• GCC &amp; G++: <ref type="bibr">5.4.0 (20160609)</ref> • The complete codebase for this work is also available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepLabv3+ Tensorflow research codebase 2 extensions</head><p>• convert rgb to index.py (altered to strip 3 dimensional segmentation labels to 1 dimensional)</p><p>• build dd data.py (altered for DroneDeploy compatiblity)</p><p>• data generator.py (altered for DroneDeploy compatiblity)</p><p>• train-dd-full.sh, eval-dd.sh, vis-dd.sh (dataset adaptations inspired by this GitHub repo 3 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DroneDeploy benchmark codebase 4 extensions</head><p>• custom training.py (implementation Focal loss function for fastai u-net)</p><p>• custom training keras.py (implementation Focal loss function for Keras u-net)</p><p>• images2chips.py (added test images conversion to tiles for DeepLabv3+ compatibility)</p><p>• scoring.py (added mean IOU and IOU score per class metric)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">From tensorflow/models/research/ directory run</title>
	</analytic>
	<monogr>
		<title level="m">export PYTHONPATH=$PYTHONPATH:&apos;pwd&apos;:&apos;pwd&apos;/slim</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Airbus Defence and Space. SPOT 6 -SPOT 7 High Resolution Broad Coverage Intelligence</title>
		<ptr target="URLwww.geostore.com" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1707.02937" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conditional Random Fields Meet Deep Neural Networks for Semantic Segmentation: Combining Probabilistic Graphical Models with Deep Learning for Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Måns</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2017.2762355</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="52" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Waterfall Atrous Spatial Pooling Architecture for Efficient Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Artacho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
		</author>
		<idno type="DOI">10.3390/s19245361</idno>
		<ptr target="https://www.mdpi.com/1424-8220/19/24/5361" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">5361</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">YOLOv4: Optimal Speed and Accuracy of Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2004.10934" />
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Introduction to Remote Sensing, Fifth Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J B</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R H</forename><surname>Wynne</surname></persName>
		</author>
		<ptr target="https://books.google.nl/books?id=NkLmDjSS8TsC" />
		<imprint>
			<date type="published" when="2011" />
			<publisher>Guilford Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Liang Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2699184</idno>
		<ptr target="http://liangchiehchen.com/projects/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), volume 11211 LNCS</title>
		<imprint>
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-030-01234-2{\}49</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on object detection in optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2016.03.014</idno>
		<idno>doi: 10.1016/ j.isprsjprs.2016.03.014</idno>
		<ptr target="https://doi.org/10.1016/j.isprsjprs.2016.03.014" />
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deformable Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.89</idno>
		<ptr target="https://github.com/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deforestation history of the eastern rain forests of Madagascar from satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sussman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">248</biblScope>
			<biblScope unit="issue">4952</biblScope>
			<biblScope unit="page" from="212" to="215" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. Lecture Notes in Computer Science (including subseries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">http:/arxiv.org/abs/1406.4729http:/dx.doi.org/10.1007/978-3-319-10578-9_23</idno>
		<idno>doi: 10. 1007/978-3-319-10578-9{\ }23</idno>
		<ptr target="http://arxiv.org/abs/1406.4729http://dx.doi.org/10.1007/978-3-319-10578-9_23" />
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
	<note>LNCS(PART</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<idno type="DOI">http:/arxiv.org/abs/2002.04688http:/dx.doi.org/10.3390/info11020108</idno>
		<ptr target="http://arxiv.org/abs/2002.04688http://dx.doi.org/10.3390/info11020108" />
		<title level="m">fastai: A Layered API for Deep Learning. Information (Switzerland)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GSP 216 Introduction to remote sensing</title>
		<ptr target="http://gsp.humboldt.edu/OLM/Courses/GSP_216_" />
	</analytic>
	<monogr>
		<title level="m">Humboldt State University: Resolution</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Humboldt State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Online/lesson3-1/resolution.html</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">USA for UNHCR Launches Satellite Imagery and Crowdsourcing Project to Improve Refugee Camp Planning and Maintenance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Ko</surname></persName>
		</author>
		<ptr target="https://bit.ly/3kW2MEU" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.02002" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An Improved Algorithm for Semantic Segmentation of Remote Sensing Images Based on Deeplabv3+</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangxin</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3369985.3370027</idno>
		<ptr target="https://doi.org/10.1145/3369985.3370027" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Communication and Information Processing</title>
		<meeting>the 5th International Conference on Communication and Information Processing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1411.4038" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SkyScapes-Fine-Grained Semantic Understanding of Aerial Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Seyed Majid Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vig</surname></persName>
		</author>
		<ptr target="https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760Aerialimagewithoverlaidannotation:dense" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploration of Optimized Semantic Segmentation Architectures for edge-Deployment on Drones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayani</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manan</forename><surname>Suri</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2007.02839" />
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">GitHub -dronedeploy/ddml-segmentation-benchmark: DroneDeploy Machine Learning Segmentation Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Pilkington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacey</forename><surname>Svetlichnaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Holmes</surname></persName>
		</author>
		<ptr target="https://github.com/dronedeploy/dd-ml-segmentation-benchmark" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rates and drivers of mangrove deforestation in Southeast Asia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="344" to="349" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno>9783319245737. doi: 10.1007/ 978-3-319-24574-4{\ }28</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Roser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Ortiz-Ospina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Extreme Poverty. Our World in Data</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully residual convolutional neural networks for aerial image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Nguyen Duc</surname></persName>
		</author>
		<idno type="DOI">10.1145/3287921.3287970</idno>
		<ptr target="http://dl.acm.org/citation.cfm?doid=3287921.3287970" />
	</analytic>
	<monogr>
		<title level="m">ACM International Conference Proceeding Series</title>
		<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Shermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Van Etten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Pacifici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronny</forename><surname>Haensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Bastidas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Soenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Bacastow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2004.06500" />
	</analytic>
	<monogr>
		<title level="m">Multi-Sensor All Weather Mapping Dataset</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="768" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rooftop detection using aerial drone imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kritik</forename><surname>Soman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3297001.3297041</idno>
		<ptr target="http://dl.acm.org/citation.cfm?doid=3297001.3297041" />
	</analytic>
	<monogr>
		<title level="m">ACM International Conference Proceeding Series</title>
		<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="281" to="284" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Hierarchical Multi-Scale Attention for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2005.10821" />
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully Residual Con-volutional Neural Networks for, Aerial Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Nguyen Duc</surname></persName>
		</author>
		<idno type="DOI">10.1145/3287921.3287970</idno>
		<ptr target="https://doi.org/10.1145/3287921.3287970" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Symposium on Information and Communication Technology -SoICT</title>
		<meeting>the Ninth International Symposium on Information and Communication Technology -SoICT<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accurate Semantic Segmentation in Remote Sensing Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3373509.3373535</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3373509.3373535" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 8th International Conference on Computing and Pattern Recognition</title>
		<meeting>the 2019 8th International Conference on Computing and Pattern Recognition<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="173" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Weigand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Taubenböck</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2019.02.006</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Future Intelligent Earth Observing Satellites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.501232</idno>
	</analytic>
	<monogr>
		<title level="j">Proc SPIE</title>
		<imprint>
			<biblScope unit="volume">5151</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving Semantic Segmentation via Video Propagation and Label Relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1812.01593" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rethinking Pre-training and Self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2006.06882" />
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
