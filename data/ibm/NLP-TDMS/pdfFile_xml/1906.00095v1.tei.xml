<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Pupil Has Become the Master: Teacher-Student Model-Based Word Embedding Distillation with Ensemble Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonggun</forename><surname>Shin</surname></persName>
							<email>bonggun.shin@emory.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Emory University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<email>haoyang@visa.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Visa Research</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
							<email>jinho.choi@emory.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Emory University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Pupil Has Become the Master: Teacher-Student Model-Based Word Embedding Distillation with Ensemble Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in deep learning have facilitated the demand of neural models for real applications. In practice, these applications often need to be deployed with limited resources while keeping high accuracy. This paper touches the core of neural models in NLP, word embeddings, and presents a new embedding distillation framework that remarkably reduces the dimension of word embeddings without compromising accuracy. A novel distillation ensemble approach is also proposed that trains a highefficient student model using multiple teacher models. In our approach, the teacher models play roles only during training such that the student model operates on its own without getting supports from the teacher models during decoding, which makes it eighty times faster and lighter than other typical ensemble methods. All models are evaluated on seven document classification datasets and show significant advantage over the teacher models for most cases. Our analysis depicts insightful transformation of word embeddings from distillation and suggests a future direction to ensemble approaches using neural models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As deep learning starts dominating the field of machine learning, there have been growing interests in deploying deep neural models for real applications. <ref type="bibr" target="#b4">[Hinton et al., 2014]</ref> stated that academic research on model development had mostly focused on accuracy improvement, whereas the deployment of deep neural models would also require the optimization of other practical aspects such as speed, memory, storage, power, etc. To satisfy these requirements, several neural model compression methods have been proposed, which can be categorized into the following four: weight pruning <ref type="bibr" target="#b1">[Denil et al., 2013;</ref><ref type="bibr" target="#b2">Han et al., 2015;</ref><ref type="bibr" target="#b6">Jurgovsky et al., 2016]</ref>, weight quantization <ref type="bibr" target="#b3">[Han et al., 2016;</ref><ref type="bibr" target="#b6">Jurgovsky et al., 2016;</ref><ref type="bibr">Ling et al., 2016]</ref>, lossless compression <ref type="bibr" target="#b13">[Van Leeuwen, 1976;</ref><ref type="bibr" target="#b2">Han et al., 2015]</ref>, and distillation <ref type="bibr" target="#b10">[Mou et al., 2016]</ref>. This paper focuses on distillation methods that can remarkably reduce the model size, resulting in much less memory usage and fewer computations.</p><p>Distillation aims to extract core elements from a complex network and transfer them to a simpler network so it gives comparable results to the complex network. It has been shown that the core elements can be transferred to various types of networks i.e., deep to shallow networks <ref type="bibr" target="#b0">[Ba and Caruana, 2014]</ref>, recurrent to dense networks <ref type="bibr" target="#b0">[Chan et al., 2015]</ref>, and vice versa <ref type="bibr" target="#b11">[Romero et al., 2014;</ref><ref type="bibr">Tang et al., 2016]</ref>. Lately, embedding distillation was suggested <ref type="bibr" target="#b10">[Mou et al., 2016]</ref>, which transferred the output of the projection layer in the source network as input to the target network, although accuracy drop was expected with this approach. Considering the upper bound of a distilled network, that is the accuracy achieved by the original network <ref type="bibr" target="#b0">[Ba and Caruana, 2014]</ref>, enough room is left for the improvement of embedding distillation. Distilled embeddings can significantly enhance the efficiency of deep neural models in NLP, where the majority of model space is occupied by word embeddings.</p><p>In this paper, we first propose a new embedding distillation method based on three teacher-student frameworks, which is a more advanced way of embedding distillation, because the previous one <ref type="bibr" target="#b10">[Mou et al., 2016</ref>] is a standalone embedding distillation (Section 2.4) with limited knowledge transfer. Our distilled embeddings not only enable the target network to outperform the previous state of the art <ref type="bibr" target="#b10">[Mou et al., 2016]</ref>, but also are eight times smaller than the original word embeddings yet allow the target network to achieve compatible (sometimes higher) accuracy to the source network. We then present a novel ensemble approach which extends this distillation framework by allowing multiple teacher models when training a student model. After learning from multiple teachers during training, the student model runs on its own during decoding such that it performs faster and lighter than any of the teacher models yet pushes the accuracy much beyond them with just 1.25% ( 50 /4000) the size of other typical ensemble models. All models are evaluated on seven document classification datasets; our experiments show the effectiveness of the proposed frameworks, and our analysis illustrates an interesting nature of the distilled word embeddings. To the best of our knowledge, this is the first time that embedding distillation is thoroughly examined for natural language processing and used in ensemble to achieve such promising results. 1 (a) Logit Matching (Sec. 2.1) (b) Noisy Logit Matching (Sec. 2.2) (c) Softmax Tau Matching (Sec. 2.3) <ref type="figure">Figure 1</ref>: Three teacher-student methods described in Background section, which uses different cost functions to transfer trained knowledge from the teacher model to the student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Our embedding distillation framework is based on teacherstudent models <ref type="bibr" target="#b0">[Ba and Caruana, 2014;</ref><ref type="bibr" target="#b12">Sau and Balasubramanian, 2016;</ref>, where teacher models are trained on deep neural networks and transfer their knowledge to student models on simpler networks. The following subsections describe three popular teacher-student methods applied to our framework. The main difference between these three methods is in their cost functions ( <ref type="figure">Figure 1</ref>). The last subsection discusses embedding encoding that is used to extract distilled embeddings from the projection layer. Throughout this section, a logit refers to a vector representing the layer immediately before the softmax layer in a neural network, where z i and v i are the teacher's and student's logit values for the class i, respectively. Note that the student models are not necessarily optimized for only the gold labels but also optimized for the logit values from the teacher models in these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Logit Matching (LM)</head><p>Proposed by <ref type="bibr" target="#b0">[Ba and Caruana, 2014]</ref>, the cost function of this teacher-student method is defined by the logit differences between the teacher and the student models (D: the total number of classes):</p><formula xml:id="formula_0">L LM = 1 2 · D D i=1 |z i − v i | 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Noisy Logit Matching (NLM)</head><p>Proposed by <ref type="bibr" target="#b12">[Sau and Balasubramanian, 2016]</ref>, this method is similar to Logit Matching except that Gaussian noise is introduced during the distillation, simulating variations in the teacher models, which gives a similar effect for the student to learn from multiple teachers. The cost function takes random noise η drawn from Gaussian distribution such that the logit of each teacher model is</p><formula xml:id="formula_1">z i = (1 + η) · z i . Thus, the final cost function becomes L N LM = 1 2D ∀i |z i − v i | 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Softmax Tau Matching (STM)</head><p>Proposed by <ref type="bibr" target="#b4">[Hinton et al., 2014]</ref>, this method is based on softmax matching where softmax values are compared between the teacher and the student models instead of logits. Later, <ref type="bibr" target="#b4">[Hinton et al., 2014]</ref> added two hyperparameters to further generalize this method. The first hyperparameter, λ, is for the weighted average of two sub-cost functions, where the first sub-cost function measures a cross-entropy between the student's softmax predictions and the truth values, represented as L 1 = − ∀i y i log p i (i indexes classes, y is the gold label, p i ∈ (0, 1) is the prediction for a sample). Another cost function involves the second hyperparameter, τ , that is a temperature variable normalizing the output of the teacher's logit value:</p><p>s i (z, τ ) = e z i/τ D j=1 e z j/τ Given s i , the second cost function can be defined as L 2 = − ∀i s i (z, τ ) log p i . Therefore, the final cost function becomes L ST M = λL 1 + (1 − λ)L 2 . If λ weights more on L 1 , the student model values more on the gold labels than teacher's predictions. If τ is greater, the teacher's output becomes more uniformed, implying that the probability values are spread out more throughout all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Embedding Encoding (ENC)</head><p>Embedding distillation was first proposed by <ref type="bibr" target="#b10">[Mou et al., 2016]</ref> for NLP tasks. Unlike our framework, their method does not rely on teacher-student models, but rather directly trains a single model with an encoding layer inserted between the embedding layer and its upper layer in the network <ref type="bibr">(Figure 2a)</ref>. Each word w i is entered to an embedding layer φ that yields a large embedding vector φ(w i ). This vector is projected into a smaller embedding space by W enc with an activation function f . As a result, a smaller embedding vector φ (w i ) is produced for w i as follows (b enc : a bias for the projection):</p><formula xml:id="formula_2">φ (w i ) = f (W enc · φ(w i ) + b enc )</formula><p>The smaller embedding φ (w i ) generated by this projection contains distilled knowledge from the larger embedding φ(w i ). The cost function of this method simply measures the crossentropy between gold labels and the softmax output values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Embedding Distillation</head><p>Our proposed embedding distillation framework begins by training a teacher model using the original embeddings. After training, the teacher model generates the corresponding logit value for each input in the training data. Then, a student model that comprises a projection layer is optimized for the logit (or softmax) values from the teacher model. After training the student model, small embeddings are distilled from the projection layer ( <ref type="figure" target="#fig_3">Figure 3a</ref>).  The original large embeddings as well as weights in the projection layer are discarded for deployment such that the small embeddings can be referenced directly from the word indices in the student model during decoding <ref type="figure" target="#fig_3">(Figure 3b</ref>). Such distillation significantly reduces the model size and computations in the network, which is welcomed in production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Distillation via Teacher-Student Models</head><p>Projecting a vector into a lower dimensional space generally entails information loss, although it does not have to be the case under two conditions. First, the source embeddings comprise both relevant and irrelevant information for the target task, therefore, there is a room to discard the irrelevant information. Second, the projection layer in the target network is capable of preserving the relevant information from the source embeddings. The first condition is met for NLP because most vector space models such as Word2Vec <ref type="bibr" target="#b9">[Mikolov et al., 2013]</ref>, <ref type="bibr">Glove [Pennington et al., 2014</ref><ref type="bibr">], or FastText [Bojanowski et al., 2017</ref> are trained on a vast amount of text, where only a small portion is germane to a specific task.</p><p>To meet the second condition, teacher-student models are adapted to our distillation framework, where the projection layer in a student model learns the relevant information for the target task from a teacher model. The output dimension of the projection layer is generally much smaller than the size of the embedding layer in a teacher model. Note that it is possible to integrate multiple projection layers in a student model; Experiment section discusses performance difference by adding different numbers of projection layers in the student model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Projection Layer Initialization</head><p>Unlike <ref type="bibr" target="#b10">[Mou et al., 2016]</ref> who randomly initialized the projection layer, it is initialized with vectors pre-trained by an autoencoder <ref type="bibr" target="#b4">[Hinton and Zemel, 1994]</ref> in our framework. This initialization stabilizes and improves optimization of neural networks during training, resulting more robust models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distillation Ensemble</head><p>Ensemble methods generally achieve higher accuracy than a standalone model; however, slow speed is expected due to the runs from multiple models in ensemble. This section presents a novel ensemble approach based on our distillation framework using logit matching that produces a light-weighted student model trained by multiple teachers <ref type="figure" target="#fig_5">(Figure 4</ref>). The premise of this approach is that it is possible to have multiple teacher models train a student model by combining their logit values such that the student no longer needs the teachers during decoding because it already learned "enough" from them during training. As a result, our ensemble approach ensures higher efficiency for the student model than for any of the teacher models during decoding. The following sections describe two different ensemble methods applied to our framework.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Routing by Agreement Ensemble (RAE)</head><p>This method gives more weights to the majority, by adopting the dynamic routing algorithm presented by <ref type="bibr" target="#b11">[Sabour et al., 2017]</ref>. It first collects the consensus of all teachers, then boosts weights of the teachers who strongly agree with that consensus whereas suppresses the influence of the teachers who do not agree as much. The procedure of calculating the representing logit is described in Algorithm 1. The squash function in lines 4 and 9 is a non-linear activation function that ensures the norm of the output vector to be in [0, 1] <ref type="bibr" target="#b11">[Sabour et al., 2017]</ref>. This vectorized activation is important for a routing algorithm because both magnitude and direction play an important role in the agreement calculation by the dot product, which enforces the consensus into the direction with strong confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Routing by Disagreement Ensemble (RDE)</head><p>This method focuses on the minority vote instead, because minority opinions may cast important information for the task.</p><p>The algorithm is the same as RAE, except for the sign of the weight update (line 2 in Algorithm 1).   </p><formula xml:id="formula_3">∈ R C . 2 k ← 1 if b is RAE else −1 3 for t ∈ {1, . . . , T } do 4 x t ← squash(z t ), wt ← 0 5 while n iterations do 6 c ← softmax(w) 7 z rep = T t=1 ct · z t 8 s ← squash(z rep ) 9 if not last iteration</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Word Embeddings</head><p>For sentiment analysis, raw text from the Amazon Review dataset 2 is used to train word embeddings, resulting 2.67M word vectors. For the other tasks, combined text from Wikipedia and the New York Times Annotated corpus 3 are used, resulting 1.96M word vectors. For each group, two sets of embeddings are trained with dimensions of 50 and 400 by Word2Vec <ref type="bibr" target="#b9">[Mikolov et al., 2013]</ref>. While training, default hyper-parameters are used without an explicit hyper-parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Network Configuration</head><p>Two types of teacher models are developed using Convolutional Neural Networks (CNN) and Long Short-Term Memory Networks (LSTM); comparing different types of teacher models provides more generalized insights for our distillation framework. All teacher models use 400 dimensional word embeddings, and all student models are based on CNN. The CNN-based teacher and student models share the followings: filter sizes = [2, 3, 4, 5], # of filters = 32, dimension of the hidden layer right below the softmax layer = 50. Teacher models add a dropout of 0.8 to the hidden layer, wheres student models add dropouts of 0.1 to both the hidden layer and the the projection layer. On the other hand, the LSTM-based teacher models use two bidirectional LSTM layers. Only the last output vector is fed into the 50 dimensional hidden layer, which becomes the input to the softmax layer. A dropout of 0.2 is applied to all hidden layers, both in and out of the LSTM. For both CNN and LSTM ensembles, all 10 teachers share the same model structures with different initializations. Although this limited teacher diversity, our ensemble method produces remarkably good results (Section 5.7). Each student model may consist of one or two projection layers. The one-layered projection adds one 50 dimensional hidden layer above the embedding layer that transfers core knowledge from the original embeddings to the distilled embeddings. The two-layered projection comprises two layers; the size of the lower layer is the same as the size of teacher's embedding layer, 400, and the size of the upper layer is 50, which is the dimension of the distilled embeddings. This twolayered projection is empirically found to be more robust than various combinations of network architectures including wider and deeper layers from our experiments. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Pre-trained Weights</head><p>An autoencoder comprising a 50-dimensional encoder and a 400-dimensional decoder is used to pre-train weights for the two-layered projection in student models, where the encoder and the decoder have the same and inversed shapes as the upper and lower layers of the projection. Note that results by using pre-trained weights for the one-layered projection are not reported in <ref type="table">Table 2</ref> due to the limited space, but we consistently see robust improvement using pre-trained weights for the projection layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Embedding Distillation</head><p>Six models are evaluated on the seven datasets in <ref type="table" target="#tab_0">Table 1</ref> to show the effectiveness of our embedding distillation framework: logit matching (LM), noisy logit matching (NLM), softmax tau matching (STM) models with teacher-student based embedding distillation (TSED), and another three models with the autoencoder pre-trained weights (*+PT) and the two layered projection network (*+2L). Teacher models using 400-dim embeddings (*-400) are also presented along with the baseline model using 50-dim word embeddings (*-50) and the previous distillation model (ENC). The comparison to these  <ref type="table">Table 2</ref>: Results from our embedding distillation models on the evaluation sets in <ref type="table" target="#tab_0">Table 1</ref> using the CNN-based (the rows 5-10) and the LSTM-based teacher models (rows 13-18) along with the teacher models (*-400), baseline model (*-50) and previous distillation model (ENC). All models are tuned on the development sets and the best performing models are tested on the evaluation sets. Since neural models produce different results at any training due to the random initialization, five models are developed for each approach to avoid (un)lucky peaks, except for *-400 and *-50 where the results are achieved by selecting the best models among ten trials on the development sets. Each score is based on these five trials and represented as a pair of [Average ± Standard Deviation]. two models highlights the strength of our distilled models, significantly outperforming them with the same dimensional word embeddings (50-dim). <ref type="table">Table 2</ref> shows the results achieved by all models. While the two existing models, *-50 and ENC, show marginal differences, our proposed models, *+TSED, outperform the previous embedding distillation SOTA (ENC). Our final models, *+PT+2L, outperform all the other models, reaching similar (or even higher) accuracy to the teacher models (*-400), which confirms that the proposed embedding distillation framework can successfully transfer the core knowledge from the original embeddings with respect to the target tasks, independent from the network structures of the teacher models.</p><p>The fact that the best model for each task comes from a different teacher-student strategy appears to be random. However, considering that it is the nature of neural network models whose accuracy deviates at every training, this is not surprising although it signifies the need of ensemble approaches to take advantage of multiple teachers and produce a more robust student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Lexical Analysis</head><p>Embedding distillation for a task such as sentiment analysis can be viewed as a vector transformation that adjusts similarities between word embeddings with respect to their sentiments. Thus, we hypothesize that distilled word embeddings from sentiment analysis should bring similar sentiment words together while disperse opposite ones in vector space.</p><p>To verify this, lexical analysis on both the original and the distilled word embeddings is conducted using the four sentiment datasets: SST-1, SST-2, MR, and CR. First, positive and negative words are collected from two publicly available lexicons, the MaxDiff Twitter Sentiment Lexicon <ref type="bibr" target="#b7">[Kiritchenko et al., 2014]</ref> and the Bing Liu Opinion Lexicon <ref type="bibr" target="#b5">[Hu and Liu, 2004]</ref>. Then, two groups of sentiment word sets are constructed, (P t , N t ) and (P o , N o ), where P * and N * compose positive and negative words, and * t and * o are collected from the Twitter and the Opinion lexicons, respectively. Next, the intersection between each type of sentiment word sets is found, that are P to = P t ∩ P o and N to = N t ∩ N o , where |P to | = 72 and |N to | = 89. Finally, the intersections between * to and the vocabulary set from each of the four sentiment datasets are found (e.g., P = A ∩ P to , where is one of the four datasets  For each , cosine similarities are measured for all possible word pairs (w i , w j ) ∈ P N × P N where P N = P ∪ N , using the original and distilled embeddings. <ref type="figure" target="#fig_6">Figure 5</ref> illustrates the similarity distributions. It is clear that similar sentiment words generally give high similarity scores with the distilled embeddings (the plots drawn by red circles), whereas opposite sentiment words give low similarity scores (the plots drawn by red crosses). On the other hand, low similarity scores are shown for any case with the original embeddings (the plots drawn by blue circles and crosses). The normal distributions derived from the distilled embeddings (the red lines) are more symmetric and spread than the ones from the original embeddings (the blue lines), implying that distilled embeddings are more stable for the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Distillation Ensemble</head><p>All ensemble models are based on our distillation framework using logit matching (LM) where the teacher models compose of 10 CNN-based or 10 LSTM-based models. Two ensemble methods are evaluated: Routing by Agreement (RAE) and Routing by Disagreement (RDE), and <ref type="figure" target="#fig_7">Figure 6</ref> shows comparisons between these ensemble models against the teacher models.</p><p>The most notable finding is that RDE significantly outperforms the teacher, if the dataset is big. For example, RDE outperforms the teacher models on average, except for CR and MPQA, whose training set is relatively small ( <ref type="table" target="#tab_0">Table 1)</ref>. The insight behind this trend might be that if there are many data samples, then the probability of exploring different knowledge from minor opinions could be increased, which would positively affect to the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Model Reduction</head><p>The deployment models from either distillation or ensemble are notably smaller than the teacher models. Since word embeddings occupy a large portion of neural models in NLP, reducing the size of word embeddings through distillation decreases the size of the entire model roughly by the same ratio as its reduction; in our case, eight times ( 400 /50). Furthermore, if the proposed distillation ensemble method is compared to other typical ensemble methods, this reduction ratio becomes even larger. <ref type="table" target="#tab_5">Table 3</ref> shows the number of neurons required for previous ensemble methods and the proposed one. When training, the proposed one comprises 10% more parameters due to the distillation process. However, when deploying, the reduction of neuron is about eighty times ( 4000 /50). This is because the proposed framework doesn't require repetitive evaluation of teacher models when testing, while other ensemble methods require evaluation of all sub models (teachers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous</head><p>Proposed Reduction Train O(400M * 10) O(400M * 11) ×0.91 Deploy O(400M * 10) O(50M ) ×80 It is worth mentioning that the deployment models produced by distillation ensemble not only outperform the teacher models in accuracy but also are significantly smaller such that they operate much faster and lighter than the teacher models upon deployment. This is very welcoming for those who want to embed these models into low-resource platforms such as mobile environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes a new embedding distillation framework based on several teacher-student methods. Our experiments show that the proposed distillation models outperform the previous distillation model and give compatible accuracy to the teacher models, yet they are significantly smaller. Lexical analysis on sentiments reveals the comprehensiveness of the distilled embeddings. Moreover, a novel distillation ensemble approach is proposed, which shows huge advantage in both speed and accuracy over any teacher model. Our distillation ensemble approach consistently shows more robust results when the size of training data is sufficiently large.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) The core elements from large embeddings are distilled during training using gold labels and transferred to smaller embeddings. (b) Only the small embeddings are kept for the deployment, resulting less space and fewer computations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Embedding distillation.(b) Model deployment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) Our proposed embedding distillation framework using teacher-student models. (b) Only the small embeddings are kept for deployment, similarly toFigure 2b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Embedding distill ensemble.(b) Model deployment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>(a) Our proposed embedding distillation ensemble model. One representing logit (R. LOGIT) is calculated from a set of multiple teachers' logits by the proposed ensemble methods. (b) No need to evaluate teachers at deployment, unlike other ensemble methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Similarity distributions among sentiment word pairs. Blue and Red colors distinguish histograms from the original and distilled embeddings, respectively. The solid and dashed lines show the distributions from all word pairs regardless of their sentiments. Circles are for sentimentally similar word pairs, that are similarities between positive word pairs and negative word pairs such that (wi, wj) ∈ (P × P )|(N × N ). Crosses are for sentimentally opposite word pairs, that are similarities across positive and negative word pairs such that (wi, wj) ∈ P × N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Accuracy comparisons between the ensemble and the teacher models. To avoid (un)lucky peaks, each method is evaluated 20 times where each trial produces a different result. These evaluation results are shown as boxplots in thisfigure.and A is the set of all words in ):• = SST 1 ⇒ |P | = 66 and |N | = 83. • = SST 2 ⇒ |P | = 67 and |N | = 83. • = MR ⇒ |P | = 67 and |N | = 82. • = CR ⇒ |P | = 19 and |N | = 57.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Get R. LOGIT for RAE and RDE Input: Teachers' logits Z ∈ R T ×C , and an algorithm selector b ∈ {RAE, RDE} 1 . Output: The representing logit, z rep</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Seven datasets used for our experiments. C: num- ber of classes, TRN/DEV/TST: number of instances in train- ing/development/evaluation set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>11± 0.97 44.94± 1.26 83.71± 1.41 90.64± 0.49 90.60± 1.10 80.88± 1.22 88.65± 0.60 63± 0.37 48.71± 0.73 85.04± 0.64 91.91± 0.29 92.48± 0.73 81.84± 0.57 89.14± 0.25 NLM+TSED 50 78.10± 0.40 48.66± 0.83 85.17± 0.16 92.03± 0.36 92.36± 0.91 83.04± 0.74 88.90± 0.34 STM+TSED 50 77.81± 0.33 49.10± 0.34 84.72± 0.70 92.25± 0.38 92.76± 0.65 80.31± 0.42 89.13± 0.28 LM+TSED+PT+2L 50 79.06± 0.59 49.82± 0.54 85.75± 0.42 92.63± 0.23 92.58± 0.86 83.40± 0.76 89.44± 0.20</figDesc><table><row><cell>Model</cell><cell>Emb.Size</cell><cell>MR</cell><cell>SST-1</cell><cell>SST-2</cell><cell>Subj</cell><cell>TREC</cell><cell>CR</cell><cell>MPQA</cell></row><row><cell cols="3">ENC 77.CNN-400 50 400 79.07</cell><cell>49.86</cell><cell>86.22</cell><cell>92.34</cell><cell>93.60</cell><cell>83.82</cell><cell>88.78</cell></row><row><cell>CNN-50</cell><cell>50</cell><cell>78.07</cell><cell>45.07</cell><cell>84.51</cell><cell>90.81</cell><cell>91.00</cell><cell>80.89</cell><cell>86.40</cell></row><row><cell cols="9">LM+TSED 77.NLM+TSED+PT+2L 50 50 78.60± 0.70 49.90± 0.59 85.31± 0.75 92.26± 0.20 92.80± 0.62 83.82± 0.49 89.61± 0.14</cell></row><row><cell>STM+TSED+PT+2L</cell><cell>50</cell><cell cols="7">78.77± 0.70 49.19± 0.71 85.83± 0.59 92.38± 0.53 93.48± 0.30 83.57± 0.85 89.95± 0.28</cell></row><row><cell>LSTM-400</cell><cell>400</cell><cell>79.28</cell><cell>49.23</cell><cell>86.22</cell><cell>92.71</cell><cell>92.00</cell><cell>82.98</cell><cell>89.73</cell></row><row><cell>LSTM-50</cell><cell>50</cell><cell>77.16</cell><cell>43.76</cell><cell>83.36</cell><cell>90.02</cell><cell>86.00</cell><cell>80.06</cell><cell>85.66</cell></row><row><cell>LM+TSED</cell><cell>50</cell><cell cols="7">78.61± 0.80 48.79± 0.27 85.81± 0.77 91.74± 0.36 92.56± 0.89 82.76± 0.36 89.59± 0.28</cell></row><row><cell>NLM+TSED</cell><cell>50</cell><cell cols="7">78.89± 0.73 48.81± 0.44 85.55± 0.74 91.87± 0.32 91.80± 1.29 82.96± 0.50 89.63± 0.10</cell></row><row><cell>STM+TSED</cell><cell>50</cell><cell cols="7">78.85± 0.60 48.77± 0.83 86.11± 0.36 91.99± 0.16 92.36± 0.43 82.96± 0.72 89.60± 0.16</cell></row><row><cell>LM+TSED+PT+2L</cell><cell>50</cell><cell cols="7">80.33± 0.40 49.37± 0.39 86.12± 0.47 92.53± 0.29 91.91± 0.63 83.54± 0.78 90.15± 0.13</cell></row><row><cell>NLM+TSED+PT+2L</cell><cell>50</cell><cell cols="7">79.33± 0.66 48.87± 0.53 85.89± 0.43 92.35± 0.18 92.08± 0.46 83.32± 0.61 89.92± 0.35</cell></row><row><cell>STM+TSED+PT+2L</cell><cell>50</cell><cell cols="7">80.09± 0.49 49.14± 0.62 86.95± 0.44 92.34± 0.49 92.96± 0.62 82.73± 0.25 89.83± 0.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The number of neurons in previous ensemble methods and the proposed distillation ensemble method for training and deploying. M represents the basic unit of model size for the embedding dimension 1. This table assumes ensemble with 10 teachers.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://snap.stanford.edu/data/web-Amazon.html 3 https://catalog.ldc.upenn.edu/LDC2008T19</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Among convolutional, relational, and dense-networks with different configurations, the dense-network with the reported configuration produces the best results.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caruana ; Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01483</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Nan Rosemary Ke, and Ian Lane</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Transferring knowledge from a rnn to a dnn</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Denil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zemel ; Geoffrey E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Minqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating memory efficiency and robustness of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jurgovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="200" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="723" to="762" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word embeddings with limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth ; Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Ling et al., 2016] Shaoshi Ling, Yangqiu Song, and Dan Roth</editor>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="387" to="392" />
		</imprint>
	</monogr>
	<note>Proceedings of the 19th international conference on Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">271</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 42nd annual meeting on Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee ; Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar; Nicolas Ballas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramanian ; Bharat Bhusan</forename><surname>Sau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Vineeth N Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09650</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<editor>Tang et al., 2016] Zhiyuan Tang, Dong Wang, and Zhiyong Zhang</editor>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="5900" to="5904" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>2016 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Leeuwen ; Jan Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICALP</title>
		<imprint>
			<date type="published" when="1976" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
	<note>On the construction of huffman trees</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
