<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collaborative Similarity Embedding for Recommender Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
							<email>cjwang@citi.sinica.edu.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
							<email>mftsai@nccu.edu.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
							<email>yang@citi.sinica.edu.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Chengchi University Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Academia Sinica Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National Chengchi University Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Academia Sinica Taipei</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Collaborative Similarity Embedding for Recommender Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ACM Reference Format: 2019. Collaborative Similarity Embedding for Recommender Systems. In Pro-ceedings of ACM conference (WWW&apos;19). ACM, New York, NY, USA, Article 4, 9 pages. https://doi.org/10.475/123_4</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present collaborative similarity embedding (CSE), a unified framework that exploits comprehensive collaborative relations available in a user-item bipartite graph for representation learning and recommendation. In the proposed framework, we differentiate two types of proximity relations: direct proximity and k-th order neighborhood proximity. While learning from the former exploits direct user-item associations observable from the graph, learning from the latter makes use of implicit associations such as user-user similarities and item-item similarities, which can provide valuable information especially when the graph is sparse. Moreover, for improving scalability and flexibility, we propose a sampling technique that is specifically designed to capture the two types of proximity relations. Extensive experiments on eight benchmark datasets show that CSE yields significantly better performance than state-of-theart recommendation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The task of recommender systems is to produce a list of recommendation results that match user preferences given their past behavior. Collaborative filtering (CF), a common yet powerful approach, generates user recommendations by taking advantage of the collective wisdom from all users <ref type="bibr" target="#b13">[14]</ref>. Many CF-based recommendation algorithms have been shown to work well across various domains and been used in many real-world applications <ref type="bibr" target="#b26">[27]</ref>.</p><p>The core idea of model-based CF algorithms is to learn lowdimensional representations of users and items from either explicit user-item associations such as user-item ratings or implicit feedback such as playcounts and dwell time. This can be done by training a rating-based model with matrix completion to learn from observed user-item associations (either explicit or implicit feedback) to predict associations that are unobserved <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>. In addition to this rating-based approach, ranking-based methods have been proposed based on optimizing ranking loss; the ranking-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref> have been found more suitable for implicit feedback. However, many existing model-based CF algorithms leverage only the user-item associations available in a given user-item bipartite graph. Thus, when the available user-item associations are sparse, these algorithms may not work well.</p><p>It has been noted that it is possible to mine from a user-item bipartite graph other types of collaborative relations, such as user-user similarities and item-item similarities, since users and items can be indirectly connected in the graph. Moreover, by taking random walks on the graph, it is possible to exploit higher-order proximity among users and items. Using item-item similarities in the learning process has been firstly studied by Liang et al. <ref type="bibr" target="#b17">[18]</ref>, who propose to jointly decompose the user-item interaction matrix and the itemitem co-occurrence matrix with shared item latent factors. Hsieh et al. <ref type="bibr" target="#b11">[12]</ref> propose to learn a joint metric space to encode both user preferences and user-user and item-item similarities. A recent work presented by Yu et al. <ref type="bibr" target="#b34">[35]</ref> shows that jointly modeling user-item, user-user, and item-item relations outperforms competing methods that consider only user-item relations. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>, the higher-order proximity has been shown useful in graph embedding methods. In general, exploiting additional collaborative relations shows promise in learning better representations of vertexes in an information graph.</p><p>We note that these prior arts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref> share the same core idea: using some specific methods to sample auxiliary information from a graph to augment the data for representation learning. However, there is a lack of a unified and efficient model that generalizes the underlying computation and aims at recommendation problems. For example, Liang et al. <ref type="bibr" target="#b17">[18]</ref> consider only the item-item similarities but no other collaborative relations; Yu et al. <ref type="bibr" target="#b34">[35]</ref> consider only ranking-based loss functions but not rating-based ones. Higherorder proximity is exploited in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref>, which however deal with the general graph embedding problem not the recommendation one. Moreover, the model presented by <ref type="bibr" target="#b11">[12]</ref> fails to manage large-scale user-item associations <ref type="bibr" target="#b28">[29]</ref>.</p><p>To address this discrepancy, in this paper we present collaborative similarity embedding (CSE), a unified representation learning        framework for collaborative recommender systems with the aim of modeling the direct and in-direct edges of user-item interactions in a simple and effective way. CSE involves a direct similarity embedding module for modeling user-item associations as well as a neighborhood similarity embedding module for modeling user-user and item-item similarities. The former module provides the flexibility to implement various types of modeling techniques for user-item associations, whereas the later module models user-user and item-item relations via k-order neighborhood proximity. To simultaneously manage the two modules, we introduce triplet embedding into the proposed framework to ideally model user-user, item-item clustering and user-item relations in a single and joint-learning model, while most prior arts use only one or two embedding mappings in their methods. Moreover, the two sub-modules are fused by a carefully designed sampling technique for scalability and flexibility For scalability, the space complexity and time of convergence are both only linear with respect to the number of observed user-item associations. In addition, with the proposed sampling techniques, CSE provides the flexibility to shape different relation distributions in its optimization. Extensive experiments were conducted on eight recommendation datasets that cover different user-item interaction types, levels of data sparsity, and data sizes. We compare the performance of CSE with classic methods such as matrix factorization (MF) and Bayesian personalized ranking (BPR) <ref type="bibr" target="#b24">[25]</ref>, recent methods that incorporate user-user and/or item-item relations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref>, as well as several general graph embedding methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>. The evaluation shows that CSE outperforms the competing methods for seven out of the eight datasets.</p><formula xml:id="formula_0">I c k = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L K g C x c u K t o H t C F M p p N 2 6 G Q S Z i a F E v I n b l w o 4 t Y / c e f f O G m z 0 N Y D A 4 d z 7 u W e O U H C m d K O 8 2 1 V 1 t Y 3 N r e q 2 7 W d 3 b 3 9 A / v w q K P i V B L a J j G P Z S / A i n I m a F s z z W k v k R R H A a f d Y H J T + N 0 p l Y r F 4 k n P E u p F e C R Y y A j W R v J t e x B h</formula><formula xml:id="formula_1">I c k = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L K g C x c u K t o H t C F M p p N 2 6 G Q S Z i a F E v I n b l w o 4 t Y / c e f f O G m z 0 N Y D A 4 d z 7 u W e O U H C m d K O 8 2 1 V 1 t Y 3 N r e q 2 7 W d 3 b 3 9 A / v w q K P i V B L a J j G P Z S / A i n I m a F s z z W k v k R R H A a f d Y H J T + N 0 p l Y r F 4 k n P E u p F e C R Y y A j W R v J t e x B h</formula><formula xml:id="formula_2">v 0 z T F Z I y H t G u p x I L q M J 8 f P E V n V h m g O F G 2 p E F z 9 f d E j o X W E x H Z T o H N S C 9 7 M / E / r 5 u Z + D r M m U w z Q y V Z L I o z j k y C Z t + j A V O U G D 6 x B B P F 7 K 2 I j L D C x N i M K j Y E b / n l V d K 6 q H t u 3 b u 7 r D V u i z j K c A K n c A 4 e X E E D b s C H A A g I e I Z X</formula><formula xml:id="formula_3">v 0 z T F Z I y H t G u p x I L q M J 8 f P E V n V h m g O F G 2 p E F z 9 f d E j o X W E x H Z T o H N S C 9 7 M / E / r 5 u Z + D r M m U w z Q y V Z L I o z j k y C Z t + j A V O U G D 6 x B B P F 7 K 2 I j L D C x N i M K j Y E b / n l V d K 6 q H t u 3 b u 7 r D V u i z j K c A K n c A 4 e X E E D b s C H A A g I e I Z X</formula><formula xml:id="formula_4">v 0 z T F Z I y H t G u p x I L q M J 8 f P E V n V h m g O F G 2 p E F z 9 f d E j o X W E x H Z T o H N S C 9 7 M / E / r 5 u Z + D r M m U w z Q y V Z L I o z j k y C Z t + j A V O U G D 6 x B B P F 7 K 2 I j L D C x N i M K j Y E b / n l V d K 6 q H t u 3 b u 7 r D V u i z j K c A K n c A 4 e X E E D b s C H A A g I e I Z X</formula><formula xml:id="formula_5">v 0 z T F Z I y H t G u p x I L q M J 8 f P E V n V h m g O F G 2 p E F z 9 f d E j o X W E x H Z T o H N S C 9 7 M / E / r 5 u Z + D r M m U w z Q y V Z L I o z j k y C Z t + j A V O U G D 6 x B B P F 7 K 2 I j L D C x N i M K j Y E b / n l V d K 6 q H t u 3 b u 7 r D V u i z j K c A K n c A 4 e X E E D b s C H A A g I e I Z X</formula><p>In summary, we propose a simple yet effective representation learning framework aiming at making the best use of information embedded in observed user-item associations for recommendation. Our framework advances the state-of-the-art recommendation algorithms along the following five dimensions.</p><p>(1) The CSE serves as a generalized framework that models comprehensive pairwise relations among users and items with a unified objective function in a simple and effective manner. datasets covering different user-item interaction types, levels of data sparsity, and data sizes, demonstrating the robustness, efficiency, and effectiveness of our framework. (5) For reproducibility, we share the source code of CSE online at a GitHub repo, 1 by which the learning process can be done within an hour for each dataset performed in this work. The rest of this paper is organized as follows. In Section 2, we present the proposed CSE framework in detail, including the problem definition, the two similarity embedding modules, and the sampling techniques. We then provide model analyses for flexibility, scalability and time and space complexity of the proposed CSE in Section 3. Experimental results are provided and discussed in Section 4. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROPOSED CSE FRAMEWORK</head><p>Problem Formulation. A recommender system provides a list of ranked items to users based on their historical interactions with items. Let U and I denote the sets of users and items, respectively. User-item associations can be presented as a bipartite graph G = (V , E), where V = {v 1 , . . . , v |V | } = U ∪ I , and E represents the set of observed user-item associations. Note that for explicit rating data, the weights of the user-item preference edges can be positive real numbers, whereas for implicit interactions, the bipartite graph becomes a binary graph. The goal of the CSE framework is to obtain an embedding matrix Φ ∈ R |V |×d that maps each user and item into a d-dimensional embedding vector for item recommendation; that is, with the learned embedding matrix Φ, for a user v i ∈ U , the proposed framework generates the top-N recommended items via computing the similarity between the embedding vector of the user, i.e., Φ v i , and those of all items, i.e., Φ v j for all v j ∈ I , where Φ v x denotes the row vector for vertex v x ∈ V from matrix Φ.</p><p>Framework Overview. <ref type="figure" target="#fig_7">Figure 1</ref> provides an overview of CSE. In the figure, CSE consists of two similarity embedding modules: a direct similarity embedding (DSEmbed) module to model user-item associations, and a neighborhood similarity embedding (NSEmbed) module to model user-user and item-item similarities. The DSEmbed model provides the flexibility to implement two mainstream types of modeling techniques: rating-based and ranking-based models to preserve direct proximity of user-item associations; NSEmbed, in turn, models user-user and item-item relations using the contexts within a k-step random walk, as shown in <ref type="figure" target="#fig_7">Fig. 1(b)</ref>, to preserve k-order neighborhood proximity between users and items. To minimize the sum of the losses from DSEmbed and NSEmbed modules, which are denoted as L DS and L N S respectively, the objective function of the proposed framework is designed as</p><formula xml:id="formula_6">L = L DS + λL N S ,</formula><p>where λ controls the balance between the two losses. The rationale behind this design is that L DS controls the optimization of the embedding vectors towards preserving direct user-item associations, and L N S encourages users/items sharing similar neighbors to be close to one another in the learned embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Direct Similarity Embedding (DSEmbed) Module</head><p>Definition 2.1. (Direct Proximity) Given a bipartite graph G = (V , E), the direct proximity between a user v i ∈ U and an item v j ∈ I is represented by the presence of an edge (v i , v j ) ∈ E between these two vertices. If there is no edge between user v i and item v j , then their direct proximity is defined as 0.</p><p>The DSEmbed module is designed to model the direct proximity of the user-item associations defined in Definition 2.1. For a ratingbased approach, the objective is to find the embedding matrix Φ that maximizes the log-likelihood function of observed user-item pairs:</p><p>arg max</p><formula xml:id="formula_7">Φ (v i ,v j )∈E log p(v i , v j |Φ) = arg min Φ (v i ,v j )∈E − log p(v i , v j |Φ).<label>(1)</label></formula><p>In contrast, a ranking-based approach cares more about whether we can predict stronger association between a 'positive' user-item pair (v i , v j ) ∈ E than a 'negative' user-item pair (v i , v k ) ∈Ē <ref type="bibr" target="#b24">[25]</ref>, whereĒ denotes the set of edges for all the unobserved useritem associations. This can be approached by maximizing the loglikelihood function of observed user-item pairs over unobserved user-item pairs for each user:</p><formula xml:id="formula_8">arg max Φ (v i ,v j ,v k ) log p(v j &gt; i v k |Φ) = arg min Φ (v i ,v j ,v k ) − log p(v j &gt; i v k |Φ),<label>(2)</label></formula><p>where v i ∈ U and v j , v k ∈ I , and &gt; i indicates that user v i prefers item v j over item v k . In the above two equations, p(v i , v j |Φ) and</p><formula xml:id="formula_9">p(v j &gt; i v k |Φ) is calculated by p(v i , v j |Φ) = σ Φ v i · Φ v j , and p(v j &gt; i v k |Φ) = σ Φ v i · Φ v j − Φ v i · Φ v k ,</formula><p>respectively, and σ (·) denotes the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neighborhood Similarity Embedding (NSEmbed) Module</head><p>Definition 2.2. (k-Order Neighborhood Proximity) Given a bipartite graph G = (V , E) representing the observed user-item associations of the set of users and items in V = U ∪ I , the korder neighborhood proximity of a pair of users (or items) is defined as the similarity between their neighborhood network structures retrieved by k-step random walks. Mathematically speaking, given the k-order neighborhood structures of a pair of users (or items), v i , v j ∈ U (or v i , v j ∈ I , respectively), which are denoted as two sets of neighbor nodes N v i and N v j , with |N v i | = |N v j | = k, the k-order neighborhood proximity between v i and v j is decided by the similarity between these two sets N v i and N v j . If there are no shared neighbors between v i and v j , the neighborhood proximity between them is 0.</p><p>The NSEmbed module is designed to model k-order neighborhood proximity for capturing user-user and item-item similarities. Given a set of neighborhood relations for users (or items)</p><formula xml:id="formula_10">S U = {(v i , v j )| ∀v i ∈ U , v j ∈ N v i } (or S I = {(v i , v j )| ∀v i ∈ I, v j ∈ N v i },</formula><p>respectively), the NSEmbed module seeks a set of embedding matrices Φ, Φ U C , Φ IC ∈ R |V |×d that maximizes the likelihood of all pairs in S U (or S I , respectively), where Φ is a vertex mapping matrix akin to that used in the DSEmbed module, and Φ U C and Φ IC are two context mapping matrices. Note that each vertex (representing a user or an item) plays two roles for modeling the neighborhood proximity: 1) the vertex itself and 2) the context of other vertices <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>. With this design, the embedding vectors of vertices that share similar contexts are thus closely located in the learned vector space. Therefore, the maximization of the likelihood function can be defined as arg max</p><formula xml:id="formula_11">Φ,Φ U C ,Φ I C (v i ,v j )∈S U p(v j |v i ; Φ; Φ U C ) + (v i ,v j )∈S I p(v j |v i ; Φ; Φ IC ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WWW'19, May 2019, San Francisco</head><p>Chih-Ming Chen, Chuan-Ju Wang, Ming-Feng Tsai, and Yi-Hsuan Yang Similar to Eqs. (1) and (2), the above objective function becomes arg min</p><formula xml:id="formula_12">Φ,Φ U C ,Φ I C (v i ,v j )∈S U − log p(v j |v i ; Φ; Φ U C ) + (v i ,v j )∈S I − log p(v j |v i ; Φ; Φ I C ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_13">p(v j |v i ; Θ) =        σ Φ v i · Φ U C v j if v i ∈ U , σ Φ v i · Φ I C v j if v i ∈ I .<label>(4)</label></formula><p>It is worth mentioning that most prior arts use only one or two embedding mappings; while the former approach fails to consider high-order neighbors (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>), the later one cannot model user-user, item-item, and user-item relations simultaneously (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>). Our newly designed triplet embedding solution (i.e., Θ = {Φ, Φ U C , Φ U C }) can ideally model user-user, item-item clustering and user-item relations in a single and jointlearning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sampling-based Expectation Loss</head><p>In order to minimize the above objective functions, we need to go through all the pairs in E for Eq. (1), E andĒ for Eq. (2), and S U and S I for Eq. (3), to compute all the pairwise losses. This is not feasible in real-world recommendation scenarios as the complexity is O(|V | × |V |). To address this, we propose a sampling technique to work in tandem with the above two modules to enhance CSE's scalability and flexibility in learning user and item representations from large-scale datasets.</p><p>In CSE, the DSEmbed and NSEmbed modules are fused with the shareable data sampling technique described below. For each parameter update, we first sample an observed user-item pair (v i , v j ) ∈ E, as shown as U1 and I1 in <ref type="figure" target="#fig_7">Fig. 1(b)</ref>, where v i ∈ U and v j ∈ I . Then, we search for the k-order neighborhood structures of user v i and item v j via the k-step random walks. To improve computational efficiency, we use negative sampling <ref type="bibr" target="#b27">[28]</ref>. Consequently, for a rating-based approach (see Eq. (1)), the expected sampled loss of the DSEmbed module can be re-written as</p><formula xml:id="formula_14">L DS = E (v i ,v j )∼E − log p(v i , v j |Φ) + M E (v k ,v h )∼Ē [ log p(v k , v h |Φ) ] ,<label>(5)</label></formula><p>where M denotes the number of negative pairs adopted. For a ranking-based approach (see Eq. <ref type="formula" target="#formula_8">(2)</ref>), the DSEmbed module can be re-written as</p><formula xml:id="formula_15">L DS = E (v i ,v k )∼Ē E (v i ,v j )∼E − log p(v j &gt; i v k |Φ) | v i . (6)</formula><p>Note that for the ranking-based approach, there is no need to explicitly include M negative sample pairs as this kind of method naturally involves negative pairs fromĒ. Similarly, given a user or an item vertex v i , its k-order neighborhood structure N v i is composed of nodes in the k-step random walks surfing on G,</p><formula xml:id="formula_16">W v i = (W 0 v i , W 1 v i , W 2 v i , . . . , W k v i ),</formula><p>where the vertex for W j v i is randomly chosen from the neighbors of the vertex v given W</p><formula xml:id="formula_17">j−1 v i = v and W 0 v i = v i .</formula><p>The expected sampled loss of the NSEmbed module can be re-written as</p><formula xml:id="formula_18">L N S = E (v i ,v j )∼S U − log p(v j |v i ; Φ; Φ U C ) + M E (v i ,v j )∼Ē log p(v j |v i ; Φ; Φ U C ) + E (v i ,v j )∼S I − log p(v j |v i ; Φ; Φ IC ) + M E (v i ,v j )∼Ē log p(v j |v i ; Φ; Φ IC ) .<label>(7)</label></formula><p>Since L DS and L N S are described in a sampling-based expectation form, CSE provides the flexibility for accommodating arbitrary distributions of positive and negative data. In the following experiments, we produce the positive data according to primitive edges distribution of given user-item graph. As to negative sampling, we propose to directly sample the negative data from whole data collection instead of unobserved data collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optimization</head><p>In the optimization stage, we use asynchronous stochastic gradient descent (ASGD) <ref type="bibr" target="#b23">[24]</ref> to efficiently update the parameters in parallel. The model parameters are composed of the three embedding matrices Θ = {Φ, Φ U C , Φ IC }, each having the size O(|V |d). They are updated with learning rate α according to</p><formula xml:id="formula_19">Θ ← Θ − α ∂L DS ∂Θ + λ ∂L N S ∂Θ − λ V ∥Φ∥ ,<label>(8)</label></formula><p>where λ V is a hyper-parameter for reducing the risk of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL ANALYSIS</head><p>The CSE framework not only modularizes the modeling of pairwise user-item, user-user, and item-item relations, but also integrates them into a single objective function through shared embedding vectors. Together with the DSEmbed and NSEmbed sub-modules for modeling these relations, CSE involves a novel sampling technique to improve scalability and flexibility. To give a clear view of CSE, we further provide the comparison to general graph embedding and deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scalability</head><p>Typical factorization methods usually work on a sparse user-item matrix and do not explicitly model high-order connections from the corresponding user-item bipartite graph G = (V , E). Several methods, including our CSE, propose to explicitly incorporate high-order connections for modeling user-user and item-item relations into the recommendation models to improve performance. As discussed in <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>, such modeling can then be seen as conducting matrix factorization on a |V | × |V | point-wise mutual information (PMI) matrix:</p><formula xml:id="formula_20">PMI (v i , v j ) = log p(v i , v j ) p(v i )p(v j ) − log M .</formula><p>Recall that V = U ∪ I , and M denotes the number of negative pairs adopted in Eqs. (5) and <ref type="bibr" target="#b6">(7)</ref>. However, given the high-order connections for modeling user-user and item-item relations, most  and space, and is therefore infeasible in many large-scale recommendation scenarios.</p><formula xml:id="formula_21">PMI(v i , v j ) for v i , v j ∈ U (or v i , v j ∈ I )</formula><p>To explicitly consider all the collaborative relations into our model while keeping it practical for large-scale datasets, the CSE uses the sampling technique together with k-step random walks surfing on G to preserve direct proximity of user-item associations as well as harvest the high-order neighborhood structures of users and items. By doing so, we approximate factorization of the corresponding PMI matrix and thus reduce the complexity in space and the training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Flexibility</head><p>In the CSE, the DSEmbed and NSEmbed modules are united with the sampling technique. In addition to improved scalability, such a sampling perspective facilitates the shaping of different relation distributions for optimization via different weighting schemes or sampling strategies. Here we resort to use the perspective of KL divergence to explain this model characteristic. Specifically, minimizing the losses in our framework (see Eqs. <ref type="bibr" target="#b4">(5)</ref>, <ref type="bibr" target="#b5">(6)</ref>, and <ref type="formula" target="#formula_18">(7)</ref>) can be related to minimizing the KL divergence of two probability distributions <ref type="bibr" target="#b27">[28]</ref>. Suppose there are two distributions over the space V × V :p(·, ·) and p(·, ·), denoting the empirical distribution and the target distribution, respectively, we have arg min p KL(p(·, ·), p(·, ·))</p><formula xml:id="formula_22">= arg min p − (v i ,v j )∈Ep (v i , v j ) log p(v i , v j ) p(v i , v j ) ∝ arg min p − (v i ,v j )∈Ep (v i , v j ) log p(v i , v j ).<label>(9)</label></formula><p>In Eq. (9), the empirical distributionp(·, ·) can be treated as the probability density (mass) function of the distribution in our loss functions, from which each pair of vertices (v i , v j ) is sampled. This indicates that applying different weighting schemes or different sampling strategies (i.e., differentp(·, ·)) in CSE shapes different relation distributions for learning representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complexity</head><p>The time and space complexity of the proposed method depends on the implementation. The training procedure of CSE framework involves a sampling step and an optimization step. Since all the required training pairs, including observed associations and unobserved associations, can be derived from the user-item bipartite graph G, we adopt the compressed sparse alias rows (CSAR) data structure to perform weighted edge sampling for direct similarity and weighted random walk for neighborhood similarity <ref type="bibr" target="#b1">[2]</ref>. With CSAR data structure, sampling an edge requires only O(1) and the overall demand space complexity is linearly increased with the number of positive edges O(|E|). <ref type="bibr" target="#b1">2</ref> As to the optimization, SGDbased update has a closed form so that updating the embedding of a vertex in a batch depends only on the dimension size O(d). As for the time of convergence, many studies on graph embedding as well as our method empirically show that the required total training time for the convergence of embedding learning is also linear in |E| <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison to General Graph Embedding Models</head><p>General graph embedding algorithms, such as DeepWalk <ref type="bibr" target="#b21">[22]</ref> and node2vec <ref type="bibr" target="#b6">[7]</ref>, can be used for the task of recommendation. Yet, we do not focus on comparing the proposed CSE with general graph embedding models and only provide the results of DeepWalk because many prior works on recommendation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36]</ref> have shown that many of the baseline methods considered in our paper outperform these general graph embedding algorithms. The main reason for this phenomenon is that most graph embedding methods cluster vertices that have similar neighbors together, and thus make the users apart from the items because user-item interactions typically form a bipartite graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison to Deep Learning Models</head><p>Our method, and the existing methods we discussed and compared in the experiments, focus on improving the modeling quality of user and item embeddings that can be directly and later used for useritem recommendation with similarity computation. Many approximation techniques, such as approximate nearest neighbor 3 (ANN), can be applied to speed up the similarity computation between user and item embeddings, which facilitates real-time online predictions and makes the recommendation scalable to large-scale realworld datasets. In contrast, many deep learning methods, including NCF <ref type="bibr" target="#b10">[11]</ref>, DeepFM <ref type="bibr" target="#b9">[10]</ref>, etc, do not learn the directly comparable embeddings of users or items. There are a few deep learning methods (e.g., Collaborative Deep Embedding <ref type="bibr" target="#b31">[32]</ref> and DropoutNet <ref type="bibr" target="#b29">[30]</ref>) that can produce user and item embeddings, but to our knowledge, efficiency is still a major concern of these methods. Therefore, improving the embedding quality is still a critical research issue for building up a recommender system especially when the computation power is limited in real-world application scenarios. For readability and to maintain the focus of this work, we opt for not comparing the deep learning methods in our paper. It is also worth mentioning that our solution can obtain user and item embeddings within only an hour for every large dataset listed in the paper; the efficiency and scalability is thus one of the highlights of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 4.1 Settings</head><p>4.1.1 Datasets and Preprocessing. To examine the capability and scalability of the proposed CSE framework, we conducted experiments on eight publicly available real-world datasets that vary in terms of domain, size, and density, as shown in <ref type="table" target="#tab_1">Table 1</ref>. For each of the datasets, we discarded the users who have less than ten associated interactions with items. In addition, we converted each data into implicit feedback: 4 1) for 5-star rating datasets, we transformed ratings higher than or equal to 3.5 to 1 and the rest to 0; 2) for count-based datasets, we transformed counts higher than or equal to 3 to 1 and the rest to 0; 3) for the CiteULike dataset, no transformation was conducted as it is already a binary preference dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baseline Algorithms.</head><p>We compare the performance of our model with the following eight baseline methods: 1) POP, a naive popularity model that ranks the items by their degrees, 2) Deep-Walk <ref type="bibr" target="#b21">[22]</ref>, a classic algorithm of network embedding, 3) WALS <ref type="bibr" target="#b12">[13]</ref>, a weighted rating-based factorization model, 4) ranking-based factorization models: BPR <ref type="bibr" target="#b24">[25]</ref>, WARP <ref type="bibr" target="#b32">[33]</ref>, and K-OS <ref type="bibr" target="#b33">[34]</ref>, 5) BiNE <ref type="bibr" target="#b5">[6]</ref>, a network embedding model specialized for bipartite networks, and 6) recent advanced models considering user-user/item-item relations: coFactor <ref type="bibr" target="#b17">[18]</ref>, CML <ref type="bibr" target="#b11">[12]</ref> and WalkRanker <ref type="bibr" target="#b34">[35]</ref>, Note that except for POP, the embedding vectors for users and items learned by these competitors as well as by our method can be directly used for item recommendations. Additionally, while CML adopts Euclidean distance as the scoring function, all other methods including ours utilize the dot product to calculate the score of a pair of user-item embedding vectors. The experiments for WALS and BPR were conducted using the matrix factorization library QMF, <ref type="bibr" target="#b4">5</ref> and those for WARP and K-OS were conducted using LightFM; <ref type="bibr" target="#b5">6</ref> for coFactor, CML, and WalkRanker, we used the code provided by the respective authors. For our model, the learning rate α was set to 0.1, λ V was set to 0.025; the hyper-parameter λ was set to 0.05 and 0.1 for rating-based CSE and ranking-based CSE, respectively, and k was set to 2 as the default value. The sensitivity of CSE parameters are additionally reported. For each dataset, the sample time for convergence depends on the number of non-zero user-item interaction edges and is set to 80 × |E|. Sensitivity analysis for k and λ and convergence analysis are later provided in the section for convergence analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Evaluations.</head><p>The performance is evaluated between the recommended list R u containing top-N recommended items and the corresponding ground truth list T u for each user u. We consider following two commonly-used metrics over these N recommended results:</p><p>• Recall: Denoted as Recall@N , which describes the fraction of the ground truth (i.e., the user preferred items) that are successfully recommended by the recommendation algorithm:  </p><formula xml:id="formula_23">Recall@N = u ∈U 1 |U | v ∈R u 1(v ∈ T u ) min(N , |T u |</formula><formula xml:id="formula_24">= 1 |U | u ∈U N k =1 P u (k) × 1(r k ∈ T u ) min(N , |T u |) ,</formula><p>where r k is the k-th recommended item and P u (k) denotes the precision at k for user u. This is a rank-aware evaluation metric because it considers the positions of each recommended item. For each dataset, the reported performance was averaged over 10 times; in each time, we randomly split the data into 80% training set and 20% testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Recommendation Performance Comparison.</head><p>The results for the ten baseline methods along with the proposed method are listed in <ref type="table" target="#tab_4">Table 2</ref>, where RATE-CSE and RANK-CSE denote two versions of our method that employ respectively rating-based and rankingbased loss functions for user-item associations. Note that the best results are always indicated by the bold font, and for coFactor and BiNE we report only the experimental results on Frappe and CiteULike because of resource limitations. <ref type="bibr" target="#b6">7</ref> As discussed in Section 3.4, DeepWalk is not suitable for user-item recommendation as it make the users apart from items in the embedding space. In addition, observe that BiNE does not perform well in our experiments; such a result is due to the fact that BiNE is a general network embedding model and thus does not incorporate the regularizer in their objective function, which is however an important factor for the robustness of recommendation performance. Comparing the performance of the other baseline methods, we observe that That is, these methods achieve the top performance among all the baselines on several datasets. The performance of WalkRanker and CML, on the other hand, seems satisfactory only on two rather small datasets -Frappe and CiteULike -and performs more poorly on most of the other datasets. We observe that our method achieves the best results in terms of both Recall@10 and mAP@10 for most datasets. Moreover, RANK-CSE generally outperforms RATE-CSE in the experiments, re-confirming that using a ranking-based loss is indeed better for datasets with binary implicit feedbacks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Specifically, except for Frappe, RATE-CSE or RANK-CSE achieves significantly much better performance than the best performing baseline methods with a maximum improvement of +20.7%. <ref type="figure" target="#fig_8">Figure 2</ref> shows the results of the sensitivity analysis on two hyper-parameters k and λ in the first and second rows, respectively, and those of the convergence analysis based on sample times in the third row. <ref type="bibr" target="#b7">8</ref> We first observe that increasing the order k of modeling neighborhood proximity between users or items improves the performance in general. We first observe from <ref type="figure" target="#fig_8">Figure 2</ref> is that the optimal value of k is data dependent and has to be empirically tuned considering the trade-off between accuracy and time/space complexity. In general, a larger k leads to better result, and from our experience, the result would reach a plateau when k is sufficiently large (e.g., when k &gt; 3). <ref type="bibr" target="#b7">8</ref> Note that due to space limits, we report the results for the four largest datasets only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Parameter Sensitivity and Convergence Analyses.</head><p>The second row of <ref type="figure" target="#fig_8">Figure 2</ref> shows how the balancing parameter λ affects performance: RANK-CSE obtains better performance with a value around 0.05, while RATE-CSE performs well with a value around 0.1. Finally, we empirically show that the required total sample times for convergence is linear with respect to |E| as illustrated in the third row, where the vertical dash line indicates the boundary of |E| × 80 as we applied to the previous recommendation experiment. As the training time depends linearly on the sample times, it can be said that both RATE-CSE and RANK-CSE converge with less than a constant multiple of |E| sample times. This demonstrates the nice scalability of CSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We present CSE, a unified representation learning framework that exploits comprehensive collaborative relations available in a useritem bipartite graph for recommender systems. Two types of proximity relations are modeled by the proposed DSEmbed and NSEmbed modules. Moreover, we propose a sampling technique to enhance the scalability and flexibility of the model. Experimental results show that CSE yields superior recommendation performance over a wide range of datasets with different sizes, densities, and types than many state-of-the-art recommendation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " S 2 X W e Z L f l / D B c q k m C 1 8 a d c u t e d A = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K t h b a U D b b T b N 0 P 8 L u R i i h f 8 G L B 0 W 8 + o e 8 + W / c t D l o 6 4 O B x 3 s z z M y L U s 6 M 9 f 1 v r 7 K 2 v r G 5 V d 2 u 7 e z u 7 R / U D 4 + 6 R m W a 0 A 5 R X O l e h A 3 l T N K O Z Z b T X q o p F h G n j 9 H k t v A f n 6 g 2 T M k H O 0 1 p K P B Y s p g R b A t p 0 E 7 Y s N 7 w m / 4 c a J U E J W l A i f a w / j U Y K Z I J K i 3 h 2 J h + 4 K c 2 z L G 2 j H A 6 q w 0 y Q 1 N M J n h M + 4 5 K L K g J 8 / m t M 3 T m l B G K l X Y l L Z q r v y d y L I y Z i s h 1 C m w T s + w V 4 n 9 e P 7 P x d Z g z m W a W S r J Y F G c c W Y W K x 9 G I a U o s n z q C i W b u V k Q S r D G x L p 6 a C y F Y f n m V d C + a g d 8 M 7 i 8 b r Z s y j i q c w C m c Q w B X 0 I I 7 a E M H C C T w D K / w 5 g n v x X v 3 P h a t F a + c O Y Y / 8 D 5 / A O K 5 j h 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 2 X W e Z L f l / D B c q k m C 1 8 a d c u t e d A = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K t h b a U D b b T b N 0 P 8 L u R i i h f 8 G L B 0 W 8 + o e 8 + W / c t D l o 6 4 O B x 3 s z z M y L U s 6 M 9 f 1 v r 7 K 2 v r G 5 V d 2 u 7 e z u 7 R / U D 4 + 6 R m W a 0 A 5 R X O l e h A 3 l T N K O Z Z b T X q o p F h G n j 9 H k t v A f n 6 g 2 T M k H O 0 1 p K P B Y s p g R b A t p 0 E 7 Y s N 7 w m / 4 c a J U E J W l A i f a w / j U Y K Z I J K i 3 h 2 J h + 4 K c 2 z L G 2 j H A 6 q w 0 y Q 1 N M J n h M + 4 5 K L K g J 8 / m t M 3 T m l B G K l X Y l L Z q r v y d y L I y Z i s h 1 C m w T s + w V 4 n 9 e P 7 P x d Z g z m W a W S r J Y F G c c W Y W K x 9 G I a U o s n z q C i W b u V k Q S r D G x L p 6 a C y F Y f n m V d C + a g d 8 M 7 i 8 b r Z s y j i q c w C m c Q w B X 0 I I 7 a E M H C C T w D K / w 5 g n v x X v 3 P h a t F a + c O Y Y / 8 D 5 / A O K 5 j h 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 2 X W e Z L f l / D B c q k m C 1 8 a d c u t e d A = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K t h b a U D b b T b N 0 P 8 L u R i i h f 8 G L B 0 W 8 + o e 8 + W / c t D l o 6 4 O B x 3 s z z M y L U s 6 M 9 f 1 v r 7 K 2 v r G 5 V d 2 u 7 e z u 7 R / U D 4 + 6 R m W a 0 A 5 R X O l e h A 3 l T N K O Z Z b T X q o p F h G n j 9 H k t v A f n 6 g 2 T M k H O 0 1 p K P B Y s p g R b A t p 0 E 7 Y s N 7 w m / 4 c a J U E J W l A i f a w / j U Y K Z I J K i 3 h 2 J h + 4 K c 2 z L G 2 j H A 6 q w 0 y Q 1 N M J n h M + 4 5 K L K g J 8 / m t M 3 T m l B G K l X Y l L Z q r v y d y L I y Z i s h 1 C m w T s + w V 4 n 9 e P 7 P x d Z g z m W a W S r J Y F G c c W Y W K x 9 G I a U o s n z q C i W b u V k Q S r D G x L p 6 a C y F Y f n m V d C + a g d 8 M 7 i 8 b r Z s y j i q c w C m c Q w B X 0 I I 7 a E M H C C T w D K / w 5 g n v x X v 3 P h a t F a + c O Y Y / 8 D 5 / A O K 5 j h 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 2 X W e Z L f l / D B c q k m C 1 8 a d c u t e d A = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K t h b a U D b b T b N 0 P 8 L u R i i h f 8 G L B 0 W 8 + o e 8 + W / c t D l o 6 4 O B x 3 s z z M y L U s 6 M 9 f 1 v r 7 K 2 v r G 5 V d 2 u 7 e z u 7 R / U D 4 + 6 R m W a 0 A 5 R X O l e h A 3 l T N K O Z Z b T X q o p F h G n j 9 H k t v A f n 6 g 2 T M k H O 0 1 p K P B Y s p g R b A t p 0 E 7 Y s N 7 w m / 4 c a J U E J W l A i f a w / j U Y K Z I J K i 3 h 2 J h + 4 K c 2 z L G 2 j H A 6 q w 0 y Q 1 N M J n h M + 4 5 K L K g J 8 / m t M 3 T m l B G K l X Y l L Z q r v y d y L I y Z i s h 1 C m w T s + w V 4 n 9 e P 7 P x d Z g z m W a W S r J Y F G c c W Y W K x 9 G I a U o s n z q C i W b u V k Q S r D G x L p 6 a C y F Y f n m V d C + a g d 8 M 7 i 8 b r Z s y j i q c w C m c Q w B X 0 I I 7 a E M H C C T w D K / w 5 g n v x X v 3 P h a t F a + c O Y Y / 8 D 5 / A O K 5 j h 8 = &lt; / l a t e x i t &gt; L NS &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N Ii I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J ou c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; L NS &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; L DS &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t z V / / 7 + H w z s 9 G r 8 g q 6 8 d t b N x I c k = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L K g C x c u K t o H t C F M p p N 2 6 G Q S Z i a F E v I n b l w o 4 t Y / c e f f O G m z 0 N Y D A 4 d z 7 u W e O U H C m d K O 8 2 1 V 1 t Y 3 N r e q 2 7 W d 3 b 3 9 A / v w q K P i V B L a J j G P Z S / A i n I m a F s z z W k v k R R H A a f d Y H J T + N 0 p l Y r F 4 k n P E u p F e C R Y y A j W R v J t e x B h P S a Y Z / e 5 n 9 0 + 5 r 5 d d x r O H G i V u C W p Q 4 m W b 3 8 N h j F J I y o 0 4 V i p v u s k 2 s u w 1 I x w m t c G q a I J J h M 8 o n 1 D B Y 6 o 8 r J 5 8 h y d G W W I w l i a J z S a q 7 8 3 M h w p N Y s C M 1 n k V M t e I f 7 n 9 V M d X n s Z E 0 m q q S C L Q 2 H K k Y 5 R U Q M a M k m J 5 j N D M J H M Z E V k j C U m 2 p R V M y W 4 y 1 9 e J Z 2 L h u s 0 3 I f L e r N V 1 l G F E z i F c 3 D h C p p w B y 1 o A 4 E p P M M r v F m Z 9 W K 9 W x + L 0 Y p V 7 h z D H 1 i f P 7 0 Z k 8 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t z V / / 7 + H w z s 9 G r 8 g q 6 8 d t b N x I c k = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L K g C x c u K t o H t C F M p p N 2 6 G Q S Z i a F E v I n b l w o 4 t Y / c e f f O G m z 0 N Y D A 4 d z 7 u W e O U H C m d K O 8 2 1 V 1 t Y 3 N r e q 2 7 W d 3 b 3 9 A / v w q K P i V B L a J j G P Z S / A i n I m a F s z z W k v k R R H A a f d Y H J T + N 0 p l Y r F 4 k n P E u p F e C R Y y A j W R v J t e x B h P S a Y Z / e 5 n 9 0 + 5 r 5 d d x r O H G i V u C W p Q 4 m W b 3 8 N h j F J I y o 0 4 V i p v u s k 2 s u w 1 I x w m t c G q a I J J h M 8 o n 1 D B Y 6 o 8 r J 5 8 h y d G W W I w l i a J z S a q 7 8 3 M h w p N Y s C M 1 n k V M t e I f 7 n 9 V M d X n s Z E 0 m q q S C L Q 2 H K k Y 5 R U Q M a M k m J 5 j N D M J H M Z E V k j C U m 2 p R V M y W 4 y 1 9 e J Z 2 L h u s 0 3 I f L e r N V 1 l G F E z i F c 3 D h C p p w B y 1 o A 4 E p P M M r v F m Z 9 W K 9 W x + L 0 Y p V 7 h z D H 1 i f P 7 0 Z k 8 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t z V / / 7 + H w z s 9 G r 8 g q 6 8 d t b N x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>P S a Y Z / e 5 n 9 0 + 5 r 5 d d x r O H G i V u C W p Q 4 m W b 3 8 N h j F J I y o 0 4 V i p v u s k 2 s u w 1 I x w m t c G q a I J J h M 8 o n 1 D B Y 6 o 8 r J 5 8 h y d G W W I w l i a J z S a q 7 8 3 M h w p N Y s C M 1 n k V M t e I f 7 n 9 V M d X n s Z E 0 m q q S C L Q 2 H K k Y 5 R U Q M a M k m J 5 j N D M J H M Z E V k j C U m 2 p R V M y W 4 y 1 9 e J Z 2 L h u s 0 3 I f L e r N V 1 l G F E z i F c 3 D h C p p w B y 1 o A 4 E p P M M r v F m Z 9 W K 9 W x + L 0 Y p V 7 h z D H 1 i f P 7 0 Z k 8 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t z V / / 7 + H w z s 9 G r 8 g q 6 8 d t b N x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>P S a Y Z / e 5 n 9 0 + 5 r 5 d d x r O H G i V u C W p Q 4 m W b 3 8 N h j F J I y o 0 4 V i p v u s k 2 s u w 1 I x w m t c G q a I J J h M 8 o n 1 D B Y 6 o 8 r J 5 8 h y d G W W I w l i a J z S a q 7 8 3 M h w p N Y s C M 1 n k V M t e I f 7 n 9 V M d X n s Z E 0 m q q S C L Q 2 H K k Y 5 R U Q M a M k m J 5 j N D M J H M Z E V k j C U m 2 p R V M y W 4 y 1 9 e J Z 2 L h u s 0 3 I f L e r N V 1 l G F E z i F c 3 D h C p p w B y 1 o A 4 E p P M M r v F m Z 9 W K 9 W x + L 0 Y p V 7 h z D H 1 i f P 7 0 Z k 8 E = &lt; / l a t e x i t &gt; UC &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 6 1 l c g c 8 I F n r 6 w J / P a / b 6 q 6 i T s U = " &gt; A A A B 8 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O h F y 9 C B N N W 2 l g 2 2 0 2 7 d H c T d j d C C f 0 V X j w o 4 t W f 4 8 1 / 4 7 b N Q V s f D D z e m 2 F m X p R y p o 3 r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 N J J p g g N S M I T 1 Y m w p p x J G h h m O O 2 k i m I R c d q O x s 2 Z 3 3 6 i S r N E 3 p t J S k O B h 5 L F j G B j p Y e e P 2 K P e d C c 9 q s 1 t + 7 O g V a J V 5 A a F P D 7 1 a / e I C G Z o N I Q j r X u e m 5 q w h w r w w i n 0 0 o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>e H O U 8 + K 8 O x + L 1 p J T z B z D H z i f P 5 b p k E o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 6 1 l c g c 8 I F n r 6 w J / P a / b 6 q 6 i T s U = " &gt; A A A B 8 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O h F y 9 C B N N W 2 l g 2 2 0 2 7 d H c T d j d C C f 0 V X j w o 4 t W f 4 8 1 / 4 7 b N Q V s f D D z e m 2 F m X p R y p o 3 r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 N J J p g g N S M I T 1 Y m w p p x J G h h m O O 2 k i m I R c d q O x s 2 Z 3 3 6 i S r N E 3 p t J S k O B h 5 L F j G B j p Y e e P 2 K P e d C c 9 q s 1 t + 7 O g V a J V 5 A a F P D 7 1 a / e I C G Z o N I Q j r X u e m 5 q w h w r w w i n 0 0 o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>e H O U 8 + K 8 O x + L 1 p J T z B z D H z i f P 5 b p k E o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 6 1 l c g c 8 I F n r 6 w J / P a / b 6 q 6 i T s U = " &gt; A A A B 8 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O h F y 9 C B N N W 2 l g 2 2 0 2 7 d H c T d j d C C f 0 V X j w o 4 t W f 4 8 1 / 4 7 b N Q V s f D D z e m 2 F m X p R y p o 3 r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 N J J p g g N S M I T 1 Y m w p p x J G h h m O O 2 k i m I R c d q O x s 2 Z 3 3 6 i S r N E 3 p t J S k O B h 5 L F j G B j p Y e e P 2 K P e d C c 9 q s 1 t + 7 O g V a J V 5 A a F P D 7 1 a / e I C G Z o N I Q j r X u e m 5 q w h w r w w i n 0 0 o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>e H O U 8 + K 8 O x + L 1 p J T z B z D H z i f P 5 b p k E o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 6 1 l c g c 8 I F n r 6 w J / P a / b 6 q 6 i T s U = " &gt; A A A B 8 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O h F y 9 C B N N W 2 l g 2 2 0 2 7 d H c T d j d C C f 0 V X j w o 4 t W f 4 8 1 / 4 7 b N Q V s f D D z e m 2 F m X p R y p o 3 r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 N J J p g g N S M I T 1 Y m w p p x J G h h m O O 2 k i m I R c d q O x s 2 Z 3 3 6 i S r N E 3 p t J S k O B h 5 L F j G B j p Y e e P 2 K P e d C c 9 q s 1 t + 7 O g V a J V 5 A a F P D 7 1 a / e I C G Z o N I Q j r X u e m 5 q w h w r w w i n 0 0 o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>e H O U 8 + K 8 O x + L 1 p J T z B z D H z i f P 5 b p k E o = &lt; / l a t e x i t &gt; IC &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a N w P 4 n O Y 1 o 3 M T B M g j U L e b Q U B G z 8 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j I R Q 9 C B P O Q Z A 2 z k 9 l k y D y W m V k h L P k K L x 4 U 8 e r n e P N v n C R 7 0 M S C h q K q m + 6 u K O H M W N / / 9 l Z W 1 9 Y 3 N g t b x e 2 d 3 b 3 9 0 s F h 0 6 h U E 9 o g i i v d j r C h n E n a s M x y 2 k 4 0 x S L i t B W N a l O / 9 U S 1 Y U r e 2 3 F C Q 4 E H k s W M Y O u k h 2 5 9 y B 6 z m 9 q k V y r 7 F X 8 G t E y C n J Q h R 7 1 X + u r 2 F U k F l Z Z w b E w n 8 B M b Z l h b R j i d F L u p o Q k m I z y g H U c l F t S E 2 e z g C T p 1 S h / F S r u S F s 3 U 3 x M Z F s a M R e Q 6 B b Z D s + h N x f + 8 T m r j q z B j M k k t l W S + K E 4 5 s g p N v 0 d 9 p i m x f O w I J p q 5 W x E Z Y o 2 J d R k V X Q j B 4 s v L p H l e C f x K c H d R r t 7 m c R T g G E 7 g D A K 4 h C p c Q x 0 a Q E D A M 7 z C m 6 e 9 F + / d + 5 i 3 r n j 5 z B H 8 g f f 5 A 4 S h k D 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a N w P 4 n O Y 1 o 3 M T B M g j U L e b Q U B G z 8 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j I R Q 9 C B P O Q Z A 2 z k 9 l k y D y W m V k h L P k K L x 4 U 8 e r n e P N v n C R 7 0 M S C h q K q m + 6 u K O H M W N / / 9 l Z W 1 9 Y 3 N g t b x e 2 d 3 b 3 9 0 s F h 0 6 h U E 9 o g i i v d j r C h n E n a s M x y 2 k 4 0 x S L i t B W N a l O / 9 U S 1 Y U r e 2 3 F C Q 4 E H k s W M Y O u k h 2 5 9 y B 6 z m 9 q k V y r 7 F X 8 G t E y C n J Q h R 7 1 X + u r 2 F U k F l Z Z w b E w n 8 B M b Z l h b R j i d F L u p o Q k m I z y g H U c l F t S E 2 e z g C T p 1 S h / F S r u S F s 3 U 3 x M Z F s a M R e Q 6 B b Z D s + h N x f + 8 T m r j q z B j M k k t l W S + K E 4 5 s g p N v 0 d 9 p i m x f O w I J p q 5 W x E Z Y o 2 J d R k V X Q j B 4 s v L p H l e C f x K c H d R r t 7 m c R T g G E 7 g D A K 4 h C p c Q x 0 a Q E D A M 7 z C m 6 e 9 F + / d + 5 i 3 r n j 5 z B H 8 g f f 5 A 4 S h k D 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a N w P 4 n O Y 1 o 3 M T B M g j U L e b Q U B G z 8 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j I R Q 9 C B P O Q Z A 2 z k 9 l k y D y W m V k h L P k K L x 4 U 8 e r n e P N v n C R 7 0 M S C h q K q m + 6 u K O H M W N / / 9 l Z W 1 9 Y 3 N g t b x e 2 d 3 b 3 9 0 s F h 0 6 h U E 9 o g i i v d j r C h n E n a s M x y 2 k 4 0 x S L i t B W N a l O / 9 U S 1 Y U r e 2 3 F C Q 4 E H k s W M Y O u k h 2 5 9 y B 6 z m 9 q k V y r 7 F X 8 G t E y C n J Q h R 7 1 X + u r 2 F U k F l Z Z w b E w n 8 B M b Z l h b R j i d F L u p o Q k m I z y g H U c l F t S E 2 e z g C T p 1 S h / F S r u S F s 3 U 3 x M Z F s a M R e Q 6 B b Z D s + h N x f + 8 T m r j q z B j M k k t l W S + K E 4 5 s g p N v 0 d 9 p i m x f O w I J p q 5 W x E Z Y o 2 J d R k V X Q j B 4 s v L p H l e C f x K c H d R r t 7 m c R T g G E 7 g D A K 4 h C p c Q x 0 a Q E D A M 7 z C m 6 e 9 F + / d + 5 i 3 r n j 5 z B H 8 g f f 5 A 4 S h k D 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a N w P 4 n O Y 1 o 3 M T B M g j U L e b Q U B G z 8 = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 B j I R Q 9 C B P O Q Z A 2 z k 9 l k y D y W m V k h L P k K L x 4 U 8 e r n e P N v n C R 7 0 M S C h q K q m + 6 u K O H M W N / / 9 l Z W 1 9 Y 3 N g t b x e 2 d 3 b 3 9 0 s F h 0 6 h U E 9 o g i i v d j r C h n E n a s M x y 2 k 4 0 x S L i t B W N a l O / 9 U S 1 Y U r e 2 3 F C Q 4 E H k s W M Y O u k h 2 5 9 y B 6 z m 9 q k V y r 7 F X 8 G t E y C n J Q h R 7 1 X + u r 2 F U k F l Z Z w b E w n 8 B M b Z l h b R j i d F L u p o Q k m I z y g H U c l F t S E 2 e z g C T p 1 S h / F S r u S F s 3 U 3 x M Z F s a M R e Q 6 B b Z D s + h N x f + 8 T m r j q z B j M k k t l W S + K E 4 5 s g p N v 0 d 9 p i m x f O w I J p q 5 W x E Z Y o 2 J d R k V X Q j B 4 s v L p H l e C f x K c H d R r t 7 m c R T g G E 7 g D A K 4 h C p c Q x 0 a Q E D A M 7 z C m 6 e 9 F + / d + 5 i 3 r n j 5 z B H 8 g f f 5 A 4 S h k D 4 = &lt; / l a t e x i t &gt; (a) The bottom part depicts the direct similarity embedding module for user-item associations, whereas the upper left (right) part corresponds to modeling user-user (item-item, respectively) similarity with the neighborhood similarity embedding module. An optimization step includes a sampled pair of a user and an item in DSEmbed and multiple high-order relation pairs in NSEmbed. t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j C L Z w 1 9 V s L 5 2 u W Z 0 2 8 h J m v N I i I = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L L g x o V I R f u A N o T J d N I O n U z C z K R Q Q v 7 E j Q t F 3 P o n 7 v w b J 2 0 W 2 n p g 4 H D O v d w z J 0 g 4 U 9 p x v q 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / Y B 8 e d V S c S k L b J O a x 7 A V Y U c 4 E b W u m O e 0 l k u I o 4 L Q b T G 4 K v z u l U r F Y P O l Z Q r 0 I j w Q L G c H a S L 5 t D y K s x w T z 7 C 7 3 s / v H 3 L f r T s O Z A 6 0 S t y R 1 K N H y 7 a / B M C Z p R I U m H C v V d 5 1 E e x m W m h F O 8 9 o g V T T B Z I J H t G + o w B F V X j Z P n q M z o w x R G E v z h E Z z 9 f d G h i O l Z l F g J o u c a t k r x P + 8 f q r D a y 9 j I k k 1 F W R x K E w 5 0 j E q a k B D J i n R f G Y I J p K Z r I i M s c R E m 7 J q p g R 3 + c u r p H P R c J 2 G + 3 B Z b 7 b K O q p w A q d w D i 5 c Q R N u o Q V t I D C F Z 3 i F N y u z X q x 3 6 2 M x W r H K n W P 4 A + v z B 8 x V k 8 s = &lt; / l a t e x i t &gt; L DS &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t z V / / 7 + H w z s 9 G r 8 g q 6 8 d t b N x I c k = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L K g C x c u K t o H t C F M p p N 2 6 G Q S Z i a F E v I n b l w o 4 t Y / c e f f O G m z 0 N Y D A 4 d z 7 u W e O U H C m d K O 8 2 1 V 1 t Y 3 N r e q 2 7 W d 3 b 3 9 A / v w q K P i V B L a J j G P Z S / A i n I m a F s z z W k v k R R H A a f d Y H J T + N 0 p l Y r F 4 k n P E u p F e C R Y y A j W R v J t e x B h P S a Y Z / e 5 n 9 0 + 5 r 5 d d x r O H G i V u C W p Q 4 m W b 3 8 N h j F J I y o 0 4 V i p v u s k 2 s u w 1 I x w m t c G q a I J J h M 8 o n 1 D B Y 6 o 8 r J 5 8 h y d G W W I w l i a J z S a q 7 8 3 M h w p N Y s C M 1 n k V M t e I f 7 n 9 V M d X n s Z E 0 m q q S C L Q 2 H K k Y 5 R U Q M a M k m J 5 j N D M J H M Z E V k j C U m 2 p R V M y W 4 y 1 9 e J Z 2 L h u s 0 3 I f L e r N V 1 l G F E z i F c 3 D h C p p w B y 1 o A 4 E p P M M r v F m Z 9 W K 9 W x + L 0 Y p V 7 h z D H 1 i f P 7 0 Z k 8 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t z V / / 7 + H w z s 9 G r 8 g q 6 8 d t b N x I c k = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L K g C x c u K t o H t C F M p p N 2 6 G Q S Z i a F E v I n b l w o 4 t Y / c e f f O G m z 0 N Y D A 4 d z 7 u W e O U H C m d K O 8 2 1 V 1 t Y 3 N r e q 2 7 W d 3 b 3 9 A / v w q K P i V B L a J j G P Z S / A i n I m a F s z z W k v k R R H A a f d Y H J T + N 0 p l Y r F 4 k n P E u p F e C R Y y A j W R v J t e x B h P S a Y Z / e 5 n 9 0 + 5 r 5 d d x r O H G i V u C W p Q 4 m W b 3 8 N h j F J I y o 0 4 V i p v u s k 2 s u w 1 I x w m t c G q a I J J h M 8 o n 1 D B Y 6 o 8 r J 5 8 h y d G W W I w l i a J z S a q 7 8 3 M h w p N Y s C M 1 n k V M t e I f 7 n 9 V M d X n s Z E 0 m q q S C L Q 2 H K k Y 5 R U Q M a M k m J 5 j N D M J H M Z E V k j C U m 2 p R V M y W 4 y 1 9 e J Z 2 L h u s 0 3 I f L e r N V 1 l G F E z i F c 3 D h C p p w B y 1 o A 4 E p P M M r v F m Z 9 W K 9 W x + L 0 Y p V 7 h z D H 1 i f P 7 0 Z k 8 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t z V / / 7 + H w z s 9 G r 8 g q 6 8 d t b N x I c k = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L K g C x c u K t o H t C F M p p N 2 6 G Q S Z i a F E v I n b l w o 4 t Y / c e f f O G m z 0 N Y D A 4 d z 7 u W e O U H C m d K O 8 2 1 V 1 t Y 3 N r e q 2 7 W d 3 b 3 9 A / v w q K P i V B L a J j G P Z S / A i n I m a F s z z W k v k R R H A a f d Y H J T + N 0 p l Y r F 4 k n P E u p F e C R Y y A j W R v J t e x B h P S a Y Z / e 5 n 9 0 + 5 r 5 d d x r O H G i V u C W p Q 4 m W b 3 8 N h j F J I y o 0 4 V i p v u s k 2 s u w 1 I x w m t c G q a I J J h M 8 o n 1 D B Y 6 o 8 r J 5 8 h y d G W W I w l i a J z S a q 7 8 3 M h w p N Y s C M 1 n k V M t e I f 7 n 9 V M d X n s Z E 0 m q q S C L Q 2 H K k Y 5 R U Q M a M k m J 5 j N D M J H M Z E V k j C U m 2 p R V M y W 4 y 1 9 e J Z 2 L h u s 0 3 I f L e r N V 1 l G F E z i F c 3 D h C p p w B y 1 o A 4 E p P M M r v F m Z 9 W K 9 W x + L 0 Y p V 7 h z D H 1 i f P 7 0 Z k 8 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t z V / / 7 + H w z s 9 G r 8 g q 6 8 d t b N x I c k = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q i 6 L K g C x c u K t o H t C F M p p N 2 6 G Q S Z i a F E v I n b l w o 4 t Y / c e f f O G m z 0 N Y D A 4 d z 7 u W e O U H C m d K O 8 2 1 V 1 t Y 3 N r e q 2 7 W d 3 b 3 9 A / v w q K P i V B L a J j G P Z S / A i n I m a F s z z W k v k R R H A a f d Y H J T + N 0 p l Y r F 4 k n P E u p F e C R Y y A j W R v J t e x B h P S a Y Z / e 5 n 9 0 + 5 r 5 d d x r O H G i V u C W p Q 4 m W b 3 8 N h j F J I y o 0 4 V i p v u s k 2 s u w 1 I x w m t c G q a I J J h M 8 o n 1 D B Y 6 o 8 r J 5 8 h y d G W W I w l i a J z S a q 7 8 3 M h w p N Y s C M 1 n k V M t e I f 7 n 9 V M d X n s Z E 0 m q q S C L Q 2 H K k Y 5 R U Q M a M k m J 5 j N D M J H M Z E V k j C U m 2 p R V M y W 4 y 1 9 e J Z 2 L h u s 0 3 I f L e r N V 1 l G F E z i F c 3 D h C p p w B y 1 o A 4 E p P M M r v F m Z 9 W K 9 W x + L 0 Y p V 7 h z D H 1 i f P 7 0 Z k 8 E = &lt; / l a t e x i t &gt; The left part shows that a target user-item pair (U1-I1)can be directly sampled from the observed edges for DSEmbed; the right part shows that for U1 (or I1), a 2-step random walk is applied to obtain the contexts used in the NSEmbed module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 :</head><label>1</label><figDesc>An overview of the proposed CSE framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 2 )</head><label>2</label><figDesc>The proposed sampling technique enables the suitability of CSE for large-scale user-item recommendations. (3) We provide model analyses for flexibility, scalability and time and space complexity of the proposed CSE. (4) We report extensive experiments over eight recommendation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2 :</head><label>2</label><figDesc>Sensitivity and convergence analyses the performance of WALS, WARP and K-OS is very competitive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets considered in our experiments.</figDesc><table><row><cell>a http://baltrunas.info/research-menu/frappe</cell><cell>f http://jmcauley.ucsd.edu/data/amazon/</cell></row><row><cell>b http://www.wanghao.in/CDL.htm</cell><cell>g http://www.trustlet.org/epinions.html</cell></row><row><cell>c http://academictorrents.com/</cell><cell>h https://labrosa.ee.columbia.edu/millionsong/</cell></row><row><cell>d https://grouplens.org/</cell><cell>tasteprofile</cell></row><row><cell cols="2">e http://www.dtic.upf.edu//~ocelma/MusicRecommendationDataset/</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>4.1.3 Experimental Setup.For all the experiments, the dimension of embedding vectors was fixed to 100; the values of the hyperparameters for the compared method were decided via implementing a grid search over different settings, and the combination that leads to the best performance was picked. The ranges of hyperparameters we searched for the compared methods are listed as follows.</figDesc><table><row><cell>• learning rate: [0.0025, 0.01, 0.025, 0.1]</cell></row><row><cell>• regularization: [0.00025, 0.001, 0.0025, 0.01, 0.025, 0.1]</cell></row><row><cell>• training epoch: [10, 20, 40, 80, 160]</cell></row><row><cell>• sampling time: [20 × |E|, 40 × |E|, 60 × |E|, 80 × |E|, 100 × |E|]</cell></row><row><cell>• walk time: [10, 40, 80]</cell></row><row><cell>• walk length: [40, 60, 80]</cell></row><row><cell>• window size: [2, 3, 4, 5, 6, 8, 10]</cell></row><row><cell>• stopping probability for random walk: [0.15, 0.25, 0.5, 0.75]</cell></row><row><cell>• k-order: [1, 2, 3]</cell></row><row><cell>• rank margin: 1 (commonly used default value)</cell></row><row><cell>• number of negative samples: 5 (commonly used default value)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Recommendation performance. The † symbol indicates the best performing method among all the baseline methods; '*' and '%Improv.' denote statistical significance at p &lt; 0.01 with a paired t -test and the percentage improvement of the proposed method, respectively, with respect to the best performing baseline.</figDesc><table><row><cell>u as</cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP@N =</cell><cell>1 |U | u ∈U</cell><cell>AP u @k</cell><cell>(10)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/cnclabs/proNet-core</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the space for the learned embedding matrices Θ = {Φ, Φ U C , Φ I C } is O(|V |) and |V | ≪ |E |.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/erikbern/ann-benchmarks<ref type="bibr" target="#b3">4</ref> Note that in real-world scenarios, most feedback is not explicit but implicit<ref type="bibr" target="#b24">[25]</ref>; we here converted the datasets into implicit feedback as most of the recent developed methods focus on dealing with such type of data. However, our method is not limited to binary preference since the presented sampling technique has the flexibility to manage arbitrary weighted edge distributions and rating estimation is also allowed in the proposed RATE-CSE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/quora/qmf 6 https://github.com/lyst/lightfm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">While the memory usage of coFactor implementation is O(|V | 2 ), BiNE's requires extensive computational time, e.g., more than 24 hours to learn the embedding for the large dataset, Movielens-Latest.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Item2Vec: Neural item embedding for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop IEEE MLSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vertex-Context Sampling for Weighted Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Querybased Music Recommendations via Preference Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM RecSys</title>
		<meeting>ACM RecSys</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIBMF: A Library for Parallel Matrix Factorization in Shared-memory Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Wen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Local Item-Item Models For Top-N Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelia</forename><surname>Christakopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM RecSys</title>
		<meeting>ACM RecSys</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BiNE: Bipartite Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leihui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoying</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Node2Vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RaRE: Social Rank Regulated Large-scale Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TrustSVD: Collaborative Filtering with Both the Explicit and Implicit Influence of User Trust and of Item Ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Yorke-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collaborative Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collaborative Filtering for Implicit Feedback Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICDM</title>
		<meeting>IEEE ICDM</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recommender systemsâĂŤbeyond matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Zanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM KDD</title>
		<meeting>ACM KDD</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Word Embedding As Implicit Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaan</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM RecSys</title>
		<meeting>ACM RecSys</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and Their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SLIM: Sparse Linear Methods for Top-N Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICDM</title>
		<meeting>IEEE ICDM</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning User-Item Relatedness from Knowledge Graphs for Top-N Item Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Palumbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphaël</forename><surname>Troncy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM RecSys</title>
		<meeting>ACM RecSys</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Walklets: Multiscale Graph Embeddings for Interpretable Network Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HOG-WILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey of collaborative filtering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in artificial intelligence</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>n. d.. n. d.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DropoutNet: Addressing Cold Start in Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomi</forename><surname>Poutanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Collaborative Deep Learning for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM KDD</title>
		<meeting>ACM KDD</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collaborative Deep Learning for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">WSABIE: Scaling Up to Large Vocabulary Image Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to Rank Recommendations with the K-order Statistic Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM RecSys</title>
		<meeting>ACM RecSys</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">WalkRanker: A Unified Pairwise Ranking Model with Multiple Relations for Item</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations for Recommender Systems with a Network Embedding Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIRS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable Graph Embedding for Asymmetric Proximity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
