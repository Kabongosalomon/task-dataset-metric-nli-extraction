<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Scale Adversarial Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">Donahue</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">Simonyan</forename><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">Large Scale Adversarial Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -including image generators and encoders -are available on TensorFlow Hub 1 .</p><p>1 Models available at https://tfhub.dev/s?publisher=deepmind&amp;q=bigbigan, with a Colab notebook demo at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years we have seen rapid progress in generative models of visual data. While these models were previously confined to domains with single or few modes, simple structure, and low resolution, with advances in both modeling and hardware they have since gained the ability to convincingly generate complex, multimodal, high resolution image distributions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Intuitively, the ability to generate data in a particular domain necessitates a high-level understanding of the semantics of said domain. This idea has long-standing appeal as raw data is both cheapreadily available in virtually infinite supply from sources like the Internet -and rich, with images comprising far more information than the class labels that typical discriminative machine learning models are trained to predict from them. Yet, while the progress in generative models has been undeniable, nagging questions persist: what semantics have these models learned, and how can they be leveraged for representation learning?</p><p>The dream of generation as a means of true understanding from raw data alone has hardly been realized. Instead, the most successful approaches for unsupervised learning leverage techniques adopted from the field of supervised learning, a class of methods known as self-supervised learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b10">11]</ref>. These approaches typically involve changing or holding back certain aspects of the data in some way, and training a model to predict or generate aspects of the missing information. For example, <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> proposed colorization as a means of unsupervised learning, where a model is given a subset of the color channels in an input image, and trained to predict the missing channels.</p><p>Generative models as a means of unsupervised learning offer an appealing alternative to selfsupervised tasks in that they are trained to model the full data distribution without requiring any modification of the original data. One class of generative models that has been applied to representation learning is generative adversarial networks (GANs) <ref type="bibr" target="#b12">[13]</ref>. The generator in the GAN  <ref type="figure">Figure 1</ref>: The structure of the BigBiGAN framework. The joint discriminator D is used to compute the loss . Its inputs are data-latent pairs, either (x ∼ P x ,ẑ ∼ E(x)), sampled from the data distribution P x and encoder E outputs, or (x ∼ G(z), z ∼ P z ), sampled from the generator G outputs and the latent distribution P z . The loss includes the unary data term s x and the unary latent term s z , as well as the joint term s xz which ties the data and latent distributions.</p><p>framework is a feed-forward mapping from randomly sampled latent variables (also called "noise") to generated data, with learning signal provided by a discriminator trained to distinguish between real and generated data samples, guiding the generator's outputs to follow the data distribution. The adversarially learned inference (ALI) <ref type="bibr" target="#b9">[10]</ref> or bidirectional GAN (BiGAN) <ref type="bibr" target="#b6">[7]</ref> approaches were proposed as extensions to the GAN framework that augment the standard GAN with an encoder module mapping real data to latents, the inverse of the mapping learned by the generator.</p><p>In the limit of an optimal discriminator, <ref type="bibr" target="#b6">[7]</ref> showed that a deterministic BiGAN behaves like an autoencoder minimizing 0 reconstruction costs; however, the shape of the reconstruction error surface is dictated by a parametric discriminator, as opposed to simple pixel-level measures like the 2 error. Since the discriminator is usually a powerful neural network, the hope is that it will induce an error surface which emphasizes "semantic" errors in reconstructions, rather than low-level details.</p><p>In <ref type="bibr" target="#b6">[7]</ref> it was demonstrated that the encoder learned via the BiGAN or ALI framework is an effective means of visual representation learning on ImageNet for downstream tasks. However, it used a DCGAN <ref type="bibr" target="#b30">[31]</ref> style generator, incapable of producing high-quality images on this dataset, so the semantics the encoder could model were in turn quite limited. In this work we revisit this approach using BigGAN <ref type="bibr" target="#b1">[2]</ref> as the generator, a modern model that appears capable of capturing many of the modes and much of the structure present in ImageNet images. Our contributions are as follows:</p><p>• We show that BigBiGAN (BiGAN with BigGAN generator) matches the state of the art in unsupervised representation learning on ImageNet.</p><p>• We propose a more stable version of the joint discriminator for BigBiGAN.</p><p>• We perform a thorough empirical analysis and ablation study of model design choices.</p><p>• We show that the representation learning objective also improves unconditional image generation, and demonstrate state-of-the-art results in unconditional ImageNet generation.</p><p>• We open source pretrained BigBiGAN models on TensorFlow Hub 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BigBiGAN</head><p>The BiGAN <ref type="bibr" target="#b6">[7]</ref> or ALI <ref type="bibr" target="#b9">[10]</ref> approaches were proposed as extensions of the GAN <ref type="bibr" target="#b12">[13]</ref> framework which enable the learning of an encoder that can be employed as an inference model <ref type="bibr" target="#b9">[10]</ref> or feature representation <ref type="bibr" target="#b6">[7]</ref>. Given a distribution P x of data x (e.g., images), and a distribution P z of latents z (usually a simple continuous distribution like an isotropic Gaussian N (0, I)), the generator G models a conditional distribution P (x|z) of data x given latent inputs z sampled from the latent prior P z , as in the standard GAN generator <ref type="bibr" target="#b12">[13]</ref>. The encoder E models the inverse conditional distribution P (z|x), predicting latents z given data x sampled from the data distribution P x .</p><p>Besides the addition of E, the other modification to the GAN in the BiGAN framework is a joint discriminator D, which takes as input data-latent pairs (x, z) (rather than just data x as in a standard GAN), and learns to discriminate between pairs from the data distribution and encoder, versus the generator and latent distribution. Concretely, its inputs are pairs (x ∼ P x ,ẑ ∼ E(x)) and (x ∼ G(z), z ∼ P z ), and the goal of the G and E is to "fool" the discriminator by making the two joint distributions P xE and P Gz from which these pairs are sampled indistinguishable. The adversarial minimax objective in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, analogous to that of the GAN framework <ref type="bibr" target="#b12">[13]</ref>, was defined as follows:</p><formula xml:id="formula_0">min GE max D E x∼Px,z∼EΦ(x) [log(σ(D(x, z)))] + E z∼Pz,x∼GΦ(z) [log(1 − σ(D(x, z)))]</formula><p>Under this objective, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> showed that with an optimal D, G and E minimize the Jensen-Shannon divergence between the joint distributions P xE and P Gz , and therefore at the global optimum, the two joint distributions P xE = P Gz match, analogous to the results from standard GANs <ref type="bibr" target="#b12">[13]</ref>. Furthermore, <ref type="bibr" target="#b6">[7]</ref> showed that in the case where E and G are deterministic functions (i.e., the learned conditional distributions P G (x|z) and P E (z|x) are Dirac δ functions), these two functions are inverses at the global optimum: e.g., ∀ x∈supp(Px) x = G(E(x)), with the optimal joint discriminator effectively imposing 0 reconstruction costs on x and z.</p><p>While the crux of our approach, BigBiGAN, remains the same as that of BiGAN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, we have adopted the generator and discriminator architectures from the state-of-the-art BigGAN <ref type="bibr" target="#b1">[2]</ref> generative image model. Beyond that, we have found that an improved discriminator structure leads to better representation learning results without compromising generation ( <ref type="figure">Figure 1</ref>). Namely, in addition to the joint discriminator loss proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> which ties the data and latent distributions together, we propose additional unary terms in the learning objective, which are functions only of either the data x or the latents z. Although <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> prove that the original BiGAN objective already enforces that the learnt joint distributions match at the global optimum, implying that the marginal distributions of x and z match as well, these unary terms intuitively guide optimization in the "right direction" by explicitly enforcing this property. For example, in the context of image generation, the unary loss term on x matches the original GAN objective and provides a learning signal which steers only the generator to match the image distribution independently of its latent inputs. (In our evaluation we will demonstrate empirically that the addition of these terms results in both improved generation and representation learning.)</p><p>Concretely, the discriminator loss L D and the encoder-generator loss L EG are defined as follows, based on scalar discriminator "score" functions s * and the corresponding per-sample losses * :</p><formula xml:id="formula_1">s x (x) = θ x F Θ (x) s z (z) = θ z H Θ (z) s xz (x, z) = θ xz J Θ (F Θ (x), H Θ (z)) EG (x, z, y) = y (s x (x) + s z (z) + s xz (x, z)) y ∈ {−1, +1} L EG (P x , P z ) = E x∼Px,ẑ∼EΦ(x) [ EG (x,ẑ, +1)] + E z∼Pz,x∼GΦ(z) [ EG (x, z, −1)] D (x, z, y) = h(ys x (x)) + h(ys z (z)) + h(ys xz (x, z)) y ∈ {−1, +1} L D (P x , P z ) = E x∼Px,ẑ∼EΦ(x) [ D (x,ẑ, +1)] + E z∼Pz,x∼GΦ(z) [ D (x, z, −1)]</formula><p>where h(t) = max(0, 1 − t) is a "hinge" used to regularize the discriminator <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37]</ref> 3 , also used in BigGAN <ref type="bibr" target="#b1">[2]</ref>. The discriminator D includes three submodules: F , H, and J. F takes only x as input and H takes only z, and learned projections of their outputs with parameters θ x and θ z respectively give the scalar unary scores s x and s z . In our experiments, the data x are images and latents z are unstructured flat vectors; accordingly, F is a ConvNet and H is an MLP. The joint score s xz tying x and z is given by the remaining D submodule, J, a function of the outputs of F and H.</p><p>The E and G parameters Φ are optimized to minimize the loss L EG , and the D parameters Θ are optimized to minimize loss L D . As usual, the expectations E are estimated by Monte Carlo samples taken over minibatches.</p><p>Like in BiGAN <ref type="bibr" target="#b6">[7]</ref> and ALI <ref type="bibr" target="#b9">[10]</ref>, the discriminator loss L D intuitively trains the discriminator to distinguish between the two joint data-latent distributions from the encoder and the generator, pushing it to predict positive values for encoder input pairs (x, E(x)) and negative values for generator input pairs (G(z), z). The generator and encoder loss L EG trains these two modules to fool the discriminator into incorrectly predicting the opposite, in effect pushing them to create matching joint data-latent distributions. (In the case of deterministic E and G, this requires the two modules to invert one another <ref type="bibr" target="#b6">[7]</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Most of our experiments follow the standard protocol used to evaluate unsupervised learning techniques, first proposed in <ref type="bibr" target="#b40">[41]</ref>. We train a BigBiGAN on unlabeled ImageNet, freeze its learned representation, and then train a linear classifier on its outputs, fully supervised using all of the training set labels. We also measure image generation performance, reporting Inception Score <ref type="bibr" target="#b34">[35]</ref> (IS) and Fréchet Inception Distance <ref type="bibr" target="#b17">[18]</ref> (FID) as the standard metrics there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ablation</head><p>We begin with an extensive ablation study in which we directly evaluate a number of modeling choices, with results presented in <ref type="table" target="#tab_10">Table 1</ref>. Where possible we performed three runs of each variant with different seeds and report the mean and standard deviation for each metric.</p><p>We start with a relatively fully-fledged version of the model at 128 × 128 resolution (row Base), with the G architecture and the F component of D taken from the corresponding 128 × 128 architectures in BigGAN, including the skip connections and shared noise embedding proposed in <ref type="bibr" target="#b1">[2]</ref>. z is 120 dimensions, split into six groups of 20 dimensions fed into each of the six layers of G as in <ref type="bibr" target="#b1">[2]</ref>. The remaining components of D -H and J -are 8-layer MLPs with ResNet-style skip connections (four residual blocks with two layers each) and size 2048 hidden layers. The E architecture is the ResNet-v2-50 ConvNet originally proposed for image classification in <ref type="bibr" target="#b15">[16]</ref>, followed by a 4-layer MLP (size 4096) with skip connections (two residual blocks) after ResNet's globally average pooled output. The unconditional BigGAN training setup corresponds to the "Single Label" setup proposed in <ref type="bibr" target="#b26">[27]</ref>, where a single "dummy" label is used for all images (theoretically equivalent to learning a bias in place of the class-conditional batch norm inputs). We then ablate several aspects of the model, with results detailed in the following paragraphs. Additional architectural and optimization details are provided in Appendix A. Full learning curves for many results are included in Appendix D.</p><p>Latent distribution P z and stochastic E. As in ALI <ref type="bibr" target="#b9">[10]</ref>, the encoder E of our Base model is nondeterministic, parametrizing a distribution N (µ, σ). µ andσ are given by a linear layer at the output of the model, and the final standard deviation σ is computed fromσ using a non-negative "softplus" non-linearity σ = log(1 + exp(σ)) <ref type="bibr" target="#b8">[9]</ref>. The final z uses the reparametrized sampling from <ref type="bibr" target="#b22">[23]</ref>, with z = µ + σ, where ∼ N (0, I). Compared to a deterministic encoder (row Deterministic E) which predicts z directly without sampling (effectively modeling P (z|x) as a Dirac δ distribution), the non-deterministic Base model achieves significantly better classification performance (at no cost to generation). We also compared to using a uniform P z = U(−1, 1) (row Uniform P z ) with E deterministically predicting z = tanh(ẑ) given a linear outputẑ, as done in BiGAN <ref type="bibr" target="#b6">[7]</ref>. This also achieves worse classification results than the non-deterministic Base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unary loss terms.</head><p>We evaluate the effect of removing one or both unary terms of the loss function proposed in Section 2, s x and s z . Removing both unary terms (row No Unaries) corresponds to the original objective proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. It is clear that the x unary term has a large positive effect on generation performance, with the Base and x Unary Only rows having significantly better IS and FID than the z Unary Only and No Unaries rows. This result makes intuitive sense as it matches the standard generator loss. It also marginally improves classification performance. The z unary term makes a more marginal difference, likely due to the relative ease of modeling relatively simple distributions like isotropic Gaussians, though also does result in slightly improved classification and generation in terms of FID -especially without the x term (z Unary Only vs. No Unaries). On the other hand, IS is worse with the z term. This may be due to IS roughly measuring the generator's coverage of the major modes of the distribution (the classes) rather than the distribution in its entirety, the latter of which may be better captured by FID and more likely to be promoted by a good encoder E. The requirement of invertibility in a (Big)BiGAN could be encouraging the generator to produce distinguishable outputs across the entire latent space, rather than "collapsing" large volumes of latent space to a single mode of the data distribution.</p><p>G capacity. To address the question of the importance of the generator G in representation learning, we vary the capacity of G (with E and D fixed) in the Small G rows. With a third of the capacity of the Base G model (Small G (32)), the overall model is quite unstable and achieves significantly worse classification results than the higher capacity base model <ref type="bibr" target="#b3">4</ref> With two-thirds capacity (Small G (64)), generation performance is substantially worse (matching the results in <ref type="bibr" target="#b1">[2]</ref>) and classification performance is modestly worse. These results confirm that a powerful image generator is indeed important for learning good representations via the encoder. Assuming this relationship holds in the future, we expect that better generative models are likely to lead to further improvements in representation learning.</p><p>Standard GAN. We also compare BigBiGAN's image generation performance against a standard unconditional BigGAN with no encoder E and only the standard F ConvNet in the discriminator, with only the s x term in the loss (row No E (GAN)). While the standard GAN achieves a marginally better IS, the BigBiGAN FID is about the same, indicating that the addition of the BigBiGAN E and joint D does not compromise generation with the newly proposed unary loss terms described in Section 2.</p><p>(In comparison, the versions of the model without unary loss term on x -rows z Unary Only and No Unaries -have substantially worse generation performance in terms of FID than the standard GAN.) We conjecture that the IS is worse for similar reasons that the s z unary loss term leads to worse IS. Next we will show that with an enhanced E taking higher input resolutions, generation with BigBiGAN in terms of FID is substantially improved over the standard GAN.</p><p>High resolution E with varying resolution G. BiGAN <ref type="bibr" target="#b6">[7]</ref> proposed an asymmetric setup in which E takes higher resolution images than G outputs and D takes as input, showing that an E taking 128 × 128 inputs with a 64 × 64 G outperforms a 64 × 64 E for downstream tasks. We experiment with this setup in BigBiGAN, raising the E input resolution to 256 × 256 -matching the resolution used in typical supervised ImageNet classification setups -and varying the G output and D input resolution in {64, 128, 256}. Our results in <ref type="table" target="#tab_10">Table 1</ref> (rows High Res E (256) and Low/High Res G (*)) show that BigBiGAN achieves better representation learning results as the G resolution increases, up to the full E resolution of 256 × 256. However, because the overall model is much slower to train with G at 256 × 256 resolution, the remainder of our results use the 128 × 128 resolution for G.</p><p>Interestingly, with the higher resolution E, generation improves significantly (especially by FID), despite G operating at the same resolution (row High Res E (256) vs. Base). This is an encouraging result for the potential of BigBiGAN as a means of improving adversarial image synthesis itself, besides its use in representation learning and inference. E architecture. Keeping the E input resolution fixed at 256, we experiment with varied and often larger E architectures, including several of the ResNet-50 variants explored in <ref type="bibr" target="#b23">[24]</ref>. In particular, we expand the capacity of the hidden layers by a factor of 2 or 4, as well as swap the residual block structure to a reversible variant called RevNet <ref type="bibr" target="#b11">[12]</ref> with the same number of layers and capacity as the corresponding ResNets. (We use the version of RevNet described in <ref type="bibr" target="#b23">[24]</ref>.) We find that the base ResNet-50 model (row High Res E (256)) outperforms RevNet-50 (row RevNet), but as the network widths are expanded, we begin to see improvements from RevNet-50, with double-width RevNet outperforming a ResNet of the same capacity (rows RevNet ×2 and ResNet ×2). We see further gains with an even larger quadruple-width RevNet model (row RevNet ×4), which we use for our final results in Section 3.2.  <ref type="table" target="#tab_10">Table 1</ref>: Results for variants of BigBiGAN, given in Inception Score <ref type="bibr" target="#b34">[35]</ref> (IS) and Fréchet Inception Distance <ref type="bibr" target="#b17">[18]</ref> (FID) of the generated images, and ImageNet top-1 classification accuracy percentage (Cls.) of a supervised logistic regression classifier trained on the encoder features <ref type="bibr" target="#b40">[41]</ref>, computed on a split of 10K images randomly sampled from the training set, which we refer to as the "train val " split. The Encoder (E) columns specify the E architecture (A.) as ResNet (S) or RevNet (V), the depth (D., e.g. 50 for ResNet-50), the channel width multiplier (C.), with 1 denoting the original widths from <ref type="bibr" target="#b15">[16]</ref>, the input image resolution (R.), whether the variance is predicted and a z vector is sampled from the resulting distribution (Var.), and the learning rate multiplier η relative to the G learning rate. The Generator (G) columns specify the BigGAN G channel multiplier (C.), with 96 corresponding to the original width from <ref type="bibr" target="#b1">[2]</ref>, and output image resolution (R.). The Loss columns specify which terms of the BigBiGAN loss are present in the objective. The P z column specifies the input distribution as a standard normal N (0, 1) or continuous uniform U(−1, 1). Changes from the Base setup in each row are highlighted in blue. Results with margins of error (written as "µ ± σ") are the means and standard deviations over three runs with different random seeds. (Experiments requiring more computation were run only once.) (* Result for vanilla GAN (No E (GAN)) selected with early stopping based on best FID; other results selected with early stopping based on validation classification accuracy (Cls.).)</p><formula xml:id="formula_2">Encoder (E) Gen. (G) Loss L * Results A. D. C. R. Var. η C. R. sxz sx sz Pz IS (↑) FID (↓) Cls. (↑)</formula><p>Decoupled E/G optimization. As a final improvement, we decoupled the E optimizer from that of G, and found that simply using a 10× higher learning rate for E dramatically accelerates training and improves final representation learning results. For ResNet-50 this improves linear classifier accuracy by nearly 3% (ResNet (↑ E LR) vs. High Res E (256)). We also applied this to our largest E architecture, RevNet-50 ×4, and saw similar gains (RevNet ×4 (↑ E LR) vs. RevNet ×4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with prior methods</head><p>Representation learning. We now take our best model by train val classification accuracy from the above ablations and present results on the official ImageNet validation set, comparing against the state of the art in recent unsupervised learning literature. For comparison, we also present classification results for our best performing variant with the smaller ResNet-50-based E. These models correspond to the last two rows of <ref type="table" target="#tab_10">Table 1</ref>, ResNet (↑ E LR) and RevNet ×4 (↑ E LR).</p><p>Results are presented in <ref type="table" target="#tab_2">Table 2</ref>. (For reference, the fully supervised accuracy of these architectures is given in Appendix A, <ref type="table" target="#tab_5">Table 4</ref>.) Compared with a number of modern self-supervised approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref> and combinations thereof <ref type="bibr" target="#b5">[6]</ref>, our BigBiGAN approach based purely on generative models performs well for representation learning, state-of-the-art among recent unsupervised learning results, improving upon a recently published result from <ref type="bibr" target="#b23">[24]</ref> of 55.4% to 60.8% top-1 accuracy using rotation prediction pre-training with the same representation learning architecture <ref type="bibr" target="#b4">5</ref>     <ref type="bibr" target="#b26">[27]</ref>. We specify the "pseudo-labeling" method as SL (Single Label) or Clustering. For comparison we train BigBiGAN for the same number of steps (500K) as the BigGAN-based approaches from <ref type="bibr" target="#b26">[27]</ref>, but also present results from additional training to 1M steps in the last row and observe further improvements. All results above include the median m as well as the mean µ and standard deviation σ across three runs, written as "m (µ ± σ)". The BigBiGAN result is selected with early stopping based on best FID vs. Train. labeled as AvePool in <ref type="table" target="#tab_2">Table 2</ref>, and matches the results of the concurrent work in <ref type="bibr" target="#b16">[17]</ref> based on contrastic predictive coding (CPC).</p><p>We also experiment with learning linear classifiers on a different rendering of the AvePool feature, labeled BN+CReLU, which boosts our best results with RevNet ×4 to 61.3% top-1 accuracy. Given the global average pooling output a, we first compute h = BatchNorm(a), and the final feature is computed by concatenating [ReLU(h), ReLU(−h)], sometimes called a "CReLU" (concatened ReLU) non-linearity <ref type="bibr" target="#b35">[36]</ref>. BatchNorm denotes parameter-free Batch Normalization <ref type="bibr" target="#b18">[19]</ref>, where the scale (γ) and offset (β) parameters are not learned, so training a linear classifier on this feature does not involve any additional learning. The CReLU non-linearity retains all the information in its inputs and doubles the feature dimension, each of which likely contributes to the improved results.</p><p>Finally, in Appendix C we consider evaluating representations by zero-shot k nearest neighbors classification, achieving 43.3% top-1 accuracy in this setting. Qualitative examples of nearest neighbors are presented in <ref type="figure" target="#fig_2">Figure 13</ref>.</p><p>Unsupervised image generation. In <ref type="table" target="#tab_3">Table 3</ref> we show results for unsupervised generation with BigBiGAN, comparing to the BigGAN-based <ref type="bibr" target="#b1">[2]</ref> unsupervised generation results from <ref type="bibr" target="#b26">[27]</ref>. Note that these results differ from those in <ref type="table" target="#tab_10">Table 1</ref> due to the use of the data augmentation method of [27] 6  <ref type="figure">E(x)</ref>). Unlike most explicit reconstruction costs (e.g., pixel-wise), the reconstruction cost implicitly minimized by a (Big)BiGAN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> tends to emphasize more semantic, high-level details. Additional reconstructions are presented in Appendix B.</p><p>(rather than ResNet-style preprocessing used for all results in our <ref type="table" target="#tab_10">Table 1</ref> ablation study). The lighter augmentation from <ref type="bibr" target="#b26">[27]</ref> results in better image generation performance under the IS and FID metrics. The improvements are likely due in part to the fact that this augmentation, on average, crops larger portions of the image, thus yielding generators that typically produce images encompassing most or all of a given object, which tends to result in more representative samples of any given class (giving better IS) and more closely matching the statistics of full center crops (as used in the real data statistics to compute FID). Besides this preprocessing difference, the approaches in <ref type="table" target="#tab_3">Table 3</ref> have the same configurations as used in the Base or High Res E (256) row of <ref type="table" target="#tab_10">Table 1</ref>.</p><p>These results show that BigBiGAN significantly improves both IS and FID over the baseline unconditional BigGAN generation results with the same (unsupervised) "labels" (a single fixed label in the SL (Single Label) approach -row BigBiGAN + SL vs. BigGAN + SL). We see further improvements using a high resolution E (row BigBiGAN High Res E + SL), surpassing the previous unsupervised state of the art (row BigGAN + Clustering) under both IS and FID. (Note that the image generation results remain comparable: the generated image resolution is still 128 × 128 here, despite the higher resolution E input.) The alternative "pseudo-labeling" approach from <ref type="bibr" target="#b26">[27]</ref>, Clustering, which uses labels derived from unsupervised clustering, is complementary to BigBiGAN and combining both could yield further improvements. Finally, observing that results continue to improve significantly with training beyond 500K steps, we also report results at 1M steps in the final row of <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reconstruction</head><p>As shown in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, the (Big)BiGAN E and G can reconstruct data instances x by computing the encoder's predicted latent representation E(x) and then passing this predicted latent back through the generator to obtain the reconstruction G(E(x)). We present BigBiGAN reconstructions in <ref type="figure" target="#fig_1">Figure 2</ref>. These reconstructions are far from pixel-perfect, likely due in part to the fact that no reconstruction cost is explicitly enforced by the objective -reconstructions are not even computed at training time. However, they may provide some intuition for what features the encoder E learns to model. For example, when the input image contains a dog, person, or a food item, the reconstruction is often a different instance of the same "category" with similar pose, position, and texture -for example, a similar species of dog facing the same direction. The extent to which these reconstructions tend to retain the high-level semantics of the inputs rather than the low-level details suggests that BigBiGAN training encourages the encoder to model the former more so than the latter. Additional reconstructions are presented in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>A number of approaches to unsupervised representation learning from images based on selfsupervision have proven very successful. Self-supervision generally involves learning from tasks designed to resemble supervised learning in some way, but in which the "labels" can be created automatically from the data itself with no manual effort. An early example is relative location prediction <ref type="bibr" target="#b4">[5]</ref>, where a model is trained on input pairs of image patches and predicts their relative locations. Contrastive predictive coding (CPC) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b16">17]</ref> is a recent related approach where, given an image patch, a model predicts which patches occur in other image locations. Other approaches include colorization <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, motion segmentation <ref type="bibr" target="#b29">[30]</ref>, rotation prediction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4]</ref>, GAN-based discrimination <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4]</ref>, and exemplar matching <ref type="bibr" target="#b7">[8]</ref>. Rigorous empirical comparisons of many of these approaches have also been conducted <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>. A key advantage offered by BigBiGAN and other approaches based on generative models, relative to most self-supervised approaches, is that their input may be the full-resolution image or other signal, with no cropping or modification of the data needed (though such modifications may be beneficial as data augmentation). This means the resulting representation can typically be applied directly to full data in the downstream task with no domain shift.</p><p>A number of relevant autoencoder and GAN variants have also been proposed. Associative compression networks (ACNs) <ref type="bibr" target="#b14">[15]</ref> learn to compress at the dataset level by conditioning data on other previously transmitted data which are similar in code space, resulting in models that can "daydream" semantically similar samples, similar to BigBiGAN reconstructions. VQ-VAEs <ref type="bibr" target="#b39">[40]</ref> pair a discrete (vector quantized) encoder with an autoregressive decoder to produce faithful reconstructions with a high compression factor and demonstrate representation learning results in reinforcement learning settings. In the adversarial space, adversarial autoencoders <ref type="bibr" target="#b27">[28]</ref> proposed an autoencoder-style encoder-decoder pair trained with pixel-level reconstruction cost, replacing the KL-divergence regularization of the prior used in VAEs <ref type="bibr" target="#b22">[23]</ref> with a discriminator. In another proposed VAE-GAN hybrid <ref type="bibr" target="#b24">[25]</ref> the pixel-space reconstruction error used in most VAEs is replaced with feature space distance from an intermediate layer of a GAN discriminator. Other hybrid approaches like AGE <ref type="bibr" target="#b37">[38]</ref> and α-GAN <ref type="bibr" target="#b32">[33]</ref> add an encoder to stabilize GAN training. An interesting difference between many of these approaches and the BiGAN <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7]</ref> framework is that BiGAN does not train the encoder or generator with an explicit reconstruction cost. Though it can be shown that (Big)BiGAN implicitly minimizes a reconstruction cost, qualitative reconstruction results (Section 3.3) suggest that this reconstruction cost is of a different flavor, emphasizing high-level semantics over pixel-level details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We have shown that BigBiGAN, an unsupervised learning approach based purely on generative models, achieves state-of-the-art results in image representation learning on ImageNet. Our ablation study lends further credence to the hope that powerful generative models can be beneficial for representation learning, and in turn that learning an inference model can improve large-scale generative models.</p><p>In the future we hope that representation learning can continue to benefit from further advances in generative models and inference models alike, as well as scaling to larger image databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Model and optimization details</head><p>Our optimizer matches that of BigGAN [2] -we use Adam <ref type="bibr" target="#b20">[21]</ref> with batch size 2048 and the same learning rates and other hyperparameters, using the G optimizer to update E simultaneously, with the same alternating optimization: two D updates followed by a single joint update of G and E. (We do not use orthogonal regularization used in <ref type="bibr" target="#b1">[2]</ref>, finding it gave worse results in the unconditional setting, matching the findings of <ref type="bibr" target="#b26">[27]</ref>.) Spectral normalization <ref type="bibr" target="#b28">[29]</ref> is used in G and D, but not in E. Full cross-replica batch normalization is used in both G and E (including for the linear classifier training on E features used for evaluations). We also apply exponential moving averaging (EMA) with a decay of 0.9999 to the G and E weights in all evaluations. (We find this results in only a small improvement for E evaluations, but a substantial one for G evaluations.)</p><p>At BigBiGAN training time, as well as linear classification evaluation training time, we preprocess inputs with ResNet <ref type="bibr" target="#b15">[16]</ref>-style data augmentation, though with crops of size 128 or 256 rather than 224 <ref type="bibr" target="#b6">7</ref> .</p><p>For linear classification evaluations in the ablations reported in <ref type="table" target="#tab_10">Table 1</ref>, we hold out 10K randomly selected images from the official ImageNet <ref type="bibr" target="#b33">[34]</ref> training set as a validation set and report accuracy on that validation set, which we call train val . All results in <ref type="table" target="#tab_10">Table 1</ref> are run for 500K steps, with early stopping based on linear classifier accuracy on our train val split. In all of these models the linear classifier is initialized to 0 and trained for 5K Adam steps with a (high) learning rate of 0.01 and EMA smoothing with decay 0.9999. We have found it helpful to monitor representation learning progress during BigBiGAN training by periodically rerunning this linear classification evaluation from scratch given the current E weights, resetting the classifier weights to 0 before each evaluation.</p><p>In <ref type="table" target="#tab_2">Table 2</ref> we extend the BigBiGAN training time to 1M steps, and report results on the official validation set of 50K images for comparison with prior work. The classifier in these results is trained for 100K Adam steps, sweeping over learning rates {10 −4 , 3 · 10 −4 , 10 −3 , 3 · 10 −3 , 10 −2 }, again applying EMA with decay 0.9999 to the classifier weights. Hyperparameter selection and early stopping is again based on classification accuracy on train val . As in <ref type="bibr" target="#b1">[2]</ref>, FID is reported against statistics over the full ImageNet training set, preprocessed by resizing the minor axis to the G output resolution and taking the center crop along the major axis, except as noted in <ref type="table" target="#tab_3">Table 3</ref>, where we also report FID against the validation set for comparison with <ref type="bibr" target="#b26">[27]</ref>.</p><p>All models were trained via TensorFlow <ref type="bibr" target="#b0">[1]</ref> and Sonnet <ref type="bibr" target="#b31">[32]</ref> with data parallelism on TPU pod slices <ref type="bibr" target="#b13">[14]</ref> using 32 to 512 cores, coordinated by TF-Replicator <ref type="bibr" target="#b2">[3]</ref>.</p><p>Supervised model performance. In <ref type="table" target="#tab_5">Table 4</ref> we present the results of fully supervised training with the model architectures used in our experiments in Section 3 for comparison purposes.  First layer convolutional filters. In <ref type="figure" target="#fig_2">Figure 3</ref> we visualize the learned convolutional filters for the first convolutional layer of our BigBiGAN encoders E using the largest RevNet ×4 E architecture. Note the difference between the filters in (a) and (b) (corresponding to rows RevNet ×4 and RevNet ×4 (↑ E LR) in <ref type="table" target="#tab_10">Table 1</ref>). In (b) we use the higher E learning rate and see a corresponding qualitative improvement in the appearance of the learned filters, with less noise and more Gabor-like and color filters, as observed in BiGAN <ref type="bibr" target="#b6">[7]</ref>. This suggests that examining the convolutional filters of the input layer can serve as a diagnostic for undertrained models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Samples and reconstructions</head><p>In this Appendix we present BigBiGAN samples and reconstructions from several variants of the method. <ref type="table" target="#tab_6">Table 5</ref> includes pointers to samples and reconstruction images, as well as relevant metrics. The samples were selected by best FID vs. training set statistics, and we show the IS and FID along with sample images at that point. The reconstructions were selected by best (lowest) relative pixel-wise 1 error, the error metric presented in <ref type="table" target="#tab_6">Table 5</ref>, computed as:</p><formula xml:id="formula_3">E Rel 1 = E x∼Px ||x − G(E(x))|| 1 E x,x ∼Px ||x − G(E(x))|| 1</formula><p>, where x and x are independent data samples, and ||x − G(E(x))|| 1 serves as a "baseline" reconstruction error relative to a "random" input. For example, with a random initialization of G and E, we have E Rel 1 ≈ 1. This relative metric penalizes degenerate reconstructions, such as the mean image, which would sometimes achieve low absolute reconstruction error despite having no perceptual similarity to the inputs. despite that the resulting images having no perceptual similarity to the inputs. In practice, given N data samples x 0 , x 1 , . . . , x N −1 (we use N = 50K), we estimate the denominator by comparing each sample x i with a single neighbor x (i+1) mod N , computing:</p><formula xml:id="formula_4">E Rel 1 ≈ N −1 i=0 ||x i − G(E(x i ))|| 1 N −1 i=0 ||x (i+1) mod N − G(E(x i ))|| 1</formula><p>Iterated reconstruction To further explore the behavior of a BigBiGAN (or any other model capable of approximately reconstructing its input), we can "iterate" the reconstruction operation. In particular, let R i (x) be defined for non-negative integers i and input images x as:</p><formula xml:id="formula_5">R 0 (x) = x R i+1 (x) = G(E(R i (x)))</formula><p>In <ref type="figure" target="#fig_1">Figure 12</ref> we show the results of up to 500 steps of this process for a few sample images. Qualitatively, the first several steps of this process often appear to retain some semantics of the input image x. After dozens or hundreds of iterations, however, little content from the original input apparently remains intact.          <ref type="table" target="#tab_10">Table 1</ref>, computed by recursively running the reconstruction operation G(E(·)) on its own output as described in Appendix B. In each pair of columns, the left column shows a real input image R 0 at the top, and R 1 through R 9 in the remaining rows, the results of iterating reconstruction one to nine times, The right column shows the result of up to 500 iterations sampled at longer intervals, displaying R 10 , R 20  <ref type="table">Table 6</ref>: Accuracy of k nearest neighbors classifiers in BigBiGAN feature space on the ImageNet validation set. We report results under the normalized 1 distance D 1 as well as the normalized 2 (cosine) distance D 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Nearest neighbors</head><p>In this Appendix we consider an alternative way of evaluating representations -by means of k nearest neighbors classification, which does not involve learning any parameters during evaluation and is even simpler than learning a linear classifier as done in Section 3. For all results in this section, we use the outputs of the global average pooling layer (a flat 8192D feature) of our best performing model, RevNet ×4, ↑ E LR. We do not do any data augmentation for either the training or validation sets: we simply crop each image at the center of its larger axis and resize to 256 × 256.</p><p>We use a normalized 1 or 2 distance metric as our nearest neighbors criterion, defined as D p (a, b) = a ||a||p − b ||b||p p , for p ∈ {1, 2}. (D 2 corresponds to cosine distance.) For label predictions with multiple neighbors (k &gt; 1), we use a simple counting scheme: the label with the most votes is selected as the prediction. Ties (multiple labels with the same number of votes) are broken by k = 1 nearest neighbor classification among the data with the tied labels.</p><p>Quantitative results. In <ref type="table">Table 6</ref> we present k nearest neighbors classification results for k ∈ {1, 5, 25, 50}. Across all k, the 1 -based metric D 1 outperforms D 2 , and the remainder of our discussion refers to the D 1 results. With just a single neighbor (k = 1) we achieve a top-1 accuracy around 38%. Top-1 accuracy reaches 43% with k = 25, dropping off slightly at k = 50 as votes from more distant neighbors are added.</p><p>Qualitative results. <ref type="figure" target="#fig_2">Figure 13</ref> shows sample nearest neighbors in the ImageNet training set for query images in the validation set. Despite being fully unsupervised, the neighbors in many cases match the query image in terms of high-level semantic content such as the category of the object of interest, demonstrating BigBiGAN's ability to capture high-level attributes of the data in its unsupervised representations. Where applicable, the object's pose and position in the image appears to be important as well -for example, the nearest neighbors of the RV (row 2, column 2) are all RVs facing roughly the same direction. In other cases, the nearest neighbors appear to be selected primarily based on the background or color scheme.</p><p>Discussion. While our quantitative k nearest neighbors classification results are far from the state of the art for ImageNet classification and significantly below the linear classifier-based results reported in <ref type="table" target="#tab_2">Table 2</ref>, note that in this setup, no supervised learning of model parameters from labels occurs at any point: labels are predicted purely based on distance in a feature space learned from BigBiGAN training on image pixels alone. We believe this makes nearest neighbors classification an interesting additional benchmark for future approaches to unsupervised representation learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Learning curves</head><p>In this Appendix we present learning curves showing how the image generation and representation learning metrics that we measured evolve throughout training, as a more detailed view of the results in Section 3, <ref type="table" target="#tab_10">Table 1</ref>. We include plots for the following results:</p><p>• Image generation ( <ref type="figure" target="#fig_3">Figure 14</ref>   <ref type="figure" target="#fig_4">Figure 15</ref>: Image generation and representation learning curves for the latent space variations explored in Section 3. Legend entries correspond to the following rows in  <ref type="figure" target="#fig_7">Figure 18</ref>: Image generation and representation learning curves for high resolution E with varying resolution G explored in Section 3. Legend entries correspond to the following rows in  <ref type="figure" target="#fig_8">Figure 19</ref>: Image generation and representation learning curves for the E architecture variations explored in Section 3. Legend entries correspond to the following rows in <ref type="table" target="#tab_10">Table 1</ref>: High Res E (256), ResNet-101, ResNet ×2, RevNet, RevNet ×2, and RevNet ×4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Selected reconstructions from an unsupervised BigBiGAN model (Section 3.3). Top row images are real data x ∼ P x ; bottom row images are generated reconstructions of the above image x computed by G(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of first layer convolutional filters for our unsupervised BigBiGAN models with the RevNet ×4 E architecture, which includes 1024 filters. (Best viewed with zoom.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>128 × 128 samplesx ∼ G(z) from an unsupervised BigBiGAN generator G, trained using the Base method from Table 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>128 × 128 reconstructions from an unsupervised BigBiGAN model, trained using the Base method from Table 1. The top rows of each pair are real data x ∼ P x , and bottom rows are generated reconstructions computed by G(E(x)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>128 × 128 samplesx ∼ G(z) from an unsupervised BigBiGAN generator G, trained using the lighter augmentation from<ref type="bibr" target="#b26">[27]</ref> with generation results reported inTable 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>128 × 128 reconstructions from an unsupervised BigBiGAN model, trained using the lighter augmentation from<ref type="bibr" target="#b26">[27]</ref> with generation results reported inTable 3. The top rows of each pair are real data x ∼ P x , and bottom rows are generated reconstructions computed by G(E(x)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>128 × 128 samplesx ∼ G(z) from an unsupervised BigBiGAN generator G, trained using the High Res E (256) configuration from Table 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>128 × 128 reconstructions of 256 × 256 encoder input images from an unsupervised BigBiGAN model, trained using the High Res E (256) configuration from Table 1. Reconstructions are upsampled from 128 × 128 to 256 × 256 for visualization. The top rows of each pair are real data x ∼ P x , and bottom rows are generated reconstructions computed by G(E(x)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>256 × 256 samplesx ∼ G(z) from an unsupervised BigBiGAN generator G, trained with a high-resolution E and G (High Res G (256) fromTable 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>256 × 256 reconstructions from an unsupervised BigBiGAN model, trained with a highresolution E and G (High Res G (256) fromTable 1). The top rows of each pair are real data x ∼ P x , and bottom rows are generated reconstructions computed by G(E(x)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>R 0 : R 9 RFigure 12 :</head><label>912</label><figDesc>10 : R 500 R 0 : R 9 R 10 : R 500 R 0 : R 9 R 10 : Iterated reconstructions from an unsupervised BigBiGAN model, trained using the ResNet (↑ E LR) method from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>) • Latent distribution P z and stochastic E (Figure 15) • Unary loss terms (Figure 16) • G capacity (Figure 17) • High resolution E with varying resolution G (Figure 18) • E architecture (Figure 19)• Decoupled E/G learning rates(Figure 20)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>N 22.66 ± 0.18 31.19 ± 0.37 48.10 ± 0.13 Deterministic E S 50 1 128 (-) 1 96 128 N 22.79 ± 0.27 31.31 ± 0.30 46.97 ± 0.35 Uniform Pz S 50 1 128 (-) 1 96 128 (U) 22.83 ± 0.24 31.52 ± 0.28 45.11 ± 0.93 x N 23.19 ± 0.28 31.99 ± 0.30 47.74 ± 0.20 z N 19.52 ± 0.39 39.48 ± 1.00 47.78 ± 0.28</figDesc><table><row><cell>Base</cell><cell>S 50 1 128</cell><cell>1 96 128</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Unary Only (-) Unary Only S 50 1 128 1 96 128 S 50 1 128 1 96 128 (-)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">No Unaries (BiGAN) S 50 1 128</cell><cell>1 96 128</cell><cell cols="4">(-) (-) N 19.70 ± 0.30 42.92 ± 0.92 46.71 ± 0.88</cell></row><row><cell>Small G (32)</cell><cell>S 50 1 128</cell><cell>1 (32) 128</cell><cell cols="4">N 3.28 ± 0.18 247.30 ± 10.31 43.59 ± 0.34</cell></row><row><cell>Small G (64)</cell><cell>S 50 1 128</cell><cell>1 (64) 128</cell><cell cols="4">N 19.96 ± 0.15 38.93 ± 0.39 47.54 ± 0.33</cell></row><row><cell>No E (GAN) *</cell><cell>(-)</cell><cell>96 128 (-)</cell><cell cols="3">(-) N 23.56 ± 0.37 30.91 ± 0.23</cell><cell>-</cell></row><row><cell>High Res E (256)</cell><cell>S 50 1 (256)</cell><cell>1 96 128</cell><cell cols="4">N 23.45 ± 0.14 27.86 ± 0.13 50.80 ± 0.30</cell></row><row><cell>Low Res G (64)</cell><cell>S 50 1 (256)</cell><cell>1 96 (64)</cell><cell cols="4">N 19.40 ± 0.19 15.82 ± 0.06 47.51 ± 0.09</cell></row><row><cell>High Res G (256)</cell><cell>S 50 1 (256)</cell><cell>1 96 (256)</cell><cell>N</cell><cell>24.70</cell><cell>38.58</cell><cell>51.49</cell></row><row><cell>ResNet-101</cell><cell>S (101) 1 (256)</cell><cell>1 96 128</cell><cell>N</cell><cell>23.29</cell><cell>28.01</cell><cell>51.21</cell></row><row><cell>ResNet ×2</cell><cell>S 50 (2) (256)</cell><cell>1 96 128</cell><cell>N</cell><cell>23.68</cell><cell>27.81</cell><cell>52.66</cell></row><row><cell>RevNet</cell><cell>(V) 50 1 (256)</cell><cell>1 96 128</cell><cell cols="4">N 23.33 ± 0.09 27.78 ± 0.06 49.42 ± 0.18</cell></row><row><cell>RevNet ×2</cell><cell>(V) 50 (2) (256)</cell><cell>1 96 128</cell><cell>N</cell><cell>23.21</cell><cell>27.96</cell><cell>54.40</cell></row><row><cell>RevNet ×4</cell><cell>(V) 50 (4) (256)</cell><cell>1 96 128</cell><cell>N</cell><cell>23.23</cell><cell>28.15</cell><cell>57.15</cell></row><row><cell>ResNet (↑ E LR)</cell><cell>S 50 1 (256)</cell><cell>(10) 96 128</cell><cell cols="4">N 23.27 ± 0.22 28.51 ± 0.44 53.70 ± 0.15</cell></row><row><cell cols="2">RevNet ×4 (↑ E LR) (V) 50 (4) (256)</cell><cell>(10) 96 128</cell><cell>N</cell><cell>23.08</cell><cell>28.54</cell><cell>60.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>+ SL (ours) 500K 25.43 (25.45 ± 0.04) 22.34 (22.36 ± 0.04) 22.94 (23.00 ± 0.15)</figDesc><table><row><cell>Method</cell><cell>Steps</cell><cell>IS (↑)</cell><cell>FID vs. Train (↓)</cell><cell>FID vs. Val. (↓)</cell></row><row><cell>BigGAN + SL [27]</cell><cell>500K</cell><cell>20.4 (15.4 ± 7.57)</cell><cell>-</cell><cell>25.3 (71.7 ± 66.32)</cell></row><row><cell>BigGAN + Clustering [27]</cell><cell>500K</cell><cell>22.7 (22.8 ± 0.42)</cell><cell>-</cell><cell>23.2 (22.7 ± 0.80)</cell></row><row><cell>BigBiGAN + SL (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Comparison of BigBiGAN models on the official ImageNet validation set against recent competing approaches with a supervised logistic regression classifier. BigBiGAN results are selected with early stopping based on highest accuracy on our train val subset of 10K training set images. ResNet-50 results correspond to row ResNet (↑ E LR) in Table 1, and RevNet-50 ×4 corresponds to RevNet ×4 (↑ E LR).500K 25.38 (25.33 ± 0.17) 22.78 (22.63 ± 0.23) 23.60 (23.56 ± 0.12) BigBiGAN High Res EBigBiGAN High Res E + SL (ours) 1M 27.94 (27.80 ± 0.21) 20.32 (20.27 ± 0.09) 21.61 (21.62 ± 0.09)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison of our BigBiGAN for unsupervised (unconditional) generation vs. previously reported results for unsupervised BigGAN from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>ImageNet validation set accuracy for fully supervised end-to-end training of the model architectures used in our representation learning experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Links to BigBiGAN samples and reconstructions with associated metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>R 30 , R 40 , R 50 , R 100 , R 200 , R 300 , R 400 , and R 500 . 38.09 / -41.28 / 58.56 43.32 / 65.12 42.73 / 66.22 D 2 35.68 / -38.61 / 55.59 40.65 / 62.23 40.15 / 63.42</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Top-1 / Top-5 Acc. (%)</cell><cell></cell></row><row><cell>Metric</cell><cell>k = 1</cell><cell>k = 5</cell><cell>k = 25</cell><cell>k = 50</cell></row><row><cell>D 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Figure 13 :Table 1 :</head><label>131</label><figDesc>Nearest neighbors in BigBiGAN E feature space, from our best performing model (RevNet ×4, ↑ E LR). In each row, the first (left) column is a query image, and the remaining columns are its three nearest neighbors from the training set (the leftmost being the nearest, next being the second nearest, etc.). The query images above are the first 24 images in the ImageNet validation set.Figure 14: Image generation learning curves for several of the ablations in Section 3, including a comparison of BigBiGAN to standard GAN. Legend entries correspond to the following rows in Base, No E (GAN), and High Res E (256).</figDesc><table><row><cell></cell><cell></cell><cell>Inception Score (IS)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>16 18</cell><cell></cell><cell></cell><cell></cell><cell>23.99@497500: No E (GAN) 23.89@490500: No E (GAN) 23.84@487250: High Res E 23.62@466000: High Res E 23.39@491000: High Res E 23.31@495500: Base 23.31@486500: No E (GAN) 23.03@481750: Base 22.77@465750: Base</cell><cell></cell></row><row><cell>0</cell><cell>100K</cell><cell>200K</cell><cell>300K</cell><cell>400K</cell><cell>500K</cell></row><row><cell></cell><cell></cell><cell cols="2">Fréchet Inception Distance (FID)</cell><cell></cell><cell></cell></row><row><cell>70 80</cell><cell></cell><cell></cell><cell></cell><cell>31.40@496500: Base 31.22@467000: No E (GAN) 30.85@499000: Base 30.84@478000: No E (GAN) 30.67@492000: No E (GAN) 30.50@500000: Base 27.91@462000: High Res E 27.74@498000: High Res E 27.50@473250: High Res E</cell><cell></cell></row><row><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>100K</cell><cell>200K</cell><cell>300K</cell><cell>400K</cell><cell>500K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 1 :</head><label>1</label><figDesc>Base, Deterministic E, and Uniform P z . No Unaries 42.70@418500: No Unaries 41.49@475000: No Unaries 40.32@460250: z Unary Only 39.02@475250: z Unary Only 38.39@493250: z Unary Only 32.07@477250: x Unary Only 31.79@493750: x Small G (32) 60.67@193500: Small G (32) 60.51@195000: Small G (32) 38.81@499500: Small G (64) 38.56@492000: Small G (64) 38.32@489750: Small G (64) High Res E, High Res G 27.91@462000: High Res E 27.74@498000: High Res E 27.50@473250: High Res E 15.77@492750: High Res E, Low Res G 15.68@486500: High Res E, Low Res G 15.64@499000: High Res E, Low Res G High Res E, High Res G 51.21@488750: High Res E 50.63@494250: High Res E 50.55@498000: High Res E 47.63@474000: High Res E, Low Res G 47.49@461500: High Res E, Low Res G 47.41@496500: High Res E, Low Res G High Res E, High Res G 72.03@490250: High Res E 71.89@406000: High Res E 71.06@491000: High Res E 68.74@381500: High Res E, Low Res G 68.40@464000: High Res E, Low Res G 68.24@494000: High Res E, Low Res G</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Fréchet Inception Distance (FID)</cell><cell></cell><cell></cell></row><row><cell>60 70 80</cell><cell></cell><cell></cell><cell></cell><cell>43.64@481750: Unary Only 31.40@496500: Base 31.31@497000: x Unary Only 30.85@499000: Base 30.50@500000: Base</cell><cell></cell></row><row><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>100K</cell><cell>200K</cell><cell>300K</cell><cell>400K</cell><cell>500K</cell></row><row><cell></cell><cell></cell><cell cols="2">Top 1 Classification Accuracy (%), Val. (Cls.)</cell><cell></cell><cell></cell></row><row><cell>45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30 35</cell><cell></cell><cell></cell><cell></cell><cell>48.29@461000: Base 48.10@457250: z Unary Only 48.06@477250: Base 47.98@482000: x Unary Only 47.96@497250: Base 47.82@497750: z Unary Only 47.76@492250: x Unary Only 47.66@493750: No Unaries 47.48@496000: x Unary Only 47.42@490250: z Unary Only 46.93@498250: No Unaries 45.53@494500: No Unaries</cell><cell></cell></row><row><cell>0</cell><cell>100K</cell><cell>200K</cell><cell>300K</cell><cell>400K</cell><cell>500K</cell></row><row><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>100K</cell><cell>200K</cell><cell>300K</cell><cell>400K</cell><cell>500K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1 :</head><label>1</label><figDesc>High Res E (256), Low Res G (64), and High Res G (256). High Res E 26.68@969750: High Res E, RevNet ×2 26.60@974000: High Res E 26.59@908250: High Res E, RevNet ×4 26.57@988000: High Res E, ResNet-101 26.41@950500: High Res E, ResNet ×2 26.39@970250: High Res E, RevNet 26.37@971750: High Res E, RevNet 26.25@964250: High Res E 26.18@998750: High Res E, RevNet High Res E, RevNet ×4 56.12@992250: High Res E, RevNet ×2 54.40@886750: High Res E, ResNet ×2 53.54@995500: High Res E, ResNet-101 53.08@958750: High Res E 53.07@980750: High Res E 52.55@995250: High Res E 51.97@960250: High Res E, RevNet 51.79@922000: High Res E, RevNet 51.59@976750: High Res E, RevNet High Res E 71.26@987250: High Res E 70.99@989250: High Res E, RevNet 70.89@976000: High Res E, RevNet ×2 70.75@989250: High Res E, ResNet ×2 70.72@653750: High Res E, ResNet-101 70.60@800250: High Res E 70.55@997250: High Res E, RevNet ×4 70.26@999750: High Res E, RevNet 70.00@993750: High Res E, RevNet</figDesc><table><row><cell></cell><cell>Fréchet Inception Distance (FID)</cell><cell></cell></row><row><cell>25 30 35 40 45 50 55 60 50 55 40 45 70 71 72 73 74 75 76 77</cell><cell>0 26.69@942500: 0 200K 400K 600K 800K Top 1 Classification Accuracy (%), Val. (Cls.) 200K 400K 600K 800K 200K 400K 600K 800K Relative L1 Reconstruction Error (%) 58.68@869250: 0 71.37@958750:</cell><cell>1M 1M 1M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See footnote 1 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We also considered an alternative discriminator loss D which invokes the "hinge" h just once on the sum of the three loss terms -D (x, z, y) = h(y (sx(x) + sz(z) + sxz(x, z))) -but found that this performed significantly worse than D above which clamps each of the three loss terms separately.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Though the generation performance by IS and FID in row Small G (32) is very poor at the point we measured -when its best validation classification performance (43.59%) is achieved -this model was performing more reasonably for generation earlier in training, reaching IS 14.69 and FID 60.67.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Our RevNet ×4 architecture matches the widest architectures used in<ref type="bibr" target="#b23">[24]</ref>, labeled as ×16 there.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">See the "distorted" preprocessing method from the Compare GAN framework: https://github.com/ google/compare_gan/blob/master/compare_gan/datasets.py.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Preprocessing code from the TensorFlow ResNet TPU model: https://github.com/tensorflow/tpu/ tree/master/models/official/resnet.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Aidan Clark, Olivier Hénaff, Aäron van den Oord, Sander Dieleman, and many other colleagues at DeepMind for useful discussions and feedback on this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 20</ref><p>: Image generation and representation learning curves showing the effect of decoupling the E and G optimizers to train E with 10× higher learning rate. Legend entries correspond to the following rows in <ref type="table">Table 1</ref>: High Res E (256), ResNet (↑ E LR), RevNet ×4, and RevNet ×4 (↑ E LR).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur ; Pete Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
	</analytic>
	<monogr>
		<title level="j">Josh Levenberg, Dandelion Mané</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Buchlovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aedan</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tf-Replicator</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00465</idno>
		<title level="m">Distributed machine learning for researchers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-supervised GANs via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Incorporating second-order functional knowledge for better option pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Belisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Garcia</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Cloud</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/tpu/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Associative compression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02476</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09005</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Boesen Lindbo Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geometric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-fidelity image generation with fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Open sourcing Sonnet -a new library for constructing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Puigdomènech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Racanière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viola</surname></persName>
		</author>
		<ptr target="https://deepmind.com/blog/open-sourcing-sonnet/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Variational approaches for auto-encoding generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04987</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenling</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">It takes (only) two: Adversarial generator-encoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">48@348250: x Unary Only 69.43@486500: x Unary Only Figure 16: Image generation and representation learning curves for the unary loss component variations explored in Section 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>73.17@451250: z Unary Only 72.73@462250: No Unaries 72.20@484250: Base 71.93@414000: No Unaries 71.80@495250: Base 71.33@403750: x Unary Only 71.14@408750: Base 69</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2016. Relative L1 Reconstruction Error (%) 75.97@402500: z Unary Only 74.01@493750: No Unaries 73.48@458750: z Unary Only</title>
		<imprint/>
	</monogr>
	<note>Split-brain autoencoders: Unsupervised learning by cross-channel prediction. Legend entries correspond to the following rows in Table 1: Base, x Unary Only, z Unary Only, and No Unaries (BiGAN</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image generation and representation learning curves for the G size variations explored in Section 3</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">64</biblScope>
		</imprint>
	</monogr>
	<note>Legend entries correspond to the following rows in Table 1: Base, Small G (32</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">Fréchet Inception Distance (FID) 27.27@928000: High Res E ( ↑ E LR) 27.17@944500: High Res E ( ↑ E LR) 27.02@994000: High Res E, RevNet ×4 ( ↑ E LR) 26.69@942500: High Res E 26.64@949750: High Res E ( ↑ E LR) 26.60@974000: High Res E 26.59@908250: High Res E</title>
		<imprint/>
	</monogr>
	<note>RevNet ×4 26.25@964250: High Res E</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">Top 1 Classification Accuracy (%), Val. (Cls.) 61.62@907000: High Res E, RevNet ×4 ( ↑ E LR) 58.68@869250: High Res E, RevNet ×4 55.68@999250: High Res E ( ↑ E LR) 55.48@932000: High Res E ( ↑ E LR) 55.31@932000: High Res E ( ↑ E LR) 53.08@958750: High Res E 53.07@980750: High Res E 52.55@995250: High Res E</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<title level="m">Relative L1 Reconstruction Error (%) 73.59@952750: High Res E ( ↑ E LR) 72.55@889500: High Res E ( ↑ E LR) 72.45@667000: High Res E, RevNet ×4 ( ↑ E LR) 72.40@971750: High Res E ( ↑ E LR) 71.37@958750: High Res E 71.26@987250: High Res E 70.60@800250: High Res E 70.55@997250: High Res E, RevNet ×4</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
