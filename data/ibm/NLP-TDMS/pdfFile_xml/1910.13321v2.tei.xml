<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Object Accuracy for Generative Text-to-Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Semantic Object Accuracy for Generative Text-to-Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial networks conditioned on textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image models is challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address these challenges we introduce a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption. The SOA uses a pre-trained object detector to evaluate if a generated image contains objects that are mentioned in the image caption, e.g. whether an image generated from "a car driving down the street" contains a car. We perform a user study comparing several text-to-image models and show that our SOA metric ranks the models the same way as humans, whereas other metrics such as the Inception Score do not. Our evaluation also shows that models which explicitly model objects outperform models which only model global image characteristics.</p><p>Index Terms-text-to-image synthesis, generative adversarial network (GAN), evaluation of generative models, generative models !</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>G ENERATIVE adversarial networks (GANs) <ref type="bibr" target="#b0">[1]</ref> are capable of generating realistic-looking images that adhere to characteristics described in a textual manner, e.g. an image caption. For this, most networks are conditioned on an embedding of the textual description. Often, the textual description is used on multiple levels of resolution, e.g. first to obtain a course layout of the image at lower levels and then to improve the details of the image on higher resolutions. This approach has led to good results on simple, well-structured data sets containing a specific class of objects (e.g. faces, birds, or flowers) at the image center.</p><p>Once images and textual descriptions become more complex, e.g. by containing more than one object and having a large variety in backgrounds and scenery settings, the image quality drops drastically. This is likely because, until recently, almost all approaches only condition on an embedding of the complete textual description, without paying attention to individual objects. Recent approaches have started to tackle this by either relying on specific scene layouts <ref type="bibr" target="#b1">[2]</ref> or by explicitly focusing on individual objects <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. In this work, we extend this approach by additionally focusing specifically on salient objects within the generated image. However, generating complex scenes containing multiple objects from a variety of classes is still a challenging problem.</p><p>The most commonly used evaluation metrics for GANs, the Inception Score (IS) <ref type="bibr" target="#b4">[5]</ref> and the Fr√©chet Inception Distance (FID) <ref type="bibr" target="#b5">[6]</ref>, are not designed to evaluate images that contain multiple objects and depict complex scenes. In fact, both of these metrics depend on an image classifier (the Inception-Net), which is pre-trained on ImageNet, a data set whose images almost always contain only a single object at the image center. They also do not evaluate the consistency between image description and generated image and, therefore, can not evaluate whether a model generates images that actually depict what is described in the caption. Even evaluation metrics specifically designed for text-to-image synthesis evaluation such as the R-precision <ref type="bibr" target="#b6">[7]</ref> often fail to evaluate more detailed aspects of an image, such as the quality of individual objects. As such, our contributions are twofold: first, we introduce a novel GAN architecture called OP-GAN that focuses specifically on individual objects while simultaneously generating a background that fits with the overall image description. Our approach relies on an object pathway similar to <ref type="bibr" target="#b2">[3]</ref>, which iteratively attends to all objects that need to be generated given the current image description. In parallel, a global pathway generates the background features which later on get merged with the object features. Second, we introduce an evaluation metric specifically for text-to-image synthesis tasks which we call Semantic Object Accuracy (SOA). In contrast to most current evaluation metrics, our metric focuses on individual objects and parts of an image and also takes the caption into consideration when evaluating an image. Image descriptions often explicitly or implicitly mention what kind of objects are seen in an image, e.g. an image described by the caption "a person holding a cell phone" should depict both a person and a cell phone. To evaluate this, we sample all image captions from the COCO validation set that explicitly mention one of the 80 main object categories (e.e. "person", "dog", "car", etc.) and use them to generate images. We then use a pre-trained object detector <ref type="bibr" target="#b7">[8]</ref> and check whether it detects the explicitly mentioned objects within the generated images. We perform a user study over several current text-to-image models and show that SOA is highly compatible with human evaluation whereas other metrics, such as the Inception Score, are not.</p><p>We evaluate several variations of our proposed model as well as several state-of-the-art approaches that provide pre-arXiv:1910.13321v2 [cs.CV] 2 Jun 2020 trained models. Our results show that current architectures are not able to generate images that contain objects of the same quality as the original images. While some models already achieve results close to or better than real images on scores such as the IS and R-precision, none of the models comes close to generating images that achieve SOA scores close to the real images. However, our results and user study also show that models that attend to individual objects in one way or another tend to perform better than models, which only focus on global image semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Modern architectures are able to synthesize realistic, highresolution images of many domains. In order to generate images of high resolution many GAN <ref type="bibr" target="#b0">[1]</ref> architectures use multiple discriminators at various resolutions <ref type="bibr" target="#b8">[9]</ref>. Additionally, most GAN architectures use some form of attention for improved image synthesis <ref type="bibr" target="#b6">[7]</ref> as well as matching aware discriminators <ref type="bibr" target="#b9">[10]</ref> which identify whether images correspond to a given textual description.</p><p>Originally, most GAN approaches for text-to-image synthesis encoded the textual description into a single vector which was used as a condition in a conditional GAN (cGAN) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. However, this faces limitations when the image content becomes more complex as e.g. in the COCO data set <ref type="bibr" target="#b10">[11]</ref>. As a result, many approaches now use attention mechanisms to attend to specific words of the sentence <ref type="bibr" target="#b6">[7]</ref>, use intermediate representations such as scene layouts <ref type="bibr" target="#b1">[2]</ref>, condition on additional information such as object bounding boxes <ref type="bibr" target="#b2">[3]</ref> or perform interactive image refinement <ref type="bibr" target="#b11">[12]</ref>. Other approaches generate images directly from semantic layouts without additional textual input <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>or perform a translation from text to images and back <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Direct Text-to-Image Synthesis Approaches that do not use intermediate representations such as scene layouts use only the image caption as conditional input. <ref type="bibr" target="#b9">[10]</ref> use a GAN to generate images from captions directly and without any attention mechanism. Captions are embedded and used as conditioning vector and they introduce the widely adopted matching aware discriminator. The matching aware discriminator is trained to distinguish between real and matching caption-image pairs ("real"), real but mismatching caption-image pairs ("fake"), and matching captions with generated images ("fake"). <ref type="bibr" target="#b16">[17]</ref> modify the sampling procedure during training to obtain a curriculum of mismatching caption-image pairs and introduce an auxiliary classifier that specifically predicts the semantic consistency of a given caption-image pair. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref> use multiple generators and discriminators and are one of the first ones to achieve good image quality at resolutions of 256 √ó 256 on complex data sets. <ref type="bibr" target="#b18">[19]</ref> have a similar architecture as <ref type="bibr" target="#b17">[18]</ref> with multiple discriminators but only use one generator while <ref type="bibr" target="#b19">[20]</ref> generate realistic high-resolution images from text with a single discriminator and generator.</p><p>[7] extend <ref type="bibr" target="#b8">[9]</ref> and are the first ones to introduce an attention mechanism to the text-to-image synthesis task with GANs. The attention mechanism attends to specific words in the caption and conditions different image regions on different words to improve the image quality. <ref type="bibr" target="#b20">[21]</ref> extend this and also consider semantics from the text description during the generation process. <ref type="bibr" target="#b21">[22]</ref> introduce a dynamic memory part that selects "bad" parts of the initial image and tries to refine them based on the most relevant words. <ref type="bibr" target="#b22">[23]</ref> refine the attention module by having spatial and channel-wise wordlevel attention and introduce a word-level discriminator to provide fine-grained feedback based on individual words and image regions. <ref type="bibr" target="#b23">[24]</ref> decompose the text-to-image process into three distinct phases by first learning a prior over the text-image space, then sampling from this prior, and lastly using the prior to generate the image.</p><p>Text-to-Image Synthesis with Layouts When using more complex data sets that contain multiple objects per image, generating an image directly becomes difficult. Therefore, many approaches use additional information such as bounding boxes for objects or intermediate representations such as scene graphs or scene layouts which can be generated automatically <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b28">[29]</ref> build on <ref type="bibr" target="#b9">[10]</ref> by additionally conditioning the generator on bounding boxes or keypoints of relevant objects. <ref type="bibr" target="#b29">[30]</ref> decomposition textual descriptions into basic visual primitives to generate images in a compositional manner. <ref type="bibr" target="#b1">[2]</ref> introduce the concept of generating a scene graph based on a caption. This scene graph is then used to generate an image layout and finally the image. Similar to <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b30">[31]</ref> use the caption to infer a scene layout which is used to generate images. <ref type="bibr" target="#b31">[32]</ref> predict convolution kernels conditioned on the semantic layout, making it possible to control the generation process based on semantic information at different locations.</p><p>Given a coarse image layout (bounding boxes and object labels) <ref type="bibr" target="#b32">[33]</ref> generate images by disentangling each object into a specified part (e.g. object label) and unspecified part (appearance). <ref type="bibr" target="#b2">[3]</ref> generate images conditioned on bounding boxes for the individual foreground objects by introducing an object pathway that generates individual objects. <ref type="bibr" target="#b3">[4]</ref> update the grid-based attention mechanism <ref type="bibr" target="#b6">[7]</ref> by combining attention with scene layouts. Additionally, an object discriminator is introduced which focuses on individual objects and provides feedback whether the object is at the right location. <ref type="bibr" target="#b33">[34]</ref> refine the grid-based attention mechanism between word phrases and specific image regions of various sizes based on an initial set of bounding boxes. <ref type="bibr" target="#b34">[35]</ref> introduce a new feature normalization method and fine-grained mask maps to generate visually different images from a given layout. <ref type="bibr" target="#b35">[36]</ref> generate images from scene graphs and allow the model to crop objects from other images to paste them into the generated image. <ref type="bibr" target="#b36">[37]</ref> generate a visual-relation scene layout based on the caption. For this, they introduce a dedicated module which generates bounding boxes for objects at a given caption in order to condition the network during the image generation process.</p><p>Semantic Image Manipulation Finally, there are methods that allow humans to directly describe the image in an iterative process or that allow for direct semantic manipulation of images. <ref type="bibr" target="#b11">[12]</ref> condition generation process on a dialogue describing the image instead of a single caption. <ref type="bibr" target="#b37">[38]</ref> facilitate semantic image manipulation by allowing users to modify image layouts which are then used to generate images. <ref type="bibr" target="#b38">[39]</ref> allow users to input object instance masks into an existing image represented by a semantic layout. <ref type="bibr" target="#b39">[40]</ref> generate images iteratively from consecutive textual commands, <ref type="bibr" target="#b40">[41]</ref> provide interactive image editing based on a current image and instructions on how to update the image, and <ref type="bibr" target="#b41">[42]</ref> generate individual images for a sequence of sentences. <ref type="bibr" target="#b42">[43]</ref> do interactive image generation but do not use text as direct input but instead update a scene graph from text over the course of the interaction. <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, and <ref type="bibr" target="#b45">[46]</ref> modify visual attributes of individual objects in an image while leaving text irrelevant parts of the image unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>A traditional generative adversarial network (GAN) <ref type="bibr" target="#b0">[1]</ref> consists of two networks: a generator G which generates new data points from randomly sampled inputs, and a discriminator D which tries to distinguish between generated and real data samples. In conditional GANs (cGANs) <ref type="bibr" target="#b46">[47]</ref> both the discriminator and the generator are conditioned on additional information, e.g. a class label or textual information. This has been shown to improve performance and leads to more control over the data generating process. For a conventional cGAN with generator G, discriminator D, condition c (e.g. a class label), data point x, and a randomly sampled noise vector z the training objective V is:</p><formula xml:id="formula_0">min G max D V (D, G) = E (x,c)‚àºp data [log D(x, c)]+ E (z)‚àºpz,(c)‚àºp data [log(1 ‚àí D(G(z, c), c))].<label>(1)</label></formula><p>We use the AttnGAN <ref type="bibr" target="#b6">[7]</ref> as our baseline architecture and add our object-centric modifications to it. The AttnGAN is a conditional GAN for text-to-image synthesis that uses attention and a novel additional loss to improve the quality of the generated images. It consists of a generator and three discriminators as shown in the top row of <ref type="figure" target="#fig_1">Figure 1</ref>. Attention is used such that different words of the caption have more or less influence on different regions of the image. This means that, for example, the word "sky" has more influence on the generation of the top half of the image than the word "grass" even if both words are present in the image caption.</p><p>[7] also introduce the Deep Attentional Multimodal Similarity Model (DAMSM) which computes the similarity between images and captions. This DAMSM is used during training to provide additional, fine-grained feedback to the generator about how well the generated image matches its caption. We adapt the AttnGAN architecture with multiple object pathways which are learned end-to-end in both the discriminator and the generator, see B and C in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>These object pathways are conditioned on individual object labels (e.g. "person", "car", etc.) and the same object pathway is applied multiple times at a given image resolution at different locations and for different objects. This is similar to the approach introduced by <ref type="bibr" target="#b2">[3]</ref>. However, <ref type="bibr" target="#b2">[3]</ref> only use one object pathway in the generator at a small resolution and only one discriminator was equipped with an object pathway. In our approach, the generator contains three object pathways at various resolutions (16 √ó 16, 64 √ó 64, and 128 √ó 128) to further refine object features at higher resolutions and each of our three discriminators is equipped with its own object pathway, see D in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>For a given image caption œï we have several objects which are associated with this caption and which we represent with one-hot vectors œÉ i , i = 1...n (e.g. œÉ 0 = person, œÉ 1 = car, etc.). Each object pathway at a given resolution is applied iteratively for each of the objects œÉ i . The location is determined by a bounding box describing the object's location and size. Each object pathway starts with an "empty" zero-tensor œÅ and the features that are generated (generator) or extracted (discriminator) are added onto œÅ at the location of the specific object's bounding box. After the object pathway has processed each object, œÅ contains features at each object location and is zero everywhere else.</p><p>For the generator, we first concatenate the image caption's embedding œï, the one-hot label œÉ i , and a randomly sampled noise vector z. We use this concatenated vector to obtain the final conditioning label Œπ i for the current object œÉ i :</p><formula xml:id="formula_1">Œπ i = F(œï, z, œÉ i ),<label>(2)</label></formula><p>where F is a fully connected layer followed by a non-linearity (A in <ref type="figure" target="#fig_1">Figure 1</ref>). The generator's first object pathway (B.2 in <ref type="figure" target="#fig_1">Figure 1</ref>) takes this conditioning label Œπ i and uses it to generate features for the given object at a spatial resolution of 16√ó16. The features are then transformed onto œÅ into the location of the respective bounding box with a spatial transformer network (STN) <ref type="bibr" target="#b47">[48]</ref>. This procedure is repeated for each object œÉ i associated with the given caption œï.</p><p>The global pathway in the first generator also gets the locations and labels Œπ i for the individual objects. It spatially replicates these labels at the locations of the respective bounding boxes and then applies convolutional layers to the resulting layout to obtain a layout encoding (B.1 in <ref type="figure" target="#fig_1">Figure 1</ref>). This layout encoding, the image caption œï, and the noise vector z are used to generate coarse features for the image at a low resolution.</p><p>At higher levels in the generator, the object pathways are conditioned on the object features of the current object and the one-hot label œÉ i for that object (C.2 in <ref type="figure" target="#fig_1">Figure 1</ref>). For this, we again use an STN to extract the features at the bounding box location of the object œÉ i and resize the features to a spatial resolution of 16 √ó 16 (second object pathway) or 32 √ó 32 (third object pathway). We obtain a conditioning label in the same manner as for the first object pathway (Equation 2), replicate it spatially to the same dimension as the extracted object features, and concatenate it with the object features along the channel axis. Following this, we apply multiple convolutional layers and upsampling to update the features of the given object. Finally, as in the first object pathway, we use an STN to transform the features into the bounding box location and add them onto œÅ. The global pathway in the higher layers (C.1 in <ref type="figure" target="#fig_1">Figure 1</ref>) stays unchanged from the baseline architecture <ref type="bibr" target="#b6">[7]</ref>.</p><p>Our final loss function for the generator is the same as in the original AttnGAN and consists of an unconditional, a conditional, and a caption-image matching part. The unconditional loss is</p><formula xml:id="formula_2">L uncon G = ‚àíE (x)‚àºp G [log D(x))],<label>(3)</label></formula><p>the conditional loss is</p><formula xml:id="formula_3">L con G = ‚àíE (x)‚àºp G ,(c)‚àºp data [log D(x, c))],<label>(4)</label></formula><p>and the caption-image matching loss is which measures text-image similarity at the word level and is calculated with the pre-trained models provided by <ref type="bibr" target="#b6">[7]</ref>. The complete loss for the generator then is:</p><formula xml:id="formula_4">L DAMSM G = ‚àíE (x)‚àºp G ,(c)‚àºp data [log D(x, c))],<label>(5)</label></formula><formula xml:id="formula_5">L G = L uncon G + L con G + ŒªL DAMSM G ,<label>(6)</label></formula><p>where we set Œª = 50 as in the original implementation. As in our baseline architecture, we employ three discriminators at three spatial resolutions: 64 √ó 64, 128 √ó 128, and 256√ó256. Each discriminator possesses a global and an object pathway which extract features in parallel (D in <ref type="figure" target="#fig_1">Figure 1</ref>). In the object pathway we use an STN to extract the features of object œÉ i and concatenate them with the one-hot vector œÉ i describing the object. The object pathway then applies multiple convolutional layers before adding the extracted features onto œÅ at the location of the bounding box.</p><p>The global pathway in each of the discriminators works on the full input image and applies convolutional layers with stride two to decrease the spatial resolution (D.1). Once the spatial resolution reaches that of the tensor œÅ we concatenate the two tensors (full image features and object features œÅ) along the channel axis and use convolutional layers with stride two to further reduce the spatial dimension until we reach a resolution of 4 √ó 4.</p><p>We calculate both a conditional (image and image caption) and an unconditional (only image) loss for each of the discriminators. The conditional input c during training consists of the image caption embedding œï and the information about objects œÉ i (bounding boxes and object labels) associated with the image x, i.e. c = {œï, œÉ i }. In the unconditional case the discriminators are trained to classify images as real or generated without any influence of the image caption by minimizing the following loss:</p><formula xml:id="formula_6">L uncon Di = ‚àíE (x)‚àºp data [log D(x)] ‚àí E (x)‚àºp G [log(1 ‚àí D(x))].<label>(7)</label></formula><p>In order to optimize the conditional loss we concatenate the extracted features with the image caption embedding œï along the channel axis and minimize</p><formula xml:id="formula_7">L con Di = ‚àíE (x,c)‚àºp data [log D(x, c)] ‚àíE (x)‚àºp G ,(c)‚àºp data [log(1 ‚àí D(x, c))].<label>(8)</label></formula><p>for each discriminator. Finally, to specifically train the discriminators to check for caption-image consistency we use the matching aware discriminator loss <ref type="bibr" target="#b9">[10]</ref> with mismatching caption-image pairs and minimize</p><formula xml:id="formula_8">L cls Di = ‚àíE (x,œÉ)‚àºp data ,(œï)‚àºp data [log(1 ‚àí D(x, c))],<label>(9)</label></formula><p>where image x and caption œï are sampled individually and randomly from the data distribution and are, therefore, unlikely to align. We introduce an additional loss term similar to the matching aware discriminator loss V cls (D) which works on individual objects. Instead of using mismatching imagecaption pairs, we use correct image-caption pairs, but with incorrect bounding boxes and minimize:</p><formula xml:id="formula_9">L obj Di = ‚àíE (x,œï)‚àºp data ,(œÉ)‚àºp data [log(1 ‚àí D(x, c))].<label>(10)</label></formula><p>Thus, the complete objective we minimize for each individual discriminator is:</p><formula xml:id="formula_10">L Di = L uncon Di + L con Di + L cls Di + L obj Di .<label>(11)</label></formula><p>We leave all other training parameters as in the original implementation <ref type="bibr" target="#b6">[7]</ref> and the training procedure itself also stays the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION OF TEXT-TO-IMAGE MODELS</head><p>Quantitatively evaluating generative models is difficult <ref type="bibr" target="#b48">[49]</ref>. While there are several evaluation metrics that are commonly used to evaluate GANs, many of them have known weaknesses and are not designed specifically for text-to-image synthesis tasks. In the following, we first discuss some of the common evaluation metrics for GANs, their weaknesses, and why they might be inadequate for evaluating text-to-image synthesis models. Following this, we introduce our novel evaluation metric, Semantic Object Accuracy (SOA), and describe how it can be used to evaluate text-to-image models in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current Evaluation Metrics</head><p>Inception Score and Fr√©chet Inception Distance Most GAN approaches are trained on relatively simple images which only contain one object at the center (e.g. ImageNet, CelebA, etc). These methods are evaluated with metrics such as the Inception Score (IS) <ref type="bibr" target="#b4">[5]</ref> and Fr√©chet Inception Distance (FID) <ref type="bibr" target="#b5">[6]</ref>, which use an Inception-Net usually pre-trained on ImageNet. The IS evaluates roughly how distinctive an object in each image is (i.e. ideally the classification layer of the Inception-Net has small entropy) and how many different objects the GAN generates overall (i.e. high entropy in the output of different images). The FID measures how similar generated images are to a control set of images, usually the validation set by calculating the distance in feature space between generated and real images. Consequently, the IS should be as high as possible, while the FID should be as small as possible. Both evaluation metrics have known weaknesses <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. For example, the IS does not measure the similarity between objects of the same class, so a network that only generates one "perfect" sample for each class can achieve a very good IS despite showing an intra-class mode dropping behavior. Li et al. <ref type="bibr" target="#b3">[4]</ref> also note that the IS overfits within the context of text-to-image synthesis and can be "gamed" by increasing the batch size at the end of the training. Furthermore, the IS uses the output of the classification layer of an Inception-Net pre-trained on the ImageNet data set. This might not be the best approach for a more complex data set in which each image contains multiple objects at distinct locations throughout the image, as opposed to the ImageNet data set which consists of images usually depicting one object in the image center. <ref type="figure" target="#fig_2">Figure 2</ref> shows some exemplary failure cases of the IS on images sampled from the COCO data set. Examples when IS fails for COCO images. The top row shows images for which the Inception-Net has very high entropy in its output layer, possibly because the images contain more than one object and are often not centered. The second row shows images containing different objects and scenes which were nonetheless all assigned to the same class by the Inception-Net, thereby negatively affecting the overall predicted diversity in the images.</p><p>The FID relies on representative ground truth data to compare the generated data against and also assumes that features are of Gaussian distribution, which is often not the case. For more complex data sets the FID also still suffers from the problem that the image statistics are obtained with a network pre-trained on ImageNet which might not be a representative data set. Finally, neither the IS nor the FID take the image caption into account during their evaluation.</p><p>VS similarity and R-precision <ref type="bibr" target="#b18">[19]</ref> introduce the visualsemantic similarity (VS similarity) metric which measures the distance between a generated image and its caption. Two models are trained to embed images and captions respectively and then minimize the cosine distance between embeddings of matching image-caption pairs while maximizing the cosine distance between mismatching imagecaption pairs. A good model then achieves high VS similarity between a generated image and its associated caption.</p><p>[7] use the R-precision metric to evaluate how well an image matches a given description or caption. The Rprecision score is similar to VS similarity, but instead of scoring the VS similarity between a given image and caption it instead performs a ranking of the similarity between the real caption and randomly sampled captions for a given generated image. For this, first, an image is generated conditioned on a given caption. Then, another 99 captions are chosen randomly from the data set. Both the generated images and the 100 captions are then encoded with the respective image and text encoder. Similar to VS similarity the cosine distance between the image embedding and each caption embedding is used as proxy for the similarity between the given image and caption. The 100 captions are then ordered in descending similarity and the top k (usually k=1) most similar captions are used to calculate the R-precision. Intuitively, R-precision calculates if the real caption is more similar to the generated image (in feature space) than 99 randomly sampled captions.</p><p>The drawback of both metrics is that they do not evaluate the quality of individual objects. For example the real caption could state that "a person stands on a snowy hill" while the 99 random captions do not mention "snow" (which usually covers most of the background in the generated image) or "person" (but e.g. giraffe, car, bedroom, etc). In this case, an image with only white background (snow) would already make the real caption rank very highly in the R-precision metric and achieve a high VS similarity. See <ref type="figure" target="#fig_3">Figure 3</ref> for a visualization of this. As such, this metric does not focus on the quality of individual objects but rather concentrates on global background and salient features.</p><p>Classification Accuracy Score <ref type="bibr" target="#b51">[52]</ref> introduce the Classification Accuracy Score (CAS) to evaluate conditional image generation models, similar to <ref type="bibr" target="#b52">[53]</ref>. For this, a classifier is trained on images generated by the conditional generative model. The classifier's performance is then evaluated on the original test set of the data set that was used to train the generative model. If the classifier achieves high accuracy on the test set, this indicates that the data it was trained on is representative of the real distribution. The authors find that neither the IS, the FID, nor combinations thereof are predictive of the CAS, further indicating that the IS and FID are only of limited use for evaluating image quality.</p><p>Caption Generation <ref type="bibr" target="#b30">[31]</ref> suggest evaluating text-toimage models by comparing original captions with captions obtained from generated images. The intuition is that if the generated image is relevant to its caption, then it should be possible to infer the original text from it. To this end, <ref type="bibr" target="#b30">[31]</ref> use a pre-trained caption generator <ref type="bibr" target="#b57">[57]</ref> to generate captions for each synthesized image and compare these to the original ones through standard language similarity metrics, i.e. BLEU, METEOR, and CIDEr. Except for CIDEr, these metrics were originally developed to evaluate machine translation and text summarization methods and were only later adopted for the evaluation of image captions.</p><p>One challenge with this caption generation approach is that often many different captions are valid for a given image. Even if two captions are not similar, this does not necessarily imply that they do not describe the same image <ref type="bibr" target="#b53">[54]</ref>. Furthermore, it has been shown that metrics such as BLEU, METEOR, and CIDEr are primarily sensitive to ngram overlap which is neither necessary nor sufficient for two sentences to convey the same meaning <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> and do also not necessarily correlate with human judgments of captions <ref type="bibr" target="#b57">[57]</ref>, <ref type="bibr" target="#b58">[58]</ref>. Finally, there is no requirement that captions, either real or generated, need to focus on specific objects. Instead, captions can also describe the general layout of a given scene (e.g. a busy street with lots of traffic) without explicitly mentioning specific objects. Some of these limitations might potentially be overcome in the future by novel image caption evaluation metrics that focus more on objects and semantic content in the scene <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b59">[59]</ref>.</p><p>Other Approaches In contrast to the IS, which measures the diversity of a whole set of images, the diversity score <ref type="bibr" target="#b32">[33]</ref> measures the perceptual difference between a pair of images in feature space. This metric can be useful when images are generated from conditional inputs (e.g. labels or scene layouts) to examine whether a model can generate diverse outputs for a given condition. However, the metric does not convey anything directly about the quality of the generated images or their congruence with any conditional information. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b60">[60]</ref>, <ref type="bibr" target="#b61">[61]</ref> run a semantic segmentation network on generated images and compare the predicted segmentation mask to the ground truth segmentation mask used as input for the model. However, this metric needs a ground truth semantic segmentation mask and does not provide information about specific objects within the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Object Accuracy (SOA)</head><p>So far, most evaluation metrics are designed to evaluate the holistic image quality but do not evaluate individual areas or objects within an image. Furthermore, except for Caption Generation and R-precision, none of the scores take the image caption into account when evaluating generated images. To address the challenges and issues mentioned above we introduce a novel evaluation metric based on a pre-trained object detection network. <ref type="bibr" target="#b0">1</ref> The pre-trained object detector evaluates images by checking if it recognizes objects that the image should contain based on the caption. For example, if the image caption is "a person is eating a pizza" we can infer that the image should contain both a person and a pizza and the object detector should be able to recognize both objects within the image. Since this evaluation measures directly whether objects specifically mentioned in the caption are recognizable in an image we call this metric Semantic Object Accuracy (SOA). Some previous works have used similar approaches to evaluate the quality of the generated images. <ref type="bibr" target="#b2">[3]</ref> evaluate how often expected objects (based on the caption) are detected by an object detector. However, only a subset of the captions is evaluated and the evaluated captions contain false positives (e.g. captions containing the phrase "hot dog" are evaluated based on the assumption that the image should contain a dog). <ref type="bibr" target="#b14">[15]</ref> introduce a detection score that calculates (roughly) whether a pre-trained object detector detects an object in a generated image with high certainty. However, no information from the caption is taken into account, meaning any detection with high confidence is "good" even if the detected object does not make sense in the context of the caption. <ref type="bibr" target="#b62">[62]</ref> use a pre-trained object detector to calculate the mean average precision and report precision-recall curves. However, the evaluation is done on synthetic data sets and without textual information as conditional input. <ref type="bibr" target="#b32">[33]</ref> use classification accuracy as an evaluation metric in which they report the object classification accuracy in generated images. For this, they use a ResNet-101 model which is trained on real objects cropped and resized from the original data. However, in order to calculate the score, the size and location of each object in the generated image must be known, so this evaluation is not directly applicable to approaches that do not use scene layouts or similar representations. <ref type="bibr" target="#b36">[37]</ref> use recall and intersection-over-union (IoU) to evaluate the bounding boxes in their generated scene layout but do not apply these evaluations to generated images directly.</p><p>SOA Since we work with the COCO data set we filter all captions in the validation set for specific keywords that are related to the available labels for objects (e.g. person, car, zebra, etc). For each of the 80 available labels in the COCO data set we find all captions that imply the existence of the respective object and generate three images for each of the captions. The supplementary material gives a detailed overview of how exactly the captions were chosen for each label. We then run the YOLOv3 network <ref type="bibr" target="#b7">[8]</ref> pre-trained on the COCO data set on each of the generated images and check whether it recognizes the given object. We report the recall as a class average (SOA-C), i.e. in how many images per class the YOLOv3 on average detects the given object, and as an image average (SOA-I), i.e. on average in how many images a desired object was detected. Specifically, the SOA-C is calculated as</p><formula xml:id="formula_11">SOA-C = 1 |C| c‚ààC 1 |I c | ic‚ààIc YOLOv3(i c ),<label>(12)</label></formula><p>for object classes c ‚àà C and images i ‚àà I c that are supposed to contain an object of class c. The SOA-I is calculated as</p><formula xml:id="formula_12">SOA-I = 1 c‚ààC |I c | c‚ààC ic‚ààIc YOLOv3(i c ),<label>(13)</label></formula><p>and</p><p>YOLOv3(i c ) = 1 if YOLOv3 detected an object of class c 0 otherwise .</p><p>(14) Since many images can also contain objects that are not specifically mentioned (for example an image described by "lots of cars are on the street" could still contain persons, dogs, etc) in the caption we do not calculate a false negative rate but instead only focus on the recall, i.e. the true positives.</p><p>SOA-Intersection over Union Several approaches (e.g. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b36">[37]</ref>) use additional conditioning information such as scene layouts or bounding boxes. For these approaches, our evaluation metric can also calculate the intersection over union (IoU) between the location at which different objects should be and locations at which they are detected, which we call SOA-IoU. To calculate the IoU we use every image in which the YOLOv3 network detected the respective object. Since many images contain multiple instances of a given object we calculate the IoU between each predicted bounding box for the given object and each ground truth bounding box. The final IoU for a given image and object is then the maximum of the values, i.e. the reported IoU is an upper bound on the actual IoU.</p><p>Overall this approach allows a more fine-grained evaluation of the image content since we can now focus on individual objects and their features. To get a better idea of the overall performance of a model we calculate both the class average recall/IoU (SOA-C/SOA-IoU-C) and image average recall/IoU (SOA-I/SOA-IoU-I). Additionally, we report the SOA-C for the forty most and least common labels (SOA-C-Top40 and SOA-C-Bot40) to see how well the model can generate objects of common and less common classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We perform multiple experiments and ablation studies. In a first step, we add the object pathway (OP) on multiple layers of the generator and to each discriminator and call this model OPv2. We also train this model with the additional bounding box loss we introduced in section 3. When the model is trained with the additional bounding box loss we refer to it as BBL.</p><p>Different approaches differ in how many objects per image are used during training. If an image layout is used, typically all objects (foreground and background) are used as conditioning information. Other approaches limit the number of objects during per training <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. To examine the effect of training with different numbers of objects per image we train our approach with either a maximum of three objects per image (standard) or with up to ten objects per image, which we refer to as many objects (MO). When training with a maximum of three objects per image we sample randomly from the training set at train time, i.e. each batch contains images which contain zero to three objects. If an image contains more than three objects we choose the three largest ones in terms of area of the bounding box. When training with up to ten objects per image we slightly change our sampling strategy so that each batch consists of images that contain the same amount of objects. This means that, e.g., each image in a batch contains exactly four objects, while in the next batch each image might contain exactly seven objects. This increases the training efficiency as most of the images contain fewer than five objects.</p><p>As a result of the different settings we perform the following experiments: 1) OPv2: apply the object pathway (OP) on multiple layers of the generator and on all discriminators, training without the bounding box loss and with a maximum of three objects per image. 2) OPv2 + BBL: same as OPv2 but with the bounding box loss added to the discriminator loss term. 3) OPv2 + MO: same as OPv2 but with a maximum of ten objects per image. 4) OPv2 + BBL + MO (OP-GAN): combination of all three approaches.</p><p>We train each model three times on the 2014 split of the COCO data set. At test time we use bounding boxes generated by a network <ref type="bibr" target="#b3">[4]</ref> as the conditioning information. Therefore, except for the image caption no other ground truth information is used at test time. <ref type="table">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref> give an overview of our results for the COCO data set. The first half of the table shows the results on the original images from the data set and from related literature while the second half shows our results. To make a direct comparison we calculated the IS, FID, CIDEr, 1 Inception Score (IS), Fr√©chet Inception Distance (FID), R-precision, Caption Generation with CIDEr, and Semantic Object Accuracy on Class (SOA-C) and Image Average (SOA-I) on the MS-COCO data set. Results of our models are obtained with generated bounding boxes. Scores for models marked with ‚Ä† were calculated with a pre-trained model provided by the respective authors. and R-precision scores ourselves for all models which are provided by the authors. As such, the values from AttnGAN <ref type="bibr" target="#b6">[7]</ref>, AttnGAN+OP <ref type="bibr" target="#b2">[3]</ref>, Obj-GAN <ref type="bibr" target="#b3">[4]</ref>, and DM-GAN <ref type="bibr" target="#b21">[22]</ref> are the ones most directly comparable to our reported values since they were calculated in the same way. Note that there is some inconsistency in how the FID is calculated in prior works. Some approaches, e.g. <ref type="bibr" target="#b3">[4]</ref>, compare the statistics of the generated images only with the statistics of the respective "original" images (i.e. images corresponding to the captions that were used to generate a given image). We, on the other hand, generate 30,000 images from 30,000 randomly sampled captions and compare their statistics with the statistics of the full validation set. Many of the recent publications also do not report the FID or R-precision. This makes a direct comparison difficult as we show that the IS is likely the least meaningful score of the three since it easily overfits <ref type="bibr" target="#b3">[4]</ref> and due to the reasons mentioned in section 4. We calculate each of the reported values of our models three times for each trained model (nine times in total) and report the average and standard deviation. To calculate the SOA scores we generate three images for each caption in the given class, except for the "person" class, for which we randomly sample 30,000 captions (from over 60,000) and generate one image for each of the 30,000 captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION AND ANALYSIS</head><formula xml:id="formula_13">Model IS ‚Üë FID ‚Üì R-precision (k=1) ‚Üë CIDEr ‚Üë SOA-C ‚Üë SOA-I ‚Üë Original<label>Images</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head><p>Overall Results As <ref type="table">Table 1</ref> shows, all our models outperform the baseline AttnGAN in all metrics. The IS is improved by 16 ‚àí 19%, the R-precision by 6 ‚àí 7%, the SOA-C by 28 ‚àí 33%, the SOA-I by 22 ‚àí 25%, the FID by 20 ‚àí 25%, and CIDEr by 15 ‚àí 18%. This was achieved by adding our object pathways to the baseline model without any further tuning of the architecture, hyperparameters, or the training procedure. Our approach also outperforms all other approaches based on FID, SOA-C, and SOA-I. While there are two approaches that report a IS higher than our models, it has previously been observed that this score is likely the least meaningful for this task and can be gamed to achieve higher numbers <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Our user study also shows that the IS is the score that has the least predictive value for human evaluation.</p><p>We also calculated each score using the original images of the COCO data set. For the IS we sampled three times 30,000 images from the validation set and resized them to 256 √ó 256 pixels. These images were also used to calculate the CIDEr score. To calculate the FID we randomly sampled three times 30,000 images from the training set and compared them to the statistics of the validation set. The R-precision was calculated on three times 30,000 randomly sampled images and the corresponding caption from the validation set and the SOA-C and SOA-I were calculated on the real images corresponding to the originally chosen captions.</p><p>As we can see, the IS is close to the current state of the art models with a value of 34.88. It is possible to achieve a much higher IS on other, simpler data sets, e.g. IS &gt; 100 on the ImageNet data set <ref type="bibr" target="#b63">[63]</ref>. This indicates that the IS is indeed not a good evaluation metric, especially for complex images consisting of multiple objects and various locations. The difference between the R-precision on real and generated images is even larger. On the original images, the R-precision score is only 68.58, which is much worse than what current models can achieve (&gt; 88).</p><p>One reason for this might be that the R-precision calculates the cosine similarity between an image embedding and a caption embedding and measures how often the caption that was used to generate an image is more similar than 99 other, randomly sampled captions. However, the same encoders that are used to calculate the R-precision are also used during training to minimize the cosine similarity between an image and the caption it was generated from. As a result, the model might already overfit to this metric through the training procedure. Our observation is that the models tend to heavily focus on the background to make it match a specific word in the caption (e.g. images tend to be very white when the caption mentions "snow" or "ski", very blue when the caption mentions "surf" or "beach", very green when the caption mentions "grass" or "savanna", etc.) This matching might lead to a high R-precision score since it leads, on average, to a large cosine similarity. Real images do not always reflect this, since a large part of the image might be occupied by a person or an animal, essentially "blocking out" the background information. We see a similar trend for the CIDEr evaluation where many models achieve a score similar to the score reached by real images. Regardless of what the actual reason is, the question remains whether evaluation metrics like the IS, R-precision, and CIDEr are meaning-and helpful when models that can not (as of now) generate images that would be confused as "real" achieve scores comparable to or better than real images.</p><p>The FID and the SOA values are the only two evaluation metrics (that we used) for which none of the current state of the art models can come close to the values obtained with the original images. The FID is still much smaller on the real data (6.09) compared to what current models can achieve (&gt; 24 for the best models). While the FID still uses a network pretrained on ImageNet it compares activations of convolutional layers for different images and is, therefore, likely still more meaningful and less dependent on specific object settings than the IS. Similarly, the SOA-C (SOA-I) on real data is 74.97 (80.84), while current models achieve values of around 30‚àí36 <ref type="bibr">(40‚àí50)</ref>. Since the network used to calculate the SOA values is not part of the training loop the models can not easily overfit to this evaluation metric like they can for the R-precision. Furthermore, the results of the SOA evaluation confirm the impression that none of the models is able to generate images with multiple distinct objects of a quality similar to real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of the Object Pathway</head><p>To get a clearer understanding of how the evaluation metrics might be impacted by the object pathway we calculate our scores for a different number of generated objects. More specifically, we only apply the object pathway for a maximum given number of objects (0, 1, 3, or 10) per image. Intuitively, we would assume that without the application of the object pathway the IS and FID should be decreased, since the object pathway is not used to generate any object features and the images should, therefore, consist mostly of background. Additionally, we can get an intuition of how important the object pathway is for the overall performance of the network by looking at how it affects the R-precision, SOA-C, and SOA-I.</p><p>As <ref type="table">Table 1</ref> shows, all models perform markedly worse when the object pathway is not used (0 obj). We find that the models trained with up to ten objects per image seem to rely more heavily on the object pathway than models trained with three objects per image. For models trained with only three objects per image (OPv2 and OPv2 + BBL) the IS decreases by around 1 ‚àí 2, the R-precision decreases by around 4 ‚àí 5, the SOA-C (SOA-I) decreases by around 7 ‚àí 9 (11 ‚àí 14), CIDEr decreases by around 6‚àí8%, and the FID increases by around 4‚àí7. On the other hand, models trained with up to 10 objects suffer much more when the object pathway is removed, with the IS decreasing by around 3 ‚àí 6, the R-precision decreasing by around 9 ‚àí 15, the SOA-C (SOA-I) decreasing by around 12 ‚àí 18 (17 ‚àí 28), CIDEr decreasing by around 16 ‚àí 30%, and the FID increasing by around 10 ‚àí 20. These results indicate that the object pathways are an important part of the model and are responsible for at least some of the improvements compared to the baseline architecture.</p><p>Impact of Bounding Box Loss Adding the bounding box loss to the object pathways has a small negative effect on all scores, but does slightly improve the IoU scores (see <ref type="table" target="#tab_2">Table 2</ref>). Note that the weighting of the bounding box loss in the overall loss term was not optimized but simply weighted with the same strength as the matching aware discriminator loss L cls D . It is possible that the positive effect of the bounding box loss could be increased by weighting it differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Training on Many Objects</head><p>Training the model with up to ten objects per image has only minor effects on the IS and SOA scores, but improves the FID and R-precision. However, we observe that the models trained with only three objects per image slightly decrease in their performance once the object pathway is applied multiple times. Usually, the models trained on only three objects achieve their best performance when applying the object pathway three times as at training time. Once the model is trained on up to ten objects though, we do not observe this behavior anymore and instead achieve comparable or even better results when applying the object pathway ten times per image.</p><p>SOA Scores <ref type="table" target="#tab_2">Table 2</ref> shows the results for the SOA and SOA-IoU. The SOA-I values are consistently higher than the SOA-C values. Since the SOA-I is calculated on image average (instead of class average like the SOA-C) it is skewed by objects that often occur in captions and images (e.g. persons, cats, dogs, etc.). The SOA values for the most and least common 40 objects show that the models perform much better on the more common objects. Actually, most models perform about two times better on the common objects showing their problem in generating objects that are not often observed during training. For a detailed overview of how each model performed on the individual labels please refer to the supplementary material.</p><p>When we look at the IoU scores we see that the Obj-GAN <ref type="bibr" target="#b3">[4]</ref> achieves by far the best IoU scores (around 0.5), albeit at the cost of lower SOA scores. Our models usually achieve an IoU of around 0.2 ‚àí 0.3 on average. Training with up to ten objects per image and using the bounding box loss slightly increases the IoU. However, similar to previous work <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> we find that the AttnGAN architecture tends to place salient object features at many locations of the image which affects the IoU scores negatively.</p><p>When looking at the SOA for individual objects (see <ref type="figure" target="#fig_6">Figure 5</ref>) we find that there are objects for which we can achieve very high SOA values (e.g. person, cat, dog, zebra, pizza, etc.). Interestingly, we find that all tested methods perform "good" or "bad" at the same objects. We found that objects need to have three characteristics to achieve a high SOA and the highest SOA scores are achieved when objects possess all three characteristics. The first important characteristic is easily predictable: the higher the occurrence of an object in the training data, the better (on average) the final performance on this object. Secondly, large objects, i.e. objects that usually cover a large part of the image (e.g. bus or elephant), are usually modeled better than objects that are usually small (spoon or baseball glove). The final and more subtle characteristic is the surface texture of an object. Objects with highly distinct surface textures (e.g. zebra, giraffe, pizza, etc.) achieve high SOA scores because the object detection network relies on these textures to detect objects. However, while the models are able to correctly match the surface texture (e.g. black and white stripes for a zebra) they are still not capable of generating a realisticlooking shape of many objects. As a result, many of these objects possess the "correct" surface texture but their shape is more a general "blob" consisting of the texture and not a distinct form (e.g. a snout and for legs for a zebra). See <ref type="figure">Figure 6</ref> for a visualization of this. This is one of the weaknesses of the SOA score as it might give the wrong impression that an 80% object detection rate means in 80% of the cases the object is recognizable and of real-world quality. This is not the case, as the SOA scores are calculated with a pre-trained object detector which might focus more on texture and less on shapes of objects <ref type="bibr" target="#b64">[64]</ref>. Consequently, the results of the SOA are more aptly interpreted as cases where a model was able to generate features that an independently pre-trained object detector would classify as a given object. The overall quality of the metric is, therefore, strongly dependent on the object detector   <ref type="figure">6</ref>. Generated images and objects recognized by the pre-trained object detector (YOLOv3) which was used to calculate the SOA scores. The results highlight that, like most other CNN based object detectors, YOLOv3 focuses much more on texture and less on actual shapes. and future improvements in this area might also lead to more meaningful interpretations of the SOA scores. <ref type="figure" target="#fig_4">Figure 4</ref> shows images generated by our different models. All images shown in this paper were generated without ground truth bounding boxes but instead use generated bounding boxes <ref type="bibr" target="#b3">[4]</ref>. The first column shows the respective image from the data set, while the next four columns show the generated images. We can see that all models are capable of generating recognizable foreground objects. It is often difficult to find qualitative differences in the images generated by the different models. However, we find that the models using the bounding box loss usually improve the generation of rare objects. Training with ten objects per image usually leads to a slightly better image quality overall, especially for images that contain many objects.</p><p>As we saw in the quantitative evaluation, the object pathway can have a large impact on the image quality. <ref type="figure" target="#fig_8">Figure 7</ref> shows what happens when (some of) the object pathways are not used in the full model (OPv2 + BBL + MO).</p><p>Again, the first column shows the original image from the data set and the second column shows images generated without the use any of the object pathways. The next three columns show generated images when we consecutively use the object pathways, starting with the lowest object pathway and iteratively adding the next object pathway until we reach the full model. When no object pathway is used (first column) we clearly see that only background information is generated. Once the first object pathway is added we also get foreground objects and their quality gets slightly better <ref type="bibr">TABLE 3</ref> Human evaluation results (ratio of 1st by human ranking) of five models on the MS-COCO data set given a caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AttnGAN-OP [3]</head><p>14.65% ¬± 0.35 AttnGAN <ref type="bibr" target="#b6">[7]</ref> 16.80% ¬± 0.43 Obj-GAN <ref type="bibr" target="#b3">[4]</ref> 20.96% ¬± 0.33 DM-GAN <ref type="bibr" target="#b21">[22]</ref> 22.42% ¬± 0.41 OP-GAN (ours) <ref type="bibr" target="#b24">25</ref>.17% ¬± 0.43 by adding the higher-level object pathways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Study</head><p>In order to further validate our results, we performed a user study on Amazon Mechanical Turk. Similar to other approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b30">[31]</ref> we sampled 5,000 random captions from the COCO validation set. For each caption, we generated one image with each of the following models: our OP-GAN, the AttnGAN <ref type="bibr" target="#b6">[7]</ref>, the AttnGAN-OP <ref type="bibr" target="#b2">[3]</ref>, the Obj-GAN <ref type="bibr" target="#b3">[4]</ref>, and the DM-GAN <ref type="bibr" target="#b21">[22]</ref>. We showed each user a given caption and the respective five images from the models in random order and asked them to choose the image that depicts the given caption best. We evaluated each image caption twice, for a total of 10,000 evaluations with the help of 200 participants. <ref type="table">Table 3</ref> shows how often each model was chosen as having produced the best image given a caption (variance was estimated by bootstrap <ref type="bibr" target="#b65">[65]</ref>). This evaluation reveals that the human ranking closely reflects the ranking obtained through the SOA and FID scores. One notable exception are the two worst performing models (AttnGAN and AttnGAN-OP), which we measure to perform similar according to the SOA and FID scores, but obtain different results in the user study. We find that the IS score is not predictive of the performance in the user study. The R-precision and CIDEr are somewhat predictive, but predict a different ranking of the top-three performing models. Overall, we find that our OP-GAN performs best according to both the SOA scores and the human evaluation. As hypothesized in section 4 we also observe that the FID and SOA scores are the best predictors for a model's performance in a human user evaluation.  <ref type="figure" target="#fig_9">Figure 8</ref> shows examples of images generated by our model (OPv2 + BBL + MO) and those generated by several other models <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b21">[22]</ref>. We observe that our model often generates images with foreground objects that are more recognizable than the ones generated by other models. For more common objects (e.g. person, bus or plane) all models manage to generate features that resemble the object but in most cases do not generate a coherent representation from these features and instead distribute them throughout the image. As a result, we notice features that are associated with an object but not necessarily form one distinct and coherent appearance of that object. Our model, on the other hand, is often able to generate one (or multiple) coherent object(s) from the features, see e.g. the generated images containing a bus, cattle, or the plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head><p>When generating rare objects (e.g. cake or hot dog) we observe that our model generates a much more distinct object than the other models. Indeed, most models fail completely to generate rare objects and instead only generate colors associated with these objects. Finally, when we inspect more complex scenes we see that our model is also capable of generating multiple diverse objects within an image. As opposed to the other images for "room showing a sink and some drawers" we can recognize a sink-like shape and drawers in the image generated by our model. Similarly, our model can also generate an image containing a reasonable shape of a banana and a cup of coffee, whereas the other models only seem to generate the texture of a banana without the shape and completely ignore the cup of coffee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we introduced a novel GAN architecture (OP-GAN) that specifically models individual objects based on some textual image description. This is achieved by adding object pathways to both the generator and discriminator which learn features for individual objects at different resolutions and scales. Our experiments show that this consistently improves the baseline architecture based on quantitative and qualitative evaluations.</p><p>We also introduce a novel evaluation metric named Semantic Object Accuracy (SOA) which evaluates how well a model can generate individual objects in images. This new SOA evaluation allows to evaluate text-to-image synthesis models in more detail and to detect failure and success modes for individual objects and object classes. A user study with 200 participants shows that the SOA score is consistent with the ranking obtained by human evaluation, whereas other scores such as the Inceptions Score are not. Evaluation of several state-of-the-art approaches using SOA shows that no current approach is able to generate realistic foreground objects for the 80 classes in the COCO data set. While some models achieve high accuracy for several of the most common objects, all of them fail when it comes to modeling rare objects or objects that do not have an easily recognizable surface structure. However, using the SOA as an evaluation metric on text-to-image models provides more detailed information about how well they perform for different object classes or image captions and is well aligned with human evaluation.  <ref type="table" target="#tab_3">Table 4</ref> gives a detailed overview of how we chose the captions for each label to calculate the Semantic Object Accuracy (SOA) scores. The second column shows how many captions we found in total for the given label. The third column shows which words we filtered the captions for to obtain captions for the given label. This means that we chose all captions that contained at least one of those words as a valid caption for the given label. In the fourth column we show (were applicable) which words were explicitly excluded when looking for captions for the given label. Finally, the last column shows some examples of "false positives", i.e. captions that are included in the set of captions for the given label even though they do not necessarily explicitly ask for the presence of the given label as understood by humans. Code to use the SOA can be found here: https://github.com/tohinz/semanticobject-accuracy-for-generative-text-to-image-synthesis. <ref type="figure">Figure 9</ref> shows generated images with the ground truth bounding boxes (red) provided as input to the model and the bounding boxes detected by YOLO (blue). When the Intersection over Union (IoU) is small (right column) we observe that this is usually due to the fact that the generated object is much larger than the originally provided bounding box. This agrees with our hypothesis that the reason for the relatively small IoU numbers for our model is because it tends to put salient object features even at locations outside of the provided bounding box. Note that our model rarely generates the desired object at a location completely different from the provided bounding box. Rather, it tends to increase the object's size, especially when the provided bounding box is small. However, we can also see that most objects are not clearly recognizable to humans even though they are "correctly" detected by the YOLO network. This is in line with our observation that YOLO, like many other CNNs, tend to be focused on textural cues much more than on shapes. As a result, future improvements in object detection models can also help increase the information provided by our SOA score. <ref type="table" target="#tab_4">Table 5</ref> shows our model's architecture. More details and the code can be found here: https: //github.com/tohinz/semantic-object-accuracy-forgenerative-text-to-image-synthesis. We train our model on four NVIDIA GeForce GTX 1080Ti GPUs. Training one model takes between two and four weeks, depending on the exact setting. <ref type="table" target="#tab_5">Table 6</ref> and <ref type="table" target="#tab_6">Table 7</ref> show the detailed results of the YOLOv3 detection network on the individual labels for all models.  <ref type="figure">Fig. 9</ref>. Examples of our model and YOLOv3 predictions on the generated images. The bounding boxes in red are the bounding boxes provided to the network at test time for the given objects. The blue bounding boxes are the bounding boxes provided by YOLOv3 for the given object. When the Intersection over Union (IoU) is small (right column) we observe that this is usually due to the fact that the generated object is much larger than the originally provided bounding box. Only in few cases is the generated object at a completely different location than the provided red bounding box.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INFORMATION ABOUT CAPTIONS FOR SOA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INSPECTION OF YOLO PREDICTIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FURTHER RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>‚Ä¢</head><label></label><figDesc>The authors are with the Knowledge Technology Group, University of Hamburg, Germany, Email: {hinz, heinrich, wermter}@informatik.unihamburg.de.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of our model architecture called OP-GAN. The top row shows a high-level summary of our architecture, while the bottom two rows show details of the individual generators and discriminators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples when IS fails for COCO images. The top row shows images for which the Inception-Net has very high entropy in its output layer, possibly because the images contain more than one object and are often not centered. The second row shows images containing different objects and scenes which were nonetheless all assigned to the same class by the Inception-Net, thereby negatively affecting the overall predicted diversity in the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Examples when R-precision fails for COCO images. The top row shows images from the COCO data set. The middle row shows the correct caption and the bottom row gives examples for characteristics of captions that are rated as being more similar than the original caption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of images generated by different variations of our models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For example, all models perform reasonably well on objects such as person and pizza (many examples in the training set) as well as e.g. plane and traffic light (few examples in the training set). Conversely, all models fail on objects such as table and skateboard (many examples in the training set) as well as e.g. hair drier and toaster (few examples in the training set).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of SOA scores: SOA per class with degree of a bin reflecting relative frequency of that class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.</head><label></label><figDesc>Fig. 6. Generated images and objects recognized by the pre-trained object detector (YOLOv3) which was used to calculate the SOA scores. The results highlight that, like most other CNN based object detectors, YOLOv3 focuses much more on texture and less on actual shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison of images generated by our model (OP-GAN) with OPs switched on and off.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison of images generated by our model (OP-GAN) with images generated by other current models. Stefan Wermter is Full Professor at the University of Hamburg, Germany, and Director of the Knowledge Technology Research Group. His main research interests are in the fields of neural networks, hybrid knowledge technology, neuroscience-inspired computing, cognitive robotics, and human-robot interaction. He has been associate editor of the journal 'Transactions on Neural Networks and Learning Systems', is associate editor of 'Connection Science' and 'International Journal for Hybrid Intelligent Systems', and is on the editorial board of the journals 'Cognitive Systems Research', 'Cognitive Computation' and 'Journal of Computational Intelligence'. Currently, he serves as co-coordinator of the international collaborative research centre on Crossmodal Learning (TRR-169) and is the coordinator of the European Training Network SECURE on safety for cognitive robots. He is the elected President for the European Neural Network Society for 2020-2022.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Sink 0.196 0.131 0.172 0.106 0.206 0.090 0.195 0.113 Stop Sign 0.584 0.279 0.453 0.280 0.494 0.237 0.449 0.270 Banana 0.464 0.274 0.517 0.289 0.426 0.280 0.504 0.284 Monitor 0.510 0.209 0.502 0.225 0.535 0.222 0.581 0.225 Hotdog 0.481 0.297 0.443 0.305 0.478 0.311 0.576 0.322 Skis 0.021 0.111 0.037 0.121 0.042 0.115 0.039 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Comparison of the recall values for the different models. We used generated bounding boxes to calculate the values. Numbers in brackets show scores when the object pathway was not used at test time.<ref type="bibr" target="#b24">25</ref>.88 / ‚àí‚àí 39.01 / ‚àí‚àí 37.47 / ‚àí‚àí 14.29 / ‚àí‚àí AttnGAN + OP [3] 25.46 / 0.236 40.48 / 0.311 39.77 / 0.308 11.15 / 0.164 Obj-GAN [4] 27.14 / 0.513 41.24 / 0.598 39.88 / 0.587 14.40 / 0.438 OPv2 33.82 (26.04) / 0.207 48.39 (37.56) / 0.270 48.34 (36.53) / 0.260 19.31 (15.55) / 0.152 OPv2 + BBL 33.19 (24.00) / 0.210 48.24 (34.01) / 0.270 47.96 (32.96) / 0.261 18.43 (15.04) / 0.159 OPv2 + MO 33.46 (21.15) / 0.214 47.93 (30.24) / 0.275 47.84 (28.15) / 0.264 19.07 (14.15) / 0.163 OPv2 + BBL + MO 34.51 (16.55) / 0.217 48.90 (22.76) / 0.278 49.70 (22.19) / 0.269 19.32 (10.91) / 0.165</figDesc><table><row><cell>Model</cell><cell>SOA-C / IoU</cell><cell>SOA-I / IoU</cell><cell>SOA-C-Top40 / IoU</cell><cell>SOA-C-Bot40 / IoU</cell></row><row><cell>Original Images</cell><cell>74.97 / 0.550</cell><cell>80.84 / 0.570</cell><cell>78.77 / 0.546</cell><cell>71.18 / 0.554</cell></row><row><cell>AttnGAN [7]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DM-GAN [22]</cell><cell>33.44 / ‚àí‚àí</cell><cell>48.03 / ‚àí‚àí</cell><cell>47.73 / ‚àí‚àí</cell><cell>19.15 / ‚àí‚àí</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 :</head><label>4</label><figDesc>Words that were used to identify given labels in the image caption for the YOLOv3 object detection test (plural of each word also included, different forms of spelling also included).</figDesc><table><row><cell>Label</cell><cell cols="2"># Sent. Words in Captions</cell><cell>Excluded Strings</cell><cell>False Positives</cell></row><row><cell>Person</cell><cell>61586</cell><cell>person, people, human, man, men, woman, women, child, children</cell><cell></cell><cell>A sign advertising an eatery in which people can eat burgers.</cell></row><row><cell>Dining Table</cell><cell>7678</cell><cell>table, desk</cell><cell></cell><cell>A sweet dish is kept in a bowl on a table mat.</cell></row><row><cell>Cat</cell><cell>6609</cell><cell>cat, kitten</cell><cell></cell><cell>A double parking meter decorated with cat art</cell></row><row><cell>Dog</cell><cell>5614</cell><cell>dog, pup</cell><cell>hot dog, hotdog, hot-dog, cheese dog, chili dog, corn dog</cell><cell>Two stuffed dogs under a blanket looking at a picture book.</cell></row><row><cell>Train</cell><cell>5397</cell><cell>train</cell><cell></cell><cell>A red train engine sits on the tracks</cell></row><row><cell>Bus</cell><cell>4027</cell><cell>bus</cell><cell></cell><cell>The sign is pointing the direction of the bus route.</cell></row><row><cell>Clock</cell><cell>3870</cell><cell>clock</cell><cell></cell><cell></cell></row><row><cell>Giraffe</cell><cell>3866</cell><cell>giraffe</cell><cell></cell><cell>A woman standing in front of a giraffe pen</cell></row><row><cell>Pizza</cell><cell>3655</cell><cell>pizza</cell><cell></cell><cell>Would you prefer fresh basil on your pizza or sans basil?</cell></row><row><cell>Horse</cell><cell>3615</cell><cell>horse</cell><cell></cell><cell>A close-up of a man hating the horses face.</cell></row><row><cell>Elephant</cell><cell>3133</cell><cell>elephant</cell><cell>toy elephant, stuffed elephant</cell><cell>Outdoor art display of elephant sculptures of various colorings.</cell></row><row><cell>Zebra</cell><cell>3070</cell><cell>zebra</cell><cell></cell><cell>An animal that is part horse and part zebra by another horse.</cell></row><row><cell>Bed</cell><cell>2923</cell><cell>bed</cell><cell></cell><cell>A large truck has a flat bed trailer attached</cell></row><row><cell>Boat</cell><cell>2819</cell><cell>boat, ship</cell><cell></cell><cell>a upside down boat is on top of a big hil</cell></row><row><cell>Toilet</cell><cell>2796</cell><cell>toilet</cell><cell></cell><cell>You can pick either toilet stall in this clean restroom.</cell></row><row><cell>Bird</cell><cell>2691</cell><cell>bird</cell><cell></cell><cell>a clock with a painting of a bird on a branch on it</cell></row><row><cell>Skateboard</cell><cell>2665</cell><cell>skateboard</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>train car, car window,</cell><cell></cell></row><row><cell>Car</cell><cell>2650</cell><cell>car, auto</cell><cell>side car, passenger car, subway car, car tire, rail car, tram car,</cell><cell>A museum sign showing the main entrance and car park</cell></row><row><cell></cell><cell></cell><cell></cell><cell>street car, trolly car</cell><cell></cell></row><row><cell>Bench</cell><cell>2633</cell><cell>bench</cell><cell></cell><cell></cell></row><row><cell>Laptop</cell><cell>2376</cell><cell>laptop</cell><cell></cell><cell></cell></row><row><cell>Surfboard</cell><cell>2270</cell><cell>surfboard</cell><cell></cell><cell></cell></row><row><cell>Truck</cell><cell>2213</cell><cell>truck</cell><cell></cell><cell></cell></row><row><cell>Umbrella</cell><cell>2107</cell><cell>umbrella</cell><cell></cell><cell>a man playing with a white ball on a red umbrella</cell></row><row><cell>Kite</cell><cell>2025</cell><cell>kite</cell><cell>kite board, kiteboard</cell><cell></cell></row><row><cell>Sports Ball</cell><cell>2001</cell><cell>ball</cell><cell></cell><cell>Female tennis player looks on as she waits for the ball serve</cell></row><row><cell>Cake</cell><cell>2012</cell><cell>cake</cell><cell>cupcake</cell><cell></cell></row><row><cell>Cow</cell><cell>1981</cell><cell>cow</cell><cell></cell><cell>A young boy sitting on top of a cow statue.</cell></row><row><cell>Bicycle</cell><cell>1920</cell><cell>bike, bicycle</cell><cell>motorbike, motor bike, motorcycle, dirt bike</cell><cell>A man drives his bike taxi with luggage in the back.</cell></row><row><cell>Chair</cell><cell>1884</cell><cell>chair</cell><cell></cell><cell></cell></row><row><cell>Frisbee</cell><cell>1775</cell><cell>frisbee</cell><cell></cell><cell></cell></row><row><cell>Bear</cell><cell>1740</cell><cell>bear</cell><cell>teddy bear, stuffed bear, care bear, toy bear</cell><cell>a very old panda bear doll with a handkerchief</cell></row><row><cell>Sandwich</cell><cell>1649</cell><cell>sandwich</cell><cell></cell><cell></cell></row><row><cell>Sheep</cell><cell>1626</cell><cell>sheep</cell><cell></cell><cell>furniture shaped like sheep on a open field</cell></row><row><cell>Vase</cell><cell>1597</cell><cell>vase</cell><cell></cell><cell></cell></row><row><cell>Bowl</cell><cell>1570</cell><cell>bowl</cell><cell>toilet bowl</cell><cell></cell></row><row><cell>Sink</cell><cell>1529</cell><cell>sink</cell><cell></cell><cell></cell></row><row><cell>Stop Sign</cell><cell>1491</cell><cell>stop sign</cell><cell></cell><cell>That sign almost looks like a stop sign with no words on it.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>Overview of the individual layers used in our networks to generate images of resolution 256 √ó 256 pixels. Values in brackets (C, H, W ) represent the tensor's shape. Numbers in the columns after convolutional, residual, or dense layers describe the number of filters / units in that layer. (fs=x, s=y, p=z, BN=B) describes the filter size, stride, padding, and batch norm for that convolutional / residual layer. Everything not specifically mentioned or explained (e.g. RNN-Encoder, DAMSM) is the same as in the AttnGAN(Xu, Tao et al., CVPR, 2018).</figDesc><table><row><cell>Optimizer: Adam</cell><cell>(Œ≤ 1 = 0.5, Œ≤ 2 = 0.999)</cell><cell>Learning Rate</cell><cell>0.0002</cell></row><row><cell></cell><cell>Relu (RL), Leaky</cell><cell>Training Epochs</cell><cell>120</cell></row><row><cell>Activation Functions</cell><cell>RL (LR), Gated</cell><cell>Batch Size</cell><cell>24</cell></row><row><cell></cell><cell>Linear Unit (GLU )</cell><cell>Z-Dim / Img-Caption-Dim</cell><cell>100 / 256</cell></row><row><cell>Attention Mask</cell><cell>see AttnGAN</cell><cell>Layout Encoder</cell><cell></cell></row><row><cell>Upsample Block</cell><cell></cell><cell>Input Shape</cell><cell>(100, 16, 16)</cell></row><row><cell>Upsampling</cell><cell>Nearest Neighbor</cell><cell>Conv (fs=3, s=2, p=1, BN=0)</cell><cell>50, LR</cell></row><row><cell>Conv (fs=3, s=1, p=1, BN=1)</cell><cell>X, GLU</cell><cell>Conv x 2 (fs=3, s=2, p=1, BN=1)</cell><cell>25, LR, 12, LR</cell></row><row><cell>Residual Block</cell><cell></cell><cell>Output Shape</cell><cell>(12, 2, 2)</cell></row><row><cell>Conv x 2 (fs=3, s=1, p=1, BN=1)</cell><cell>X, GLU , X</cell><cell>Discriminator 64 √ó 64</cell><cell></cell></row><row><cell>Add original input to output</cell><cell></cell><cell>Global Pathway</cell><cell></cell></row><row><cell>of previous conv</cell><cell></cell><cell>Input Shape</cell><cell>(3, 64, 64)</cell></row><row><cell>Prepare Label</cell><cell></cell><cell>Conv (fs=4, s=2, p=1, BN=0)</cell><cell>96, LR</cell></row><row><cell>Input Shape (Label œÉ i )</cell><cell>(81, )</cell><cell>Conv (fs=4, s=2, p=1, BN=1)</cell><cell>192, LR</cell></row><row><cell>Dense (BN=1)</cell><cell>100, RL</cell><cell>Output Shape</cell><cell>(192, 16, 16)</cell></row><row><cell>Reshape</cell><cell>(100, 1, 1)</cell><cell>Object Pathway</cell><cell></cell></row><row><cell>Replicate</cell><cell>(100, X, X)</cell><cell>Input Shape</cell><cell>(3, 64, 64)</cell></row><row><cell>Initial Generator</cell><cell></cell><cell>Extract Object Feat w/ STN</cell><cell>(3, 16, 16)</cell></row><row><cell>Global Pathway Input</cell><cell>noise, sentence emb, layout enc</cell><cell>Concatenate with labels œÉ i Conv (fs=4, s=1, p=1)</cell><cell>(84, 16, 16) 192, LR</cell></row><row><cell>Input Shape</cell><cell>(304, )</cell><cell>Transform Object Feat w/ STN</cell><cell>(192, 16, 16)</cell></row><row><cell>Dense (BN=1)</cell><cell>49152, GLU</cell><cell>Output Shape</cell><cell>(192, 16, 16)</cell></row><row><cell>Reshape</cell><cell>(1536, 4, 4)</cell><cell>Concat Pathways</cell><cell>(384, 16, 16)</cell></row><row><cell>Upsample x 2 (fs=3, s=1, p=1)</cell><cell>768, 384</cell><cell cols="2">Conv x 2 (fs=4, s=2, p=1, BN=1) 384, LR, 768, LR</cell></row><row><cell>Object Pathway Input</cell><cell>object labels œÉ i</cell><cell>Concat w/ Sentence Embedding</cell><cell>(1024, 4, 4)</cell></row><row><cell>Prepare Label</cell><cell>(100, 4, 4)</cell><cell>Conv (fs=3, s=1, p=1, BN=1)</cell><cell>768, LR</cell></row><row><cell>Upsample x 2 (fs=3, s=1, p=1)</cell><cell>768, 384</cell><cell>Conv (fs=4, s=4, p=1, BN=1)</cell><cell>1, Sigmoid</cell></row><row><cell>Transform with STN</cell><cell></cell><cell>Discriminator 128 √ó 128</cell><cell></cell></row><row><cell>Concat Pathways</cell><cell>(768, 16, 16)</cell><cell>Global Pathway</cell><cell></cell></row><row><cell>Upsample x 2 (fs=3, s=1, p=1)</cell><cell>192, 96</cell><cell>Input Shape</cell><cell>(3, 128, 128)</cell></row><row><cell>Output Shape</cell><cell>(96, 64, 64)</cell><cell>Conv (fs=4, s=2, p=1, BN=0)</cell><cell>96, LR</cell></row><row><cell>Generator 128 √ó 128</cell><cell></cell><cell>Conv (fs=4, s=2, p=1, BN=1)</cell><cell>192, LR</cell></row><row><cell>Global Pathway Input</cell><cell>(96, 64, 64)</cell><cell>Output Shape</cell><cell>(192, 32, 32)</cell></row><row><cell>Input Shape</cell><cell>(96, 64, 64)</cell><cell>Object Pathway</cell><cell></cell></row><row><cell>Attention Mask</cell><cell>(96, 64, 64)</cell><cell>Input Shape</cell><cell>(3, 128, 128)</cell></row><row><cell>Concatenate</cell><cell>(192, 64, 64)</cell><cell>Extract Object Feat w/ STN</cell><cell>(3, 32, 32)</cell></row><row><cell>Residual x 3</cell><cell>192</cell><cell>Concatenate with labels œÉ i</cell><cell>(84, 32, 32)</cell></row><row><cell>Object Pathway Input</cell><cell>object labels œÉ i prev G output</cell><cell>Conv (fs=4, s=1, p=1) Transform Object Feat w/ STN</cell><cell>192, LR (192, 32, 32)</cell></row><row><cell>Input Shape (Label œÉ i )</cell><cell>(81, ), (96, 64, 64)</cell><cell>Output Shape</cell><cell>(192, 32, 32)</cell></row><row><cell>Prepare Label</cell><cell>(128, 16, 16)</cell><cell>Concat Pathways</cell><cell>(384, 32, 32)</cell></row><row><cell>Extr Obj Feat w/ STN Concatenate</cell><cell>(96, 16, 16) (224, 16, 16)</cell><cell>Conv x 4 (fs=4, s=2, p=1, BN=1)</cell><cell>384, LR, 768, LR 1536, LR, 768, LR</cell></row><row><cell>Upsample x 2 (fs=3, s=1, p=1)</cell><cell>192, 96</cell><cell>Concat w/ Sentence Embedding</cell><cell>(1024, 4, 4)</cell></row><row><cell>Transf Obj Feat w/ STN</cell><cell>(192, 64, 64)</cell><cell>Conv (fs=3, s=1, p=1, BN=1)</cell><cell>768, LR</cell></row><row><cell>Concat Pathways</cell><cell>(288, 64, 64)</cell><cell>Conv (fs=4, s=4, p=1, BN=1)</cell><cell>1, Sigmoid</cell></row><row><cell>Upsample (fs=3, s=1, p=1)</cell><cell>96</cell><cell>Discriminator 256 √ó 256</cell><cell></cell></row><row><cell>Output Shape</cell><cell>(96, 128, 128)</cell><cell>Global Pathway</cell><cell></cell></row><row><cell>Generator 256 √ó 256</cell><cell></cell><cell>Input Shape</cell><cell>(3, 256, 256)</cell></row><row><cell>Global Pathway Input</cell><cell>(96, 128, 128)</cell><cell>Conv (fs=4, s=2, p=1, BN=0)</cell><cell>96, LR</cell></row><row><cell>Input Shape</cell><cell>(96, 128, 128)</cell><cell>Conv (fs=4, s=2, p=1, BN=1)</cell><cell>192, LR</cell></row><row><cell>Attention Mask</cell><cell>(96, 128, 128)</cell><cell>Output Shape</cell><cell>(192, 64, 64)</cell></row><row><cell>Concatenate</cell><cell>(192, 128, 128)</cell><cell>Object Pathway</cell><cell></cell></row><row><cell>Residual x 3</cell><cell>192</cell><cell>Input Shape</cell><cell>(3, 256, 256)</cell></row><row><cell>Object Pathway Input</cell><cell>object labels œÉ i prev G output</cell><cell>Extract Object Feat w/ STN Concatenate with labels œÉ i</cell><cell>(3, 64, 64) (84, 64, 64)</cell></row><row><cell>Input Shape (Label œÉ i )</cell><cell>(81, ), (96, 128, 128)</cell><cell>Conv (fs=4, s=1, p=1)</cell><cell>192, LR</cell></row><row><cell>Prepare Label</cell><cell>(128, 32, 32)</cell><cell>Transform Object Feat w/ STN</cell><cell>(192, 64, 64)</cell></row><row><cell>Extr Obj Feat w/ STN</cell><cell>(96, 32, 32)</cell><cell>Output Shape</cell><cell>(192, 64, 64)</cell></row><row><cell>Concatenate</cell><cell>(224, 32, 32)</cell><cell>Concat Pathways</cell><cell>(384, 64, 64)</cell></row><row><cell>Upsample x 2 (fs=3, s=1, p=1)</cell><cell>192, 96</cell><cell></cell><cell>384, LR, 768, LR</cell></row><row><cell>Transf Obj Feat w/ STN</cell><cell>(192, 128, 128)</cell><cell>Conv x 6 (fs=4, s=2, p=1, BN=1)</cell><cell>1536, LR, 3072, LR</cell></row><row><cell>Concat Pathways</cell><cell>(288, 128, 128)</cell><cell></cell><cell>1536, LR 768, LR</cell></row><row><cell>Upsample (fs=3, s=1, p=1)</cell><cell>96</cell><cell>Concat w/ Sentence Embedding</cell><cell>(1024, 4, 4)</cell></row><row><cell>Conv (fs=3, s=1, p=1, BN=1)</cell><cell>3, T anh</cell><cell>Conv (fs=3, s=1, p=1, BN=1)</cell><cell>768, LR</cell></row><row><cell>Output Shape</cell><cell>(3, 256, 256)</cell><cell>Conv (fs=4, s=4, p=1, BN=1)</cell><cell>1, Sigmoid</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6 :</head><label>6</label><figDesc>Results of YOLOv3 detections on generated and original images. Recall provides the fraction of images in which YOLOv3 detected the given object. IoU (Intersection over Union) measures the maximum IoU per image in which the given object was detected. No ground truth information besides the caption was used for all measurements.</figDesc><table><row><cell>Label</cell><cell cols="5">Orig. Img. AttnGAN AttnGAN + OP DM-GAN Recall IoU Recall Recall IoU Recall</cell><cell cols="2">Obj-GAN Recall IoU Recall OP-GAN (Ours) IoU</cell></row><row><cell>Person</cell><cell>0.953 0.624</cell><cell>0.698</cell><cell>0.730</cell><cell>0.357</cell><cell>0.840</cell><cell>0.708 0.640 0.793</cell><cell>0.289</cell></row><row><cell>Dining Table</cell><cell>0.379 0.566</cell><cell>0.104</cell><cell>0.061</cell><cell>0.453</cell><cell>0.094</cell><cell>0.031 0.600 0.157</cell><cell>0.495</cell></row><row><cell>Cat</cell><cell>0.868 0.644</cell><cell>0.734</cell><cell>0.697</cell><cell>0.264</cell><cell>0.790</cell><cell>0.632 0.653 0.656</cell><cell>0.339</cell></row><row><cell>Dog</cell><cell>0.813 0.610</cell><cell>0.651</cell><cell>0.778</cell><cell>0.323</cell><cell>0.764</cell><cell>0.846 0.695 0.850</cell><cell>0.355</cell></row><row><cell>Train</cell><cell>0.826 0.627</cell><cell>0.491</cell><cell cols="2">0.654 0.370</cell><cell>0.463</cell><cell>0.641 0.670 0.561</cell><cell>0.377</cell></row><row><cell>Bus</cell><cell>0.848 0.651</cell><cell>0.615</cell><cell>0.665</cell><cell>0.511</cell><cell>0.766</cell><cell>0.685 0.804 0.793</cell><cell>0.366</cell></row><row><cell>Clock</cell><cell>0.900 0.502</cell><cell>0.469</cell><cell>0.184</cell><cell>0.359</cell><cell>0.528</cell><cell>0.649 0.587 0.587</cell><cell>0.077</cell></row><row><cell>Giraffe</cell><cell>0.949 0.662</cell><cell>0.581</cell><cell>0.725</cell><cell>0.486</cell><cell>0.829</cell><cell>0.679 0.585 0.868</cell><cell>0.368</cell></row><row><cell>Pizza</cell><cell>0.876 0.630</cell><cell>0.793</cell><cell>0.847</cell><cell>0.363</cell><cell>0.883</cell><cell>0.683 0.737 0.893</cell><cell>0.449</cell></row><row><cell>Horse</cell><cell>0.891 0.611</cell><cell>0.650</cell><cell>0.723</cell><cell>0.528</cell><cell>0.827</cell><cell>0.634 0.685 0.827</cell><cell>0.328</cell></row><row><cell>Elephant</cell><cell>0.937 0.647</cell><cell>0.373</cell><cell>0.653</cell><cell>0.522</cell><cell>0.705</cell><cell>0.476 0.737 0.665</cell><cell>0.360</cell></row><row><cell>Zebra</cell><cell>0.915 0.650</cell><cell>0.902</cell><cell>0.882</cell><cell>0.420</cell><cell>0.909</cell><cell>0.921 0.735 0.931</cell><cell>0.407</cell></row><row><cell>Bed</cell><cell>0.732 0.601</cell><cell>0.704</cell><cell>0.661</cell><cell>0.472</cell><cell>0.796</cell><cell>0.655 0.573 0.754</cell><cell>0.444</cell></row><row><cell>Boat</cell><cell>0.736 0.502</cell><cell>0.211</cell><cell>0.284</cell><cell>0.208</cell><cell>0.244</cell><cell>0.136 0.557 0.323</cell><cell>0.198</cell></row><row><cell>Toilet</cell><cell>0.912 0.591</cell><cell>0.281</cell><cell>0.325</cell><cell>0.315</cell><cell>0.178</cell><cell>0.382 0.750 0.543</cell><cell>0.238</cell></row><row><cell>Bird</cell><cell>0.797 0.551</cell><cell>0.358</cell><cell>0.430</cell><cell>0.284</cell><cell>0.637</cell><cell>0.546 0.612 0.554</cell><cell>0.267</cell></row><row><cell>Skateboard</cell><cell>0.822 0.427</cell><cell>0.040</cell><cell>0.119</cell><cell>0.126</cell><cell>0.153</cell><cell>0.164 0.536 0.127</cell><cell>0.116</cell></row><row><cell>Car</cell><cell>0.752 0.488</cell><cell>0.143</cell><cell>0.202</cell><cell>0.124</cell><cell>0.336</cell><cell>0.196 0.430 0.310</cell><cell>0.102</cell></row><row><cell>Bench</cell><cell>0.760 0.547</cell><cell>0.107</cell><cell>0.079</cell><cell>0.311</cell><cell>0.216</cell><cell>0.339 0.637 0.259</cell><cell>0.225</cell></row><row><cell>Laptop</cell><cell>0.876 0.617</cell><cell>0.071</cell><cell>0.252</cell><cell>0.337</cell><cell>0.229</cell><cell>0.027 0.425 0.349</cell><cell>0.323</cell></row><row><cell>Surfboard</cell><cell>0.794 0.414</cell><cell>0.140</cell><cell>0.091</cell><cell>0.218</cell><cell>0.225</cell><cell>0.117 0.548 0.321</cell><cell>0.172</cell></row><row><cell>Truck</cell><cell>0.835 0.631</cell><cell>0.472</cell><cell>0.524</cell><cell>0.442</cell><cell>0.622</cell><cell>0.413 0.685 0.634</cell><cell>0.341</cell></row><row><cell>Umbrella</cell><cell>0.884 0.548</cell><cell>0.074</cell><cell>0.150</cell><cell>0.177</cell><cell>0.292</cell><cell>0.230 0.591 0.381</cell><cell>0.213</cell></row><row><cell>Kite</cell><cell>0.822 0.410</cell><cell>0.291</cell><cell>0.163</cell><cell>0.310</cell><cell>0.302</cell><cell>0.370 0.384 0.414</cell><cell>0.160</cell></row><row><cell>Sports Ball</cell><cell>0.507 0.161</cell><cell>0.112</cell><cell>0.064</cell><cell>0.027</cell><cell>0.295</cell><cell>0.198 0.297 0.165</cell><cell>0.004</cell></row><row><cell>Cake</cell><cell>0.726 0.570</cell><cell>0.471</cell><cell>0.365</cell><cell>0.206</cell><cell>0.385</cell><cell>0.286 0.626 0.423</cell><cell>0.305</cell></row><row><cell>Cow</cell><cell>0.886 0.598</cell><cell>0.425</cell><cell>0.566</cell><cell>0.472</cell><cell>0.649</cell><cell>0.365 0.638 0.614</cell><cell>0.341</cell></row><row><cell>Bicycle</cell><cell>0.686 0.546</cell><cell>0.281</cell><cell>0.251</cell><cell>0.284</cell><cell>0.498</cell><cell>0.249 0.503 0.486</cell><cell>0.297</cell></row><row><cell>Chair</cell><cell>0.717 0.566</cell><cell>0.175</cell><cell>0.142</cell><cell>0.157</cell><cell>0.269</cell><cell>0.070 0.257 0.258</cell><cell>0.130</cell></row><row><cell>Frisbee</cell><cell>0.803 0.350</cell><cell>0.025</cell><cell>0.018</cell><cell>0.050</cell><cell>0.061</cell><cell>0.099 0.625 0.101</cell><cell>0.025</cell></row><row><cell>Bear</cell><cell>0.638 0.637</cell><cell>0.812</cell><cell>0.794</cell><cell>0.431</cell><cell>0.800</cell><cell>0.712 0.761 0.737</cell><cell>0.341</cell></row><row><cell>Sandwich</cell><cell>0.674 0.630</cell><cell>0.505</cell><cell>0.634</cell><cell>0.310</cell><cell>0.508</cell><cell>0.585 0.568 0.667</cell><cell>0.402</cell></row><row><cell>Sheep</cell><cell>0.910 0.593</cell><cell>0.303</cell><cell>0.403</cell><cell>0.239</cell><cell>0.545</cell><cell>0.573 0.642 0.559</cell><cell>0.251</cell></row><row><cell>Vase</cell><cell>0.858 0.600</cell><cell>0.114</cell><cell>0.152</cell><cell>0.468</cell><cell>0.175</cell><cell>0.150 0.306 0.276</cell><cell>0.271</cell></row><row><cell>Bowl</cell><cell>0.675 0.633</cell><cell>0.315</cell><cell>0.113</cell><cell>0.170</cell><cell>0.212</cell><cell>0.066 0.598 0.330</cell><cell>0.216</cell></row><row><cell>Sink</cell><cell>0.712 0.431</cell><cell>0.075</cell><cell>0.128</cell><cell>0.127</cell><cell>0.144</cell><cell>0.165 0.340 0.184</cell><cell>0.102</cell></row><row><cell>Stop Sign</cell><cell>0.874 0.608</cell><cell>0.183</cell><cell>0.225</cell><cell>0.207</cell><cell>0.522</cell><cell>0.510 0.830 0.591</cell><cell>0.292</cell></row><row><cell>Banana</cell><cell>0.788 0.578</cell><cell>0.552</cell><cell cols="2">0.593 0.208</cell><cell>0.433</cell><cell>0.287 0.572 0.444</cell><cell>0.308</cell></row><row><cell>Monitor</cell><cell>0.754 0.594</cell><cell>0.278</cell><cell>0.225</cell><cell>0.477</cell><cell>0.445</cell><cell>0.385 0.759 0.606</cell><cell>0.213</cell></row><row><cell>Skis</cell><cell>0.576 0.315</cell><cell>0.010</cell><cell>0.023</cell><cell>0.057</cell><cell>0.023</cell><cell>0.023 0.512 0.040</cell><cell>0.146</cell></row><row><cell>Hot Dog</cell><cell>0.711 0.621</cell><cell>0.404</cell><cell>0.355</cell><cell>0.227</cell><cell>0.452</cell><cell>0.371 0.671 0.592</cell><cell>0.332</cell></row><row><cell>Fire Hydrant</cell><cell>0.927 0.613</cell><cell>0.414</cell><cell>0.256</cell><cell>0.388</cell><cell>0.420</cell><cell>0.274 0.666 0.426</cell><cell>0.282</cell></row><row><cell>Sofa</cell><cell>0.834 0.584</cell><cell>0.253</cell><cell>0.179</cell><cell>0.259</cell><cell>0.397</cell><cell>0.221 0.470 0.331</cell><cell>0.292</cell></row><row><cell>Teddy Bear</cell><cell>0.806 0.643</cell><cell>0.637</cell><cell cols="2">0.688 0.336</cell><cell>0.615</cell><cell>0.410 0.707 0.455</cell><cell>0.328</cell></row><row><cell>Aeroplane</cell><cell>0.916 0.575</cell><cell>0.612</cell><cell>0.382</cell><cell>0.211</cell><cell>0.571</cell><cell>0.297 0.513 0.665</cell><cell>0.318</cell></row><row><cell>Tie</cell><cell>0.800 0.574</cell><cell>0.138</cell><cell cols="2">0.074 0.157</cell><cell>0.095</cell><cell>0.113 0.385 0.117</cell><cell>0.117</cell></row><row><cell cols="2">Tennis Racket 0.830 0.432</cell><cell>0.019</cell><cell>0.044</cell><cell>0.071</cell><cell>0.048</cell><cell>0.141 0.518 0.058</cell><cell>0.093</cell></row><row><cell>Cell Phone</cell><cell>0.590 0.513</cell><cell>0.036</cell><cell>0.054</cell><cell>0.067</cell><cell>0.105</cell><cell>0.134 0.563 0.264</cell><cell>0.201</cell></row><row><cell>Refrigerator</cell><cell>0.881 0.631</cell><cell>0.593</cell><cell>0.252</cell><cell>0.408</cell><cell>0.456</cell><cell>0.409 0.518 0.558</cell><cell>0.375</cell></row><row><cell>Cup</cell><cell>0.706 0.586</cell><cell>0.061</cell><cell>0.054</cell><cell>0.022</cell><cell>0.131</cell><cell>0.040 0.430 0.067</cell><cell>0.137</cell></row><row><cell>Broccoli</cell><cell>0.756 0.575</cell><cell>0.130</cell><cell>0.137</cell><cell>0.240</cell><cell>0.255</cell><cell>0.248 0.601 0.528</cell><cell>0.267</cell></row><row><cell>Donut</cell><cell>0.854 0.655</cell><cell>0.076</cell><cell>0.089</cell><cell>0.213</cell><cell>0.138</cell><cell>0.207 0.712 0.304</cell><cell>0.297</cell></row><row><cell>Bottle</cell><cell>0.782 0.590</cell><cell>0.072</cell><cell>0.047</cell><cell>0.020</cell><cell>0.148</cell><cell>0.027 0.002 0.069</cell><cell>0.053</cell></row><row><cell>Suitcase</cell><cell>0.851 0.612</cell><cell>0.049</cell><cell>0.043</cell><cell>0.407</cell><cell>0.070</cell><cell>0.118 0.662 0.122</cell><cell>0.318</cell></row><row><cell>Snowboard</cell><cell>0.746 0.411</cell><cell>0.055</cell><cell>0.030</cell><cell>0.085</cell><cell>0.101</cell><cell>0.080 0.356 0.073</cell><cell>0.114</cell></row><row><cell>Book</cell><cell>0.628 0.500</cell><cell>0.006</cell><cell>0.032</cell><cell>0.340</cell><cell>0.064</cell><cell>0.007 0.390 0.051</cell><cell>0.184</cell></row><row><cell>Remote</cell><cell>0.619 0.440</cell><cell>0.014</cell><cell>0.015</cell><cell>0.120</cell><cell>0.123</cell><cell>0.044 0.488 0.038</cell><cell>0.133</cell></row><row><cell>Traffic Light</cell><cell>0.942 0.450</cell><cell>0.607</cell><cell>0.565</cell><cell>0.409</cell><cell>0.724</cell><cell>0.619 0.559 0.653</cell><cell>0.215</cell></row><row><cell>Keyboard</cell><cell>0.783 0.495</cell><cell>0.397</cell><cell>0.083</cell><cell>0.064</cell><cell>0.687</cell><cell>0.095 0.701 0.350</cell><cell>0.154</cell></row><row><cell>Apple</cell><cell>0.588 0.593</cell><cell>0.054</cell><cell>0.021</cell><cell>0.162</cell><cell>0.119</cell><cell>0.121 0.709 0.140</cell><cell>0.237</cell></row><row><cell>Oven</cell><cell>0.699 0.606</cell><cell>0.067</cell><cell>0.074</cell><cell>0.520</cell><cell>0.174</cell><cell>0.055 0.338 0.213</cell><cell>0.304</cell></row><row><cell>Motorcycle</cell><cell>0.910 0.597</cell><cell>0.422</cell><cell>0.409</cell><cell>0.396</cell><cell>0.476</cell><cell>0.206 0.528 0.629</cell><cell>0.363</cell></row><row><cell>Carrot</cell><cell>0.590 0.537</cell><cell>0.081</cell><cell>0.045</cell><cell>0.097</cell><cell>0.083</cell><cell>0.044 0.545 0.116</cell><cell>0.153</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7 :</head><label>7</label><figDesc>Results of YOLOv3 detections on ablations of our model. Recall provides the fraction of images in which YOLOv3 detected the given object. IoU (Intersection over Union) measures the maximum IoU per image in which the given object was detected. No ground truth information besides the caption was used for all measurements.</figDesc><table><row><cell>Label</cell><cell cols="2">OPv2 Recall IoU Recall IoU Recall IoU Recall OPv2 + BBL OPv2 + MO OPv2 + BBL + MO IoU</cell></row><row><cell>Person</cell><cell>0.783 0.279 0.769 0.278 0.789 0.288 0.771</cell><cell>0.286</cell></row><row><cell>Dining Table</cell><cell>0.095 0.462 0.126 0.453 0.106 0.466 0.106</cell><cell>0.467</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. Code for the evaluation metric and all experiments: https://github.com/tohinz/semantic-object-accuracy-for-generativetext-to-image-synthesis</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors gratefully acknowledge partial support from the German Research Foundation DFG under project CML (TRR 169). We also thank the NVIDIA Corporation for their support through the GPU Grant Program. Stefan Heinrich received his Diplom (German MSc) in computer science and cognitive psychology from the University of Paderborn, and his PhD in Computer Science from the Universit√§t Hamburg, Germany. He is a postdoctoral researcher at the International Research Center for Neurointelligence of the University of Tokyo and previously was appointmened as a postdoctoral research associate in the international collaborative research centre Crossmodal Learning (TRR-169). His research interest is located in between artificial intelligence, cognitive psychology, and computational neuroscience. Here, he aims to explore computational principles in the brain, such as timescales, compositionality, and uncertainty, to foster our fundamental understanding of the brain's mechanisms but also to exploit them in developing machine learning methods for intelligent systems.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0.065 0.000 0.055 0.000 0.085 0.000 0.075 0.000 Baseball Glove 0.041 0.042 0.029 0.210 0.033 0.211 0.022 0.137 Hair Drier 0.002 0.000 0.000 0.000 0.000 0.000 0.002 0.000</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating multiple objects at spatially distinct locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object-driven text-to-image synthesis via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stackgan++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1947" to="1962" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis</title>
		<meeting>Europ. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Chatpainter: Improving text to image generation using dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suhubdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations Workshop</title>
		<meeting>Int. Conf. Learn. Representations Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to generate images of outdoor scenes from attributes and semantic layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00215</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantically invariant text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shringi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ptucha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Image Process</title>
		<meeting>IEEE Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3783" to="3787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mirrorgan: Learning text-toimage generation by redescription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1505" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial learning of semantic relevance in text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3272" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Photographic text-to-image synthesis with a hierarchically-nested adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6199" to="6208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchically-fused generative adversarial network for text to realistic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer and Robot Vis</title>
		<meeting>IEEE Conf. Computer and Robot Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantics disentangling for text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2327" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Controllable text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learn, imagine and create: Text-to-image generation from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="885" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Layoutgan: Generating graphic layouts with wireframe discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Layoutvae: Stochastic scene layout generation from a label set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Seq-sg2sl: Inferring semantic layout from scene graph through sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<title level="m">Generating interpretable images with controllable structure</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Compositional generation of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst. ViGIL</title>
		<meeting>Advances Neural Inf. ess. Syst. ViGIL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7986" to="7994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to predict layoutto-image conditional convolutions for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8584" to="8593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Realistic image generation using region-phrase attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oppermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf</title>
		<meeting>Asian Conf</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image synthesis from reconfigurable layout and style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pastegan: A semi-parametric method to generate image from scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Visual-relation conscious image generation from structured-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01741</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning hierarchical semantic image manipulation through structured representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf</title>
		<meeting>Advances Neural Inf</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2712" to="2722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Contextaware synthesis and placement of object instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf</title>
		<meeting>Advances Neural Inf</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="413" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tell, draw, and repeat: Generating and modifying images based on continual linguistic instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sequential attention gan for interactive image editing via dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08352</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Storygan: A sequential conditional gan for story visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6329" to="6338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interactive image generation using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marwah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations Workshop</title>
		<meeting>Int. Conf. Learn. Representations Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Text-adaptive generative adversarial networks: manipulating images with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf</title>
		<meeting>Advances Neural Inf</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Text guided person image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3663" to="3672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simgan: Photorealistic semantic image manipulation using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Image Process</title>
		<meeting>IEEE Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="734" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pros and cons of gan evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="41" to="65" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. Workshop</title>
		<meeting>Int. Conf. Mach. Learn. Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Classification accuracy score for conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How good is my gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis</title>
		<meeting>Europ. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis</title>
		<meeting>Europ. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Linguistic features for automatic evaluation of heterogenous mt systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gim√©nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>M√†rquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL Workshop on Statistical Machine Translation</title>
		<meeting>of the ACL Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="256" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Vifidel: Evaluating the visual fidelity of image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th</title>
		<meeting>the 57th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6539" to="6550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Show and tell: Lessons learned from the 2015 mscoco image captioning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="652" to="663" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Reevaluating automatic metrics for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kilickaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Europ. Chapter of the Assoc. for Comput. Linguistics</title>
		<meeting>Conf. Europ. Chapter of the Assoc. for Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panagiotou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>D√≠az-Rodr√≠guez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Machine Learning in Real Life</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1520" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Probabilistic neural programmed networks for scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf</title>
		<meeting>Advances Neural Inf</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4028" to="4038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Bootstrap methods: another look at the jackknife</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="569" to="593" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
