<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coherent Semantic Attention for Image Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">Hunan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">Hunan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">Hunan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">Hunan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Coherent Semantic Attention for Image Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The latest deep learning-based approaches have shown promising results for the challenging task of inpainting missing regions of an image. However, the existing methods often generate contents with blurry textures and distorted structures due to the discontinuity of the local pixels. From a semantic-level perspective, the local pixel discontinuity is mainly because these methods ignore the semantic relevance and feature continuity of hole regions. To handle this problem, we investigate the human behavior in repairing pictures and propose a fined deep generative model-based approach with a novel coherent semantic attention (CSA) layer, which can not only preserve contextual structure but also make more effective predictions of missing parts by modeling the semantic relevance between the holes features. The task is divided into rough, refinement as two steps and model each step with a neural network under the U-Net architecture, where the CSA layer is embedded into the encoder of refinement step. To stabilize the network training process and promote the CSA layer to learn more effective parameters, we propose a consistency loss to enforce the both the CSA layer and the corresponding layer of the CSA in decoder to be close to the VGG feature layer of a ground truth image simultaneously. The experiments on CelebA, Places2, and Paris StreetView datasets have validated the effectiveness of our proposed methods in image inpainting tasks and can obtain images with a higher quality as compared with the existing state-of-the-art approaches. The codes and pretrained models will be available at https://github. com/KumapowerLIU/CSA-inpainting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image inpainting is the task to synthesize the missing or damaged parts of a plausible hypothesis, and can be utilized in many applications such as removing unwanted objects, completing occluded regions, restoring damaged or corrupted parts. The core challenge of image inpainting is to maintain global semantic structure and generate realistic texture details for the missing regions.</p><p>Traditional works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">34]</ref> mostly develop texture synthesis techniques to address the problem of hole filling. In <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr">Barnes et al.</ref> propose the Patch-Match algorithm which iteratively searches for the best fitting patches from hole boundaries to synthesize the contents of the missing parts. Wilczkowiak et al. <ref type="bibr" target="#b33">[34]</ref> take further steps and detect desirable search regions to find better match patches. However, these methods fall short of understanding high-level semantics and struggle at reconstructing patterns that are locally unique. In contrast, early deep convolution neural networks based approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref> learn data distribution to capture the semantic information of the image, and can achieve plausible inpainting results. However, these methods fail to effectively utilize contextual information to generate the contents of holes, often leading to the results containing noise patterns.</p><p>Some recent studies effectively utilize the contextual information and obtain better inpainting results. These methods can be divided into two types. The first type <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">42]</ref> utilizes spatial attention which takes surrounding image features as references to restore missing regions. These methods can ensure the semantic consistency of generated content with contextual information. However, they just focus on rectangular shaped holes, and the results always tend to show pixel discontinuous and have semantic chasm (See in <ref type="figure" target="#fig_0">Fig 1(b, c)</ref>). The second type <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41]</ref> is to make the prediction of the missing pixels condition on the valid pixels in the original image. These methods can handle irregular holes properly, but the generated contents still meet problems of semantic fault and boundary artifacts (See in <ref type="figure" target="#fig_0">Fig  1(g, h)</ref>). The reason that the above mentioned methods do not work well is because they ignore the semantic relevance and feature continuity of generated contents, which is crucial for the local pixel continuity.</p><p>In order to achieve better image restoration effect, we investigate the human behavior in inpainting pictures and find that such process involves two steps as conception and painting to guarantee both global structure consistency and local pixel continuity of a picture. To put it more concrete, a man first observes the overall structure of the image and , Shift-net <ref type="bibr" target="#b35">[36]</ref>, Partial Conv <ref type="bibr" target="#b25">[26]</ref>, and Gated Conv <ref type="bibr" target="#b40">[41]</ref>. First line, from left to right are: image with centering mask, Shift-net <ref type="bibr" target="#b35">[36]</ref>, Contextual Attention [42], our model, Ground Truth, respectively. Second line, from left to right are: image with irregular mask, Partial Conv <ref type="bibr" target="#b25">[26]</ref>, Gated Conv <ref type="bibr" target="#b40">[41]</ref>, our model, Ground Truth, respectively. The size of images are 256×256.</p><p>conceives the contents of missing parts during conception process, so that the global structure consistency of the image can be maintained. Then the idea of the contents will be stuffed into the actual image during painting process. In the painting process, one always continues to draw new lines and coloring from the end nodes of the lines drawn previously, which actually ensures the local pixel continuity of the final result.</p><p>Inspired by this process, we propose a coherent semantic attention layer (CSA), which fills in the unknown regions of the image feature maps with the similar process. Initially, each unknown feature patch in the unknown region is initialized with the most similar feature patch in the known regions. Thereafter, they are iteratively optimized by considering the spatial consistency with adjacent patches. Consequently, the global semantic consistency is guaranteed by the first step, and the local feature coherency is maintained by the optimizing step.</p><p>Similar to [42], we divide the image inpainting into two steps. The first step can be constructed by training a rough network to rough out the missing contents. A refinement network with the CSA layer in encoder guides the second step to refine the rough predictions. In order to make network training process more stable and motivate the CSA layer to learn more effective features, we propose a consistency loss to measure not only the distance between the VGG feature layer and the CSA layer but also the distance between the VGG feature layer and the the corresponding layer of the CSA in decoder. Meanwhile, in addition to a patch discriminator <ref type="bibr" target="#b17">[18]</ref>, we improve the details by introducing a feature patch which is simpler in formulation, faster and more stable for training than conventional one <ref type="bibr" target="#b28">[29]</ref>. Except for the consistency loss, reconstruction loss, and relativistic average LS adversarial loss <ref type="bibr" target="#b27">[28]</ref> are incorporated as constraints to instruct our model to learn meaningful parameters.</p><p>We conduct experiments on standard datasets CelebA <ref type="bibr" target="#b26">[27]</ref>, Places2 <ref type="bibr">[44]</ref>, and Paris StreetView <ref type="bibr" target="#b7">[8]</ref>. Both the qualitative and quantitative tests demonstrate that our method can generate higher-quality inpainting results than existing ones. (See in <ref type="figure" target="#fig_0">Fig 1(d, i)</ref>).</p><p>Our contributions are summarized as follows:</p><p>• We propose a novel coherent semantic attention layer to construct the correlation between the deep features of hole regions. No matter whether the unknown region is irregular or centering, our algorithm can achieve state-of-the-art inpainting results.</p><p>• To enhance the performance of the CSA layer and training stability, we introduce the consistency loss to guide the CSA layer and the corresponding decoder layer to learn the VGG features of ground truth. Meanwhile, a feature patch discriminator is designed and jointed to achieve better predictions.</p><p>• Our approach achieves higher-quality results in comparison with <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr">42]</ref> and generates more coherent textures. Even the inpainting task is completed in two stages, our full network can be trained in an end to end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image inpainting</head><p>In the literature, previous image inpainting researches can generally be divided into two categories: Non-learning inpainting approaches and Learning inpainting approaches. The former is traditional diffusion-based or patch-based <ref type="bibr">Figure 2</ref>. The architecture of our model. We add the CSA layer at the resolution of 32×32 in refinement network. methods with low-level features. The latter learns the semantics of image to fulfill the inpainting task and generally trains deep convolutional neural networks to infer the content of the missing regions.</p><p>Non-learning approaches such as <ref type="bibr">[1, 3, 4, 6, 7, 10-13, 19, 21, 23, 31, 35]</ref> fill in missing regions by propagating neighboring information or copying information from similar patch of the background. Huang et al. <ref type="bibr" target="#b15">[16]</ref> blend the known regions into the target regions to minimize discontinuities. However, searching the best matching known regions is a very expensive operation. To address this challenge, Barnes et al. <ref type="bibr" target="#b1">[2]</ref> propose a fast nearest neighbor field algorithm which promotes the development of image inpainting applications. Though the non-learning approaches work well for surface textures synthesis, they can not generate semantically meaningful content, and are not suitable to deal with large missing regions.</p><p>Learning approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr">43]</ref> often use deep learning and GAN strategy to generate pixels of the hole. Context encoders <ref type="bibr" target="#b29">[30]</ref> firstly train deep neural networks for image inpainting task, which takes the adversarial training <ref type="bibr" target="#b13">[14]</ref> into a novel encoder-decoder pipeline and outputs prediction of missing regions. However, it performs poorly in generating fine-detailed textures. Soon after that, Iizuka et al. <ref type="bibr" target="#b16">[17]</ref> extend this work and propose local and global discriminators to improve the inpainting quality. However, it requires post processing steps to enforce the color coherency near the hole boundaries. Yang et al. <ref type="bibr" target="#b36">[37]</ref> take the result from context encoders <ref type="bibr" target="#b29">[30]</ref> as input and gradually increase the texture details to get high-resolution prediction. But this approach significantly increases computational costs due to its optimization process. Liu et al. <ref type="bibr" target="#b25">[26]</ref> update the mask in each layer and re-normalize the convolution weights with the mask value, which ensures that the convolution filters concentrate on the valid information from known regions to handle irregular holes. Yu et al. <ref type="bibr" target="#b40">[41]</ref> further propose to learn the mask automatically with gated convolutions, and combine with SN-PatchGAN discriminator to achieve better predictions. However, these methods do not explicitly consider the correlation between valid features, thus resulting in color inconsistency on completed image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention based image inpainting</head><p>Recently, the spatial attention based on the relationship between contextual and hole regions is often used for image inpainting tasks. Contextual Attention [42] proposes a contextual attention layer which searches for a collection of background patches with the highest similarity to the coarse prediction. Yan et al. <ref type="bibr" target="#b35">[36]</ref> introduce a shift-net powered by a shift operation and a guidance loss. The shift operation speculate the relationship between the contextual regions in the encoder layer and the associated hole region in the decoder layer. Song et al. <ref type="bibr" target="#b31">[32]</ref> introduce a patch-swap layer, which replaces each patch inside the missing regions of a feature map with the most similar patch on the contextual regions, and the feature map is extracted by VGG network. Although <ref type="bibr">[42]</ref> has the spatial propagation layer to encourage spatial coherency by the fusion of attention scores, it fails to model the correlations between patches inside the hole regions, which is also the drawbacks of the other two methods. To this end, we proposed our approach to solve this problem and achieve better results, which is detailed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our model consists of two steps: rough inpainting and refinement inpainting. This architecture helps to stabilize training and enlarge the receptive fields as mentioned in <ref type="bibr">[42]</ref>. The overall framework of our inpainting system is shown in <ref type="figure">Fig 2.</ref> Let I gt be the ground truth images, I in be the input to the rough network, the M and M denote the missing area and the known area in feature maps respectively. We first get the rough prediction I p during the rough inpainting process. Then, the refinement network with CSA layer takes the I p and I in as input pairs to output final result I r . Finally, the patch and feature patch discriminators work together to obtain higher resolution of I r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Rough inpainting</head><p>The input of rough network I in is a 3×256×256 image with center or irregular holes, which is sent to the rough net to output the rough prediction I p . The structure of our rough network is the same as the generative network in <ref type="bibr" target="#b17">[18]</ref>, which is composed of 4×4 convolutions with skip connections to concatenate the features from each layer of encoder and the corresponding layer of decoder. The rough network is trained with the L 1 reconstruction loss explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Refinement inpainting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">refinement network</head><p>We use I p conditioned on I in as input of refinement network that predicts the final result I r . This type of input stacks information of the known areas to urge the network to capture the valid features faster, which is critical for rebuilding the content of hole regions. The refinement network consists of an encoder and a decoder, where skip connection is also adopted similar to rough network. In the encoder, each of the layers is composed of a 3×3 convolution and a 4×4 dilated convolution. The 3×3 convolutions keep the same spatial size while doubling the number of channels. Layers of this size can improve the ability of obtaining deep semantic information. The 4×4 dilated convolutions reduce the spatial size by half and keep the same channel number. The dilated convolutions can enlarge the receptive fields, which can prevent excessive information loss. The CSA layer is embedded in the fourth layer of the encoder. The structure of decoder is symmetrical to the encoder without CSA layer and all 4×4 convolutions are deconvolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Coherent Semantic Attention</head><p>We believe that it is not enough to only consider the relationship between M and M in feature map to reconsturct M similar to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">42]</ref>, because the correlation between generated patches is ignored, which may result in lack of ductility and continuity in the final result.</p><p>To overcome this limitation, we consider the correlation between generated patches and propose the CSA layer. We take the centering hole as an example: the CSA layer is implemented in two phases: Search and Generate. For each (1×1) generated patch m i in M (i ∈ (1 ∼ n), n is the number of patches), the CSA layer searches the closestmatching neural patch m i in known region M to initialize m i during the search process. Then we set the m i as a main reference and the previous generated patch m i−1 as a secondary information to restore m i during the generative process. To measure the relevant degree between these patches, the following cross-correlation metric is adopted:</p><formula xml:id="formula_0">Dmax i = &lt; m i , m i &gt; ||m i ||.||m i || (1) Dad i = &lt; m i , m i−1 &gt; ||m i ||.||m i−1 ||<label>(2)</label></formula><p>where Dad i represents similarity between two adjacent generated patches, Dmax i stands for the similarity between m i and the most similar patch m i in contextual region.</p><p>Since each generated patch includes the contextual and the previous patch information, Dad i and Dmax i are normalized as the weight for the two parts of generated patch. The original patches in M are replaced with generated patches to get a new feature map. We illustrate the process in  Search: We first extract patches in M and reshape them as convolutional filters, then apply the convolution filters on M . With this operation, we can obtain a vector of values denoting the cross-correlation between each patch in M and all patches in M . In the end, for generated patch m i , we initialize it with the most similar contextual patch m i and the maximum cross-correlation value Dmax i is recorded for the next step. Generate: The top left patch is taken as the initial patch for the generative process (marked by m 1 in <ref type="figure" target="#fig_2">Figure 3</ref>). Since the m 1 has no previous patch, the Dad 1 is 0 and we replace the m 1 with m 1 directly, m 1 = m 1 . While the next patch m 2 has a previous patch m 1 as an additional reference, we therefore view the m 1 as a convolution filter to measure the cross-correlation metric Dad 2 between m 1 and m 2 . Finally, the Dad 2 and Dmax 2 are combined and normalized to the compute of new m 2 ,</p><formula xml:id="formula_1">m 2 = Dad2 Dad2+Dmax2 × m 1 + Dmax2 Dad2+Dmax2 × m 2 .</formula><p>As mentioned above, from m 1 to m n , the generative process can be summarized as:</p><formula xml:id="formula_2">m 1 = m 1 , Dad 1 = 0 m i i∈(2∼n) = Dad i Dad i + Dmax i × m (i−1) + Dmax i Dad i + Dmax i × m i<label>(3)</label></formula><p>Since the generate operation is an iterative process, the m i is related to all previous patches(m 1 to m i−1 ) and m i , each generated patch m i can obtain more contextual information in the meanwhile. We get an attention map A i which records the Dmaxi Dadi+Dmaxi and Dadi Dadi+Dmaxi × A i−1 for m i , then A 1 to A n form a attention matrix, finally the extract patches in M are reused as deconvolutional filters to reconstruct M . The process of CSA layer is shown in the Algorithm 1.</p><p>To interpret the CSA layer, we visualize the attention map of a pixel in <ref type="figure" target="#fig_5">Fig 4,</ref> where the red square marks the position of the pixel, the background is our inpainted result, dark red means the attention value is large, while light blue means the attention value is small. Use Eq (2) to calculate the Dad i 9:</p><p>Use Eq (3) to get the attention map A i for m i 10: end for 11: Combine A 1 to A n to get a attention matrix <ref type="bibr">12:</ref> Reuse M as a deconvolutional to get F out 13: End Generate 14: Return F out</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Consistency loss</head><p>Some methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref> use the perceptual loss <ref type="bibr" target="#b19">[20]</ref> to improve the recognition capacity of the network. However, perceptual loss can not directly optimize the convolutional layer, which may mislead the training process of the CSA layer. Moreover, it does not ensure consistency between the feature maps after the CSA layer and the corresponding layer in the decoder.</p><p>We adjust the form of perceptual loss and propose the consistency loss to solve this problem. As shown in   we use an ImageNet-pretrained VGG-16 to extract a high level feature space in the original image. Next, for any location in M , we set the feature space as the target for the CSA layer and the corresponding layer of the CSA in decoder respectively to compute the the L 2 distance. In order to match the shape of the feature maps, we adopt 4 − 3 layer of VGG-16 for our consistency loss. The consistency loss is defined as:</p><formula xml:id="formula_3">L c = y∈M CSA(I ip ) y − Φ n (I gt ) y 2 2 + CSA d (I ip ) y − Φ n (I gt ) y 2 2 (4)</formula><p>Where Φ n is the activation map of the selected layer in VGG-16. CSA(.) denotes the feature after the CSA layer and CSA d (.) is the corresponding feature in the decoder.</p><p>Guidance loss is similar to our consistency loss, proposed in <ref type="bibr" target="#b35">[36]</ref>. They view the ground-truth encoder features of the missing parts as a guide to stabilize training. However, extracting the ground truth features by shift-net is an expensive operation, and the semantic understanding ability of shift-net is not as good as VGG network. Moreover, it cannot optimize the specific convolution layer of the encoder and the decoder simultaneously. In summary, our consistency loss fits our requirements better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Feature Patch Discriminator</head><p>Previous image inpainting networks always use an additional local discriminator to improve results. However, the local discriminator is not suitable for irregular holes which may be with any shapes and at any locations. Motivated by Gated Conv <ref type="bibr" target="#b40">[41]</ref>, Markovian Gans <ref type="bibr" target="#b4">[5]</ref> and SRFeat <ref type="bibr" target="#b28">[29]</ref>, we develop a feature patch discriminator to discriminate completed images and original images by inspecting their feature maps. As shown in <ref type="figure" target="#fig_6">Fig 5,</ref> we use VGG-16 to extract feature map after the pool3 layer, then the feature map is treated as an input for several down-sample layers to capture the feature statistics of Markovain patches <ref type="bibr" target="#b4">[5]</ref>. Finally we directly calculate the adversarial loss in this feature map, since receptive fields of each point in this feature map can still cover the entire input image. Our feature patch discriminator combines the advantages of the conventional feature discriminator <ref type="bibr" target="#b28">[29]</ref> and patch discriminator <ref type="bibr" target="#b17">[18]</ref>, which is not only fast and stable during training but also makes the refinement network synthesize more meaningful highfrequency details.</p><p>In addition to the feature patch discriminator, we use a 70×70 patch discriminator to discriminate I r and I gt images by inspecting their pixel values similar to <ref type="bibr" target="#b28">[29]</ref>. Meanwhile, we use Relativistic Average LS adversarial loss <ref type="bibr" target="#b27">[28]</ref> for our discriminators. This loss can help refinement network benefit from the gradients from both generated data and real data in adversarial training, which is useful for the training stability. The GAN loss term D R for refinement network and the loss function D F for the discriminators are defined as:</p><formula xml:id="formula_4">D R = −E Igt [D(I gt , I r ) 2 ] − E Ir [(1 − D(I r , I gt )) 2 ] (5) D F = −E Igt [(1 − D(I gt , I r )) 2 ] − E Ir [D(I r , I gt ) 2 ] (6)</formula><p>where D stands for the discriminators, E Igt/I f [.] represents the operation of taking average for all real/fake data in the mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Objective</head><p>Following the <ref type="bibr" target="#b35">[36]</ref>, we use L 1 distance as our reconstruction loss to make the constrains that the I p and I r should approximate the ground-truth image:</p><formula xml:id="formula_5">L re = I p − I gt 1 + I r − I gt 1<label>(7)</label></formula><p>Taking consistency, adversarial, and reconstruct losses into account, the overall objective of our refinement network and rough network is defined as:</p><formula xml:id="formula_6">L = λ r L re + λ c L c + λ d D R<label>(8)</label></formula><p>where λ r , λ c , λ d are the tradeoff parameters for the reconstruction, consistency, and adversarial losses, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on three datasets: Places2 <ref type="bibr" target="#b26">[27]</ref>, CelebA <ref type="bibr">[44]</ref>, and Paris StreetView <ref type="bibr" target="#b7">[8]</ref>. We use the original train, test, and validation splits for these three datasets. Data augmentation such as flipping is also adopted during training. Our model is optimized by the Adam algorithm <ref type="bibr" target="#b21">[22]</ref> with a learning rate of 2 × 10 −4 and β 1 = 0.5. The tradeoff parameters are set as λ r =1, λ c =0.01, λ d =0.002. We train on a single NVIDIA 1080TI GPU (11GB) with a batch size of 1. The training of CelebA model, Paris StreetView model, Place2 model have taken 9 days, 5 days and 2 days, respectively.</p><p>We compare our method with four methods: To fairly evaluate, we conduct experiments on both settings of centering and irregular holes. We obtain irregular masks from the work of PC. These masks are classified based on different hole-to-image area ratios (e.g., 0-10(%), 10-20(%), etc.). For centering hole, we compare with CA and SH on image from CelebA <ref type="bibr" target="#b26">[27]</ref> and Places2 [44] validation set. For irregular holes, we compare with PC and GC using Paris StreetView <ref type="bibr" target="#b7">[8]</ref> and CelebA <ref type="bibr" target="#b26">[27]</ref> validation images. All the masks and images for training and testing are with the size of 256×256, and our full model runs at 0.82 seconds per frame on GPU for images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Qualitative Comparison</head><p>For centering mask, as shown in <ref type="figure" target="#fig_5">Fig 6, CA [42]</ref> is effective in semantic inpainting, but the results present distorted structure and confusing color. SH <ref type="bibr" target="#b35">[36]</ref> performances better due to the shift operation and guidance loss, but its predictions are to some extent blurry and detail-missing. For irregular mask, as shown in <ref type="figure" target="#fig_8">Fig 7, PC</ref>  <ref type="bibr" target="#b25">[26]</ref> and GC <ref type="bibr" target="#b40">[41]</ref> can get smooth and plausible result, but the continuities in color and lines do not hold well and some artifacts can still be observed on generated images. This is mainly due to the fact that these methods do not consider the correlations between the deep features in hole regions. In comparison to these competing methods, our model can handle these problems better, and generate visually pleasing results. Moreover, as shown in <ref type="figure" target="#fig_7">Fig 6 and Fig 7 (f, g)</ref>, A 1 and A 2 are attention maps of two adjacent pixels, the first line is the attention maps of left and right adjacent pixels, the second and third line is the attention maps of up and down adjacent pixels. We see that the attention maps of two adjacent pixels are basically the same, and the perceived areas are not limited to the most relevant contextual areas. These phenomena can prove that our approach is better at modeling the coherence of the generated content and enlarging the perception domain for each generated patch than other attention based model <ref type="bibr" target="#b35">[36,</ref><ref type="bibr">42]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative comparisons</head><p>We randomly select 500 images from Celeba validation dataset <ref type="bibr" target="#b26">[27]</ref> and generate irregular and centering holes for each image to make comparisons. Following the CA [42], we use common evaluation metrics, i.e., L1, L2, PSNR, and SSIM to quantify the performance of the models. <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref> list the evaluation results with centering mask and irregular masks respectively. It can be seen that our method outperforms all the other methods on these measurements with irregular mask or centering mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Effect of CSA layer To investigate the effectiveness of CSA, we replace the CSA layer with a conventional 3×3 layer and the contextual attention layer <ref type="bibr">[42]</ref>   <ref type="bibr" target="#b35">[36]</ref>, and Ours. − Lower is better. + Higher is better make a comparison. As shown in <ref type="figure" target="#fig_9">Fig 8(b)</ref>, the mask part fails to restore reasonable content when we use conventional conv. Although contextual attention layer <ref type="bibr">[42]</ref> can improve the performance compared to conventional convolution, the inpainting results still lack fine texture details and the pixels are not consistent with the background(see <ref type="figure" target="#fig_9">Fig 8(c)</ref>). Compared with them, our method performs better  <ref type="table">Table 2</ref>. Comparison results over Celeba with irregular mask between PC <ref type="bibr" target="#b25">[26]</ref>, GC <ref type="bibr" target="#b40">[41]</ref>, and Ours. − Lower is better. + Higher is better (see <ref type="figure" target="#fig_9">Fig 8(d)</ref>). This illustrates the fact that the global semantic structure and local coherency are constructed by the CSA layer. Effect of CSA layer at different positions Too deep or too shallow positions of CSA layer may cause loss of information details or increase calculation time overhead. <ref type="figure" target="#fig_10">Fig 9  shows</ref> the results of the CSA layer at the 2nd, 3rd, and 4th down-sample positions of refinement network. When the CSA layer is placed on the 2nd position with 64×64 size (See <ref type="figure" target="#fig_10">Fig 9(b)</ref>), our model performances well but it takes more time to process an image. When the CSA layer is placed on 4th position with 16×16 size (See <ref type="figure" target="#fig_10">Fig 9(c)</ref>), our model becomes very efficient but tends to generate the result with coarse details. By performing the CSA layer in the 3rd position with 32×32 size, better tradeoff between efficiency (i.e., 0.82 seconds per image) and performance can be obtained by our model (See <ref type="figure" target="#fig_10">Fig 9(d)</ref>).</p><p>Effect of consistency loss We conduct further experiment to evaluate the effect of consistency loss. We add and drop out the consistency loss L c to train the inpaint- ing model. <ref type="figure" target="#fig_0">Fig 10 shows</ref> the comparison results. It can be seen that, without the consistency loss, the center of the hole regions present distorted structure, which may be due to training instability and misunderstanding of image semantic [See <ref type="figure" target="#fig_0">Fig 10(b)</ref>]. The consistency loss helps to deal with these issues [See <ref type="figure" target="#fig_0">Fig 10(c)</ref>]. Effect of feature patch discriminator As shown in <ref type="figure" target="#fig_0">Fig 11(b)</ref>, when we only use the patch discriminator, the result performances distorted structure. Then we add the conventional feature discriminator <ref type="bibr" target="#b28">[29]</ref>, however the generated content still seems blurry (See <ref type="figure" target="#fig_0">Fig 11(c)</ref>). Finally, by performing the feature patch discriminator, fine details and reasonable structure can be obtained (See <ref type="figure" target="#fig_0">Fig 11(d)</ref>). Moreover, the feature patch discriminator processes each image for 0.2 seconds faster than the conventional one <ref type="bibr" target="#b28">[29]</ref>. <ref type="figure" target="#fig_0">Figure 11</ref>. The effect of feature patch discriminator. Given the input (a), (b), (c) and (d) are the results when we use patch discriminator, patch and SRFeat feature discriminators <ref type="bibr" target="#b28">[29]</ref>, patch and feature patch discriminators, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a fined deep generative model based approach which designed a novel Coherent Semantic Attention layer to learn the relationship between features of missing region in image inpainting task. The consistency loss is introduced to enhance the CSA layer learning ability for ground truth feature distribution and training stability. Moreover, a feature patch discriminator is joined into our model to achieve better predictions. Experiments have verified the effectiveness of our proposed methods. In future, we plan to extend the method to other tasks, such as style transfer and single image super-resolution.</p><p>[42] J. <ref type="bibr">Yu</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definition of Masked Region in Feature Maps</head><p>As the CSA layer works based on both the masked region M and unmasked region M in feature maps, thus we need to give a definition of masked region in feature maps. In our implementation, we introduce a masked image in which each pixel value of known regions is 0 and that for unknown regions is 1. When considering centering masks, since the CSA layer locates at the resolution of 32×32 and the centering mask covers half of the input image I in , we set the size of region M in feature maps as 16×16. While for irregular masks, following the idea of SH <ref type="bibr" target="#b35">[36]</ref>, we first define a network that has the same architecture with the encoder of rough network but with the network width of 1, the network has only convolution layers and all the elements of the filters are 1/16. Then taking the masked image as input, we obtain the feature with 32×32 resolution which is the 3rd downsample output of the network. Finally, for the value at each position of the feature, we set those values larger than 5/16 to 1, which means this position belongs to masked region M in feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architectures</head><p>As a supplement to the content of Section 3, we will report more details of our network architectures in the following. First, <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table">Table 12</ref> depict the specific design of architecture of our rough network and refinement network respectively. On one hand, the architecture of rough network is the same as pix to pix <ref type="bibr" target="#b18">[19]</ref>. On the other hand, the refinement network uses 3×3 convolutions to double the channel and uses 4×4 convolutions to reduce the spatial size to half. Then, the architecture of patch and feature patch discriminators are shown in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table" target="#tab_6">Table 5</ref> respectively, where the VGG 4-3 denotes all the layers before Relu 4 − 3 of VGG-16 network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative Comparison of Ablation Study</head><p>Effect of CSA layer When examining the effect of CSA layer, we select validation images from butte categories of Places2 dataset and replace the CSA layer with a conventional 3×3 layer and the contextual attention layer [42] respectively. <ref type="table">Table 6</ref> lists the evaluation results. From the re-   sults in <ref type="table">Table 6</ref>, we can see that the CSA layer outperforms all the other layers.</p><p>Effect of CSA layer at different positions In order to compare the effect of CSA layer at different positions, we select validation images from canyon categories of Places2 dataset to make quantitative comparisons.  Effect of consistency loss In order to verify the validity of consistency loss L c , we select validation images from butte categories of Places2 dataset to make quantitative comparisons. <ref type="table" target="#tab_9">Table 8</ref> lists the evaluation results. From the results in <ref type="table" target="#tab_9">Table 8</ref>, we can see that the consistency loss can help our model performances better. Effect of feature patch discriminator We further conduct experiments to validate the effect of feature patch discriminator. We select validation images from canyon categories of Places2 dataset to make quantitative comparisons. <ref type="table" target="#tab_10">Table 9</ref> lists the evaluation results. From the results in Table 9, it can be seen that our feature patch discriminator is better than others.  <ref type="bibr" target="#b26">[27]</ref> are also conducted. Please refer to <ref type="figure" target="#fig_0">Fig 12 and  13</ref> for more results on Places2 and CelebA with centering mask. And for comparison on irregular masks, please refer to <ref type="figure" target="#fig_0">Fig 14 and 15</ref> for results on Paris StreetView and CelebA datasets. <ref type="table">Table 10</ref> lists the evaluation results with centering mask on Place2 dataset, the scene categories selected from Places2 is butte. <ref type="table">Table 11</ref> lists the evaluation results with irregular masks on Paris StreetView dataset. It is obvious that our model outperforms state-of-the-art approaches in both structural consistency and detail richness, and the local pixel continuity is well assured since the CSA layer considers the semantic relevance between the holes features. As a side contribution, we will release the pre-trained model and codes. CelebA <ref type="figure" target="#fig_0">Fig 16 and Fig 17 show</ref> more results obtained by our full model with centering and irregular masks respectively, where the model is trained on CelebA dataset. We resize image to 256×256 for both training and evaluation.</p><formula xml:id="formula_7">L − 1 (%) L − 2 (%) SSIM + PSNR + No L</formula><formula xml:id="formula_8">L − 1 (%) L − 2 (%) SSIM + PSNR + a 3.</formula><p>Paris StreetView We also perform experiments on our full model trained on Paris StreetView dataset with irregular masks, and the results are shown in <ref type="figure" target="#fig_0">Fig 18.</ref> We resize image to 256×256 for both training and evaluation.</p><p>Places2 <ref type="figure" target="#fig_0">Fig 19</ref> shows more results obtained by our full model with centering masks, where the model is trained on Places2 dataset. The scene categories selected from Places2 dataset are canyon and butte. We also resize the images to 256×256 for both training and evaluation. <ref type="figure" target="#fig_0">Figure 13</ref>. Qualitative comparisons on Place2 with centering masks. A1 and A2 are attention maps of two adjacent pixels, the 1st, 2nd, and 3rd rows are the attention maps of up and down adjacent pixels, the 4th and 5th rows are the attention maps of left and right adjacent pixels.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Our results compared with Contextual Attention[42]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the CSA layer. Firstly, each neural patch in the hole M searches for the most similar neural patch on the boundary M . Then, the previous generated patch and the most similar contextual patch are combined to generate the current one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Process of CSA layer Input: The set of feature map for current batch F in Output: Reconstructed feature map F out 1: Search 2: Reshape M as a convolution filter and apply in M 3: Use Eq (1) to get the Dmax i and m i 4: Initialize m i with m i 5: End search 6: Generate 7: for i = 1 → n do 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig 2,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>The visualization of attention map. Dark red means the attention value is large, while light blue means the attention value is small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Architecture of our feature patch discriminator network. The number above a convolution layer represents the shape of feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparisons in centering masks cases. The first row is the testing result on Celeba image and the others are the testing result on Places2 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparisons in irregular masks cases. The first row is the testing result on Celeba image and the others are the testing result on Paris StreetView images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>The effect of CSA layer. (b), (c) are results of our model which replace the CSA layer with the conventional layer and the CA layer [42] respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>The results of CSA layer on three down-sample positions of refinement network: 2nd, 3rd, and 4th.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>The effect of consistency loss. (b), (c) are results of our model without or with consistency loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Qualitative comparisons on Celeba with centering masks. A1 and A2 are attention maps of two adjacent pixels, the 1st, 2nd, and 3rd rows are the attention maps of up and down adjacent pixels, the 4th and 5th rows are the attention maps of left and right adjacent pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 .</head><label>14</label><figDesc>Qualitative comparisons on Paris StreetView with irregular masks. A1 and A2 are attention maps of two adjacent pixels, the 1st, 2nd, and 3rd rows are the attention maps of up and down adjacent pixels, the 4th and 5th rows are the attention maps of left and right adjacent pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 .</head><label>15</label><figDesc>Qualitative comparisons on CelebA with irregular masks. A1 and A2 are attention maps of two adjacent pixels, the 1st, 2nd, and 3rd rows are the attention maps of up and down adjacent pixels, the 4th and 5th rows are the attention maps of left and right adjacent pixels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 .</head><label>16</label><figDesc>More results on CelebA with centering masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 .</head><label>17</label><figDesc>More results on CelebA with irregular masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 .</head><label>18</label><figDesc>More results on Paris StreetView with irregular masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 .</head><label>19</label><figDesc>More results on Place2 with centering masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang. Generative image inpainting with contextual attention. CVPR, 2018. [43] Y. Zhao, B. Price, and S. Cohen. Guided image inpainting: Replacing an image region by pulling content from another image. arXiv preprint arXiv: 1803.08435, 2018. [44] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. Places: A 10 million image database for scene recognition. PAMI, 2017.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>The architecture of the Rough network. IN represents In-stanceNorm and LReLU donates leaky ReLU with the slope of 0.2.</figDesc><table><row><cell>The architecture of patch discriminator</cell></row><row><cell>[layer 1] Conv. (4, 4, 64), stride=2; LReLU;</cell></row><row><cell>[layer 2] Conv. (4, 4, 128), stride=2; IN; LReLU;</cell></row><row><cell>[layer 3] Conv. (4, 4, 256), stride=2; IN; LReLU;</cell></row><row><cell>[layer 4] Conv. (4, 4, 512), stride=1; IN; LReLU;</cell></row><row><cell>[layer 5] Conv. (4, 4, 1), stride=1;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>The architecture of the patch discriminative network. IN represents InstanceNorm and LReLU donates leaky ReLU with the slope of 0.2.</figDesc><table><row><cell>The architecture of feature patch discriminator</cell></row><row><cell>[layer 1] VGG 4 − 3 layer</cell></row><row><cell>[layer 2] Conv. (4, 4, 512), stride=2; LReLU;</cell></row><row><cell>[layer 3] Conv. (4, 4, 512), stride=1; IN; LReLU;</cell></row><row><cell>[layer 4] Conv. (4, 4, 512), stride=1;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>The architecture of the feature patch discriminative network. IN represents InstanceNorm and LReLU donates leaky ReLU with the slope of 0.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>lists the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>The effect of CSA layer. − Lower is better. + Higher is better evaluation results. From the results inTable 7, we find that better tradeoff between efficiency and performance can be achieved by our model when the CSA layer is embedded into the 3th down-sample positions. The effect of CSA layer at different positions.</figDesc><table><row><cell></cell><cell cols="4">L − 1 (%) L − 2 (%) SSIM + PSNR +</cell></row><row><cell>4</cell><cell>3.06</cell><cell>0.75</cell><cell>0.797</cell><cell>22.14</cell></row><row><cell>2</cell><cell>2.92</cell><cell>0.70</cell><cell>0.803</cell><cell>22.61</cell></row><row><cell>3</cell><cell>2.83</cell><cell>0.71</cell><cell>0.802</cell><cell>22.48</cell></row></table><note>− Lower is better.+ Higher is better</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>The effect of consistency loss.</figDesc><table><row><cell>c</cell><cell>2.39</cell><cell>0.53</cell><cell>0.823</cell><cell>23.92</cell></row><row><cell>With L c</cell><cell>2.37</cell><cell>0.52</cell><cell>0.823</cell><cell>24.04</cell></row></table><note>− Lower is better.+ Higher is better</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>The effect of feature patch discriminator. a, b and c are respectively the results when we use patch discriminator,patch and SRFeat feature discriminators<ref type="bibr" target="#b28">[29]</ref>, patch and our feature patch discriminators. − Lower is better. + Higher is betterD. More Comparisons Results</figDesc><table><row><cell></cell><cell>07</cell><cell>0.77</cell><cell>0.793</cell><cell>22.12</cell></row><row><cell>b</cell><cell>2.99</cell><cell>0.77</cell><cell>0.794</cell><cell>22.16</cell></row><row><cell>c</cell><cell>2.83</cell><cell>0.71</cell><cell>0.802</cell><cell>22.48</cell></row><row><cell cols="5">More comparisons with CA [42], SH [36], PC [26]</cell></row><row><cell cols="5">and GC [41] on Paris StreetView [8], Places2 [44] and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table 10. Comparison results over Place2 (butte) with centering hole between CA [42], SH [36], and Ours. − Lower is better. 20% 28.91 29.58 32.67 PSNR + 20-30% 26.78 27.43 30.32 30-40% 23.27 23.19 24.85 40-50% 21.67 21.33 23.10 10-20% 0.937 0.945 0.972 SSIM + 20-30% 0.894 0.920 0.951 30-40% 0.815 0.846 0.873 40-50% 0.678 0.731 0.768 Table 11. Comparison results over Paris StreetView with irregular mask between PC [32], GC [19], and Ours. − Lower is better.</figDesc><table><row><cell cols="5">L − 1 (%) L − 2 (%) SSIM + PSNR +</cell></row><row><cell>CA</cell><cell>4.08</cell><cell>1.02</cell><cell>0.704</cell><cell>20.69</cell></row><row><cell>SH</cell><cell>4.04</cell><cell>0.91</cell><cell>0.738</cell><cell>21.55</cell></row><row><cell>CSA</cell><cell>2.37</cell><cell>0.52</cell><cell>0.823</cell><cell>24.04</cell></row><row><cell>Higher is better</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Mask</cell><cell>PC</cell><cell>GC</cell><cell>CSA</cell></row><row><cell></cell><cell cols="2">10-20% 1.47</cell><cell>1.14</cell><cell>1.05</cell></row><row><cell>L − 1 (%)</cell><cell cols="2">20-30% 2.12</cell><cell>1.71</cell><cell>1.41</cell></row><row><cell></cell><cell cols="2">30-40% 3.49</cell><cell>3.19</cell><cell>2.69</cell></row><row><cell></cell><cell cols="2">40-50% 4.58</cell><cell>4.49</cell><cell>3.70</cell></row><row><cell></cell><cell cols="2">10-20% 0.17</cell><cell>0.14</cell><cell>0.08</cell></row><row><cell>L − 2 (%)</cell><cell cols="2">20-30% 0.28</cell><cell>0.22</cell><cell>0.13</cell></row><row><cell></cell><cell cols="2">30-40% 0.60</cell><cell>0.57</cell><cell>0.45</cell></row><row><cell></cell><cell cols="2">40-50% 0.86</cell><cell>0.90</cell><cell>0.68</cell></row><row><cell></cell><cell>10-</cell><cell></cell><cell></cell><cell></cell></row></table><note>++ Higher is better E. More Results on CelebA, Paris StreetView, Places2</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Filling-in by joint interpolation of vector fields and gray levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verdera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1200" to="1211" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Goldman. Patchmatch: A randomized correspondence algorithm forstructural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image inpainting. SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simultaneous structure and texture image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="882" to="889" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image melding: Combining inconsistent images using patch-based synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What makes paris look like paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Eye in-painting with exemplar generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fragment-based image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="303" to="312" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Texture synthesis by nonparametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICECCS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Digital inpainting based on the mumfordcshah euler image model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Esedoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="353" to="370" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial networks. NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Haoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhenzhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Changzhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wangmeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<title level="m">Semantic image inpainting with progressive generative networks. MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image completion using planar structure guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image completion with structure propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heung-Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="861" to="868" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Coherence-enhancing diffusion filtering. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="111" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning how to inpaint from global image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Generative face completion. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05838</idno>
		<title level="m">Generative face completion</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Deep learning face attributes in the wild. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: a key element missing from standard gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Martineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00734</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Srfeat: Single image super-resolution with feature discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jay</surname></persName>
		</author>
		<title level="m">Contextual-based image inpainting: Infer, match, and translate. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Spg-net: Segmentation prediction and guidance network for image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilczkowiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tordoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<title level="m">Hole filling through photomontage. BMVC</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image inpainting by patch propagation using patch sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1153" to="1165" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<title level="m">Shift-net: Image inpainting via deep feature rearrangement. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Image inpainting using blockwise procedural training with annealed adversarial counterpart</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08943</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07539</idno>
		<title level="m">Semantic image inpainting with perceptual and contextual losses</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<title level="m">Free-form image inpainting with gated convolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Concatenate</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Concatenate</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Concatenate</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Concatenate(Layer</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Concatenate(Layer</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Concatenate(Layer</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Concatenate</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The architecture of refinement network</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Layer 1] Conv. (3, 3, 64</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">3, 128), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lrelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">3, 256), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lrelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lrelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>3, 3, 512), stride=1, padding=1; CSA; IN;</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">3, 512), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lrelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">3, 512), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lrelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">3, 512), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lrelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">3, 512), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lrelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lrelu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4, 4, 512), stride=2, padding=1; [Layer 10] ReLU</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4, 4, 512), stride=2, padding=1; IN;</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Concatenate</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Concatenate</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">3, 512), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Concatenate(Layer</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">3, 512), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Concatenate(Layer</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">3, 512), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Concatenate(Layer</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">3, 256), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Concatenate(Layer</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">3, 128), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Concatenate(Layer</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">3, 64), stride=1, padding=1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Concatenate</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">The architecture of the refinement network. IN represents InstanceNorm and LReLU donates leaky ReLU with the slope of 0</title>
		<imprint/>
	</monogr>
	<note>Table 12</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
