<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Flexible Generative Framework for Graph-based Semi-supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-02">2 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
							<email>jiaqima@umich.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Tang</surname></persName>
							<email>weijtang@umich.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
							<email>jizhu@umich.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
							<email>qmei@umich.edu</email>
						</author>
						<title level="a" type="main">A Flexible Generative Framework for Graph-based Semi-supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-02">2 Nov 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider a family of problems that are concerned about making predictions for the majority of unlabeled, graph-structured data samples based on a small proportion of labeled samples. Relational information among the data samples, often encoded in the graph/network structure, is shown to be helpful for these semi-supervised learning tasks. However, conventional graph-based regularization methods and recent graph neural networks do not fully leverage the interrelations between the features, the graph, and the labels. In this work, we propose a flexible generative framework for graph-based semi-supervised learning, which approaches the joint distribution of the node features, labels, and the graph structure. Borrowing insights from random graph models in network science literature, this joint distribution can be instantiated using various distribution families. For the inference of missing labels, we exploit recent advances of scalable variational inference techniques to approximate the Bayesian posterior. We conduct thorough experiments on benchmark datasets for graph-based semi-supervised learning. Results show that the proposed methods outperform the state-of-the-art models in most settings. * The two authors contribute equally to this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditional machine learning methods typically treat data samples as independent and approximate a mapping function from the features to the outcome of each individual sample. However, many realworld data, such as social media or scientific articles, often come with richer relational information among the individual samples. We consider a family of such scenarios where the relational information is stored in a graph structure with the data samples as nodes, and the learning task is to predict the outcomes of unlabeled nodes based on the node features, the graph structure, as well as the labels of a subset of nodes. In these scenarios, breaking the independence assumption and utilizing such relational information in the prediction models have been shown to be helpful <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>. However, there lacks a principled way to best synergize and utilize the relational information stored in the graph together with the information stored in individual nodes. In this paper, we consider the problem of graph-based semi-supervised learning and try to approach this problem by presenting a flexible generative framework that explicitly models the joint relationship among the three key types of information in this context: features, outcomes (or labels), and the graph.</p><p>There are two major classes of existing methods for graph-based semi-supervised learning. The first class includes the graph-based regularization methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>, where explicit regularizations are posed to smooth the predictions or feature representations over local neighborhoods. This class of methods share an assumption that some kind of smoothness (e.g., the outcomes of adjacent nodes are likely to be the same) should present in the local and global graph structure. The second class consists of graph neural networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21]</ref>, where the node features within a local neighborhood are aggregated into a hidden representation for the ego node and predictions are made on top of the hidden representations. These existing methods either do not treat the graph as a random variable (but rather as a fixed observation) or do not jointly model the data features, graph, and outcomes.</p><p>While having not been well-explored in graph-based semi-supervised learning, we believe that modeling the joint distribution of the data, graph, and labels with generative models has several unique advantages over the above methods.</p><p>First, generative models can learn succinct underlying structures of the graph data. Rich literature in network science <ref type="bibr" target="#b15">[16]</ref> has shown that underlying structures often exist in real-world graph data. And there have been many probabilistic generative models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> that can learn the underlying structures well from observed graph data. Most of the existing graph-based semi-supervised learning methods described above view the graph as a fixed observation and treat it as ground truth. In reality, however, an observed graph is often noisy. We expect that through treating features, outcomes, and graph as random variables, a generative model can capture more general patterns among these entities and learn low-dimensional representations of the data that can take account for the noise in the graph.</p><p>Second, modeling the joint distribution can extract more general relationship among features, outcomes, and the graph. We argue that both classes of the existing graph-based semi-supervised learning methods only utilize restricted relationships among them. The graph-based regularization methods usually make strong assumptions about smoothness over adjacent nodes. Such assumptions often restrict the model capacity, making the models fail to fully utilize the relational information. The graph neural networks, although more flexible in aggregating node features through the graph structure, usually implicitly assume conditional independence over the outcomes given node features and the graph. This might be sub-optimal in utilizing the relational information. Directly modeling the joint distribution with flexible models allows us to better utilize the relational information.</p><p>Moreover, generative models can better handle the missing-data situation. In real-world applications, we are often faced with imperfect data, where either node features or edges in the graph are missing. Generative models excel in such situations.</p><p>A few previous studies <ref type="bibr" target="#b23">[24]</ref> trying to apply generative models to graph-based semi-supervised learning have been restricted to relatively simple model families due to the difficulty in efficient training of generative models. Thanks to recent advances of scalable variational inference techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>, we are able to propose a flexible generative framework for graph-based semi-supervised learning. In this work, we use neural networks, latent space models <ref type="bibr" target="#b5">[6]</ref>, and stochastic block models <ref type="bibr" target="#b6">[7]</ref> to form the generative models. And we use graph neural networks as the approximate posterior models in the scalable variational inference. We refer such instantiations of the proposed framework as G 3 NN (Generative Graph models with Graph Neural Networks as approximate posterior). We evaluate the proposed framework with four variants of G 3 NN on three semi-supervised classification benchmark datasets. Experiments show that our models achieve better performances than the state-of-the-art models under most settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This paper mainly focuses on the problem of graph-based semi-supervised learning, where the data samples are connected by a graph and the outcome labels are only available for part of the samples. The goal is to infer the unobserved labels based on both the labeled and unlabeled data as well as the graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph-based Regularization for Semi-supervised Learning</head><p>One of the most popular types of graph-based semi-supervised learning methods is the graph-based regularization methods. The general assumption of such methods is that the data samples are located in a low-dimensional manifold where each local neighborhood is a high-dimensional Euclidean space, and the graph stores the similarity or proximity of these data samples. Various graph regularizations are posed to smooth the outcome predictions of the model or the feature representations of the data samples over the local neighborhood in the graph. Suppose there are n data samples in total and m of them are labeled, the graph-based regularization methods generally conduct semi-supervised learning by optimizing the following objective function:</p><formula xml:id="formula_0">m i=1 L i + η n i,j=1 w i,j R(f i , f j ),</formula><p>where L i is the supervised loss function of sample i; R(·, ·) is a regularization function and w i,j is a graph-based coefficient; f i , f j could be the outcome predictions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1]</ref> or the feature representations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref> of nodes i and j; η is a hyper-parameter trading-off the supervised loss and the graph-based regularization. Different methods can have different variants of the regularization term. Most commonly, it is set as a graph Laplacian regularizer <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. Such type of models heavily rely on the smoothness assumption over the graph, which restricts the modeling capacity <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks for Semi-supervised Learning</head><p>Another class of methods that have gained great attention recently are the graph neural networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21]</ref>. A graph neural network aggregates the node features within a local neighborhood into a hidden representation for the central node. Such aggregation operations can also be stacked on top of the hidden representations to form deeper neural networks. Generally, a single aggregation operation for node i at depth l can be represented as follows,</p><formula xml:id="formula_1">h l i = σ( j∈Ni α i,j W h l−1 j ),</formula><p>where h l i is the hidden representation of i at l th layer; N i is the neighbor set of i; W is a learnable linear transformation matrix; σ is an element-wise nonlinear activation function; and different models have different definitions of α i,j . For Graph Convolutional Networks <ref type="bibr" target="#b9">[10]</ref>,</p><formula xml:id="formula_2">α i,j = 1/d i or α i,j = 1/ d i d j ,</formula><p>where d i is the number of neighbors of i. For Graph Attention Networks <ref type="bibr" target="#b20">[21]</ref>, α i,j is defined as an attention function between i and j. Finally, the predictions of each node are made on top of hidden representations in the last layer. Such methods usually model the mapping from the features and the graph to the outcome of an individual node, which assume the outcomes are conditionally independent given the features and the graph. This assumption prevents the model from utilizing the joint relationship among the outcomes over the graph. Our framework models the joint distribution of the features, outcomes, and the graph, and it is not restricted to this assumption. A concurrent work <ref type="bibr" target="#b17">[18]</ref> also tries to mitigate this assumption. They take a statistical relational learning point of view and model the outcome dependency with a Markov network conditioned on the graph, while we take a generative model point of view and instantiate the joint distribution with random graph models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generative Methods for Graph-based Semi-supervised Learning</head><p>Most methods from the above two classes treat the graph as fixed observation and only a few methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref> treat the graph as a random variable and model it with generative models. Among them, Ng et al. <ref type="bibr" target="#b16">[17]</ref> focused more on an active learning setting on graphs; Zhang et al. <ref type="bibr" target="#b23">[24]</ref> modeled the graph along with a stochastic block model and did not consider the interaction between the graph and the features or the labels in the generative model of the graph; Liu <ref type="bibr" target="#b12">[13]</ref> shares the most similar generative model with our framework, but considers a supervised learning setting where the labels are fully observed. To our best knowledge, our work is the first to propose a generative framework for graph-based semi-supervised learning that models the joint distribution of features, outcomes, and the graph with flexible nonlinear models.</p><p>Finally, as a side note, there is a recently active area of deep generative models for graphs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3]</ref>. But these models focus more on generating realistic graph topology structures and are less related to the graph-based semi-supervised learning, which is the problem of interest in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setup</head><p>We start by formally introducing the problem of graph-based semi-supervised learning. Given a set of data samples D = {(x i , y i )} n i=1 , x i ∈ R d and y i ∈ R l are the feature and outcome vectors of sample i respectively. We further denote X ∈ R n×d and Y ∈ R n×l as the matrices formed by feature and outcome vectors. The dataset also comes with a graph G = (V, E) with the data samples as nodes, where V = {1, 2, · · · , n} is the set of nodes and E ⊆ V × V is the set of edges. In the semi-supervised learning setting, only 0 &lt; m &lt; n samples have observed their outcome labels and the outcome labels of other samples are missing. Without loss of generality, we assume the outcomes of the samples 1, 2, · · · , m are observed and that of m + 1, · · · , n are missing. Therefore we can partition the outcome matrix as</p><formula xml:id="formula_3">Y = Y obs Y miss .</formula><p>The goal of graph-based semi-supervised learning is to infer Y miss based on (X, Y obs , G). For discriminative methods, we are learning the conditional distribution of p(Y|X, G). This is usually done by learning a prediction model y = f (x; X, G) using empirical risk minimization, optionally with regularizations:f</p><formula xml:id="formula_4">= arg min f 1 m m i=1 L(y i , f (x i ; X, G)) + λR(f ; G),</formula><p>where L(·, ·) is a loss function, R(·; G) is a graph-based regularization term, and λ is a hyperparameter controlling the strength of the regularization. Thenf is used to predict Y miss .</p><p>There are two specific learning settings, namely transductive learning and inductive learning, which are common in graph-based semi-supervised learning. Transductive learning assumes that X and G are fully observed during both the learning stage and the inference stage while inductive learning assumes X m+1:n and the nodes of m + 1, · · · , n in G are missing during the learning stage but available during the inference stage. In the following of this paper, we will mainly focus on the transductive learning setting but our method can also be extended to the inductive learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Flexible Generative Framework for Graph-based Semi-supervised Learning</head><p>In discriminative methods, the graph G is usually viewed as a fixed observation. In reality, however, there is usually considerable noise in the graph. Moreover, we want to take advantage of the underlying structure among X, Y , and G to improve the prediction performance under this semisupervised learning setting. In this work, we propose a flexible generative framework that can model a wide range of the forms of the joint distribution p(X, Y, G), where X, Y, and G are the random variables corresponding to X, Y , and G. We also denote Y obs and Y miss as the random variable counterparts of Y obs and Y miss .</p><p>Generation process. Inspired by the random graph models from the area of network science <ref type="bibr" target="#b15">[16]</ref>, we assume the graph is generated based on the node features and outcomes. The generation process can be illustrated by the following factorization of the joint distribution:</p><formula xml:id="formula_5">p(X, Y, G) = p(G|X, Y)p(Y|X)p(X),</formula><p>where the conditional probabilities p(G|X, Y) and p(Y|X) will be modeled by some flexible parametric families distributions p θ (G|X, Y) and p θ (Y|X) with parameters θ. By "flexible" we mean that the only restriction on the PMFs of these conditional probabilities is that they need to be differentiable almost everywhere w.r.t. θ; and we do not assume the integral of the marginal distribution p θ (G|X) = p θ (Y|X)p θ (G|Y, X)dY is tractable. For simplicity, we do not specify the distribution p(X) and everything will be conditioned on X later in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model inference.</head><p>To infer the missing outcomes Y miss , we would need the posterior distribution p θ (Y miss |X, Y obs , G), which is usually intractable under many flexible generative models. Following the recent advances in scalable variational inference <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>, we introduce a recognition model q φ (Y miss |X, Y obs , G) parameterized by φ to approximate the true posterior p θ (Y miss |X, Y obs , G).</p><p>Model learning. We train the model parameters θ and φ by optimizing the Evidence Lower BOund (ELBO) of the observed data (Y obs , G) conditioned on X. The negative ELBO loss L ELBO is defined as follows,</p><formula xml:id="formula_6">log p(Y obs , G|X) ≥ E q φ (Ymiss|X,Y obs ,G) (log p θ (Y miss , Y obs , G|X) − log q φ (Y miss |X, Y obs , G)) −L ELBO (θ, φ; X, Y obs , G).</formula><p>And the optimal model parameters are obtained by minimizing the above loss:</p><formula xml:id="formula_7">θ,φ = arg min θ,φ L ELBO (θ, φ; X, Y obs , G).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">G 3 NN Instantiations</head><p>For practical use, it remains to specify the parametric forms of the generative model p θ (G|X, Y) and p θ (Y|X), and the approximate posterior model q φ (Y miss |X, Y obs , G). In this section, we instantiate the Generative Graph model with two types of random graph models, and adopt two types of Graph Neural Networks as the approximate posterior model, which leads to four variants of G 3 NN. As proof of the effectiveness of our general framework, we intend to instantiate its components with simple models and leave room for optimizing its performance with more complex instantiations. The generative framework proposed above does not restrict the type of outcomes. As proof of concept, we focus on multi-class classification outcomes in the following of the paper and denote the number of classes as K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Instantiations of the Generative Model</head><p>For p θ (Y|X) in the generative model, we simply instantiate it with a multi-layer perceptron. For p θ (G|X, Y) in the generative model, we have come up with two instantiations inspired by the generative network models from network science literature. There are two major classes of generative models for complex networks: the latent space models (LSM) <ref type="bibr" target="#b5">[6]</ref> and the stochastic block models (SBM) <ref type="bibr" target="#b6">[7]</ref>. We instantiate a simple model from each class as our generative models.</p><p>A general assumption used by both classes of generative models for networks is that the edges are conditionally independent. Let e i,j ∈ V × V be the binary edge random variable between node i and j. e i,j = 1 indicates the edge between i and j exists and 0 otherwise. Based on the conditional independence assumption of edges, the conditional probability of the graph G can be factorized as</p><formula xml:id="formula_8">p θ (G|X, Y) = i,j p θ (e i,j |X, Y).</formula><p>Next we will specify the instantiations of p θ (e i,j |X, Y).</p><p>Instantiation with an LSM. The latent space model assumes that the nodes lie in a latent space and the probability of e i,j only depends the representation of nodes i and j. i.e., p θ (e i,j |X, Y) = p θ (e i,j |x i , y i , x j , y j ). We assume it follows a logistic regression model:</p><formula xml:id="formula_9">p θ (e i,j = 1|x i , y i , x j , y j ) = σ([(U x i ) T , y T i , (U x j ) T , y T j ]w)</formula><p>, where σ(·) is the sigmoid function; w are the learnable parameters of the logistic regression model; U is a linear transformation matrix with learnable parameters (e.g., word embedding when x is a bag-of-words feature); the class labels y i , y j are represented as one-hot vectors, and we concatenate the transformed features and the class labels of a pair of nodes as input of the logistic regression model. All the learnable parameters are included in θ.</p><p>Instantiation with an SBM. The stochastic block model assumes there are C types of nodes and each node i has a (latent) type variable z i ∈ {1, 2, · · · , C} and the probability of edge e i,j only depends on node types z i and z j . In a general SBM, we have p θ (e i,j = 1|z i , z j ) = p z i ,zj , and the p u,v for all u, v ∈ {1, 2, · · · , C} form a probability matrix P and are free parameters to be fitted.</p><p>In our model, we assume the node types are the class labels, i.e., C = K and z i being the corresponding class of y i , and p θ (e i,j |X, Y) = p θ (e i,j |y i , y j ). Note that in our notation y i is a one-hot vector. We specify p θ (e i,j |y i , y j ) with the simplest SBM, which is also called the planted partition model. That is,</p><formula xml:id="formula_10">e i,j |y i , y j ∼ Bernoulli(p 0 ) if y i = y j Bernoulli(p 1 ) if y i = y j .</formula><p>This means the probability matrix P has all the diagonal elements equal to a constant p 0 and all the off-diagonal elements equal to another constant p 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Instantiations of the Approximate Posterior Model</head><p>For the approximate posterior model q φ (Y miss |X, Y obs , G), in principle we need a strong function approximator that takes (X, Y obs , G) as the input and outputs the probability of Y miss . Here we consider two recently invented graph neural networks: the Graph Convolutional Network (GCN) <ref type="bibr" target="#b9">[10]</ref> and the Graph Attention Network (GAT) <ref type="bibr" target="#b20">[21]</ref>. Note that by doing this we are making a further approximation from q φ (Y miss |X, Y obs , G) to q φ (Y miss |X, G), as the graph neural networks by design only take (X, G) as the input. This approximation is known as the mean-field method, which is commonly used in variational inference <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>Finally, we end this section by introducing two practical details of model training.</p><p>Supervised loss. As our main task is to conduct classification with the approximate posterior model, similar with Kingma et al. <ref type="bibr" target="#b7">[8]</ref>, we add an additional supervised loss to better train the approximate posterior model:</p><formula xml:id="formula_11">L s (φ; X, Y obs , G) = − log q φ (Y obs |X, G)</formula><p>. The total loss is controlled by a weight hyper-parameter η,</p><formula xml:id="formula_12">L(θ, φ) = L ELBO (θ, φ; X, Y obs , G) + η · L s (φ; X, Y obs , G).</formula><p>We could rewrite the total loss in an alternative way as follows,</p><formula xml:id="formula_13">L(θ, φ) =E q φ (Ymiss|X,G) − log p θ (G|X, Y obs , Y miss ) + D KL (q φ (Y miss |X, G) p θ (Y miss |X)) − log p θ (Y obs |X) − η · log q φ (Y obs |X, G),</formula><p>which provides a connection between the proposed generative framework and existing graph neural networks. The fourth term −η · log q φ (Y obs |X, G) provides supervised information from labeled data for the approximate posterior GCN or GAT, while the other three terms can be viewed as additional regularizations: the learned GCN or GAT is encouraged to support the generative model of the graph, and not to go far away from p θ (Y|X).</p><p>Negative edge sampling. For both LSM and SBM based models, the probability p θ (G|X, Y) factorizes to the production of probabilities of all possible edges. Calculating the log-likelihood of it requires enumeration of all possible (i, j) pairs where i, j ∈ {1, 2, · · · , n}, which results in a O(n 2 ) computational cost at each epoch. In practice, instead of going through all (i, j) pairs, we only calculate the probabilities of the edges observed in the graph and a set of "negative edges" randomly sampled from the (i, j) pairs where edges do not exist. This practical trick, named negative sampling, is commonly used in the training of word embeddings <ref type="bibr" target="#b14">[15]</ref> and graph embeddings <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the proposed variants of G 3 NN on several benchmark datasets for graphbased semi-supervised learning. We test the models under both the standard benchmark setting <ref type="bibr" target="#b22">[23]</ref> as well as two data-scarce settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Standard Benchmark Setting</head><p>We first consider a standard benchmark setting in recent graph-based semi-supervised learning literature <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Datasets. We use three standard semi-supervised learning benchmark datasets for graph neural networks, Citeseer, Cora, and Pubmed <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>. The graph G of each dataset is a citation network with documents as nodes and citations as edges. The feature vector of each node is a bag-of-words representation of the document and the class label represents the research area this document belongs to. We adopt these datasets from the PyTorch-Geometric library <ref type="bibr" target="#b3">[4]</ref> in our experiments 5 . For each  <ref type="table">Table 2</ref>: Classification accuracy under the standard benchmark setting. The upper block lists the discriminative baselines. The lower block lists the proposed variants of G 3 NN. The bold marker denotes the best performance on each dataset. The underline marker denotes that the generative model outperforms its discriminative counterpart, e.g., LSM-GCN outperforms GCN; and the asterisk (*) marker denotes the difference is statistically significant by a t-test at significance level 0.05. The (±) error bar denotes the standard deviation of the test performance of 10 independent trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cora</head><p>Pubmed Citeseer MLP 0.583 ± 0.009 0.734 ± 0.002 0.569 ± 0.008 GCN 0.815 ± 0.002 0.794 ± 0.004 0.718 ± 0.003 GAT 0.825 ± 0.005 0.785 ± 0.004 0.715 ± 0.007 LSM_GCN 0.825 ± 0.002* 0.779 ± 0.004 0.744 ± 0.003* LSM_GAT 0.829 ± 0.003 0.776 ± 0.007 0.731 ± 0.005* SBM_GCN 0.822 ± 0.002* 0.784 ± 0.006 0.745 ± 0.004* SBM_GAT 0.829 ± 0.003 0.774 ± 0.004 0.740 ± 0.003* dataset, we summarize number of classes, number of nodes, number of edges, and average number of nodes within the 2-hop neighborhood of each node in <ref type="table" target="#tab_0">Table 1</ref>. In this standard benchmark setting, we closely follow the dataset setup in Yang et al. <ref type="bibr" target="#b22">[23]</ref> and Kipf and Welling <ref type="bibr" target="#b9">[10]</ref>.</p><p>Models for comparison. For the proposed framework, we implement four variants (2 × 2) of G 3 NN by combining the two generative model instantiations with the two approximate posterior model instantiations: LSM-GCN, SBM-GCN, LSM-GAT, SBM-GAT.</p><p>For baselines, we compare against two state-of-the-art models for the graph-based semi-supervised learning, GCN <ref type="bibr" target="#b9">[10]</ref> and GAT <ref type="bibr" target="#b20">[21]</ref>. We also include a multi-layer perceptron (MLP), which is a fully connected neural network without using any graph information, as a reference.</p><p>We use the original architectures of GCN and GAT models in both the baselines and the proposed methods. We grid search the number of hidden units from <ref type="bibr" target="#b15">(16,</ref><ref type="bibr">32,</ref><ref type="bibr">64)</ref> and the learning rate from (0.001, 0.005, 0.01). GAT uses a multi-head attention mechanism. In our experiments, we fix the number of heads as 8 and try to set the total number of hidden units as <ref type="bibr" target="#b15">(16,</ref><ref type="bibr">32,</ref><ref type="bibr">64)</ref> and to set the number of hidden units of a single head as <ref type="bibr" target="#b15">(16,</ref><ref type="bibr">32,</ref><ref type="bibr">64)</ref>. In the proposed methods, we set the generative model for p θ (Y|X) as a two-layer MLP having the same number of hidden units as the corresponding GCN or GAT in the posterior model. For the MLP baseline, we also set the number of layers as 2 and grid search the number of hidden units and learning rate like other models. For the proposed generative models, we grid search the coefficient of the supervised loss η from (0.5, 1, 10). The number of negative edges is set to be the number of the observed edges in the graph. For LSM models, the dimensions of the feature transformation matrix U is fixed to 8 × d, where d is the feature size. For SBM models, we use two settings of (p 0 , p 1 ): (0.9, 0.1) and (0.5, 0.6). We use Adam optimizer to train all the models and apply early stopping with the cross-entropy loss on the validation set. We adopt the implementations of GCN and GAT from the PyTorch-Geometric <ref type="bibr" target="#b3">[4]</ref> library in all our experiments.</p><p>Results. The performance of the baselines and proposed models under the standard benchmark setting is summarized in <ref type="table">Table 2</ref>. We report the mean and the standard deviation of the test accuracy of 10 independent trials for each model. The results show that on all datasets except for Pubmed, the proposed methods achieve the best test accuracy on the standard benchmark setting. Notably, every instantiation model of the proposed generative framework outperforms their corresponding discriminative baseline (GCN or GAT) in most cases. We also note that GCN performs better than GAT and the proposed models on Pubmed. We conjecture that, when the number of classes is <ref type="table">Table 3</ref>: Classification accuracy under the missing-edge setting. The bold marker, the underline marker, the asterisk (*) marker, and the (±) error bar share the same definitions in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cora</head><p>Pubmed Citeseer MLP 0.583 ± 0.009 0.734 ± 0.002 0.569 ± 0.008 GCN 0.665 ± 0.007 0.746 ± 0.004 0.652 ± 0.005 GAT 0.682 ± 0.004 0.744 ± 0.006 0.642 ± 0.004 LSM_GCN 0.711 ± 0.005* 0.766 ± 0.006* 0.704 ± 0.002* LSM_GAT 0.710 ± 0.007* 0.766 ± 0.004* 0.691 ± 0.005* SBM_GCN 0.718 ± 0.004* 0.762 ± 0.005* 0.716 ± 0.004* SBM_GAT 0.716 ± 0.007* 0.761 ± 0.005* 0.709 ± 0.008* small and the graph is relatively dense, GCN may be already quite capable of propagating feature information from neighbors (see the average size of 2-hop neighborhoods in <ref type="table" target="#tab_0">Table 1</ref>). When there are more classes or the graph is relatively sparse (e.g., Cora, Citeseer, and the missing-edge setting of Pubmed in Section 4.2.1), the advantage of our proposed method is more evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data-Scarce Settings</head><p>Generative models usually have better sample efficiency than discriminative models. Therefore, we expect the proposed generative framework to show bigger advantage when data are scarce. Next, we evaluate the models on the citation datasets under two such settings: missing-edge setting and reduced-label setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Missing-Edge Setting</head><p>In the standard benchmark setting, we assume that all samples are connected to the graph identically and the training, validation, and test set are split randomly in the datasets. In practice, however, the samples we are interested in the test period may not be well connected to the graph. For example, we may not have connection for new users in a social network other than their profile information. In this cold-start situation, one may expect to make predictions purely based on the profile information. However, we believe that the relational information stored in the graph of the training data can still help us learn a better and more generalizable model even if some of the predictions are made only based on the node features. And we expect the proposed generative models to work better than the discriminative baselines in this case because they can better distill the relationship among the data.</p><p>To mimic such situations, we create a missing-edge setting, where we remove all the edges of the test nodes from the graph. Note that this setting is different from the inductive learning setting in previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref> where the edges for the test data are absent during the training stage but present during the test stage. In the missing-edge setting, the edges for the test data are absent during both stages. We follow the same experimental setup as in the standard benchmark setting except for the modification of the graph.</p><p>Results. The performance under the missing-edge setting is shown in <ref type="table">Table 3</ref>. Not surprisingly, as we lose part of the graph information, the performances of all models except for MLP (which does not use the graph at all) drop compared to the standard benchmark setting in <ref type="table">Table 2</ref>. However, the proposed generative models perform better than their corresponding discriminative baselines by a large margin. Remarkably, even without knowing any edges of out-of-sample nodes, the accuracy of SBM-GCN on Citeseer can achieve the state-of-the-art level of GCN under the standard benchmark setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Reduced-Label Setting</head><p>Another common data-scarce situation is the lack of labeled data. Therefore, we create a reducedlabel setting, where we drop half of the training labels for each class compared to the standard benchmark setting. All other experiment and model setups are the same as the standard benchmark setting. <ref type="table">Table 4</ref>: Classification accuracy under the reduced-label setting. The bold marker, the underline marker, the asterisk (*) marker, and the (±) error bar share the same definitions in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cora</head><p>Pubmed Citeseer MLP 0.498 ± 0.004 0.674 ± 0.005 0.493 ± 0.010 GCN 0.750 ± 0.003 0.724 ± 0.005 0.666 ± 0.003 GAT 0.771 ± 0.004 0.711 ± 0.006 0.675 ± 0.005 LSM_GCN 0.777 ± 0.002* 0.709 ± 0.003 0.691 ± 0.005* LSM_GAT 0.792 ± 0.004* 0.699 ± 0.003 0.691 ± 0.004* SBM_GCN 0.780 ± 0.002* 0.710 ± 0.004 0.703 ± 0.006* SBM_GAT 0.796 ± 0.008* 0.699 ± 0.003 0.698 ± 0.003* Results. The performance under the reduced-label setting is shown in <ref type="table">Table 4</ref>. As can be seen from the results, the proposed generative models achieve the best test accuracy on Cora and Citeseer again. And the gap of the performances between the proposed generative models and the corresponding discriminative models are larger on Cora and Citeseer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have presented a flexible generative framework for graph-based semi-supervised learning. By applying scalable variational inference, this framework is able to take the advantages of both recently developed graph neural networks and the wisdom of random graph models from classical network science literature, which leads to the G 3 NN model. We further implement 4 variants of G 3 NN, instantiations of the proposed framework where we build generative graph models with graph neural networks as the approximate posterior models. Through thorough experiments, We demonstrated that these instantiation models outperform the state-of-the-art graph-based semisupervised learning methods on most benchmark datasets under the standard benchmark setting. We also showed that the proposed generative framework has great potential in data-scarce situations.</p><p>For future work, we expect more complex instantiations of generative models to be developed using this framework and to optimize the graph-based semi-supervised learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of benchmark datasets.</figDesc><table><row><cell cols="5">Dataset # Classes # Nodes # Edges Avg. 2-Neighborhood Size</cell></row><row><cell>Cora</cell><cell>7</cell><cell>2,708</cell><cell>5,278</cell><cell>35.8</cell></row><row><cell>Pubmed</cell><cell>3</cell><cell>19,717</cell><cell>44,324</cell><cell>59.1</cell></row><row><cell>Citeseer</cell><cell>6</cell><cell>3,327</cell><cell>4,552</cell><cell>14.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The datasets loaded by the PyTorch-Geometric data loader have slightly less edges than those reported in Yang et al.<ref type="bibr" target="#b22">[23]</ref>, which is believed due to the existence of duplicate edges in the original datasets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was in part supported by the National Science Foundation under grant numbers 1633370 and 1620319.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent space approaches to social network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Handcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american Statistical association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">460</biblScope>
			<biblScope unit="page" from="1090" to="1098" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels: First steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="137" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prediction models for network-linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="164" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Statistical learning for networks with node features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A general optimization framework for smoothing language models on graph structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Networks: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian semi-supervised learning with graph gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1683" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06214</idno>
		<title level="m">Gmnn: Graph markov neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bayesian graph convolutional neural networks for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Üstebay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11103</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
