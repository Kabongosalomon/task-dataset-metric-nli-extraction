<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Back Projection Network for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University Hung Hom</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University Hung Hom</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Tak</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University Hung Hom</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University Hung Hom</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Back Projection Network for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning based single image super-resolution methods use a large number of training datasets and have recently achieved great quality progress both quantitatively and qualitatively. Most deep networks focus on nonlinear mapping from low-resolution inputs to high-resolution outputs via residual learning without exploring the feature abstraction and analysis. We propose a Hierarchical Back Projection Network (HBPN), that cascades multiple Hour-Glass (HG) modules to bottom-up and top-down process features across all scales to capture various spatial correlations and then consolidates the best representation for reconstruction. We adopt the back projection blocks in our proposed network to provide the error correlated upand down-sampling process to replace simple deconvolution and pooling process for better estimation. A new Softmax based Weighted Reconstruction (WR) process is used to combine the outputs of HG modules to further improve super-resolution. Experimental results on various datasets (including the validation dataset, NTIRE2019, of the Real Image Super-resolution Challenge) show that our proposed approach can achieve and improve the performance of the state-of-the-art methods for different scaling factors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single Image Super-Resolution (SISR) attracts a lot of attention in the research community in the past few years. It is a fundamental low-level vision problem where the aim is to form a high-resolution (HR) image Y from a lowresolution (LR) image X. Usually, SISR is described as an ill-posed problem X = HY + µ, where H is a downsampling operator, µ is additive white Gaussian noise with standard deviation σ.</p><p>To resolve the ill-posed problem, Super-Resolution (SR) images can be obtained in the perspective of model-based optimization <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref> and discriminative learning methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b9">10]</ref>. The model-based opti-mization can be formulated as,</p><formula xml:id="formula_0">Y = arg min Y 1 2 X − HY 2 + λΩ(Y)<label>(1)</label></formula><p>where λ is the regularization factor that controls the significance of the regularization term Ω(Y). Though modelbased optimization methods are flexible to handle different SR condition and noise, they are usually time-consuming and require various priors. On the contrary, discriminative approaches use external or internal paired LR-HR training samples to directly learn the nonlinear relationship. The objective is given by <ref type="bibr" target="#b1">(2)</ref> where W is the mapping model for reconstruction. The fidelity term arg min Y 1 2 Y − WX 2 determines the distortion of reconstruction and similarly, the regularization term Ω(W) controls the complexity of the mapping model. In the previous research works, patch-based approaches use classification tools , like kNN <ref type="bibr" target="#b3">[4]</ref>, to classify the patches from natural images and capture the mapping relationship for clustered patches. Taking the advantage of non-local statistical priors from external datasets, there are many successful approaches that achieve good SR performance by off-line training classifiers and regressors for efficient online reconstruction. For example, Timofte et al. <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b32">31]</ref> proposed the adjusted anchored neighbor regression (ANR and A+) which uses clustering on encoded sparse dictionary to search nearest neighbor dictionary atoms for LR patch reconstruction. Siu et al. <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b23">22]</ref> proposed random forests for binary classification to obtain fast and high qualified image SR and hierarchical decision trees to further boost up image SR performance.</p><formula xml:id="formula_1">min Θ (Ŷ, Y)s.t.Ŷ = arg min Y 1 2 Y − WX 2 + λΩ(W)</formula><p>Since Dong et al. <ref type="bibr" target="#b5">[6]</ref> proposed the first deep convolution neural network (CNN) for image SR, a large number of CNN based SR approaches have been proposed to significantly improve the image SR performance. Along with the development of other computing vision fields, i.e., image classification, object detection and so on, more deep and complex models are adopted in image SR. For example, VDSR <ref type="bibr" target="#b16">[16]</ref> uses a 20-layer convolution network for different up-sampling factors. Tai et al. <ref type="bibr" target="#b30">[29]</ref> proposed a deep recursive residual network by using recursive blocks to explore long-term correlations between LR and HR images. LapSRN <ref type="bibr" target="#b19">[18]</ref> uses Laplacian pyramid networks to gradually super-resolve LR with different up-sampling factors. Most recently, Haris et al. <ref type="bibr" target="#b9">[10]</ref> proposed Deep Back Projection Network (DBPN) for image SR by iteratively computing reconstruction errors then, fusing them back for model tuning.</p><p>Inspired by <ref type="bibr" target="#b9">[10]</ref>, we design a Hierarchical Back Projection Network (HBPN) for image SR. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our work have following contributions:</p><p>• Enhanced back projection blocks. We propose an enhanced back projection block, including the new Up-sampling Back Projection block (UBP) and Downsampling Back Projection block (DBP). Both UBP and DBP embed the back projection mechanism in the residual block to update up-sampling and downsampling errors for better results. The key modification is two 1×1 convolution layers within the back projection block to fine tune the LR and HR features. Details are explained in Section 3.</p><p>• Hierarchical SR HourGlass (SR-HG) module. We stack multiple stages of SR-HG modules to capture various spatial correlations by repeated bottom-up and top-down process across all scales. Different from HG structure used in other applications <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b26">25]</ref>, we replace the pooling and deconvolution layer by enhanced back projection blocks for better feature down-and upsampling process.</p><p>• Softmax based Weighted Reconstruction (WR). To encourage different SR-HG modules super-resolve LR images in a hierarchical order, each SR-HG module outputs one coarse SR result and one weighting map. At the final WR stage, we propose to use Softmax layer to normalize weighting maps from different SR-HG modules to obtain the global weighting map. Finally, we consolidate all coarse SR results by using the global weighting map to output the final SR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In order to compare the different SR reconstruction measurements, we can divide the convolutional neural network based SR approaches into distortion based SR and perception based SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Distortion based image super-resolution</head><p>As discussed in Section 1, to resolve Equation 2, the endto-end CNN model is a very direct and efficient method.</p><p>By inputting LR images, we can define a mean squared errors based loss function to target on optimizing the convolutional parameters to obtain the SR outputs with minimal distortion. Considering the mismatch of dimension between LR and HR images, there are different designs of CNN models for SR. In the early stage of CNN for image SR, researchers inherited the knowledge on traditional machine learning based SR approaches by initially up-sampling LR images to the desired size by simple interpolation, i.e., Bicubic, and then learn the mapping model between the up-sampled LR and HR images. SRCNN <ref type="bibr" target="#b5">[6]</ref> and many other CNN approaches <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b9">10]</ref> use this idea to build networks using cascaded convolution process. In order to grasp long-term correlation of pixels for reconstruction, we need to stack more convolution layers to cover a larger receptive field. However, building deeper convolution networks can encounter computation exploding and gradient vanishing problems. To resolve the former problem, Kim et al. <ref type="bibr" target="#b17">[17]</ref> and Tai et al. <ref type="bibr" target="#b30">[29]</ref> proposed to use recursive convolution networks to increase recursion depth rather than convolution depth without introducing new parameter for computation. For the latter problem, residual learning <ref type="bibr" target="#b11">[11]</ref> is introduced in CNN models to add shortcuts to avoid gradient vanishing. In recent SR works, Lim et al. <ref type="bibr" target="#b21">[20]</ref> proposed a state-of-the-art CNN network using residual blocks to achieve good SR performance on various datasets.</p><p>Rather than using initial interpolation to up-sample LR image to feed into CNN for training, there have also been some novel CNN works that build up-sampling process into CNN models. The deconvolution with stride larger than 1 is used in CNN working as an up-sampling process, Lai et al. <ref type="bibr" target="#b19">[18]</ref> proposed a Laplacian Pyramid network to gradually super-resolve LR image by different scales. Shi et al. <ref type="bibr" target="#b28">[27]</ref>, on the other hand, proposed the sub-pixel convolution process to work as a pixel based interpolation for enlargement. Recently, Haris et al. <ref type="bibr" target="#b9">[10]</ref> further studied the residual learning on image SR and proposed the back projection based residual block that can efficiently learn LR and HR feature maps iteratively to feedback residual errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Perception based image super-resolution</head><p>Rather than targeting on minimizing mean squared errors based loss function, perception based image SR focuses on visual quality over data fidelity. Since a pioneer work on using the Generative Adversarial Network (GAN) for image SR <ref type="bibr" target="#b20">[19]</ref>, there are a lot of studies on using adversarial loss as a measurement for SR performance. By replacing the ln-norm minimization by distribution divergence, we force the SR networks to learn the meaningful features rather than pixel differences. The idea of using GAN for image SR can be described as: the generator and discriminator learn from each other to generate a "fake" SR image that gives mini-  mal distance on the high-level feature space (features used commonly extracted from VGG19 <ref type="bibr" target="#b29">[28]</ref>). Wang et al. <ref type="bibr" target="#b34">[33]</ref> further investigated this study. They modified the generator by using Residual-in-Residual Dense Block to improve SR performance in terms of PSNR (one measurement of distortion) and then they fine tuned the network by using adversarial loss to generate SR image with better visual quality. From recent studies of GAN for image SR, one of the key issues is still the design of generators. A good generator should be able to extract rich feature maps for estimation by any criteria. Our proposed network can also be considered as a perception based image SR by using adversarial loss. However, the measurement of visual quality was only used in 4× image SR <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b34">33]</ref>. To make a good comparison, we still use distortion based evaluation (PSNR, SSIM, etc.) to make analysis among different approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hierarchical Back Projection Network</head><p>Before introducing our proposed work, let us first define some terms. As defined in Section 1, given a RGB LR image X with size h × w, we want to super-resolve it by α× to the dimension αh × αw, the HR image Y. The superresolved image is the SR imageŶ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Back projection</head><p>Let us first revisit the back projection approach that has been commonly used in image SR. Back projection was first proposed to utilize multiple LR images to estimate one SR image. <ref type="bibr" target="#b9">[10]</ref> comes up with using back projection to refine SR image to improve the quality. It is an efficient iterative process to improve the data fidelity of SR by minimizing the loss between the original LR image and the down-sampled SR image. Mathematically, description of the back projection isŶ</p><formula xml:id="formula_2">t+1 =Ŷt − λH −1 (HŶt − X)<label>(3)</label></formula><p>where H −1 is the inverse operator of H which represents the up-sampling operation process. For estimating the SR residues, we need to assume a certain known down-  <ref type="figure">Figure 2</ref>. Back Projection procedure.</p><formula xml:id="formula_3">H H 1  H Initial SR ) ( t Y ) 1 (  t Y</formula><p>sampling and up-sampling operators. λ is the trade-off parameter to control the ratio of the residual information to gradually improve the SR quality. t is the iteration number. A simple back projection process is shown in <ref type="figure">Figure</ref> 2. Back projection has been widely used in many SR approaches as a final refinement to reduce the distortion in terms of PSNR. However, it is observed that the down-and up-sampling operators need to be pre-determined as fixed parameters for estimation which may not obtain optimal results. To resolve this problem, <ref type="bibr" target="#b9">[10]</ref> proposes to embed the back projection into CNN model to learn the unknown parameters by training. By using multiple proposed back projection blocks, it can expand the iterative process as a cascading process using more parameters to minimize the SR residual information. Our study further develops this work by coming up with a hierarchical back projection network to learn LR and HR features across different scales to extract more compact and robust features for reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Enhanced back projection blocks</head><p>Let us propose our Enhanced back projection blocks, which contain both new Up-sampling Back Projection (UBP) and Down-sampling Back Projection (DBP) blocks. The UBP is the forward back projection process that estimates HR residues while the UBP is the backward back projection process that estimates LR residues. The details of two blocks are shown in <ref type="figure" target="#fig_1">Figure 3</ref>.  The process of UBP block can be described by rewriting Equation <ref type="bibr" target="#b2">3</ref> as,</p><formula xml:id="formula_4">Deconv PReLU Conv l x 1  l x Deconv 11 Conv  11 Conv  PReLU PReLU MN2C 2M2NC MN2C 2M2NC 2M2NC 11, C 11, C PReLU Conv l x 1  l x 11 Conv  11 Conv  PReLU PReLU Deconv Conv 2M2NC MN2C MN2C MN2C 2M2NC 11, 2C 11,</formula><formula xml:id="formula_5">x l+1 = ΩDx l + D (λx l − CDx l )<label>(4)</label></formula><p>Similarly, the process of DBP block can be considered as the backward of UBP that estimates the LR residues as Equation 5,</p><formula xml:id="formula_6">x l+1 = ΩCx l + C (λx l − DCx l )<label>(5)</label></formula><p>There are two key modifications between our proposed Enhanced back projection blocks and that in DBPN <ref type="bibr" target="#b9">[10]</ref>: global weighting model Ω and residual weighting model λ. They all use 1 × 1 convolution layers to work as the weighting process.</p><p>For the residual weighting model λ, it resembles the trade-off parameter in Equation 3 that provides the regularization on the update of SR residues. Without followed by any activation function, this 1 × 1 convolution layer is a linear weighted model that can tune the residual information without increasing any computation burden.</p><p>For the global weighting model, it has two jobs: first, to work as a weighted model to tune the down-and upsampled features for update so that we can introduce one extra freedom of parameters for training; second, to adjust the channel (number) of feature maps for addition. For example, from (a) in <ref type="figure" target="#fig_1">Figure 3</ref>, the global weighting model reduces the number of feature maps by half. From (b) in <ref type="figure" target="#fig_1">Figure 3</ref>, the global weighting model doubles the number of feature maps for addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hierarchical SR HourGlass (SR-HG) module</head><p>For the proposed SR-HG module, we adopt the Hour-Glass structure to cascade multiple enhanced back projection blocks in bottom-up and top-down manner. The Hour-Glass structure is commonly used in many computing vision fields. By down-sampling the size of feature maps while increasing the number of feature maps, we can extract denser and deeper features for various applications. The key differences of our proposed SR-HG module are three folds: 1) replacing pooling process by DBP blocks to avoid information loss, 2) replacing the single convolution process by DBP blocks to down-sample the feature maps and 3) output a coarse SR image and a weighting map.  The complete structure of SR-HG is shown in <ref type="figure" target="#fig_2">Figure  4</ref>. For each SR-HG module, it contains 3 DBP blocks for down-sampling process and 3 UBP blocks for up-sampling process. For DBP and UBP blocks with same feature dimension, we use 1 × 1 convolution as local shortcuts (green blocks) to share the features. For different SR-HG blocks, we use 1 × 1 convolution as global shortcuts (pink and blue blocks) to share features across different modules. For each SR-HG module, there are two branches (dash lines in <ref type="figure" target="#fig_2">Figure 4)</ref> to generate one coarse SR result and one weighting map to describe the contribution of the coarse SR. There are global and local shortcuts that share the features across different HourGlass modules and spatial scales. Each SR-HG module contains 3 UBP blocks for up-sampling and 3 DBP blocks for down-sampling and each UBP/DBP block up-/down-samples the input data by 2×. Totally, the input data are first down-sampled by 8× and then up-sampled by 8×.</p><p>In the meantime, the number of features are first increased by 8× and then decreased by 8× so that the network can learn denser and more compact features for reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Softmax based Weighted Reconstruction (WR)</head><p>For the final reconstruction, instead of concatenating coarse SR results from different SR-HG modules to generate the final SR by one convolution layer, we propose a Softmax based Weighted Reconstruction (WR) that makes use of the weighting maps to estimate the contribution of coarse SR results. It can be regarded as an adaptive weighted addition of coarse SR results. The comparison between WR process and plain process is shown in <ref type="figure" target="#fig_3">Figure 5</ref>. It concatenates the weighting maps from SR-HG modules and learns a global probability map using a Softmax normalization. The coarse SR results are weighted by the probability map to generate the final SR image.  For the plain process, it simply concatenates the coarse SR results together and learns one convolution layer to output the SR results without considering the internal correlation between coarse SR results. In the WR module, the Softmax layer is used to normalize the weighting maps from SR-HG modules in the range of [0, 1]. Then the final SR image is the weighted sum of the coarse SR results. By using Softmax normalization, we force each SR-HG module to learn the SR image at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation and training setups</head><p>Different from DBPN <ref type="bibr" target="#b9">[10]</ref> which has different structures and configurations for different up-sampling enlargement, the proposed HBPN network uses the same structure as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In UBP and DBP blocks, we use 6 × 6 convolution filters with two striding and two padding for down-and up-sampling. For shortcut connections, we use 3×3 convolution filters with one striding and 1 padding. We initialize the weights based on <ref type="bibr" target="#b12">[12]</ref>. The testing data include Set5 <ref type="bibr" target="#b2">[3]</ref>, Set14 <ref type="bibr" target="#b35">[34]</ref>, BSD100 <ref type="bibr" target="#b1">[2]</ref>, Urban100 <ref type="bibr" target="#b13">[13]</ref> and Manga109 <ref type="bibr" target="#b24">[23]</ref> on 2×, 4× and 8× SR enlargement.</p><p>The training data include 800 2K images from DIV2K <ref type="bibr" target="#b33">[32]</ref> and 2650 2K images from Flickr <ref type="bibr" target="#b21">[20]</ref>. Each image was rotated and flipped for augmentation to increase the images by 8×. The LR images were down-sampled and initially up-sampled by bicubic function in MATLAB on different scaling factors. We extracted LR-HR patch pairs from images of size 256 × 256. In order to achieve better SR performance, for different SR scaling factors, we trained our model by using different LR-HR training patches. The learning rate is set to 0.0001 for all layers. The batch size is 8 for every 5×10 5 iterations and 32 for the rest 5×10 5 iterations to achieve better results. For optimization, we used Adam with the momentum to 0.9 and the weight decay of 0.0001. All experiments were conducted using Caffe, MAT-LAB R2016b on two NVIDIA GTX1080Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model analysis</head><p>Scaling factors of UBP and DBP. For each SR-HG module, we used DBP blocks to down-sample the feature maps to the smallest size and UBP blocks as mirror reflection to up-sample feature maps to the original size. For input data with size M × N × 3, we used T DBP blocks to down-sample the input to obtain feature maps with size</p><formula xml:id="formula_7">M 2 T × N 2 T ×(64·2 T −1 ).</formula><p>To demonstrate the capability of this bottom-up and top-down structure, we conducted multiple networks HG-1, HG-2, HG-3 (which is the proposed HBPN model) and HG-4 for 4× enlargement on Set5 to make comparison. The results are shown in <ref type="figure" target="#fig_4">Figure 6</ref>. We compare different SR-HG blocks using different numbers of UBP and DBP to down-and up-scale features. Using HG-3 shows the best performance comparing with other networks. Due to the model complexity, HG-1 and HG-2 can converge faster than HG-3 and HG-4. As the best performance, HG-3 achieves 32.66 dB in terms of PSNR which is 0.2 dB and 0.4 dB better than HG-2 and HG-4.</p><p>Number of SR-HG modules. Generally, a deeper network can train more parameters to learn deeper feature representation for good performance. By stacking more and more SR-HG modules, <ref type="bibr" target="#b9">[10]</ref> shows that the network with more HG blocks can produce a better prediction. In our experiments, we conduct multiple networks with different number of SR-HG modules: S (2 SR-HG modules), M (3 SR-HG modules, which is the proposed HBPN model) and L (4 SR-HG modules).</p><p>From <ref type="figure">Figure 7</ref>, we can see that network L (4 SR-HG module) gives the highest PSNR result. For network S (2 SR-HG module), its performance is lower than network M and network L. For network L (4 SR-HG module), it requires extra 33% parameters as compared with network M but only achieves slight (0.1 dB) improvement in PSNR. This result shows that our proposed HBPN has the best trade-off between performance and number of parameters. To further study the significance of each SR-HG module, let us visualize the activation maps of the output of each SR-HG module in our HBPN network. In the <ref type="figure">Figure 8</ref>, the first row shows three activation maps of each SR-HG output on image butterfly. We believe that the reason why CNNs outperform other patch-based learning approaches is that CNNs use activation layers to introduce the nonlinearity in the network to improve the feature representation power of filters. Hence, we show the activation maps rather than the output feature maps to show how activation layer works. In our design, we use the PReLU function that assigns weight 1 to non-zero values and very small weights to negative values. We can visualize the weights as the activation maps. In our experiments, we chose the last PReLU layer of each SR-HG module to make comparison. We can observe that the activation map of the SR-HG-1 module has high activation across some of the feature maps while zero activation on others because the first layer only focuses on reconstructing the low-frequency information on averaging the whole image. This can be observed on the output of SR-HG-1 of <ref type="figure">Figure 8</ref>. For SR-HG-2 and SR-HG-3, there are more activated values on the activation maps, that focus on edge and texture regions. We calculated the percentage of activated values on SR-HG-1, SR-HG-2 and SR-HG-3 and found the value decreases from 30.55%, 25.46% to 22.35%, which explains that the convolutional filters focus more on the edge and texture reconstruction.</p><p>The effect of WR process. Finally, we compare the WR process and the plain concatenated process in <ref type="table" target="#tab_5">Table 1</ref>. We design the plain concatenated process and WR process with the structure as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. They use the same SR-HG modules for feature extraction and the only difference is the final reconstruction process. The results were conducted on Set5, Set14 dataset of 2×, 4× and 8× enlargement.</p><p>From <ref type="table" target="#tab_5">Table 1</ref>, we can see that using WR process can significantly improve the PSNR by at least 0.11 dB. The effectiveness of WR process can be further explained in <ref type="figure">Figure 8</ref>. In the second and third rows of <ref type="figure">Figure 8</ref>, we visualize the weighting maps of each SR-HG module and coarse SR outputs. For the first SR-HG module, the weighting map focuses on the low-frequency domain that reconstructs the main components of the image. For the second and third SR-HG modules, the weighting maps give high attentions to the edge regions. From the coarse SR output of each SR-HG module, we can also match the results with their weighting maps. Note that the output of SR-HG-2 focuses on the edge reconstruction on G and B channels and the output of SR-HG-3 focuses on the edge reconstruction on the R channel. From the aspect of gradient based edge detection, SR-HG-2 focuses on the first-order edge reconstruction (see the single-line edges on the output of SR-HG-2) while SR-HG-3 pays attention on the second-order edge reconstruction (see the double-line edges on the output of SR-HG-3). This can prove that using more SR-HG modules can explore deeper features in terms of the order of the pixel gradient.</p><p>From <ref type="table" target="#tab_5">Table 1</ref>, it can be found that using WR process is very efficient that can gain 0.2 dB and 0.1 higher than the plain process in terms of PSNR and SSIM, respectively. We also show the weighting maps to possibly indicate the contribution of coarse SR results. We name the weighting maps at different stages of SR-HG modules as W1, W2 and W3. The weighting maps are visualized by normalizing the pixel values in the range of [0, 255]. The weighting map corresponds to the SR-HG results giving different weights to the pixel values. The first weighting map gives a large weights on the whole image and small weights on the edges. The second and third weighting maps give higher weights to the non-edge regions (first-order edge detection) and edge regions (second-order edge detection), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the state-of-the-art SR approaches</head><p>To prove the effectiveness of the proposed methods, we conducted experiments by comparing with most (if not all) state-of-the-art SR algorithms: Bicubic, A+ <ref type="bibr" target="#b32">[31]</ref>, CRFSR <ref type="bibr" target="#b23">[22]</ref>, SRCNN <ref type="bibr" target="#b5">[6]</ref>, VDSR <ref type="bibr" target="#b16">[16]</ref>, DRCN <ref type="bibr" target="#b17">[17]</ref>, LapSRN <ref type="bibr" target="#b19">[18]</ref>, SRResNet <ref type="bibr" target="#b20">[19]</ref>, EDSR <ref type="bibr" target="#b21">[20]</ref> and DBPN <ref type="bibr" target="#b9">[10]</ref>. PSNR and SSIM are used to evaluate the proposed method and others. Generally, PSNR and SSIM are calculated by converting RGB image to YUV and only the Y-channel image taken for consideration. During the testing, we rotated and flipped LR images for augmentation to gener-ate several augmented inputs, and then applied the inverse transform and averaged all the outputs together to form the final SR results. For different scaling factors s, we exclude s pixels at boundaries to avoid boundary effect. For SR results, SRCNN, VDSR, SRResNet, EDSR and DBPN were reimplemented and provided by the authors of <ref type="bibr" target="#b9">[10]</ref> and LapSRN was provided by the authors of <ref type="bibr" target="#b19">[18]</ref>. Note that, this of our proposed approach also participated in the NTIRE2019 Real Image Super-resolution Challenge <ref type="bibr" target="#b0">[1]</ref>. <ref type="table" target="#tab_6">Table 2</ref> also includes the validation testing results of this dataset. For this competition, it targets at real daily images, with down-sampling process using different degradation and distortions, and all images were taken by DSLR cameras in natural environments. However, all the stateof-the-art SR algorithms in the literature have been trained by using bicubic down-sampled images. It would then be inappropriate to use our HBPN model to make compari- From all the results, we can see that our proposed HBPN approach can achieve better SR performance both quantitatively and qualitatively. It not only preserves the edge components, but also reconstructs the fine textures at different scaling factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a Hierarchical Back Projection Network for image Super-Resolution on different up-scaling factors. Different from the previous SR study, we focus on feature extraction by conducting a HourGlass structure to learn the features in a bottom-up and top-down manner. The back projection mechanism is embedded into the network to update the low-resolution and high-resolution feature maps to reduce the errors. Meanwhile, we propose a self-weighting process that each HourGlass module generates one intermediate SR result along with its weighting map. By using the proposed Weighted Reconstruction block, we normalize the weighting maps to tune the contribution of each intermediate SR results for generating the final SR images. Results on quantitative and quality evaluation show its advantages over other approaches. Furthermore, we have also visualized the trained feature maps to illustrate the power of feature representation of each Hour-Glass module.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Proposed HBPN structure. In a bottom-up and top-down manner, it can explore various scales to extract hierarchical features for image SR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Proposed Enhanced back projection blocks: (a) the UBP block that up-samples the LR feature maps by 2× and reduces the number of feature maps by half, and (b) the DBP block that downsamples the HR feature maps by half and increases the number of feature maps by 2×.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Proposed SR-HG module structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Proposed Weighted Reconstruction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Back projection blocks analysis with different networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>The number of SR-HG modules analysis. Note that we compare networks using different numbers of SR-HG modules for repeated feature extraction. Visualization of activation maps, weighting maps and intermediate outputs. For better observation, please check the electronic version of thisfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Visual quality comparison among different SR algorithms on 8× super-resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Enhanced Back Projection block Conv Conv Deconv PReLU 11 Conv PReLU11 Conv Hierarchical SR-HG block Softmax based Weighted Reconstruction</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Weighted</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Reconstruction block</cell><cell></cell></row><row><cell>LR</cell><cell></cell><cell></cell><cell></cell><cell>Softmax</cell><cell>Weighting maps</cell><cell>SR results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sum of dot</cell></row><row><cell>RGB to YUV</cell><cell>Concat</cell><cell>SR-HG block 1</cell><cell>SR-HG block 2</cell><cell>SR-HG block 3</cell><cell>Intermediate outputs</cell><cell>product</cell></row><row><cell></cell><cell></cell><cell>l x 25625632</cell><cell></cell><cell>1  12812864 x l</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the network using plain concatenation block or WR reconstruction block, including PSNR and SSIM for scale 2×, 4× and 8× SR on Set5 and Set14. Red indicates the best results.</figDesc><table><row><cell>Algorithm</cell><cell>Scale</cell><cell cols="3">Set5 PSNR SSIM PSNR SSIM Set14</cell></row><row><cell>Plain model</cell><cell>2</cell><cell>37.95</cell><cell>0.959 33.61</cell><cell>0.917</cell></row><row><cell>WR model</cell><cell>2</cell><cell>38.13</cell><cell>0.961 33.78</cell><cell>0.921</cell></row><row><cell>Plain model</cell><cell>4</cell><cell>32.33</cell><cell>0.889 28.55</cell><cell>0.731</cell></row><row><cell>WR model</cell><cell>4</cell><cell>32.55</cell><cell>0.900 28.67</cell><cell>0.785</cell></row><row><cell>Plain model</cell><cell>8</cell><cell>26.89</cell><cell>0.761 24.81</cell><cell>0.632</cell></row><row><cell>WR model</cell><cell>8</cell><cell>27.17</cell><cell>0.785 24.96</cell><cell>0.642</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluation of state-of-the-art SR approaches, including PSNR and SSIM for scale 2×, 4× and 8×. Red indicates the best and blue indicates the second best results.</figDesc><table><row><cell>Algorithm</cell><cell>Scale</cell><cell cols="10">Set5 PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM Set14 BSD100 Urban100 Manga109</cell></row><row><cell>Bicubic</cell><cell></cell><cell>33.65</cell><cell>0.930</cell><cell cols="2">30.34 0.870</cell><cell>29.56</cell><cell>0.844</cell><cell>27.39</cell><cell cols="2">0.841 31.05</cell><cell>0.935</cell></row><row><cell>A+ [31]</cell><cell></cell><cell>36.54</cell><cell>0.954</cell><cell cols="2">32.40 0.906</cell><cell>31.22</cell><cell>0.887</cell><cell>29.23</cell><cell cols="2">0.894 35.33</cell><cell>0.967</cell></row><row><cell>CRFSR [22]</cell><cell></cell><cell>37.29</cell><cell>0.957</cell><cell cols="2">32.61 0.909</cell><cell>31.61</cell><cell>0.891</cell><cell>30.48</cell><cell cols="2">0.907 36.78</cell><cell>0.970</cell></row><row><cell>SRCNN [6]</cell><cell></cell><cell>36.65</cell><cell>0.954</cell><cell cols="2">32.29 0.903</cell><cell>31.36</cell><cell>0.888</cell><cell>29.52</cell><cell cols="2">0.895 35.72</cell><cell>0.968</cell></row><row><cell>VDSR [16]</cell><cell></cell><cell>37.53</cell><cell>0.958</cell><cell cols="2">32.97 0.913</cell><cell>31.90</cell><cell>0.896</cell><cell>30.77</cell><cell cols="2">0.914 37.16</cell><cell>0.974</cell></row><row><cell>DRRN [29]</cell><cell>2×</cell><cell>37.74</cell><cell>0.959</cell><cell>33.23</cell><cell>0.913</cell><cell>32.05</cell><cell>0.897</cell><cell>31.23</cell><cell cols="2">0.919 37.92</cell><cell>0.976</cell></row><row><cell>SRResNet [19]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LapSRN [18]</cell><cell></cell><cell>37.52</cell><cell>0.959</cell><cell cols="2">33.08 0.913</cell><cell>31.80</cell><cell>0.895</cell><cell>30.41</cell><cell cols="2">0.910 37.27</cell><cell>0.974</cell></row><row><cell>EDSR [20]</cell><cell></cell><cell>38.11</cell><cell>0.960</cell><cell cols="2">33.92 0.919</cell><cell>32.32</cell><cell>0.901</cell><cell>32.93</cell><cell cols="2">0.935 39.10</cell><cell>0.977</cell></row><row><cell>DBPN [10]</cell><cell></cell><cell>38.09</cell><cell>0.960</cell><cell cols="2">33.85 0.919</cell><cell>32.27</cell><cell>0.900</cell><cell>32.96</cell><cell cols="2">0.931 39.10</cell><cell>0.978</cell></row><row><cell>HBPN(Ours)</cell><cell></cell><cell>38.13</cell><cell>0.961</cell><cell cols="2">33.78 0.921</cell><cell>32.33</cell><cell>0.902</cell><cell>33.12</cell><cell cols="2">0.938 39.30</cell><cell>0.979</cell></row><row><cell>Bicubic</cell><cell></cell><cell>28.42</cell><cell>0.810</cell><cell cols="2">26.10 0.704</cell><cell>25.96</cell><cell>0.669</cell><cell>23.64</cell><cell cols="2">0.659 25.15</cell><cell>0.789</cell></row><row><cell>A+ [31]</cell><cell></cell><cell>30.30</cell><cell>0.859</cell><cell cols="2">27.43 0.752</cell><cell>26.82</cell><cell>0.710</cell><cell>24.34</cell><cell cols="2">0.720 27.02</cell><cell>0.850</cell></row><row><cell>CRFSR [22]</cell><cell></cell><cell>31.10</cell><cell>0.871</cell><cell cols="2">27.87 0.765</cell><cell>27.05</cell><cell>0.719</cell><cell>24.89</cell><cell cols="2">0.744 28.12</cell><cell>0.872</cell></row><row><cell>SRCNN [6]</cell><cell></cell><cell>30.49</cell><cell>0.862</cell><cell cols="2">27.61 0.754</cell><cell>26.91</cell><cell>0.712</cell><cell>24.53</cell><cell cols="2">0.724 27.66</cell><cell>0.858</cell></row><row><cell>VDSR [16]</cell><cell></cell><cell>31.35</cell><cell>0.882</cell><cell cols="2">28.03 0.770</cell><cell>27.29</cell><cell>0.726</cell><cell>25.18</cell><cell cols="2">0.753 28.82</cell><cell>0.886</cell></row><row><cell>DRRN [29]</cell><cell>4×</cell><cell>31.68</cell><cell>0.888</cell><cell>28.21</cell><cell>0.772</cell><cell>27.38</cell><cell>0.728</cell><cell>25.44</cell><cell cols="2">0.764 29.46</cell><cell>0.896</cell></row><row><cell>SRResNet [19]</cell><cell></cell><cell>32.05</cell><cell>0.891</cell><cell cols="2">28.53 0.780</cell><cell>27.57</cell><cell>0.735</cell><cell>26.07</cell><cell>0.784</cell><cell>-</cell><cell>-</cell></row><row><cell>LapSRN [18]</cell><cell></cell><cell>31.54</cell><cell>0.885</cell><cell cols="2">28.19 0.772</cell><cell>27.32</cell><cell>0.728</cell><cell>25.21</cell><cell cols="2">0.756 29.09</cell><cell>0.890</cell></row><row><cell>EDSR [20]</cell><cell></cell><cell>32.46</cell><cell>0.897</cell><cell cols="2">28.80 0.788</cell><cell>27.71</cell><cell>0.742</cell><cell>26.64</cell><cell cols="2">0.803 31.02</cell><cell>0.915</cell></row><row><cell>DBPN [10]</cell><cell></cell><cell>32.47</cell><cell>0.898</cell><cell cols="2">28.82 0.786</cell><cell>27.72</cell><cell>0.740</cell><cell>26.60</cell><cell cols="2">0.795 31.13</cell><cell>0.914</cell></row><row><cell>HBPN(Ours)</cell><cell></cell><cell>32.55</cell><cell>0.900</cell><cell cols="2">28.67 0.785</cell><cell>27.77</cell><cell>0.743</cell><cell>27.30</cell><cell cols="2">0.818 31.57</cell><cell>0.920</cell></row><row><cell>Bicubic</cell><cell></cell><cell>24.39</cell><cell>0.657</cell><cell cols="2">23.19 0.568</cell><cell>23.67</cell><cell>0.547</cell><cell>21.24</cell><cell cols="2">0.516 21.68</cell><cell>0.647</cell></row><row><cell>A+ [31]</cell><cell></cell><cell>25.52</cell><cell>0.692</cell><cell cols="2">23.98 0.597</cell><cell>24.20</cell><cell>0.568</cell><cell>21.37</cell><cell cols="2">0.545 22.39</cell><cell>0.680</cell></row><row><cell>CRFSR [22]</cell><cell></cell><cell>26.07</cell><cell>0.732</cell><cell cols="2">23.97 0.600</cell><cell>24.20</cell><cell>0.569</cell><cell>21.36</cell><cell cols="2">0.550 22.59</cell><cell>0.688</cell></row><row><cell>SRCNN [6]</cell><cell></cell><cell>25.33</cell><cell>0.689</cell><cell cols="2">23.85 0.593</cell><cell>24.13</cell><cell>0.565</cell><cell>21.29</cell><cell cols="2">0.543 22.37</cell><cell>0.682</cell></row><row><cell>VDSR [16]</cell><cell></cell><cell>25.72</cell><cell>0.711</cell><cell cols="2">24.21 0.609</cell><cell>24.37</cell><cell>0.576</cell><cell>21.54</cell><cell cols="2">0.560 22.83</cell><cell>0.707</cell></row><row><cell>DRRN [29]</cell><cell>8×</cell><cell>26.18</cell><cell>0.738</cell><cell>24.42</cell><cell>0.622</cell><cell>24.59</cell><cell>0.587</cell><cell>21.88</cell><cell cols="2">0.583 23.60</cell><cell>0.742</cell></row><row><cell>SRResNet [19]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LapSRN [18]</cell><cell></cell><cell>26.15</cell><cell>0.738</cell><cell cols="2">24.35 0.620</cell><cell>24.54</cell><cell>0.586</cell><cell>21.81</cell><cell cols="2">0.582 23.39</cell><cell>0.735</cell></row><row><cell>EDSR [20]</cell><cell></cell><cell>26.97</cell><cell>0.775</cell><cell cols="2">24.94 0.640</cell><cell>24.80</cell><cell>0.596</cell><cell>22.47</cell><cell cols="2">0.620 24.58</cell><cell>0.778</cell></row><row><cell>DBPN [10]</cell><cell></cell><cell>27.21</cell><cell>0.784</cell><cell cols="2">25.13 0.648</cell><cell>24.88</cell><cell>0.601</cell><cell>22.69</cell><cell cols="2">0.622 24.96</cell><cell>0.799</cell></row><row><cell>HBPN(Ours)</cell><cell></cell><cell>27.17</cell><cell>0.785</cell><cell cols="2">24.96 0.642</cell><cell>24.93</cell><cell>0.602</cell><cell>23.04</cell><cell cols="2">0.647 25.24</cell><cell>0.802</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">NTIRE2019 Validation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Algorithm</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PSNR</cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell></row><row><cell></cell><cell>Bicubic</cell><cell></cell><cell></cell><cell></cell><cell cols="2">29.548</cell><cell></cell><cell></cell><cell>0.844</cell><cell></cell></row><row><cell cols="4">Using the proposed plain HBPN</cell><cell></cell><cell cols="2">33.41</cell><cell></cell><cell></cell><cell>0.889</cell><cell></cell></row><row><cell cols="4">HBPN with Weighted Reconstruction</cell><cell></cell><cell cols="2">33.88</cell><cell></cell><cell></cell><cell>0.920</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.vision.ee.ethz.ch/ntire19/" />
		<title level="m">NTIRE 2019 Real Super-Resolution Challenge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-Complexity Single-Image Super-Resolution based on Nonnegative Neighbor Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Line Alberi</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<meeting><address><addrLine>Guildford, Surrey, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Superresolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<editor>I-I</editor>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1501.00092</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1803.02735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning hierarchical decision trees for single-image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="937" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast image interpolation via random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3232" to="3245" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1511.04587</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1511.04491</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1707.02921</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast image super-resolution via randomized multi-split forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cascaded random forests for fast image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<idno>abs/1510.04389</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1603.06937</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04366</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
		<idno>abs/1612.07919</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.05158</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A+: Adjusted anchored neighborhood regression for fast superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Desmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="volume">9006</biblScope>
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ntire 2018 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">ESRGAN: enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1809.00219</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Curves and Surfaces</title>
		<meeting>the 7th International Conference on Curves and Surfaces<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
