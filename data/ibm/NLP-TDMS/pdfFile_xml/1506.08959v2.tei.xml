<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Large-Scale Car Dataset for Fine-Grained Categorization and Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Shenzhen Key Lab of CVPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Shenzhen Key Lab of CVPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Shenzhen Key Lab of CVPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Large-Scale Car Dataset for Fine-Grained Categorization and Verification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper aims to highlight vision related tasks centered around "car", which has been largely neglected by vision community in comparison to other objects. We show that there are still many interesting car-related problems and applications, which are not yet well explored and researched. To facilitate future car-related research, in this paper we present our on-going effort in collecting a large-scale dataset, "CompCars", that covers not only different car views, but also their different internal and external parts, and rich attributes. Importantly, the dataset is constructed with a cross-modality nature, containing a surveillancenature set and a web-nature set. We further demonstrate a few important applications exploiting the dataset, namely car model classification, car model verification, and attribute prediction. We also discuss specific challenges of the car-related problems and other potential applications that worth further investigations. The latest dataset can be downloaded at http://mmlab.ie.cuhk.edu.hk/ datasets/comp_cars/index.html ** Update: This technical report serves as an extension to our earlier work <ref type="bibr" target="#b27">[28]</ref> published in CVPR 2015. The experiments shown in Sec. 5 gain better performance on all three tasks, i.e. car model classification, attribute prediction, and car model verification, thanks to more training data and better network structures. The experimental results can serve as baselines in any later research works. The settings and the train/test splits are provided on the project page. ** Update 2: This update provides preliminary experiment results for fine-grained classification on the surveillance data of CompCars. The train/test splits are provided in the updated dataset. See details in Section 6.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Cars represent a revolution in mobility and convenience, bringing us the flexibility of moving from place to place. The societal benefits (and cost) are far-reaching. Cars are now indispensable from our modern life as a vehicle for transportation. In many places, the car is also viewed as a tool to help project someone's economic status, or reflects our economic stratification. In addition, the car has evolved into a subject of interest amongst many car enthusiasts in the world. In general, the demand on car has shifted over the years to cover not only practicality and reliability, but also high comfort and design. The enormous number of car designs and car model makes car a rich object class, which can potentially foster more sophisticated and robust computer vision models and algorithms. Cars present several unique properties that other objects cannot offer, which provides more challenges and facilitates a range of novel research topics in object categorization. Specifically, cars own large quantity of models that most other categories do not have, enabling a more challenging fine-grained task. In addition, cars yield large appearance differences in their unconstrained poses, which demands viewpoint-aware analyses and algorithms (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). Importantly, a unique hierarchy is presented for the car category, which is three levels from top to bottom: make, model, and released year. This structure indicates a direction to address the fine-grained task in a hierarchical way, which is only discussed by limited literature <ref type="bibr" target="#b16">[17]</ref>. Apart from the categorization task, cars reveal a number of interesting computer vision problems. Firstly, different designing styles are applied by different car manufacturers and in different years, which opens the door to fine-grained style analysis <ref type="bibr" target="#b13">[14]</ref> and fine-grained part recognition (see <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). Secondly, the car is an attractive topic for attribute prediction. In particular, cars have distinctive attributes such as car class, seating capacity, number of axles, maximum speed and displacement, which can be inferred from the appearance of the cars (see <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). Lastly, in comparison to human face verification <ref type="bibr" target="#b21">[22]</ref>, car verification, which targets at verifying whether two cars belong to the same model, is an interesting and underresearched problem. The unconstrained viewpoints make car verification arguably more challenging than traditional face verification.</p><p>Automated car model analysis, particularly the finegrained car categorization and verification, can be used for innumerable purposes in intelligent transportation system including regulation, description and indexing. For instance, fine-grained car categorization can be exploited to inexpensively automate and expedite paying tolls from the lanes, based on different rates for different types of vehicles. In video surveillance applications, car verification from appearance helps tracking a car over a multiple camera network when car plate recognition fails. In post-event investigation, similar cars can be retrieved from the database with car verification algorithms. Car model analysis also bears significant value in the personal car consumption. When people are planning to buy cars, they tend to observe cars in the street. Think of a mobile application, which can instantly show a user the detailed information of a car once a car photo is taken. Such an application will provide great convenience when people want to know the information of an unrecognized car. Other applications such as predicting popularity based on the appearance of a car, and recommending cars with similar styles can be beneficial both for manufacturers and consumers.</p><p>Despite the huge research and practical interests, car model analysis only attracts few attentions in the computer vision community. We believe the lack of high quality datasets greatly limits the exploration of the community in this domain. To this end, we collect and organize a large-scale and comprehensive image database called "Comprehensive Cars", with "CompCars" being short. The "CompCars" dataset is much larger in scale and diversity compared with the current car image datasets, containing 208, 826 images of 1, 716 car models from two scenarios: web-nature and surveillance-nature. In addition, the dataset is carefully labelled with viewpoints and car parts, as well as rich attributes such as type of car, seat capacity, and door number. The new dataset dataset thus provides a comprehensive platform to validate the effectiveness of a wide range of computer vision algorithms. It is also ready to be utilized for realistic applications and enormous novel research topics. Moreover, the multi-scenario nature enables the use of the dataset for cross modality research. The detailed description of CompCars is provided in Section 3.</p><p>To validate the usefulness of the dataset and to encourage the community to explore for more novel research topics, we demonstrate several interesting applications with the dataset, including car model classification and verification based on convolutional neural network (CNN) <ref type="bibr" target="#b12">[13]</ref>. Another interesting task is to predict attributes from novel car models (see details in Section 4.2). The experiments reveal several challenges specific to the car-related problems. We conclude our analyses with a discussion in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most previous car model research focuses on car model classification. Zhang et al. <ref type="bibr" target="#b30">[31]</ref> propose an evolutionary computing framework to fit a wireframe model to the car on an image. Then the wireframe model is employed for car model recognition. Hsiao et al. <ref type="bibr" target="#b6">[7]</ref> construct 3D space curves using 2D training images, then match the 3D curves to 2D image curves using a 3D view-based alignment technique. The car model is finally determined with the alignment result. Lin et al. <ref type="bibr" target="#b14">[15]</ref> optimize 3D model fitting and fine-grained classification jointly. All these works are restricted to a small number of car models. Recently, Krause et al. <ref type="bibr" target="#b9">[10]</ref> propose to extract 3D car representation for classifying 196 car models. The experiment is the largest scale that we are aware of. Car model classification is a fine-grained categorization task. In contrast to general object classification, fine-grained categorization targets at recognizing the subcategories in one object class. Following this line of research, many studies have proposed different datasets on a variety of categories: birds <ref type="bibr" target="#b24">[25]</ref>, dogs <ref type="bibr" target="#b15">[16]</ref>, cars <ref type="bibr" target="#b9">[10]</ref>, flowers <ref type="bibr" target="#b18">[19]</ref>, etc. But all these datasets are limited by their scales and subcategory numbers.</p><p>To our knowledge, there is no previous attempt on the car model verification task. Closely related to car model verification, face verification has been a popular topic <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>. The recent deep learning based algorithms <ref type="bibr" target="#b21">[22]</ref> first train a deep neural network on human identity classification, then train a verification model with the feature extracted from the deep neural network. Joint Bayesian <ref type="bibr" target="#b1">[2]</ref> is a widely-used verification model that models two faces jointly with an appropriate prior on the face representation. We adopt Joint Bayesian as a baseline model in car model verification.</p><p>Attribute prediction of humans is a popular research topic in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref>. However, a large portion of the labeled attributes in the current attribute datasets <ref type="bibr" target="#b3">[4]</ref>, such as long hair and short pants lack strict criteria, which causes annotation ambiguities <ref type="bibr" target="#b0">[1]</ref>. The attributes with ambiguities will potentially harm the effectiveness of evaluation on related datasets. In contrast, the attributes provided by CompCars (e.g. maximum speed, door number, seat capacity) all have strict criteria since they are set by the car manufacturers. The dataset is thus advantageous over the current datasets in terms of the attributes validity.</p><p>Other car-related research includes detection <ref type="bibr" target="#b22">[23]</ref>, tracking <ref type="bibr" target="#b17">[18]</ref>  <ref type="bibr" target="#b25">[26]</ref>, joint detection and pose estimation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>, and 3D parsing <ref type="bibr" target="#b32">[33]</ref>. Fine-grained car models are not explored in these studies. Previous research related to car parts includes car logo recognition <ref type="bibr" target="#b19">[20]</ref> and car style analysis based on mid-level features <ref type="bibr" target="#b13">[14]</ref>.</p><p>Similar to CompCars, the Cars dataset <ref type="bibr" target="#b9">[10]</ref> also targets at fine-grained tasks on the car category. Apart from the larger-scale database, our CompCars dataset offers several significant benefits in comparison to the Cars dataset. First, our dataset contains car images diversely distributed in all viewpoints (annotated by front, rear, side, front-side, and rear-side), while Cars dataset mostly consists of frontside car images. Second, our dataset contains aligned car part images, which can be utilized for many computer vision algorithms that demand precise alignment. Third, our dataset provides rich attribute annotations for each car model, which are absent in the Cars dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Properties of CompCars</head><p>The CompCars dataset contains data from two scenarios, including images from web-nature and surveillance-nature. The images of the web-nature are collected from car forums, public websites, and search engines. The images of the surveillance-nature are collected by surveillance cameras. The data of these two scenarios are widely used in the real-world applications. They open the door for cross-modality analysis of cars. In particular, the web-nature data contains 163 car makes with 1, 716 car models, covering most of the commercial car models in the recent ten years. There are a total of 136, 727 images capturing the entire cars and 27, 618 images capturing the car parts, where most of them are labeled with attributes and viewpoints. The surveillance-nature data contains 44, 481 car images captured in the front view. Each image in the surveillance-nature partition is annotated with bounding box, model, and color of the car. <ref type="figure">Fig. 2</ref> illustrates some examples of surveillance images, which are affected by large variations from lightings and haze. Note that the data from the surveillance-nature are significantly different from the web-nature data in <ref type="figure" target="#fig_0">Fig. 1</ref>, suggesting the great challenges in cross-scenario car analysis. Overall, the CompCars dataset offers four unique features in comparison to existing car image databases, namely car hierarchy, car attributes, viewpoints, and car parts. Car Hierarchy The car models can be organized into a large tree structure, consisting of three layers , namely car make, car model, and year of manufacture, from top to bottom as depicted in <ref type="figure">Fig. 3</ref>. The complexity is further compounded by the fact that each car model can be produced in different years, yielding subtle difference in their appearances. For instance, three versions of "Audi A4L" were produced between 2009 to 2011 respectively.</p><p>Car Attributes Each car model is labeled with five attributes, including maximum speed, displacement, number of doors, number of seats, and type of car. These attributes provide rich information while learning the relations or similarities between different car models. For example, we define twelve types of cars, which are MPV, SUV, hatchback, sedan, minibus, fastback, estate, pickup, sports, crossover, convertible, and hardtop convertible, as shown in    interior parts (i.e. console, steering wheel, dashboard, and gear lever). These images are roughly aligned for the convenience of further analysis. A summary and some examples are given in <ref type="table">Table 2</ref> and <ref type="figure" target="#fig_3">Fig. 5</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>In this section, we study three applications using Com-pCars, including fine-grained car classification, attribute prediction, and car verification. We select 78, 126 images from the CompCars dataset and divide them into three subsets without overlaps. The first subset (Part-I) contains 431 car models with a total of 30, 955 images capturing the entire car and 20, 349 images capturing car parts. The second subset (Part-II) consists 111 models with 4, 454 images in total. The last subset (Part-III) contains 1, 145 car models with 22, 236 images. Fine-grained car classification is conducted using images in the first subset. For attribute prediction, the models are trained on the first subset but tested on the second one. The last subset is utilized for car verification.</p><p>We investigate the above potential applications using Convolutional Neural Network (CNN), which achieves great empirical successes in many computer vision problems, such as object classification <ref type="bibr" target="#b10">[11]</ref>, detection <ref type="bibr" target="#b4">[5]</ref>, face alignment <ref type="bibr" target="#b29">[30]</ref>, and face verification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref>. Specifically, we employ the Overfeat <ref type="bibr" target="#b20">[21]</ref> model, which is pretrained on ImageNet classification task <ref type="bibr" target="#b2">[3]</ref>, and fine-tuned with the car images for car classification and attribute prediction. For car model verification, the fine-tuned model is employed as a feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fine-Grained Classification</head><p>We classify the car images into 431 car models. For each car model, the car images produced in different years are considered as a single category. One may treat them as different categories, leading to a more challenging problem because their differences are relatively small. Our experiments have two settings, comprising fine-grained classification with the entire car images and the car parts. For both settings, we divide the data into half for training and another half for testing. Car model labels are regarded as training target and logistic loss is used to fine-tune the Overfeat model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">The Entire Car Images</head><p>We compare the recognition performances of the CNN models, which are fine-tuned with car images in specific viewpoints and all the viewpoints respectively, denoted as "front (F)", "rear (R)", "side (S)", "front-side (FS)", "rearside (RS)", and "All-View". The performances of these six models are summarized in <ref type="table" target="#tab_3">Table 3</ref>, where "FS" and "RS" achieve better performances than the performances of the other viewpoint models. Surprisingly, the "All-View" model yields the best performance, although it did not leverage the information of viewpoints. This result reveals that the CNN model is capable of learning discriminative representation across different views. To verify this observation, we visualize the car images that trigger high responses with respect to each neuron in the last fully-connected layer. As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, these neurons capture car images of specific car models across different viewpoints.</p><p>Several challenging cases are given in <ref type="figure">Fig. 7</ref>, where the images on the left hand side are the testing images and the images on the right hand side are the examples  of the wrong predictions (of the "All-View" model). We found that most of the wrong predictions belong to the same car makes as the test images. We report the "top-1" accuracies of car make classification in the last row of <ref type="table" target="#tab_3">Table 3</ref>, where the "All-View" model obtain reasonable good result, indicating that a coarse-to-fine (i.e. from car make to model) classification is possible for fine-grained car recognition.</p><p>To observe the learned feature space of the "All-View"  model, we project the features extracted from the last fullyconnected layer to a two-dimensional embedding space using multi-dimensional scaling. <ref type="figure">Fig. 8</ref> visualizes the projected features of twelve car models, where the images are chosen from different viewpoints. We observe that features from different models are separable in the 2D space and features of similar models are closer than those of dissimilar models. For instance, the distances between "BWM 5 Series" and "BWM 7 Series" are smaller than those between "BWM 5 Series" and "Chevrolet Captiva".</p><p>We also conduct a cross-modality experiment, where the CNN model fine-tuned by the web-nature data is evaluated on the surveillance-nature data. <ref type="figure" target="#fig_6">Fig. 9</ref> illustrates some predictions, suggesting that the model may account for data variations in a different modality to a certain extent. This experiment indicates that the features obtained from the web-nature data have potential to be transferred to data in the other scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Car Parts</head><p>Car enthusiasts are able to distinguish car models by examining the car parts. We investigate if the CNN model can mimic this strength. We train a CNN model using   images from each of the eight car parts. The results are reported in <ref type="table" target="#tab_5">Table 4</ref>, where "taillight" demonstrates the best accuracy. We visualize taillight images that have high responses with respect to each neuron in the last fullyconnected layer. <ref type="figure" target="#fig_0">Fig. 10</ref> displays such images with respect to two neurons. "Taillight" wins among the different car parts, mostly likely due to the relatively more distinctive designs, and the model name printed close to the taillight, which is a very informative feature for the CNN model. We also combine predictions using the eight car part models by voting strategy. This strategy significantly improves the performance due to the complementary nature of different car parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attribute Prediction</head><p>Human can easily identify the car attributes such as numbers of doors and seats from a proper viewpoint, without knowing the car model. For example, a car image captured in the side view provides sufficient information of the door number and car type, but it is hard to infer these In this section, we deliberately design a challenging experimental setting for attribute recognition, where the car models presented in the test images are exclusive from the training images. We fine-tune the CNN with the sumof-square loss to model the continuous attributes, such as "maximum speed" and "displacement", but a logistic loss to predict the discrete attributes such as "door number", "seat number", and "car type". For example, the "door number" has four states, i.e. {2, 3, 4, 5} doors, while "seat number" also has four states, i.e. {2, 4, 5, &gt; 5} seats. The attribute "car type" has twelve states as discussed in Sec. 3.</p><p>To study the effectiveness of different viewpoints for attribute prediction, we train CNN models for different viewpoints separately. <ref type="table">Table 5</ref> summarizes the results, where the "mean guess" represents the errors computed by using the mean of the training set as the prediction. We observe that the performances of "maximum speed" and "displacement" are insensitive to viewpoints. However, for the explicit attributes, the best accuracy is obtained under side view. We also found that the the implicit attributes are more difficult to predict then the explicit attributes. Several test images and their attribute predictions are provided in <ref type="figure" target="#fig_0">Fig. 11.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Car Verification</head><p>In this section, we perform car verification following the pipeline of face verification <ref type="bibr" target="#b21">[22]</ref>. In particular, we adopt the classification model in Section 4.1.1 as a feature extractor of the car images, and then apply Joint Bayesian <ref type="bibr" target="#b1">[2]</ref> to train a verification model on the Part-II data. Finally, we test the performance of the model on the Part-III data, which includes 1, 145 car models. The test data is organized into three sets, each of which has different difficulty, i.e. easy, medium, and hard. Each set contains 20, 000 pairs of images, including 10, 000 positive pairs and 10, 000 negative pairs. Each image pair in the "easy set" is selected from the same viewpoint, while each pair in the "medium set" is selected from a pair of random viewpoints. Each negative pair in the "hard set" is chosen from the same car make.</p><p>Deeply learned feature combined with Joint Bayesian has been proven successful for face verification <ref type="bibr" target="#b21">[22]</ref>. Joint Bayesian formulates the feature x as the sum of two independent Gaussian variables</p><formula xml:id="formula_0">x = µ + ,<label>(1)</label></formula><p>where µ ∼ N (0, S µ ) represents identity information, and ∼ N (0, S ) the intra-category variations. Joint Bayesian models the joint probability of two objects given the intra or extra-category variation hypothesis, P (x 1 , x 2 |H I ) and P (x 1 , x 2 |H E ). These two probabilities are also Gaussian with variations</p><formula xml:id="formula_1">Σ I = S µ + S S µ S µ S µ + S<label>(2)</label></formula><p>and</p><formula xml:id="formula_2">Σ E = S µ + S 0 0 S µ + S ,<label>(3)</label></formula><p>respectively. S µ and S can be learned from data with EM algorithm. In the testing stage, it calculates the likelihood ratio   each image pair is classified by comparing the likelihood ratio produced by Joint Bayesian with a threshold. This model is denoted as (CNN feature + Joint Bayesian). The second method combines the CNN features and SVM, denoted as CNN feature + SVM. Here, SVM is a binary classifier using a pair of image features as input. The label '1' represents positive pair, while '0' represents negative pair. We extract 100, 000 pairs of image features from Part-II data for training.</p><formula xml:id="formula_3">r(x 1 , x 2 ) = log P (x 1 , x 2 |H I ) P (x 1 , x 2 |H E ) ,<label>(4)</label></formula><p>The performances of the two models are shown in <ref type="table" target="#tab_7">Table 6</ref> and the ROC curves for the "hard set" are plotted in <ref type="figure" target="#fig_0">Fig. 14.</ref> We observe that CNN feature + Joint Bayesian outperforms CNN feature + SVM with large margins, indicating the advantage of Joint Bayesian for this task. However, its benefit in car verification is not as effective as in face verification, where CNN and Joint Bayesian nearly saturated the LFW dataset <ref type="bibr" target="#b7">[8]</ref> and approached human performance <ref type="bibr" target="#b21">[22]</ref>. <ref type="figure" target="#fig_0">Fig. 12</ref> depicts several pairs of test images as well as their predictions by CNN feature + Joint Bayesian. We observe two major challenges. First, for the image pair of the same model but different viewpoints, it is difficult to obtain the correspondences directly from the raw image pixels. Second, the appearances of different car models of the same car make are extremely similar. It is difficult to distinguish these car models using the entire images. Part localization or detection is crucial for car verification.  <ref type="figure" target="#fig_0">Figure 13</ref>. The ROC curves of two baseline models for the hard flavor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Updated Results: Comparing Different Deep Models</head><p>As an extension to the experiments in Section 4, we conduct experiments for fine-grained car classification, attribute prediction, and car verification with the entire dataset and different deep models, in order to explore the different capabilities of the models on these tasks. The split of the dataset into the three tasks is similar to Section 4, where three subsets contain 431, 111, and 1, 145 car models, with 52, 083, 11, 129, and 72, 962 images respectively. The only difference is that we adopt full set of CompCars in order to establish updated baseline experiments and to make use of the dataset to the largest extent. We keep the testing sets of car verification same to those in Section 4.3.</p><p>We evaluate three network structures, namely AlexNet <ref type="bibr" target="#b10">[11]</ref>, Overfeat <ref type="bibr" target="#b20">[21]</ref>, and GoogLeNet <ref type="bibr" target="#b23">[24]</ref> for all three tasks. All networks are pre-trained on the ImageNet classification task <ref type="bibr" target="#b2">[3]</ref>, and fine-tuned with the same mini-batch size, epochs, and learning rates for each task. All predictions of the deep models are produced with a single center crop of the image. We use Caffe <ref type="bibr" target="#b8">[9]</ref> as the platform for our experiments. The experimental results can serve as baselines in any later research works. The train/test splits can be downloaded from CompCars webpage http://mmlab.ie.cuhk.edu.hk/datasets/ comp_cars/index.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Fine-Grained Classification</head><p>In this section, we classify the car images into 431 car models as in Section 4.1.1. We divide the data into 70% for training and 30% for testing. We train classification models using car images in all viewpoints. The performances of the three networks are summarized in <ref type="table">Table 7</ref>. Overfeat beats AlexNet with a large margin of 6.0% while GoogLeNet beats Overfeat by 3.3% in Top-1 accuracy, which is in consistency with their performances on the ImageNet classification task. Given more data, the accuracy rises about 11% for Overfeat compared to <ref type="table" target="#tab_3">Table 3</ref> 1 . We also release the fine-tuned GoogLeNet model on the CompCars webpage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Attribute Prediction</head><p>We predict attributes from 111 models not existed in the training set. Different from Section 4.2 where models are trained with cars in single viewpoints, we train with images in all viewpoints to build a compact model. <ref type="table">Table 8</ref> summarizes the results for the three networks, where "mean guess" represents the prediction with the mean of the values on the training set. GoogLeNet performs the best for all attributes and Overfeat is a close running-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Car Verification</head><p>The evaluation pipeline follows Section 4.3. We evaluate the three deep models combined with two verification models: Joint Bayesian <ref type="bibr" target="#b1">[2]</ref> and SVM with polynomial kernel. The feature extracted from the CNN models is reduced to 200 by PCA before training and testing in all experiments.</p><p>The performances of the three networks combined with the two verification models are shown in <ref type="table" target="#tab_9">Table 9</ref>, where each model is denoted by {name of the deep model} + {name of the verification model}. GoogLeNet + Joint Bayesian achieves the best performance in all three settings. For each deep model, Joint Bayesian outperforms SVM consistently. Compared to <ref type="table" target="#tab_7">Table 6</ref>, Overfeat + Joint Bayesian yields a performance gain of 2 ∼ 4% in the three settings, which is purely due to the increase in training data. The ROC curves for the three sets are plotted in <ref type="figure" target="#fig_0">Figure 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Fine-Grained Classification with Surveillance Data</head><p>This is a follow-up experiment for fine-grained classification with surveillance-nature data. The data includes 44, 481 images in 281 different car models. 70% images are for training and 30% are for testing. The car images are all in front views with various environment conditions such as rainy, foggy, and at night. We adopt the same three network structures (AlexNet, Overfeat, and GoogLeNet) as in the web-nature data applications for this task. The networks are also pre-trained on the ImageNet classification task, and the test is done with a single center crop. The car images are first cropped with the labeled bounding boxes with paddings of around 7% on each side. All cropped images are resized to 256 × 256 pixels. The experimental results are shown in <ref type="table">Table 10</ref>. The three networks all achieve very high accuracies for this task. The result indicates that the fixed view (front view) greatly simplifies the finegrained classification task, even when large environmental differences exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussions</head><p>In this paper, we wish to promote the field of research related to "cars", which is largely neglected by the computer vision community. To this end, we have introduced a largescale car dataset called CompCars, which contains images with not only different viewpoints, but also car parts and rich attributes. CompCars provides a number of unique properties that other fine-grained datasets do not have, such as a much larger subcategory quantity, a unique hierarchical structure, implicit and explicit attributes, and large amount of car part images which can be utilized for style analysis and part recognition. It also bears cross modality nature, consisting of web-nature data and surveillance-nature data, ready to be used for cross modality research. To validate the usefulness of the dataset and inspire the community for other novel tasks, we have conducted baseline experiments on three tasks: car model classification, car model verification, and attribute prediction. The experimental results reveal several challenges of these tasks and provide qualitative observations of the data, which is beneficial for future research.</p><p>There are many other potential tasks that can exploit CompCars. Image ranking is one of the long-lasting topics in the literature, car model ranking can be adapted from this line of research to find the models that users are mostly interested in. The rich attributes of the dataset can be used to learn the relationships between different car models. Combining with the provided 3-level hierarchy, it will yield a stronger and more meaningful relationship graph for car models. Car images from different viewpoints can be utilized for ultra-wide baseline matching and 3D reconstruction, which can benefit recognition and verification in return.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Can you predict the maximum speed of a car with only a photo? Get some cues from the examples. (b) The two SUV models are very similar in their side views, but are rather different in the front views. (c) The evolution of the headlights of two car models from 2006 to 2014 (left to right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Sample images of the surveillance-nature data. The images have large appearance variations due to the varying conditions of light, weather, traffic, etc. The tree structure of car model hierarchy. Several car models of Audi A4L in different years are also displayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Each image displays a car from the 12 car types. The corresponding model names and car types are shown below the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Each column displays 8 car parts from a car model. The corresponding car models are Buick GL8, Peugeot 207 hatchback, Volkswagen Jetta, and Hyundai Elantra from left to right, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Images with the highest responses from two sample neurons. Each row corresponds to a neuron.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Sample test images that are mistakenly predicted as another model in their makes. Each row displays two samples and each sample is a test image followed by another image showing its mistakenly predicted model. The corresponding model name is shown under each image. The features of 12 car models that are projected to a twodimensional embedding using multi-dimensional scaling. Most features are separated in the 2D plane with regard to different models. Features extracted from similar models such as BWM 5 Series and BWM 7 Series are close to each other. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Top-5 predicted classes of the classification model for eight cars in the surveillance-nature data. Below each image is the ground truth class and the probabilities for the top-5 predictions with the correct class labeled in red. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Taillight images with the highest responses from two sample neurons. Each row corresponds to a neuron.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Sample attribute predictions for four car images. The continuous predictions of maximum speed and displacement are rounded to nearest proper values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Four test samples of verification and their prediction results. All these samples are very challenging and our model obtains correct results except for the last one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 .</head><label>14</label><figDesc>The ROC curves of six verification models for (a) easy, (b) medium, and (c) hard set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2</head><label>12</label><figDesc>Quantity distribution of the labeled car images in different viewpoints. The former group contains door number, seat number, and car type, which are represented by discrete values, while the latter group contains maximum speed and displacement (volume of an engine's cylinders), represented by continuous values. Humans can easily tell the numbers of doors and seats from a car's proper viewpoint, but hardly recognize its maximum speed and displacement. We conduct interesting experiments to predict these attributes in Section 4.2.Viewpoints We also label five viewpoints for each car model, including front (F), rear (R), side (S), front-side (FS), and rear-side (RS). These viewpoints are labeled by several professional annotators. The quantity distribution of the labeled car images is shown inTable 1. Note that the numbers of viewpoint images are not balanced among different car models, because the images of some less popular car models are difficult to collect.Car Parts We collect images capturing the eight car parts for each car model, including four exterior parts (i.e. headlight, taillight, fog light, and air intake) and four</figDesc><table><row><cell cols="3">Viewpoint No. in total No. per model</cell></row><row><cell>F</cell><cell>18431</cell><cell>10.9</cell></row><row><cell>R</cell><cell>13513</cell><cell>8.0</cell></row><row><cell>S</cell><cell>23551</cell><cell>14.0</cell></row><row><cell>FS</cell><cell>49301</cell><cell>29.2</cell></row><row><cell>RS</cell><cell>31150</cell><cell>18.5</cell></row><row><cell></cell><cell></cell><cell>per model</cell></row><row><cell>headlight</cell><cell>3705</cell><cell>2.2</cell></row><row><cell>taillight</cell><cell>3563</cell><cell>2.1</cell></row><row><cell>fog light</cell><cell>3177</cell><cell>1.9</cell></row><row><cell>air intake</cell><cell>3407</cell><cell>2.0</cell></row><row><cell>console</cell><cell>3350</cell><cell>2.0</cell></row><row><cell>steering wheel</cell><cell>3503</cell><cell>2.1</cell></row><row><cell>dashboard</cell><cell>3478</cell><cell>2.1</cell></row><row><cell>gear lever</cell><cell>3435</cell><cell>2.0</cell></row></table><note>. Quantity distribution of the labeled car part images. Part No. in total No.Fig. 4. Furthermore, these attributes can be partitioned into two groups: explicit and implicit attributes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Fine-grained classification results for the models trained on car images. Top-1 and Top-5 denote the top-1 and top-5 accuracy for car model classification, respectively. Make denotes the make level classification accuracy.</figDesc><table><row><cell>Viewpoint</cell><cell></cell><cell>F</cell><cell>R</cell><cell>S</cell><cell>FS</cell><cell>RS</cell><cell>All-View</cell></row><row><cell>Top-1</cell><cell cols="6">0.524 0.431 0.428 0.563 0.598</cell><cell>0.767</cell></row><row><cell>Top-5</cell><cell cols="6">0.748 0.647 0.602 0.769 0.777</cell><cell>0.917</cell></row><row><cell>Make</cell><cell cols="6">0.710 0.521 0.507 0.680 0.656</cell><cell>0.829</cell></row><row><cell>Suzuki Plough</cell><cell></cell><cell></cell><cell>Chevrolet Captiva</cell><cell></cell><cell>Benz S-Class</cell><cell cols="2">Mitsubishi Pajero</cell></row><row><cell cols="2">Suzuki Plough</cell><cell></cell><cell>Chevrolet Captiva</cell><cell></cell><cell>Benz S-Class</cell><cell cols="2">Suzuki Plough</cell></row><row><cell cols="2">Jeep Patriot</cell><cell></cell><cell>Lexus RX hybrid</cell><cell></cell><cell>Benz C-Class AMG</cell><cell cols="2">Mitsubishi Pajero</cell></row><row><cell cols="2">Benz G-Class AMG</cell><cell></cell><cell>Citroën DS3</cell><cell></cell><cell>Benz E-Class</cell><cell cols="2">Jinbei Sea Lion</cell></row><row><cell cols="2">Mini Clubman</cell><cell></cell><cell>Kia Sorento</cell><cell></cell><cell>Dongfeng Succe</cell><cell cols="2">Changcheng V80</cell></row><row><cell cols="2">Toyota Prado</cell><cell></cell><cell>Ford Mondeo</cell><cell></cell><cell>Hyundai Equus</cell><cell cols="2">Benz GLK-Class</cell></row><row><cell>Jeep Patriot</cell><cell></cell><cell cols="2">Volkswagen Phaeton</cell><cell></cell><cell>Mazda 6</cell><cell>Audi A6L</cell></row><row><cell cols="2">Suzuki Plough</cell><cell></cell><cell>Volkswagen CC</cell><cell></cell><cell>Mazda 5</cell><cell cols="2">Audi A5 convertible</cell></row><row><cell cols="2">Jeep Patriot</cell><cell></cell><cell>Volkswagen Phaeton</cell><cell></cell><cell>Citroën DS3</cell><cell cols="2">Audi TT Coupe</cell></row><row><cell cols="2">Mini Clubman</cell><cell></cell><cell>Volkswagen Magotan</cell><cell></cell><cell>Mazda 6</cell><cell cols="2">Audi A3 hatchback</cell></row><row><cell cols="2">BWM X3</cell><cell cols="2">Volkswagen Passat Lingyu</cell><cell cols="2">Geely Emgrand hatchback</cell><cell cols="2">Lamborghini Gallardo</cell></row><row><cell cols="2">BWM X5</cell><cell></cell><cell>Volkswagen Santana</cell><cell></cell><cell>Mazda 6 Coupe</cell><cell cols="2">Audi A5 Coupe</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Fine-grained classification results for the models trained on car parts. Top-1 and Top-5 denote the top-1 and top-5 accuracy for car model classification, respectively. Exterior parts Interior parts Headlight Taillight Fog light Air intake Console Steering wheel Dashboard Gear lever Voting</figDesc><table><row><cell>Top-1</cell><cell cols="2">0.479</cell><cell>0.684</cell><cell>0.387</cell><cell></cell><cell>0.484</cell><cell>0.535</cell><cell>0.540</cell><cell>0.502</cell><cell>0.355</cell><cell>0.808</cell></row><row><cell>Top-5</cell><cell cols="2">0.690</cell><cell>0.859</cell><cell>0.566</cell><cell></cell><cell>0.695</cell><cell>0.745</cell><cell>0.773</cell><cell>0.736</cell><cell>0.589</cell><cell>0.927</cell></row><row><cell cols="7">Table 5. Attribute prediction results for the five single viewpoint</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">models. For the continuous attributes (maximum speed and</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">displacement), we display the mean difference from the ground</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">truth. For the discrete attributes (door and seat number, car type),</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">we display the classification accuracy. Mean guess denotes the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">mean error with a prediction of the mean value on the training set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Viewpoint</cell><cell></cell><cell>F</cell><cell>R</cell><cell>S</cell><cell>FS</cell><cell>RS</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">mean difference</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Maximum speed</cell><cell>20.8</cell><cell>21.3</cell><cell>20.4</cell><cell>20.1</cell><cell>21.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(mean guess)</cell><cell></cell><cell>38.0</cell><cell>38.5</cell><cell>39.4</cell><cell>40.2</cell><cell>40.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Displacement</cell><cell></cell><cell cols="5">0.811 0.752 0.795 0.875 0.822</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(mean guess)</cell><cell></cell><cell>1.04</cell><cell>0.922</cell><cell>1.04</cell><cell>1.13</cell><cell>1.08</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">classification accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Door number</cell><cell></cell><cell cols="5">0.674 0.748 0.837 0.738 0.788</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Seat number</cell><cell></cell><cell cols="5">0.672 0.691 0.711 0.660 0.700</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Car type</cell><cell></cell><cell cols="5">0.541 0.585 0.627 0.571 0.612</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">attributes from the frontal view. The appearance of a car</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">also provides hints on the implicit attributes, such as the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">maximum speed and the displacement. For instance, a car</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">model is probably designed for high-speed driving, if it has</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">a low under-pan and a streamline body.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>The verification accuracy of three baseline models.</figDesc><table><row><cell></cell><cell cols="3">Easy Medium Hard</cell></row><row><cell cols="2">CNN feature + Joint Bayesian 0.833</cell><cell>0.824</cell><cell>0.761</cell></row><row><cell>CNN feature + SVM</cell><cell>0.700</cell><cell>0.690</cell><cell>0.659</cell></row><row><cell>random guess</cell><cell></cell><cell>0.500</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>The classification accuracies of three deep models. Attribute prediction results of three deep models. For the continuous attributes (maximum speed and displacement), we display the mean difference from the ground truth (lower is better). For the discrete attributes (door and seat number, car type), we display the classification accuracy (higher is better).</figDesc><table><row><cell cols="5">Model AlexNet Overfeat GoogLeNet</cell></row><row><cell>Top-1</cell><cell>0.819</cell><cell></cell><cell>0.879</cell><cell>0.912</cell></row><row><cell>Top-5</cell><cell>0.940</cell><cell></cell><cell>0.969</cell><cell>0.981</cell></row><row><cell>Model</cell><cell cols="4">AlexNet Overfeat GoogLeNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">mean difference</cell></row><row><cell cols="2">Maximum speed</cell><cell>21.3</cell><cell>19.4</cell><cell>19.4</cell></row><row><cell>(mean guess)</cell><cell></cell><cell></cell><cell>36.9</cell></row><row><cell>Displacement</cell><cell cols="2">0.803</cell><cell>0.770</cell><cell>0.760</cell></row><row><cell>(mean guess)</cell><cell></cell><cell></cell><cell>1.02</cell></row><row><cell></cell><cell></cell><cell cols="3">classification accuracy</cell></row><row><cell>Door number</cell><cell cols="2">0.750</cell><cell>0.780</cell><cell>0.796</cell></row><row><cell>Seat number</cell><cell cols="2">0.691</cell><cell>0.713</cell><cell>0.717</cell></row><row><cell>Car type</cell><cell cols="2">0.602</cell><cell>0.631</cell><cell>0.643</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>The verification accuracies of six models.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Easy Medium Hard</cell></row><row><cell cols="2">AlexNet + SVM</cell><cell>0.822</cell><cell>0.800</cell><cell>0.729</cell></row><row><cell cols="2">AlexNet + Joint Bayesian</cell><cell>0.853</cell><cell>0.823</cell><cell>0.774</cell></row><row><cell cols="2">Overfeat + SVM</cell><cell>0.860</cell><cell>0.830</cell><cell>0.754</cell></row><row><cell cols="2">Overfeat + Joint Bayesian</cell><cell>0.873</cell><cell>0.841</cell><cell>0.780</cell></row><row><cell cols="2">GoogLeNet + SVM</cell><cell>0.880</cell><cell>0.837</cell><cell>0.764</cell></row><row><cell cols="3">GoogLeNet + Joint Bayesian 0.907</cell><cell>0.852</cell><cell>0.788</cell></row><row><cell cols="5">Table 10. The classification accuracies of three deep models on</cell></row><row><cell>surveillance data.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Model AlexNet Overfeat GoogLeNet</cell></row><row><cell>Top-1</cell><cell>0.980</cell><cell>0.983</cell><cell>0.984</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Due to the difference in testing sets, the accuracies are not directly comparable. However a rough estimate is still viable.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Describing people: Poselet-based attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pedestrian attribute recognition at far distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parameterizing object detectors in the continuous pose space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Car make and model recognition using 3d curve alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Style-aware mid-level representation for discovering visual connections in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Jointly optimizing 3D model fitting and fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dog breed classification using part localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vehicle tracking across nonoverlapping cameras using joint kinematic and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Matei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samarasekera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVGIP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vehicle logo recognition using a sift-based enhanced matching scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Psyllos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kayafas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="322" to="328" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<title level="m">On-road vehicle detection: A review. T-PAMI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular multiview object tracking with 3d aspect parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object detection and viewpoint estimation with auto-masking neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<title level="m">Panda: Pose aligned networks for deep attribute modeling. CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Threedimensional deformable-model-based localization and recognition of road vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-view perceptron: a deep model for learning face identity and view representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Are cars just 3d boxes?-jointly estimating the 3d shape of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vision</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
