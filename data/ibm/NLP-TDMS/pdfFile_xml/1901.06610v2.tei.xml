<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Attentional Hybrid Neural Networks for Document Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jader</forename><surname>Abreu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Informática</orgName>
								<orgName type="institution">Universidade Federal de Pernambuco</orgName>
								<address>
									<addrLine>50.740-560</addrLine>
									<settlement>Recife</settlement>
									<region>PE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Fred</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Informática</orgName>
								<orgName type="institution">Universidade Federal de Pernambuco</orgName>
								<address>
									<addrLine>50.740-560</addrLine>
									<settlement>Recife</settlement>
									<region>PE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Macêdo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Informática</orgName>
								<orgName type="institution">Universidade Federal de Pernambuco</orgName>
								<address>
									<addrLine>50.740-560</addrLine>
									<settlement>Recife</settlement>
									<region>PE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cleber</forename><surname>Zanchettin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Informática</orgName>
								<orgName type="institution">Universidade Federal de Pernambuco</orgName>
								<address>
									<addrLine>50.740-560</addrLine>
									<settlement>Recife</settlement>
									<region>PE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Attentional Hybrid Neural Networks for Document Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Text classification · Attention mechanisms · Document clas- sification · Convolutional Neural Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document classification is a challenging task with important applications. The deep learning approaches to the problem have gained much attention recently. Despite the progress, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not take into account the contexting importance of words and sentences. In this paper, we propose a new approach based on a combination of convolutional neural networks, gated recurrent units, and attention mechanisms for document classification tasks. The main contribution of this work is the use of convolution layers to extract more meaningful, generalizable and abstract features by the hierarchical representation. The proposed method in this paper improves the results of the current attention-based approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is one of the most classical and important tasks in the machine learning field. The document classification, which is essential to organize documents for retrieval, analysis, and curation, is traditionally performed by classifiers such as Support Vector Machines or Random Forests. As in different areas, the deep learning methods are presenting a performance quite superior to traditional approaches in this field <ref type="bibr" target="#b4">[5]</ref>. Deep learning is also playing a central role in Natural Language Processing (NLP) through learned word vector representations. It aims to represent words in terms of fixed-length, continuous and dense feature vectors, capturing semantic word relations: similar words are close to each other in the vector space.</p><p>In most NLP tasks for document classification, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not take into account the contexting importance of words and sentences. Much of these approaches do not select qualitative or informative words Authors contributed equally and are both first authors. arXiv:1901.06610v2 [cs.CL] 28 Jun 2019 and sentences since some words are more informative than others in a document. Moreover, these models are frequently based on recurrent neural networks only <ref type="bibr" target="#b5">[6]</ref>. Since CNN has leveraged strong performance on deep learning models by extracting more abundant features and reducing the number of parameters, we guess it not only improves computational performance but also yields better generalization on neural models for document classification.</p><p>A recent trend in NLP is to use attentional mechanisms to modeling information dependencies without regard to their distance between words in the input sequences. In <ref type="bibr" target="#b5">[6]</ref> is proposed a hierarchical neural architecture for document classification, which employs attentional mechanisms, trying to mirror the hierarchical structure of the document. The intuition underlying the model is that not all parts of a text are equally relevant to represent it. Further, determining the relevant sections involves modeling the interactions and importance among the words and not just their presence in the text.</p><p>In this paper, we propose a new approach for document classification based on CNN, GRU <ref type="bibr" target="#b3">[4]</ref> hidden units and attentional mechanisms to improve the model performance by selectively focusing the network on essential parts of the text sentences during the model training. Inspired by <ref type="bibr" target="#b5">[6]</ref>, we have used the hierarchical concept to better representation of document structure. We call our model as Hierarchical Attentional Hybrid Neural Networks (HAHNN). We also used temporal convolutions <ref type="bibr" target="#b1">[2]</ref>, which give us more flexible receptive field sizes. We evaluate the proposed approach comparing its results with state-of-the-art models and the model shows an improved accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Hierarchical Attentional Hybrid Neural Networks</head><p>The HAHNN model combines convolutional layers, Gated Recurrent Units, and attention mechanisms. <ref type="figure">Figure 1</ref> shows the proposed architecture. The first layer of HAHNN is a pre-processed word embedding layer (black circles in the <ref type="figure">Figure 1</ref>). The second layer contains a stack of CNN layers that consist of convolutional layers with multiple filters (varying window sizes) and feature maps. We also have performed some trials with temporal convolutional layers with dilated convolutions and gotten promising results. Besides, we used Dropout for regularization. In the next layers, we use a word encoder applying the attention mechanism on word level context vector. In sequence, a sentence encoder applying the attention on sentence-level context vector. The last layer uses a Softmax function to generate the output probability distribution over the classes.</p><p>We use CNN to extract more meaningful, generalizable and abstract features by the hierarchical representation. Combining convolutional layers in different filter sizes with both word and sentence encoder in a hierarchical architecture let our model extract more rich features and improves generalization performance in document classification. To obtain representations of more rare words, by taking into account subwords information, we used FastText <ref type="bibr" target="#b2">[3]</ref> in the word embedding initialization.</p><p>We investigate two variants of the proposed architecture. There is a basic version, as described in <ref type="figure">Figure 1</ref>, and there is another which implements a TCN <ref type="bibr" target="#b1">[2]</ref> layer. The goal is to simulate RNNs with very long memory size by adopting a combination of dilated and regular convolutions with residual connections. Dilated convolutions are considered beneficial in longer sequences as they enable an exponentially larger receptive field in convolutional layers. More formally, for a 1-D sequence input x ∈ R n and a filter f : {0, ..., k − 1} → R, the dilated convolution operation F on element s of the sequence is defined as</p><formula xml:id="formula_0">F (s) = (x * d f )(s) = k−1 i=o f (i) · x s−d·i (1)</formula><p>where d is the dilatation factor, k is the filter size, and s − d · i accounts for the past information direction. Dilation is thus equivalent to introducing a fixed step between every two adjacent filter maps. When d = 1, a dilated convolution reduces to a regular convolution. The use of larger dilation enables an output at the top level to represent a wider range of inputs, expanding the receptive field. <ref type="figure">Fig. 1</ref>: Our HAHNN Architecture include an CNN layer after the embedding layer. In addition, we have created a variant which includes a temporal convolutional layer <ref type="bibr" target="#b1">[2]</ref> after the embedding layer.</p><p>The proposed model takes into account that the different parts of a document have no similar relevant information. Moreover, determining the relevant sections involves modeling the interactions among the words, not just their isolated presence in the text. Therefore, to consider this aspect, the model includes two levels of attention mechanisms <ref type="bibr" target="#b0">[1]</ref>. One structure at the word level and other at the sentence level, which let the model pay more or less attention to individual words and sentences when constructing the document representation.</p><p>The strategy consists of different parts: 1) A word sequence encoder and a word-level attention layer; and 2) A sentence encoder and a sentence-level attention layer. In the word encoder, the model uses bidirectional GRU <ref type="bibr" target="#b0">[1]</ref> to produce annotations of words by summarizing information from both directions. Therefore, it incorporates the contextual information in the annotation. The attention levels let the model pay more or less attention to individual words and sentences when constructing the representation of the document <ref type="bibr" target="#b5">[6]</ref>.</p><p>Given a sentence with words w it , t ∈ [0 , T ] and an embedding matrix W e , a bidirectional GRU contains the forward GRU − → f which reads the sentence s i from w i1 to w iT and a backward GRU ← − f which reads from w iT to w i1 :</p><formula xml:id="formula_1">x it = W e w it , t ∈ [1 , T ], (2) − → h it = − −− → GRU (x it ), t ∈ [1 , T ], (3) ← − h it = ← −− − GRU (x it ), t ∈ [T , 1 ].<label>(4)</label></formula><p>An annotation for a given word w it is obtained by concatenating the forward hidden state and backward hidden state, i.e.,</p><formula xml:id="formula_2">h it = [ − → h it , ← − h it ]</formula><p>, which summarizes the information of the whole sentence. We use the attention mechanism to evaluates words that are important to the meaning of the sentence and to aggregate the representation of those informative words into a sentence vector. Specifically,</p><formula xml:id="formula_3">u it = tanh(W w h it + b w ) (5) α it = exp(u it u w ) t exp(u it u w ) (6) s i = α it h it (7)</formula><p>The model measures the importance of a word as the similarity of u it with a word level context vector u w and learns a normalized importance weight α it through a softmax function. After that, the architecture computes the sentence vector s i as a weighted sum of the word annotations based on the weights. The word context vector u w is randomly initialized and jointly learned during the training process.</p><p>Given the sentence vectors s i , and the document vector, the sentence attention is obtained as:</p><formula xml:id="formula_4">− → h it = − −− → GRU (s i ), i ∈ [1 , L], (8) ← − h it = ← −− − GRU (s i ), i ∈ [L, 1 ].<label>(9)</label></formula><p>The proposed solution concatenates h i = [ − → h i , ← − h i ] h i which summarizes the neighbor sentences around sentence i but still focus on sentence i . To reward sentences that are relevant to correctly classify a document, the solution again use attention mechanism and introduce a sentence level context vector u s using it to measure the importance of the sentences:</p><formula xml:id="formula_5">u it = tanh(W s h i + b s ) (10) α it = exp(u i u s ) i exp(u i u s ) (11) v = α i h i<label>(12)</label></formula><p>In the above equation, v is the document vector that summarizes all the information of sentences in a document. Similarly, the sentence level context vector u s can be randomly initialized and jointly learned during the training process. The output of the sentence attention layer feeds a fully connected softmax layer.</p><p>It gives us a probability distribution over the classes. The proposed method is openly available in the github repository 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We evaluate the proposed model on two document classification datasets using 90% of the data for training and the remaining 10% for tests. We split documents into sentences and tokenize each sentence. The word embeddings have dimension 200 and we use Adam optimizer with a learning rate of 0.001. The datasets used are the IMDb Movie Reviews 2 and Yelp 2018 3 . The former contains a set of 25k highly polar movie reviews for training and 25k for testing, whereas the classification involves detecting positive/negative reviews. The latter include users ratings and write reviews about stores and services on Yelp, being a dataset for multiclass classification (ratings from 0-5 stars). Yelp 2018 contains around 5M full review text data, but we fix in 500k the number of used samples for computational purposes.  <ref type="table" target="#tab_0">Table 1</ref> shows the experiment results comparing our results with related works. Note that HN-ATT <ref type="bibr" target="#b5">[6]</ref> obtained an accuracy of 72,73% in the Yelp test set, whereas the proposed model obtained an accuracy of 73,28%. Our results also outperformed CNN <ref type="bibr" target="#b5">[6]</ref> and VDNN <ref type="bibr" target="#b6">[7]</ref>. We can see an improvement of the results in Yelp with our approach using CNN and varying window sizes in filters. The model also performs better in the results with IMDb using both CNN and TCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention Weights Visualizations</head><p>To validate the model performance in select informative words and sentences, we present the visualizations of attention weights in <ref type="figure" target="#fig_2">Figure 2</ref>. There is an example of the attention visualizations for a positive and negative class in test reviews. Every line is a sentence. Blue color denotes the sentence weight, and red denotes the word weight in determining the sentence meaning. There is a greater focus on more important features despite some exceptions. For example, the word "loving" and "amazed" in <ref type="figure" target="#fig_2">Figure 2</ref>    Occasionally, we have found issues in some sentences, where fewer important words are getting higher importance. For example, in <ref type="figure" target="#fig_2">Figure 2</ref> (a) notes that the word "translate" has received high importance even though it represents a neutral word. These drawbacks will be taken into account in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Final Remarks</head><p>In this paper, we have presented the HAHNN architecture for document classification. The method combines CNN with attention mechanisms in both word and sentence level. HAHNN improves accuracy in document classification by incorporate the document structure in the model and employing CNN's for the extraction of more abundant features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) and "disappointment" in Figure 2 (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) A positive example of visualization of a strong word in the sentence.(b) A negative example of visualization of a strong word in the sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Visualization of attention weights computed by the proposed model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results in classification accuracies.</figDesc><table><row><cell>Method</cell><cell cols="2">Accuracy on test set</cell></row><row><cell></cell><cell>Yelp 2018 (five classes)</cell><cell>IMDb (two classes)</cell></row><row><cell>VDNN [7]</cell><cell>62.14</cell><cell>79.47</cell></row><row><cell>HN-ATT [6]</cell><cell>72.73</cell><cell>89.02</cell></row><row><cell>CNN [5]</cell><cell>71.81</cell><cell>91.34</cell></row><row><cell cols="2">Our model with CNN 73.28</cell><cell>92.26</cell></row><row><cell cols="2">Our model with TCN 72.63</cell><cell>95.17</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/luisfredgs/cnn-hierarchical-network-for-document-classification 2 http://ai.stanford.edu/ amaas/data/sentiment/ 3 https://www.yelp.com/dataset/challenge</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Dzmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Kyunghyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><forename type="middle">;</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Zico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. North Am. Chapter of the Assoc. for Comp. Ling. 2016</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01781</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
