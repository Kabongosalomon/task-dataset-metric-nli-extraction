<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE JOURNAL OF EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 1 Light Gated Recurrent Units for Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Omologo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<title level="a" type="main">IEEE JOURNAL OF EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 1 Light Gated Recurrent Units for Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TETCI.2017.2762739</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speech recognition</term>
					<term>deep learning</term>
					<term>recurrent neural networks</term>
					<term>LSTM</term>
					<term>GRU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A field that has directly benefited from the recent advances in deep learning is Automatic Speech Recognition (ASR). Despite the great achievements of the past decades, however, a natural and robust human-machine speech interaction still appears to be out of reach, especially in challenging environments characterized by significant noise and reverberation. To improve robustness, modern speech recognizers often employ acoustic models based on Recurrent Neural Networks (RNNs), that are naturally able to exploit large time contexts and long-term speech modulations. It is thus of great interest to continue the study of proper techniques for improving the effectiveness of RNNs in processing speech signals.</p><p>In this paper, we revise one of the most popular RNN models, namely Gated Recurrent Units (GRUs), and propose a simplified architecture that turned out to be very effective for ASR. The contribution of this work is two-fold: First, we analyze the role played by the reset gate, showing that a significant redundancy with the update gate occurs. As a result, we propose to remove the former from the GRU design, leading to a more efficient and compact single-gate model. Second, we propose to replace hyperbolic tangent with ReLU activations. This variation couples well with batch normalization and could help the model learn long-term dependencies without numerical issues.</p><p>Results show that the proposed architecture, called Light GRU (Li-GRU), not only reduces the per-epoch training time by more than 30% over a standard GRU, but also consistently improves the recognition accuracy across different tasks, input features, noisy conditions, as well as across different ASR paradigms, ranging from standard DNN-HMM speech recognizers to endto-end CTC models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep learning is an emerging technology that is considered one of the most promising directions for reaching higher levels of artificial intelligence <ref type="bibr" target="#b0">[1]</ref>. This paradigm is rapidly evolving and some noteworthy achievements of the last years include, among the others, the development of effective regularization methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, improved optimization algorithms <ref type="bibr" target="#b3">[4]</ref>, and better architectures <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>. The exploration of generative models <ref type="bibr" target="#b8">[9]</ref>, deep reinforcement learning <ref type="bibr" target="#b9">[10]</ref> as well as the evolution of sequence to sequence paradigms <ref type="bibr" target="#b10">[11]</ref> also represent important milestones in the field. Deep learning is now being deployed in a wide range of domains, including bioinformatics, computer vision, machine translation, dialogue systems and natural language processing, just to name a few. Another field that has been transformed by this technology is Automatic Speech Recognition (ASR) <ref type="bibr" target="#b11">[12]</ref>. Thanks to modern Deep Neural Networks (DNNs), current speech recognizers are now able to significantly outperform previous GMM-HMM systems, allowing ASR to be applied in several contexts, such as web-search, intelligent personal assistants, car control and radiological reporting.</p><p>Despite the progress of the last decade, state-of-the-art speech recognizers are still far away from reaching satisfactory robustness and flexibility. This lack of robustness typically happens when facing challenging acoustic conditions <ref type="bibr" target="#b12">[13]</ref>, characterized by considerable levels of non-stationary noise and acoustic reverberation <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b21">[22]</ref>. The development of robust ASR has been recently fostered by the great success of some international challenges such as CHiME <ref type="bibr" target="#b22">[23]</ref>, REVERB <ref type="bibr" target="#b23">[24]</ref> and ASpIRE <ref type="bibr" target="#b24">[25]</ref>, which were also extremely useful to establish common evaluation frameworks among researchers.</p><p>Currently, the dominant approach to automatic speech recognition relies on a combination of a discriminative DNN and a generative Hidden Markov Model (HMM). The DNN is normally employed for acoustic modeling purposes to predict context-dependent phone targets. The acoustic-level predictions are later embedded in an HMM-based framework, that also integrates phone-transitions, lexicon, and language model information to retrieve the final sequence of words. An emerging alternative is end-to-end speech recognition, that aims to drastically simplify the current ASR pipeline by using fully discriminative systems that learn everything from data without (ideally) any additional human effort. Popular end-to-end techniques are attention models and Connectionist Temporal Classification (CTC). Attention models are based on an encoder-decoder architecture coupled with an attention mechanism <ref type="bibr" target="#b25">[26]</ref> that decides which input information to analyze at each decoding step. CTC <ref type="bibr" target="#b26">[27]</ref> is based on a DNN predicting symbols from a predefined alphabet (characters, phones, words) to which an extra unit (blank) that emits no labels is added. Similarly to HMMs, the likelihood (and its gradient with respect to the DNN parameters) are computed with dynamic programming by summing over all the paths that are possible realizations of the ground-truth label sequence. This way, CTC allows one to optimize the likelihood of the desired output sequence directly, without the need for an explicit label alignment.</p><p>For both the aforementioned frameworks, Recurrent Neural Networks (RNNs) represent a valid alternative to standard feed-forward DNNs. RNNs, in fact, are more and more often employed in speech recognition, due to their capabilities to arXiv:1803.10225v1 [eess.AS] <ref type="bibr" target="#b25">26</ref> Mar 2018 properly manage time contexts and capture long-term speech modulations.</p><p>In the machine learning community, the research of novel and powerful RNN models is a very active research topic. General-purpose RNNs such as Long Short Term Memories (LSTMs) <ref type="bibr" target="#b27">[28]</ref> have been the subject of several studies and modifications over the past years <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. This evolution has recently led to a novel architecture called Gated Recurrent Unit (GRU) <ref type="bibr" target="#b7">[8]</ref>, that simplifies the complex LSTM cell design.</p><p>Our work continues these efforts by further revising GRUs. Differently from previous efforts, our primary goal is not to derive a general-purpose RNN, but to modify the standard GRU design in order to better address speech recognition. In particular, the major contribution of this paper is twofold: First, we propose to remove the reset gate from the network design. Similarly to <ref type="bibr" target="#b30">[31]</ref>, we found that removing it does not significantly affect the system performance, also due to a certain redundancy observed between update and reset gates. Second, we propose to replace hyperbolic tangent (tanh) with Rectified Linear Unit (ReLU) activations <ref type="bibr" target="#b31">[32]</ref> in the state update equation. ReLU units have been shown to be more effective than sigmoid non-linearities for feed-forward DNNs <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Despite its recent success, this non-linearity has largely been avoided for RNNs, due to the numerical instabilities caused by the unboundedness of ReLU activations: composing many times GRU layers (i.e., GRU units following an affine transformation) with sufficiently large weights can lead to arbitrarily large state values. However, when coupling our ReLU-based GRU with batch normalization <ref type="bibr" target="#b2">[3]</ref>, we did not experience such numerical issues. This allows us to take advantage of both techniques, that have been proven effective to mitigate the vanishing gradient problem as well as to speed up network training.</p><p>We evaluated our proposed architecture on different tasks, datasets, input features, noisy conditions as well as on different ASR frameworks (i.e., DNN-HMM and CTC). Results show that the revised architecture reduces the per-epoch training wall-clock time by more than 30%, while improving the recognition accuracy. Moreover, the proposed solution leads to a compact model, that is arguably easier to interpret, understand and implement, due to a simplified design based on a single gate.</p><p>The rest of the paper is organized as follows. Sec. II recalls the standard GRU architecture, while Sec. III illustrates in detail the proposed model and the related work. In Sec. IV, a description of the adopted corpora and experimental setup is provided. The results are then reported in Sec. V. Finally, our conclusions are drawn in Sec. VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. GATED RECURRENT UNITS</head><p>The most suitable architecture able to learn short and long-term speech dependencies is represented by RNNs <ref type="bibr" target="#b11">[12]</ref>. RNNs, indeed, can potentially capture temporal information in a very dynamic fashion, allowing the network to freely decide the amount of contextual information to use for each time step. Several works have already highlighted the effectiveness of RNNs in various speech processing tasks, such as speech recognition <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b38">[39]</ref>, speech enhancement <ref type="bibr" target="#b39">[40]</ref>, speech separation <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> as well as speech activity detection <ref type="bibr" target="#b42">[43]</ref>. Training RNNs, however, can be complicated by vanishing and exploding gradients, that might impair learning longterm dependencies <ref type="bibr" target="#b43">[44]</ref>. Although exploding gradients can be tackled with simple clipping strategies <ref type="bibr" target="#b44">[45]</ref>, the vanishing gradient problem requires special architectures to be properly addressed. A common approach relies on the so-called gated RNNs, whose core idea is to introduce a gating mechanism for better controlling the flow of the information through the various time-steps. Within this family of architectures, vanishing gradient issues are mitigated by creating effective "shortcuts", in which the gradients can bypass multiple temporal steps.</p><p>The most popular gated RNNs are LSTMs <ref type="bibr" target="#b27">[28]</ref>, that often achieve state-of-the-art performance in several machine learning tasks, including speech recognition <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b39">[40]</ref>. LSTMs rely on memory cells that are controlled by forget, input, and output gates. Despite their effectiveness, such a sophisticated gating mechanism might result in an overly complex model. On the other hand, computational efficiency is a crucial issue for RNNs and considerable research efforts have recently been devoted to the development of alternative architectures <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b45">[46]</ref>.</p><p>A noteworthy attempt to simplify LSTMs has recently led to a novel model called Gated Recurrent Unit (GRU) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b46">[47]</ref>, that is based on just two multiplicative gates. In particular, the standard GRU architecture is defined by the following equations:</p><formula xml:id="formula_0">z t = σ(W z x t + U z h t−1 + b z ), (1a) r t = σ(W r x t + U r h t−1 + b r ),<label>(1b)</label></formula><formula xml:id="formula_1">h t = tanh(W h x t + U h (h t−1 r t ) + b h ),<label>(1c)</label></formula><formula xml:id="formula_2">h t = z t h t−1 + (1 − z t ) h t .<label>(1d)</label></formula><p>where z t and r t are vectors corresponding to the update and reset gates, respectively, while h t represents the state vector for the current time frame t. Element-wise multiplications are denoted with . The activations of both gates are logistic sigmoid functions σ(·), that constrain z t and r t to take values ranging from 0 and 1. The candidate state h t is processed with a hyperbolic tangent. The network is fed by the current input vector x t (e.g., a vector of speech features), while the parameters of the model are the matrices W z , W r , W h (the feedforward connections) and U z , U r , U h (the recurrent weights). The architecture finally includes trainable bias vectors b z , b r and b h , that are added before the non-linearities are applied.</p><p>As shown in Eq. 1d, the current state vector h t is a linear interpolation between the previous activation h t−1 and the current candidate state h t . The weighting factors are set by the update gate z t , that decides how much the units will update their activations. This linear interpolation is the key component for learning long-term dependencies. If z t is close to one, in fact, the previous state is kept unaltered and can remain unchanged for an arbitrary number of time steps. On the other hand, if z t is close to zero, the network tends to favor the candidate state h t , that depends more heavily on the current input and on the closer hidden states. The candidate state h t <ref type="figure">Fig. 1</ref>: Average activations of the update and reset gates for a GRU trained on TIMIT in a chunk of the utterance "sx403" of speaker "faks0". also depends on the reset gate r t , that allows the model to possibly delete the past memory by forgetting the previously computed states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. A NOVEL GRU FRAMEWORK</head><p>The main changes to the standard GRU model concern the reset gate, ReLU activations, and batch normalization, as outlined in the next sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Removing the reset gate</head><p>From the previous introduction to GRUs, it follows that the reset gate can be useful when significant discontinuities occur in the sequence. For language modeling, this may happen when moving from one text to another that is not semantically related. In such situation, it is convenient to reset the stored memory to avoid taking a decision biased by an unrelated history.</p><p>Nevertheless, we believe that for some specific tasks like speech recognition this functionality might not be useful. In fact, a speech signal is a sequence that evolves rather slowly (the features are typically computed every 10 ms), in which the past history can virtually always be helpful. Even in the presence of strong discontinuities, for instance observable at the boundary between a vowel and a fricative, completely resetting the past memory can be harmful. On the other hand, it is helpful to memorize phonotactic features, since some phone transitions are more likely than others.</p><p>We also argue that a certain redundancy in the activations of reset and update gates might occur when processing speech sequences. For instance, when it is necessary to give more importance to the current information, the GRU model can set small values of r t . A similar effect can be achieved with the update gate only, if small values are assigned to z t . The latter solution tends to weight more the candidate state h t , that depends heavily on the current input. Similarly, a high value can be assigned either to r t or to z t , in order to place more importance on past states. This redundancy is also highlighted in <ref type="figure">Fig. 1</ref>, where a temporal correlation in the average activations of update and reset gates can be readily appreciated for a GRU trained on TIMIT. This degree of redundancy will be analyzed in a quantitative way in Sec. V using the cross-correlation metric C(z, r):</p><formula xml:id="formula_3">C(z, r) = z t r t (2)</formula><p>where z t and r t are the average activations (over the neurons) of update and reset gates, respectively, and is the crosscorrelation operator. Based on these reasons, the first variation to standard GRUs thus concerns the removal of the reset gate r t . This change leads to the following modification of Eq. 1c:</p><formula xml:id="formula_4">h t = tanh(W h x t + U h h t−1 + b h )<label>(3)</label></formula><p>The main benefits of this intervention are related to the improved computational efficiency, that is achieved thanks to a more compact single-gate model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ReLU activations</head><p>The second modification consists in replacing the standard hyperbolic tangent with ReLU activation. In particular, we modify the computation of candidate state h t (Eq. 1c), as follows:</p><formula xml:id="formula_5">h t = ReLU(W h x t + U h h t−1 + b h )<label>(4)</label></formula><p>Standard tanh activations are less used in feedforward networks because they do not work as well as piecewise-linear activations when training deeper networks <ref type="bibr" target="#b47">[48]</ref>. The adoption of ReLU-based neurons, that have shown to be effective in improving such limitations, was not so common in the past for RNNs. This was due to numerical instabilities originating from the unbounded ReLU functions applied over long time series. However, coupling this activation function with batch normalization turned out to be helpful for taking advantage of ReLU neurons without numerical issues, as will be discussed in the next sub-section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Batch Normalization</head><p>Batch normalization <ref type="bibr" target="#b2">[3]</ref> has been recently proposed in the machine learning community and addresses the so-called internal covariate shift problem by normalizing the mean and the variance of each layer's pre-activations for each training minibatch. Several works have already shown that this technique is effective both to improve the system performance and to speed-up the training procedure <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Batch normalization can be applied to RNNs in different ways. In <ref type="bibr" target="#b48">[49]</ref>, the authors suggest to apply it to feed-forward connections only, while in <ref type="bibr" target="#b49">[50]</ref> the normalization step is extended to recurrent connections, using separate statistics for each time-step. In our work, we tried both approaches, but we did not observe substantial benefits when extending batch normalization to recurrent parameters (i.e., U h and U z ). For this reason, we applied this technique to feed-forward connections only (i.e., W h and W z ), obtaining a more compact model that is almost equally performing but significantly less computationally expensive. When batch normalization is limited to feed-forward connections, indeed, all the related computations become independent at each time step and they can be performed in parallel. This offers the possibility to apply it with reduced computational efforts. As outlined in the previous sub-section, coupling the proposed model with batchnormalization <ref type="bibr" target="#b2">[3]</ref> could also help in limiting the numerical issues of ReLU RNNs. Batch normalization, in fact, rescales the neuron pre-activations, inherently bounding the values of the ReLU neurons. In this way, our model concurrently takes advantage of the well-known benefits of both ReLU activation and batch normalization. In our experiments, we found that the latter technique helps against numerical issues also when it is limited to feed-forward connections only.</p><p>Formally, removing the reset gate, replacing the hyperbolic tangent function with the ReLU activation, and applying batch normalization, now leads to the following model:</p><formula xml:id="formula_6">z t = σ(BN (W z x t ) + U z h t−1 ),<label>(5a)</label></formula><formula xml:id="formula_7">h t = ReLU(BN (W h x t ) + U h h t−1 ), (5b) h t = z t h t−1 + (1 − z t ) h t .<label>(5c)</label></formula><p>The batch normalization BN (·) works as described in <ref type="bibr" target="#b2">[3]</ref>, and is defined as follows:</p><formula xml:id="formula_8">BN (a) = γ a − µ b σ 2 b + + β<label>(6)</label></formula><p>where µ b and σ b are the minibatch mean and variance, respectively. A small constant is added for numerical stability.</p><p>The variables γ and β are trainable scaling and shifting parameters, introduced to restore the network capacity. Note that the presence of β makes the biases b h and b z redundant. Therefore, they are omitted in Eq. 5a and 5b. We called this architecture Light GRU (Li-GRU), to emphasize the simplification process conducted on a standard GRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Related work</head><p>A first attempt to remove r t from GRUs has recently led to a single-gate architecture called Minimal Gated Recurrent Unit (M-GRU) <ref type="bibr" target="#b30">[31]</ref>, that achieves a performance comparable to that obtained by standard GRUs in handwritten digit recognition as well as in a sentiment classification task. To the best of our knowledge, our contribution is the first attempt that explores this architectural variation in speech recognition. Recently, some attempts have also been done for embedding ReLU units in the RNN framework. For instance, in <ref type="bibr" target="#b50">[51]</ref> authors replaced tanh activations with ReLU neurons in a vanilla RNN, showing the capability of this model to learn long-term dependencies when a proper orthogonal initialization is adopted. In this work, we extend the use of ReLU to a GRU architecture.</p><p>In summary, the novelty of our approach consists in the integration of three key design aspects (i.e, the removal of the reset gate, ReLU activations and batch normalization) in a single model, that turned out to be particularly suitable for speech recognition. The potential benefits Li-GRUs have been preliminarily observed as part of a work on speech recognition described in <ref type="bibr" target="#b51">[52]</ref>. This study extends our previous effort in several ways. First of all, we better analyze the correlation````````F  arising between reset and update gates. We then analyze some gradient statistics, and we better study the impact of batch normalization. Moreover, we assess our approach on a larger variety of speech recognition tasks, considering several different datasets as well as noisy and reverberant conditions. Finally, we extend our experimental validation to a end-to-end CTC model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>In the following sub-sections, the considered corpora, the RNN setting as well as the HMM-DNN and CTC setups are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Corpora and tasks</head><p>The main features of the corpora considered in this work are summarized in Tab. I.</p><p>A first set of experiments with the TIMIT corpus was performed to test the proposed model in a close-talking scenario. These experiments are based on the standard phoneme recognition task, which is aligned with that proposed in the Kaldi s5 recipe <ref type="bibr" target="#b52">[53]</ref>.</p><p>To validate our model in a more realistic scenario, a set of experiments was also conducted in distant-talking conditions with the DIRHA-English corpus 2 <ref type="bibr" target="#b53">[54]</ref>. The reference context was a domestic environment characterized by the presence of non-stationary noise (with an average SNR of about 10dB) and acoustic reverberation (with an average reverberation time T 60 of about 0.7 seconds). Training was based on the original WSJ-5k corpus (consisting of 7138 sentences uttered by 83 speakers) that was contaminated with a set of impulse responses measured in a real apartment. The test phase was carried out with both real and simulated datasets, each consisting of 409 WSJ sentences uttered by six native American speakers. A development set of 310 WSJ sentences uttered by six different speakers was also used for hyperparameter tuning. To test our approach in different reverberation conditions, other contaminated versions of the latter training and test data are generated with different impulse responses. These simulations are based on the image method <ref type="bibr" target="#b54">[55]</ref> and correspond to four different reverberation times T 60 (ranging from 250 to 1000 ms). More details on the realistic impulse responses adopted in this corpus can be found in <ref type="bibr" target="#b55">[56]</ref>- <ref type="bibr" target="#b57">[58]</ref>.</p><p>Additional experiments were conducted with the CHiME 4 dataset <ref type="bibr" target="#b38">[39]</ref>, that is based on both real and simulated data recorded in four noisy environments (on a bus, cafe, pedestrian area, and street junction). The training set is composed of 43690 noisy WSJ sentences recored by five microphones (arranged on a tablet) and uttered by a total of 87 speakers. The development set (DT) is based on 3280 WSJ sentences uttered by four speakers (1640 are real utterances referred to as DT-real, and 1640 are simulated denoted as DT-sim). The test set (ET) is based on 1320 real utterances (ET-real) and 1320 simulated sentences (DT-real) from other four speakers. The experiments reported in this paper are based on the single channel setting, in which the test phase is carried out with a single microphone (randomly selected from the considered microphone setup). More information on CHiME data can be found in <ref type="bibr" target="#b22">[23]</ref>.</p><p>To evaluate the proposed model on a larger scale ASR task, some additional experiments were performed with the TEDtalk dataset, that was released in the context of the IWSLT evaluation campaigns <ref type="bibr" target="#b58">[59]</ref>. The training set is composed of 820 talks with a total of about 166 hours of speech. The development test is composed of 81 talks (16 hours), while the test sets (TST 2011 and TST 2012) are based on 8 talks (1.5 hours) and 32 talks (6.5 hours), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RNN setting</head><p>The architecture adopted for the experiments consisted of multiple recurrent layers, that were stacked together prior to the final softmax classifier. These recurrent layers were bidirectional RNNs <ref type="bibr" target="#b34">[35]</ref>, which were obtained by concatenating the forward hidden states (collected by processing the sequence from the beginning to the end) with backward hidden states (gathered by scanning the speech in the reverse time order). Recurrent dropout was used as regularization technique. Since extending standard dropout to recurrent connections hinders learning long-term dependencies, we followed the approach introduced in [60], <ref type="bibr" target="#b60">[61]</ref>, that tackles this issue by sharing the same dropout mask across all the time steps. Moreover, batch normalization was adopted exploiting the method suggested in <ref type="bibr" target="#b48">[49]</ref>, as discussed in Sec. II. The feedforward connections of the architecture were initialized according to the Glorot's scheme <ref type="bibr" target="#b47">[48]</ref>, while recurrent weights were initialized with orthogonal matrices <ref type="bibr" target="#b50">[51]</ref>. Similarly to <ref type="bibr" target="#b19">[20]</ref>, the gain factor γ of batch normalization was initialized to 0.1 and the shift parameter β was initialized to 0.</p><p>Before training, the sentences were sorted in ascending order according to their lengths and, starting from the shortest utterance, minibatches of 8 sentences were progressively processed by the training algorithm. This sorting approach minimizes the need of zero-paddings when forming minibatches, resulting helpful to avoid possible biases on batch normalization statistics. Moreover, the sorting approach exploits a curriculum learning strategy <ref type="bibr" target="#b61">[62]</ref> that has been shown to slightly improve the performance and to ensure numerical stability of gradients. The optimization was done using the Adaptive Moment Estimation (Adam) algorithm <ref type="bibr" target="#b3">[4]</ref> running for 22 epochs (35 for the TED-talk corpus) with β 1 = 0.9, β 2 = 0.999, = 10 −8 . The performance on the development set was monitored after each epoch, while the learning rate was halved when the performance improvement went below a certain threshold (th = 0.001). Gradient truncation was not applied, allowing the system to learn arbitrarily long time dependencies.</p><p>The main hyperparameters of the model (i.e., learning rate, number of hidden layers, hidden neurons per layer, dropout factor) were optimized on the development data. In particular, we guessed some initial values according to our experience, and starting from them we performed a grid search to progressively explore better configurations. A total of 20-25 experiments were conducted for all the various RNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DNN-HMM setup</head><p>In the DNN-HMM experiments, the DNN is trained to predict context-dependent phone targets. The feature extraction is based on blocking the signal into frames of 25 ms with an overlap of 10 ms. The experimental activity is conducted considering different acoustic features, i.e., 39 MFCCs (13 static+∆+∆∆), 40 log-mel filter-bank features (FBANKS), as well as 40 fMLLR features (extracted as reported in the s5 recipe of Kaldi <ref type="bibr" target="#b52">[53]</ref>).</p><p>The labels were derived by performing a forced alignment procedure on the original training datasets. See the standard s5 recipe of Kaldi for more details <ref type="bibr" target="#b52">[53]</ref>. During test, the posterior probabilities generated for each frame by the RNN are normalized by their prior probabilities. The obtained likelihoods are processed by an HMM-based decoder, that, after integrating the acoustic, lexicon and language model information in a single search graph, finally estimates the sequence of words uttered by the speaker. The RNN part of the ASR system was implemented with Theano <ref type="bibr" target="#b62">[63]</ref>, that was coupled with the Kaldi decoder <ref type="bibr" target="#b52">[53]</ref> to form a context-dependent RNN-HMM speech recognizer 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CTC setup</head><p>The models used for the CTC experiments consisted of 5 layers of bidirectional RNNs of either 250 or 465 units. Unlike in the other experiments, weight noise was used for regularization. The application of weight noise is a simplification of adaptive weight noise <ref type="bibr" target="#b63">[64]</ref> and has been successfully used before to regularize CTC-LSTM models <ref type="bibr" target="#b64">[65]</ref>. The weight noise was applied to all the weight matrices and sampled from a zero mean normal distribution with a standard deviation of 0.01. Batch normalization was used with the same initialization settings as in the other experiments. Glorot's scheme was used to initialize all the weights (also the recurrent ones). The input features for these experiments were 123 dimensional FBANK features (40 + energy + ∆+∆∆). These features were also used in the original work on CTC-LSTM models for speech recognition <ref type="bibr" target="#b64">[65]</ref>. The CTC layer itself was trained on the 61 label set. Decoding was done using the best-path method <ref type="bibr" target="#b26">[27]</ref>, without adding any external phone-based language model. After decoding, the labels were mapped to the final 39 label set. The models were trained for 50 epochs using batches of 8 utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>In the following sub-sections, we describe the experimental activity conducted to assess the proposed model. Most of the experiments reported in the following are based on hybrid DNN-HMM speech recognizers, since the latter ASR paradigm typically reaches state-of-the-art performance. However, for the sake of comparison, we also extended the experimental validation to an end-to-end CTC model. More precisely, in sub-section V-A, we first quantitatively analyze the correlations between the update and reset gates in a standard GRU. In sub-section V-B, we extend our study with some analysis of gradient statistics. The role of batch normalization and the CTC experiments are described in sub-sections V-C and V-D, respectively. The speech recognition performance will then be reported for TIMIT, DIRHA-English, CHiME as well as for the TED-talk corpus in sub-sections V-E, V-F, V-G, and V-H, respectively. The computational benefits of Li-GRU are finally discussed in sub-section V-I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Correlation analysis</head><p>The correlation between update and reset gates have been preliminarily discussed in Sec. II. In this sub-section, we take a step further by analyzing it in a quantitative way using the cross-correlation function defined in Eq. 2. In particular, the cross-correlation C(z, r) between the average activations of update z and reset r gates is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. The gate activations are computed for all the input frames and, at each time step, an average over the hidden neurons is considered. The cross-correlation C(z, r) is displayed along with the autocorrelation C(z, z), that represents the upper-bound limit of the former function. <ref type="figure" target="#fig_0">Fig. 2</ref> clearly shows a high peak of C(z, r), revealing that update and reset gates end up being redundant. This peak is about 66% of the maximum of C(z, z) and it is centered at t = 0, indicating that almost no-delay occurs between gate activations. This result is obtained with a single-layer GRU of 200 bidirectional neurons fed with MFCC features and trained with TIMIT. After the training-step, the cross-correlation is averaged over all the development sentences.</p><p>It would be of interest to examine the evolution of this correlation over the epochs. With this regard, <ref type="figure" target="#fig_1">Fig. 3</ref> reports   the peak of C(z, r) for some training epochs, showing that the GRU attributes rather quickly a similar role to update and reset gates. In fact, after 3-4 epochs, the correlation peak reaches its maximum value, that is almost maintained for all the subsequent training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gradient analysis</head><p>The analysis of the main gradient statistics can give some preliminary indications on the role played by the various parameters.</p><p>With this goal, <ref type="table" target="#tab_1">Table II</ref> reports the L2 norm of the gradient for the main parameters of the considered GRU models. Results reveal that the reset gate weight matrices (i.e., W r and U r ) have a smaller gradient norm when compared to the other parameters. This result is somewhat expected, since the reset gate parameters are processed by two different nonlinearities (i.e., the sigmoid of Eq. 1b and the tanh of 1c), that can attenuate their gradients. This would anyway indicate that, on average, the reset gate has less impact on the final cost function, further supporting its removal from the GRU design. When avoiding it, the norm of the gradient tends to increase (see for instance the recurrent weights U h of M-GRU model). This suggests that the functionalities of the reset gate, can be performed by other model parameters. The norm further increases in the case of Li-GRUs, due to the adoption of ReLu units. This non-linearity, indeed, improves the back-propagation of the gradient over both time-steps and hidden layers, making long-term dependencies easier to learn. Results are obtained with the same GRU used in subsection X X X X X X X X  V-A, considering M-GRU and Li-GRU models with the same number of hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Role of batch normalization</head><p>After the preliminary analysis on correlation and gradients done in previous sub-sections, we now compare RNN models in terms of their final speech recognition performance. To highlight the importance of batch normalization, <ref type="table" target="#tab_1">Table III</ref> compares the Phone Error Rate (PER%) achieved with and without this technique.</p><p>Results show that batch normalization is helpful to improve the ASR performance, leading to a relative improvement of about 7% for GRU and M-GRU and 18% for the proposed Li-GRU. The latter improvement confirms that our model couples particularly well with this technique, due to the adopted ReLU activations. Without batch normalization, the ReLU activations of the Li-GRU are unbounded and tend to cause numerical instabilities. According to our experience, the convergence of Li-GRU without batch normalization, can be achieved only by setting rather small learning rate values. The latter setting, however, can lead to a poor performance and, as clearly emerged from this experiment, coupling Li-GRU with this technique is strongly recommended. <ref type="table" target="#tab_1">Table IV</ref> summarizes the results of CTC on the TIMIT data set. In these experiments, the Li-GRU clearly outperforms the standard GRU, showing the effectiveness of the proposed model even in a end-to-end ASR framework. The improvement is obtained both with and without batch normalization and, similarly to what observed for hybrid systems, the latter technique leads to better performance when coupled with Li-GRU. However, a smaller performance gain is observed when batch normalization is applied to the CTC. This result could also be related to the different choice of the regularizer, as weight noise was used instead of recurrent dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CTC results</head><p>In general, PERs are higher than those of the hybrid systems. End-to-end methods, in fact, are relatively young models, that are currently still less competitive than more complex (and mature) DNN-HMM approaches. We believe that the gap between CTC and hybrid speech recognizers could be partially reduced in our experiments with a more careful setting of the hyperparameters and with the introduction of an external phone-based language model. The main focus of the paper, however, is to show the effectiveness of the proposed Li-GRU model, and a fair comparison between CTC and hybrid systems is out of the scope of this work.````````À rch.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Other results on TIMIT</head><p>The results of <ref type="table" target="#tab_1">Table III</ref> and IV highlighted that the proposed Li-GRU model outperforms other GRU architectures. In this sub-section, we extend this study by performing a more detailed comparison with the most popular RNN architectures. To provide a fair comparison, batch normalization is hereinafter applied to all the considered RNN models. Moreover, at least five experiments varying the initialization seeds were conducted for each RNN architecture. The results are thus reported as the average PER with their corresponding standard deviation. <ref type="table" target="#tab_8">Table V</ref> presents the ASR performance obtained with the TIMIT dataset. The first row reports the results achieved with a simple RNN with ReLU activations (no gating mechanisms are used here). Although this architecture has recently shown promising results in some machine learning tasks <ref type="bibr" target="#b50">[51]</ref>, our results confirm that gated recurrent networks (rows 2-5) outperform traditional RNNs. We also observe that GRUs tend to slightly outperform the LSTM model. As expected, M-GRU (i.e., the architecture without reset gate) achieves a performance very similar to that obtained with standard GRUs, further supporting our speculation on the redundant role played by the reset gate in a speech recognition application. The last row reports the performance achieved with the proposed model, in which, besides removing the reset gate, ReLU activations are used. The Li-GRU performance indicates that our architecture consistently outperforms the other RNNs over all the considered input features. A remarkable achievement is the average PER(%) of 14.9% obtained with fMLLR features. To the best of our knowledge, this result yields the best published performance on the TIMIT test-set.</p><p>In <ref type="table" target="#tab_1">Table VI</ref> the PER(%) performance is split into five different phonetic categories (vowels, liquids, nasals, fricatives and stops), showing that Li-GRU exhibits the best results for all the considered classes.</p><p>Previous results are obtained after optimizing the main hyperparameters of the model on the development set. <ref type="table" target="#tab_1">Table  VII</ref> reports the outcome of this optimization process, with the corresponding best architectures obtained for each RNN architecture. For GRU models, the best performance is achieved with 5 hidden layers of 465 neurons. It is also worth noting     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Recognition performance on DIRHA English WSJ</head><p>After a first set of experiments on TIMIT, in this sub-section we assess our model on a more challenging and realistic distant-talking task, using the DIRHA English WSJ corpus. A challenging aspect of this dataset is the acoustic mismatch between training and testing conditions. Training, in fact, is performed with a reverberated version of WSJ, while test is characterized by both non-stationary noises and reverberation.</p><p>Tables VIII and IX summarize the results obtained with the simulated and real parts of this dataset.</p><p>These results exhibit a trend comparable to that observed for TIMIT, confirming that Li-GRU still outperform GRU even in a more challenging scenario. The results are consistent over both real and simulated data as well as across the different features considered in this study.</p><p>The reset gate removal seems to play a more crucial role in the addressed distant-talking scenario. If the close-talking  performance reported in <ref type="table" target="#tab_8">Table V</ref> highlights comparable error rates between standard GRU and M-GRU, in the distanttalking case we even observe a small performance gain when removing the reset gate. We suppose that this behaviour is due to reverberation, that implicitly introduces redundancy in the signal, due to the multiple delayed replicas of each sample. This results in a forward memory effect, that can make reset gate ineffective. In <ref type="figure">Fig. 4</ref>, we extend our previous experiments by generating simulated data with different reverberation times T 60 ranging from 250 to 1000 ms, as outlined in Sec. IV-A. In order to simulate a more realistic situation, different impulse responses have been used for training and testing purposes. No additive noise is considered for these experiments.</p><p>As expected, the performance degrades as the reverberation time increases. Similarly to previous achievements, we still observe that Li-GRU outperform GRU under all the considered reverberation conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Recognition performance on CHiME</head><p>In this sub-section we extend the results to the CHiME corpus, that is an important benchmark in the ASR field, thanks to the success of CHiME challenges <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b65">[66]</ref>. In Tab. X a comparison across the various GRU architectures is presented. For the sake of comparison, the results obtained with the official CHiME 4 are also reported in the first two rows <ref type="bibr" target="#b3">4</ref> .   Results confirm the trend previously observed, highlighting a significant relative improvement of about 14% achieved when passing from GRU to the proposed Li-GRU. Similarly to our findings of the previous section, some small benefits can be observed when removing the reset gate. The largest performance gap, however, is reached when adopting ReLU units (see M-GRU and Li-GRU columns), confirming the effectiveness of this architectural variation. Note also that the GRU systems significantly outperform the DNN baseline, even when the latter is based on sequence discriminative training (DNN+sMBR) <ref type="bibr" target="#b66">[67]</ref>.</p><formula xml:id="formula_9">X X X X X X X X Arch.</formula><p>Tab. XI splits the ASR performance of the real test set into the four noisy categories. Li-GRU outperforms GRU in all the considered environments, with a performance gain that is higher when more challenging acoustic conditions are met. For instance, we obtain a relative improvement of 16% in the bus (BUS) environment (the noisiest), against the relative improvement of 9.5% observed in the street (STR) recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Recognition performance on TED-talks</head><p>Tab. XII reports a comparison between GRU and Li-GRU on the TED-talks corpus. The experiments are performed with standard MFCC features, and a four-gram language model is considered in the decoding step (see <ref type="bibr" target="#b67">[68]</ref> for more details).</p><p>Results on both test sets consistently shows the performance gain achieved with the proposed architecture. This further confirms the effectiveness of Li-GRU, even for a larger scale ASR task. In particular, a relative improvement of about 14-17% is achieved. This improvement is statistically significant according to the Matched Pairs Sentence Segment Word Error Test (MPSSW) <ref type="bibr" target="#b68">[69]</ref>, that is conducted with NIST sclite sc stat tool with a p-value of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Training time comparison</head><p>In the previous subsections, we reported several speech recognition results, showing that Li-GRU outperforms other RNNs. In this sub-section, we finally focus on another key aspect of the proposed architecture, namely its improved computational efficiency. In <ref type="table" target="#tab_1">Table XIII</ref>  The training time reduction achieved with the proposed architecture is about 30% for all the datasets. This reduction reflects the amount of parameters saved by Li-GRU, that is also around 30%. The reduction of the computational complexity, originated by a more compact model, also arises for testing purposes, making our model potentially suitable for smallfootprint ASR, <ref type="bibr" target="#b69">[70]</ref>- <ref type="bibr" target="#b74">[75]</ref>, which studies DNNs designed for portable devices with small computational capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we revised standard GRUs for speech recognition purposes. The proposed Li-GRU architecture is a simplified version of a standard GRU, in which the reset gate is removed and ReLU activations are considered. Batch normalization is also used to further improve the system performance as well as to limit the numerical instabilities originated from ReLU non-linearities.</p><p>The experiments, conducted on different ASR paradigms, tasks, features and environmental conditions, have confirmed the effectiveness of the proposed model. The Li-GRU, in fact, not only yields a better recognition performance, but also reduces the computational complexity, with a reduction of more than 30% of the training time over a standard GRU.</p><p>Future efforts will be focused on extending this work to other speech-based tasks, such as speech enhancement and speech separation as well as to explore the use of Li-GRU in other possible fields.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Auto-correlation C(z, z) and cross-correlation C(z, r) between the average activations of the update (z) and reset (r) gates. Correlations are normalized by the maximum of C(z, z) for graphical convenience.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Evolution of the peak of the cross-correlation C(z, r) over various training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Main features of the training datasets adopted in this work.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>L 2 norm of the gradient for the main parameters of the GRU models. The norm is averaged over all the training sentences and epochs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>PER(%) of the GRU models with and without batch normalization (TIMIT dataset, MFCC features).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>PER(%) obtained for the test set of TIMIT with various CTC RNN architectures.</figDesc><table><row><cell>X Arch. X X relu-RNN X</cell><cell>X X Feat. X X</cell><cell>MFCC 18.7 ± 0.18 18.3 ± 0.23 16.3 ± 0.11 FBANK fMLLR</cell></row><row><cell>LSTM</cell><cell></cell><cell>18.1 ± 0.33 17.1 ± 0.36 15.7 ± 0.32</cell></row><row><cell>GRU</cell><cell></cell><cell>17.1 ± 0.20 16.7 ± 0.36 15.3 ± 0.28</cell></row><row><cell>M-GRU</cell><cell></cell><cell>17.2 ± 0.11 16.7 ± 0.19 15.2 ± 0.10</cell></row><row><cell>Li-GRU</cell><cell></cell><cell>16.7 ± 0.26 15.8 ± 0.10 14.9 ± 0.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell>: PER(%) obtained for the test set of TIMIT with</cell></row><row><cell>various RNN architectures.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>PER(%) of the TIMIT dataset (MFCC features) split into five different phonetic categories. Silences (sil) are not considered here.</figDesc><table><row><cell>Architecture</cell><cell>Layers</cell><cell>Neurons</cell><cell># Params</cell></row><row><cell>relu-RNN</cell><cell>4</cell><cell>607</cell><cell>6.1 M</cell></row><row><cell>LSTM</cell><cell>5</cell><cell>375</cell><cell>8.8 M</cell></row><row><cell>GRU</cell><cell>5</cell><cell>465</cell><cell>10.3 M</cell></row><row><cell>M-GRU</cell><cell>5</cell><cell>465</cell><cell>7.4 M</cell></row><row><cell>Li-GRU</cell><cell>5</cell><cell>465</cell><cell>7.4 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII :</head><label>VII</label><figDesc>Optimal number of layers and neurons for each TIMIT RNN model. The outcome of the optimization process is similar for all considered features.</figDesc><table><row><cell>X Arch. X X relu-RNN X X X Feat. X X</cell><cell>MFCC 23.7 ± 0.21</cell><cell>FBANK 23.5 ± 0.30</cell><cell>fMLLR 18.9 ± 0.26</cell></row><row><cell>LSTM</cell><cell>23.2 ± 0.46</cell><cell>23.2 ± 0.42</cell><cell>18.9 ± 0.24</cell></row><row><cell>GRU</cell><cell>22.3 ± 0.39</cell><cell>22.5 ± 0.38</cell><cell>18.6 ± 0.23</cell></row><row><cell>M-GRU</cell><cell>21.5 ± 0.43</cell><cell>22.0 ± 0.37</cell><cell>18.0 ± 0.21</cell></row><row><cell>Li-GRU</cell><cell>21.3 ± 0.38</cell><cell>21.4 ± 0.32</cell><cell>17.6 ± 0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VIII</head><label>VIII</label><figDesc></figDesc><table><row><cell cols="4">: Word Error Rate (%) obtained with the DIRHA</cell></row><row><cell cols="4">English WSJ dataset (simulated part) for various RNN archi-</cell></row><row><cell>tectures.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>X Arch. X X relu-RNN X X X Feat. X X</cell><cell>MFCC 29.7 ± 0.31</cell><cell>FBANK 30.0 ± 0.38</cell><cell>fMLLR 24.7 ± 0.28</cell></row><row><cell>LSTM</cell><cell>29.5 ± 0.41</cell><cell>29.1 ± 0.42</cell><cell>24.6 ± 0.35</cell></row><row><cell>GRU</cell><cell>28.5 ± 0.37</cell><cell>28.4 ± 0.21</cell><cell>24.0 ± 0.27</cell></row><row><cell>M-GRU</cell><cell>28.4 ± 0.34</cell><cell>28.1 ± 0.30</cell><cell>23.6 ± 0.21</cell></row><row><cell>Li-GRU</cell><cell>27.8 ± 0.38</cell><cell>27.6 ± 0.36</cell><cell>22.8 ± 0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE IX :</head><label>IX</label><figDesc>Word Error Rate (%) obtained with the DIRHA English WSJ dataset (real part) for various RNN architectures. that M-GRU and Li-GRU have about 30% fewer parameters compared to the standard GRU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>± 0.38 16.1 ± 0.31 26.1 ± 0.45 30.0 ± 0.48 DNN+sMBR 14.7 ± 0.25 15.7 ± 0.23 24.0 ± 0.31 27.0 ± 0.35 GRU 15.8 ± 0.30 14.8 ± 0.25 23.0 ± 0.38 23.3 ± 0.35 M-GRU 15.9 ± 0.32 14.1 ± 0.31 22.8 ± 0.39 23.0 ± 0.41 Li-GRU 13.5 ± 0.25 12.5 ± 0.22 20.3 ± 0.31 20.0 ± 0.33</figDesc><table><row><cell></cell><cell>18</cell><cell></cell><cell>GRU</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Li-GRU</cell><cell></cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell></row><row><cell>WER(%)</cell><cell>14</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000</cell></row><row><cell></cell><cell></cell><cell></cell><cell>T 60 (ms)</cell><cell></cell></row><row><cell cols="6">Fig. 4: Evolution of the WER(%) for the DIRHA WSJ</cell></row><row><cell cols="6">simulated data over different reverberation times T 60 .</cell></row><row><cell cols="2">P Arch. P DNN P P Dataset P P</cell><cell>DT-sim 17.8</cell><cell>DT-real</cell><cell>ET-sim</cell><cell>ET-real</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE X :</head><label>X</label><figDesc>Speech recognition performance on the CHiME dataset (single channel, fMLLR features).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE XI :</head><label>XI</label><figDesc>Comparison between GRU and Li-GRU for the four different noisy conditions considered in CHiME on the real evaluation set (ET-real).```````À</figDesc><table><row><cell>rch.</cell><cell>Dataset.</cell><cell>TST-2011</cell><cell>TST-2012</cell></row><row><cell>GRU</cell><cell></cell><cell>16.3 ± 0.13</cell><cell>17.0 ± 0.16</cell></row><row><cell>Li-GRU</cell><cell></cell><cell>13.8 ± 0.12</cell><cell>14.4 ± 0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE XII :</head><label>XII</label><figDesc>Comparison between GRU and Li-GRU with the TED-talks corpus.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>, we compare the perepoch wall-clock training time of GRU and Li-GRU models.</figDesc><table><row><cell>X Arch. X X X Dataset. X X X X GRU</cell><cell>TIMIT 9.6 min</cell><cell>DIRHA 40 min</cell><cell>CHiME 312 min</cell><cell>TED 590 min</cell></row><row><cell>Li-GRU</cell><cell cols="4">6.5 min 25 min 205 min 447 min</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE XIII :</head><label>XIII</label><figDesc>Per-epoch training time (in minutes) of GRU and Li-GRU models for the various datasets on an NVIDIA K40 GPU.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. DOI: 10.1109/TETCI.2017.2762739 URL: http://ieeexplore.ieee.org/ document/8323308/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This dataset is being distributed by the Linguistic Data Consortium (LDC).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The code is available at http://github.com/mravanelli/theano-kaldi-rnn/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The results obtained in this section are not directly comparable with the best systems of the CHiME 4 competition. Due to the purpose of this work, indeed, techniques such as multi-microphone processing, data-augmentation, system combination as well as lattice rescoring are not used here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Piergiorgio Svaizer for his insightful suggestions on an earlier version of this paper. We would also thank the anonymous reviewers for their careful reading of our manuscript and their helpful comments.</p><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of a Tesla K40 GPU used for this research. Computations were also made on the Helios supercomputer from the University of Montreal, managed by Calcul Qubec and Compute Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3214" to="3218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Highway long short-term memory RNNS for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5755" to="5759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SSST</title>
		<meeting>of SSST</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Automatic Speech Recognition -A Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Speech and Audio Processing in Adverse Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hänsler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An overview of noise-robust automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="745" to="777" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hybrid acoustic models for distant and multichannel large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using neural network front-ends on far field multiple microphones based speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5542" to="5546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tachioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<title level="m">The MERL/MELCO/TUM System for the REVERB Challenge Using Deep Recurrent Neural Network Feature Enhancement</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>IEEE REVERB Workshop</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reverberant speech recognition combining deep neural networks and deep autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE REVERB Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy and Reverberant Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kellermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4380" to="4384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch-normalized joint training for dnn-based distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
		<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contaminated speech training methods for robust DNN-HMM distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="756" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A network of deep neural networks for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4880" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The third CHiME Speech Separation and Recognition Challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The reverb challenge: A Common Evaluation Framework for Dereverberation and Recognition of Reverberant Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WASPAA</title>
		<meeting>of WASPAA</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Automatic Speech recognition In Reverberant Environments (ASpIRE) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="547" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Endto-End Attention-based Large Vocabulary Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the INNS-ENNS</title>
		<meeting>of the INNS-ENNS</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LSTM: A Search Space Odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Minimal gated unit for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1603.09420" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NISP</title>
		<meeting>of NISP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Context-dependent pretrained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with Deep Bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1512.02595" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3274" to="3278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The RWTH/UPB/FORTH System Combination for the 4th CHiME Challenge Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Menne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alexandridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Golik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
	<note>in CHiME 4 challenge</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LVA/ICA</title>
		<meeting>of LVA/ICA</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discriminatively trained recurrent neural networks for single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE GlobalSIP</title>
		<meeting>of IEEE GlobalSIP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Real-life voice activity detection with LSTM Recurrent Neural Networks and an application to Hollywood movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Squartini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="483" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Batch normalized recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2657" to="2661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Recurrent batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1603.09025" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1504.00941" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving speech recognition by revising gated recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The Kaldi Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The DIRHA-English corpus and related tasks for distantspeech recognition in domestic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cristoforetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gretter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pellin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image method for efficiently simulating smallroom acoustics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="page" from="2425" to="2428" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Realistic multi-microphone data simulation for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Svaizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2786" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Impulse response estimation for robust speech recognition in a reverberant environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Svaizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EUSIPCO</title>
		<meeting>of EUSIPCO</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1668" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the selection of the impulse responses for distant-speech recognition based on contaminated speech training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1028" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Overview of the IWSLT 2011 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWSLT</title>
		<meeting>of IWSLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">RNNDROP: A novel dropout for RNNS in ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="65" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The PAS-CAL CHiME speech separation and recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="621" to="633" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sequencediscriminative training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veselý</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2345" to="2349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">FBK-IWSLT 2011</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brugnara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Falavigna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giuliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gretter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWSLT</title>
		<meeting>of IWSLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Some statistical issues in the comparison of speech recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="532" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Small-footprint Deep Neural Networks with Highway Connections for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Small-footprint high-performance deep neural network-based speech recognition using split-VQ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4984" to="4988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for smallfootprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Low-latency real-time meeting recognition and understanding using distant microphones and omni-directional camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="499" to="513" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Accurate and compact large vocabulary speech recognition on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="662" to="665" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
