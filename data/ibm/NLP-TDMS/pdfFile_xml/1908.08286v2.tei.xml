<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING STOCHASTIC DIFFERENTIAL EQUATIONS USING RNN WITH LOG SIGNATURE FEATURES A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-24">September 24, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
							<email>terry.lyons@maths.ox.ac.uk˚w</email>
							<affiliation key="aff1">
								<orgName type="department">Mathematical Institute</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eixin</forename><surname>Yang</surname></persName>
							<email>weixin.yang@maths.ox.ac.uk:</email>
							<affiliation key="aff2">
								<orgName type="department">Mathematical Institute</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ni</surname></persName>
							<email>h.ni@ucl.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING STOCHASTIC DIFFERENTIAL EQUATIONS USING RNN WITH LOG SIGNATURE FEATURES A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-24">September 24, 2019</date>
						</imprint>
					</monogr>
					<note>By testing on various datasets, i.e. synthetic data, NTU RGB+D 120 skeletal action data, and Chalearn2013 gesture data, our algorithm achieves the outstanding accuracy with superior efficiency and robustness.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper contributes to the challenge of learning a function on streamed multimodal data through evaluation. The core of the result of our paper is the combination of two quite different approaches to this problem. One comes from the mathematically principled technology of signatures and logsignatures as representations for streamed data, while the other draws on the techniques of recurrent neural networks (RNN). The ability of the former to manage high sample rate streams and the latter to manage large scale nonlinear interactions allows hybrid algorithms that are easy to code, quicker to train, and of lower complexity for a given accuracy. We illustrate the approach by approximating the unknown functional as a controlled differential equation. Linear functionals on solutions of controlled differential equations are the natural universal class of functions on data streams. Following this approach, we propose a hybrid Logsig-RNN algorithm <ref type="figure">(Figure 1</ref>) that learns functionals on streamed data . By testing on various datasets, i.e. synthetic data, NTU RGB+D 120 skeletal action data, and Chalearn2013 gesture data, our algorithm achieves the outstanding accuracy with superior efficiency and robustness. /1. 4 τ is a constant, A and B are matrices and σ is an activation function. arXiv:1908.08286v2 [cs.LG] 22 Sep 2019 dX pi1q u1 b¨¨¨b dX pinq un .</p><p>The signature of X is defined as follows:</p><p>where X k J " pX I J q I"pi1,¨¨¨,i k q , @k ě 1. Let S k pXq J denote the truncated signature of X of degree k, i.e.</p><p>Example 2.2. When X is a multi-dimensional Brownian motion, the above integration can be defined in both Stratonovich and Itô sense. It is because that the Brownian motion has samples of infinite p-variation for p P r1, 2s a.s ([30]).</p><p>Remark 2.1 (The Signature of Discrete Time Series). The discrete version of a path x D is of finite 1-variation. Thus the signature of x D is well defined. It is highlighted that Spx D q is NOT the collection of all the monomials of discrete time series! The dimension of all monomials of x D grow with |D|, while the dimension of S k px D q is invariant to |D|.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The relationship between neural networks and differential equations is an active area of research ( <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b2">[3]</ref>). For example, Funahashi et al. introduced the continuous recurrent neural network(RNN) <ref type="bibr" target="#b10">[11]</ref>; He et al. connect residual networks and discretized ODEs <ref type="bibr" target="#b5">[6]</ref>. A typical continuous RNN has the form</p><formula xml:id="formula_0">9 Y t "´Y t τ`A σpBY t q`I t ,<label>(1)</label></formula><p>where I t and Y t are an input and output at time t respectively <ref type="bibr" target="#b3">4</ref> . Rough Path Theory teaches us that is more robust to consider the differential equation of the type dY t " V pY t qdX t ,</p><p>and replace I as an input with its integral. We can rewrite <ref type="bibr" target="#b0">(1)</ref> in this form by setting X t " pt, ş t s"t0 I s dsq and V py, pt, xqq "´y τ t`AσpByqt`x. This allows the input to be of a broader type, and X need not even be differentiable for the equation to be well defined. Y inherits its regularity from X; equations in this form admit uniform estimates when X is a rough path (highly oscillatory and potentially non-differentiable). This reformulation provides a much broader class of mathematical models for functionals on streamed data, of which the continuous RNN is a special case.</p><p>In <ref type="bibr" target="#b31">[32]</ref> Lyons gives a deterministic pathwise definition to Equation (2) driven by rough signals. This analysis applies to almost all paths of e.g. vectored valued Brownian motion, diffusion processes, and also to many processes outside the SDE case, paths rougher than semi-martingales. <ref type="bibr" target="#b31">[32]</ref> articulates that in order to control the solution to Equation <ref type="bibr" target="#b1">(2)</ref>, it suffices to control the p-variation and the iterated integrals of X (the signature of X) up to degree tpu. Crucially these estimates allow p ąą 1 and allow accurate descriptions of Y t to emerge from the coarse global descriptions of X and its oscillations given by the signature. The log-signature carries the exactly the same information as the signature but is considerably more parsimonious; it is a second mathematically principled transformation, and like the signature, it is able to summarize and vectorize complex un-parameterized streams of multi-modal data effectively over a coarse time scale with a low dimensional representation.</p><p>One area where this has been worked out in detail is with the numerical analysis of stochastic differential equations (SDEs). The most effective high order numerical approximation schemes for SDEs show that describing a path through the log-signature enables one to effectively approximate the solution to the equation and any linear functional of that solution globally over interval the path is defined on, without further dependence on the fine details of the recurrent structure of the streamed data. It leads (in what is known as the log-ode method) to produce a state-of-the-art discretization method of of Inhomogenous Geometric Brownian Motion (IGBM) <ref type="bibr" target="#b7">[8]</ref>.We exploit this understanding to propose a simply but surprisingly effective neural network module (Logsig-RNN) by blending the Log-signature (Sequence) Layer with the RNN layer (see <ref type="figure" target="#fig_8">Figure 1</ref>) as an universal model for functionals on un-parameterized (and potentially complex) streamed data. 2. High frequency and continuous data: For the high frequency data case, the RNN type approach suffers from severe limitations, when applied directly <ref type="bibr" target="#b4">[5]</ref>. In this case, one has to down-sample the stream data to a coarser time grid to feed it into the RNN-type algorithm <ref type="figure" target="#fig_8">(Figure 1 (Left)</ref>). It may miss the microscopic characteristic of the streamed data and render lower accuracy. The Logsig-RNN model can handle such case or even continuous data streams very well <ref type="figure" target="#fig_8">(Figure 1</ref>).</p><p>3. Robustness to missing data. Compared with the signature feature set, the log-signature is a parsimonious representation of the signature <ref type="bibr" target="#b37">[38]</ref>, and empirically proved more robust to missing data. We validate the robustness of the Logsig-RNN model on various datasets, which outperforms that of the RNN model significantly. <ref type="bibr" target="#b3">4</ref>. Highly Oscillatory Stream data: There is a fundamental issue when one accesses a highly oscillatory stream through sampling. It is quite possible for two streams to have very different effects and yet have near identical values when sampled at very fine levels <ref type="bibr" target="#b8">[9]</ref>. Therefore, to model a functional on a general highly oscillatory stream, the RNN on the sampled stream data would be challenged, requiring huge amounts of augmentation, and very fine sampling to be effective. In contrast, the rough path theory shows that if one postulates the (log)-signature of streamed data up in advance, the Logsig-RNN model can be much effective.</p><p>In summary, the main contributions of the paper are listed as follows:</p><p>1. to introduce the Log-signature (Sequence) Layer as a transformation of sequential data, and outline its backpropagation through time algorithm. It is highlighted that the Log-signature Layer can be inserted between other neural network layers conveniently, not limited to the pre-defined feature extraction. 2. to design the novel neural network model (Logsig-RNN model) by blending the log-signature layer with RNN (Section 3.2) and prove the universality of the Logsig-RNN model for the approximation any solution map to the SDEs (Theorem 4.1); 3. to propose the PT-Logsig-RNN model by adding the linear project layer in front of the Logsig-RNN architecture to tackle the case for the high dimensional input path (Section 3.3). 4. to apply the Logsig-RNN algorithm to both synthetic data and empirical data to demonstrate its superior accuracy, effectiveness and robustness (Section 5). We achieve the state-of-the-art classification accuracy 93.27% on ChaLearn2013 gesture data by the PT-Logsig-RNN model (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Learning SDEs</head><p>SDEs of the form (2) are useful tools for modelling random phenomena and provide a general class of functionals on the path space. SDEs not only are commonly used as models for the time-evolving process of many physical, chemical and biological systems of interacting particles <ref type="bibr" target="#b11">[12]</ref>, but also are the foundational building blocks in the derivatives pricing theory, an area of huge financial impact ( <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b3">[4]</ref>). Statistical inference for SDEs has rich literature due to the importance of research outcomes and applications (see <ref type="bibr" target="#b0">[1]</ref> for the survey and overview). Most of the research focuses on the parameter estimation of (model-specific) stochastic processes; in particular <ref type="bibr" target="#b35">[36]</ref> is the pioneering work for the parameter estimation for a general stochastic process, which goes beyond diffusion processes by matching expected signature of the solution process. However, in contrast to these work, our approach is non-parametric and is used to learn the solution map without any assumption on the distribution of the stochastic process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Rough paths theory in machine learning</head><p>Recently the application of the rough path theory in machine learning has been an emerging and active research area. The empirical applications of the rough paths theory primarily focused on the signature feature, which serves as an effective feature extraction, e.g. online handwritten Chinese character/text recognition( <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b41">[42]</ref>), action classification in videos <ref type="bibr" target="#b42">[43]</ref>, and financial data analysis ( <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>). In addition, those previous work mainly combine the signature with the convolutional neural network or fully connected neural network. To our best knowledge, the proposed method is the first of the kind to integrates the sequence of log signature with the RNN. The log-signature brings many benefits (Section 2.3). The log-signature has been used as a local feature descriptor for gesture <ref type="bibr" target="#b20">[21]</ref> and action recognition <ref type="bibr" target="#b42">[43]</ref>. These used cases are bespoke; in contrast, the proposed Logsig-RNN is a general method for sequential data with outstanding performance in various datasets (See Section 5). Moreover, we extend the work on the back-propagation algorithm of the log-signature transformation in <ref type="bibr" target="#b36">[37]</ref> to the sequence of the log-signature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3">Time series modelling</head><p>In <ref type="bibr" target="#b19">[20]</ref> Levin et al. firstly proposed the signature of a path as the basis functions for a functional on the unparameterized path space and suggested the first non-parametric model for time series modelling by combining signature feature and the linear model (Sig-OLR). However, Sig-OLR has the limitation of inefficient global approximation due to the instability of the polynomial extrapolation. Despite the successful empirical applications of the signature feature sets ( <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>), the theoretical question on which learning algorithms are most appropriate to be combined with the (log)-signature feature remains open. Our work is devoted to answering this question with both theoretical justification and promising numerical evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.4">Functional Data Analysis</head><p>Learning a functional on streamed data falls under the category of the functional data analysis (FDA) <ref type="bibr" target="#b33">[34]</ref>, which models data using functions or functional parameters and analyse data providing information about curves, surfaces or anything else varying over a continuum. The representation theory of the functional on functions plays an important role in FDA study. Functional principal components analysis ( <ref type="bibr" target="#b39">[40]</ref>) is one of the main techniques of FDA to represent the function data, which express the function data as the linear coefficients of the basis functions(usually without taking into account the response variable corresponding to the function input). In contrast to it, albeit taking the functional view of sequential data, our approach focuses on the representation of the path in terms of its effect (functional on the path, i.e . the solution of the controlled differential equation driven by the path).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Log-Signature of a Path</head><p>Consider a continuous time series x over the time interval J :" rS, T s built at some very fine scale out of time stamped values xD " rx t1 , x t2 ,¨¨¨, x tn s, whereD " pt 1 ,¨¨¨, t n q. When x is highly oscillatory, to well capture effects of x, classical approaches requires sampling x at high frequency, or even collect all the ticks. In this section, we takes the functional view on xD by embedding it to a continuous path by interpolation for a unified treatment (See detailed discussion in Section 4 of <ref type="bibr" target="#b19">[20]</ref>). We start with the introduction to p-variation to measure the roughness of a continuous path. Then we introduce a graded feature, so-called log-signature feature as an effective and high order summary of a path of finite p-variation over time intervals. It follows with the key properties of the log-signature in machine learning applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Path with finite p-variation</head><p>Let E :" R d and X : J Ñ E be a continuous path endowed with a norm denoted by |¨|. To make precise about the class of paths we discuss throughout the paper, we introduce the p-variation as a measure of the roughness of the path. Definition 2.1 (p-Variation). Let p ě 1 be a real number. Let X : J Ñ E be a continuous path. The p-variation of X on the interval J is defined by</p><formula xml:id="formula_2">||X|| p,J " « sup DĂJ r´1 ÿ j"0ˇX tj`1´Xtjˇp ff 1 p ,<label>(3)</label></formula><p>where the supremum is taken over any time partition of J, i.e. D " pt 1 , t 2 ,¨¨¨, t r q. <ref type="bibr" target="#b4">5</ref> Let V p pJ, Eq denote the range of any continuous path mapping from J to E of finite p-variation. The larger p-variation is, the rougher a path is. The compactness of the time interval J can't ensure the finite 1-variation of a continuous path in general (See Example 2.1). Example 2.1. A fractional Brownian motion (fBM) with Hurst parameter H has sample paths of finite p-variation a.s. for p ą 1 H . The larger H is, the rougher fBM sample path is. For example, Brownian motion is a fBM with H " 0.5. It has finite p2`εq-variation a.s @ε ą 0, but it has infinite p-variation a.s.@p P r1, 2s.</p><p>For each p ě 1, the p-variation norm of a path X : J Ñ E of finite p-variation is denoted by ||X|| p´var and defined as follows:</p><formula xml:id="formula_3">||X|| p´var " ||X|| p,J`s up tPJ ||X t ||.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The log-signature of a path</head><p>We introduce the signature and the log signature of a path, which take value in the tensor algebra space denoted by T ppEqq endowed with the tensor multiplication and componentwise addition <ref type="bibr" target="#b29">[30]</ref>. <ref type="bibr" target="#b4">5</ref> Let J " rS, T s be a closed bounded interval. A time partition of J is an increasing sequence of real numbers D " pt0, t1,¨¨¨, trq such that S " t0 ă t1 ă¨¨¨ă tr " T . Let |D| denote the number of time points in D, i.e. |D| " r`1. ∆D denotes the time mesh of D, i.e. ∆D :"</p><formula xml:id="formula_4">r´1 max i"0 pti`1´tiq.</formula><p>Definition 2.2 (The Signature of a Path). Let J denote a compact interval and X : J Ñ E be a continuous path with finite p-variation such that the following integration makes sense. Let I " pi 1 , i 2 ,¨¨¨, i n q be a multi-index of length n where i j P t1,¨¨¨, du, @j P t1,¨¨¨, nu. Define the coordinate signature of the path X J associate with the index I as follows:</p><formula xml:id="formula_5">X I J " ż . . . ż u1ă¨¨¨ău k u 1 ,...,u k PJ</formula><p>The signature of a path has many good properties, which makes the signature an efficient representation of an unparameterized path. We refer readers to <ref type="bibr" target="#b19">[20]</ref> for an introduction to the signature feature in machine learning.</p><p>The logarithm of the element in T ppEqq is defined similar to the power series of the logarithm of a real value except for the multiplication is understood in the tensor product sense. Definition 2.3 (Logarithm map). Let a " pa 0 , a 1 ,¨¨¨q P T ppEqq be such that a 0 " 1 and t " a´1. Then the logarithm map denoted by log is defined as follows:</p><p>logpaq " logp1`tq " 8 ÿ n"1 p´1q n´1 n t bn , @a P T ppEqq.</p><p>Lemma 2.1. The logarithm map is bijective on the domain ta P T ppEqq|a 0 " 1u.</p><p>Definition 2.4 (The Log Signature of a Path). The log signature of path X by logpSpXqq is the logarithm of the signature of the path X, denoted by lSpXq. Let lS k pXq denote the truncated log signature of a path X of degree k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Properties of the log-signature</head><p>We summarize the key properties of the log-signature, which make the log-signature as a principled and effective summary of streamed data over intervals. In addition, we highlight the comparison of properties of the log-signature and the signature. More details on properties of (log)-signature can be found in the appendix, and the illustrative examples of pendigit data is provieded by pendigit demo.ipdb. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Bijection between the signature and the log-signature</head><p>As the logarithm map is bijective, there is one-to-one correspondence between the signature and the log-signature <ref type="bibr" target="#b29">[30]</ref>. The statement is also true for the truncated signature and log-signature up to the same degree. For example, by projecting both sides of Eqn (2.3) to E, the first level of signature and log-signature are both increments of the path X T´XS . For the second level of the signature pX pi,jq S,T q d i,jP"1 , it can be decomposed to its symmetric and anti-symmetric parts X pi,jq</p><formula xml:id="formula_7">S,T " 1 2 X piq S,T X pjq S,T`A pi,jq S,T ,<label>(7)</label></formula><p>where A pi,jq</p><formula xml:id="formula_8">S,T " 1 2`X p1,2q´X p2,1q˘.</formula><p>It is noted that A pi,iq " 0 and lS 2 " A p1,2q re 1 , e 2 s. This is an example to show that the signature and log signature(lower dimension) is a bijection up to degree 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Dimension Reduction</head><p>The log-signature is a parsimonious representation for the signature feature, which is of lower dimension compared with the signature feature in general. It be used for significant dimension reduction. Let us consider the linear subspace of T ppEqq equipped with the Lie bracket operation r., .s, defined as follows: <ref type="bibr" target="#b29">[30]</ref>) For any path X of finite 1-variation , there exist λ i1,¨¨¨,in such that the log-signature of X can be expressed in the following form 7 lSpXq "</p><formula xml:id="formula_9">ra, bs " a b b´b b a. Theorem 2.1. (Theorem 2.3,</formula><formula xml:id="formula_10">d ÿ i"1 λ i e i`ÿ ně2 ei 1 ,¨¨¨,ei n Pt1,¨¨¨,du λ i1,,¨¨¨,in re i1 , r¨¨¨, re n´1 , e n sss.</formula><p>The above theorem shows that the dimension of the truncated log-signature is no greater than that of the truncated signature due to the linear dependence of re i1 , re i2¨¨¨, re n´1 , e n sss. For example, re i , e j s "´re j , e i s. The analytic formula for the dimension of the truncated log signature can be found in Theorem A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Invariance under time parameterization</head><p>We say that a pathX : J Ñ E is the time re-parameterization of X : J Ñ E if and only if there exists a nondecreasing surjection λ : J Ñ J such thatX t " X λptq , @t P J. Lemma 2.2. Let X P V 1 pJ, Eq and a pathX : J Ñ E is the time re-parameterization of X. Then</p><formula xml:id="formula_11">lSpXq J " lSpXq J .<label>(8)</label></formula><p>It is an immediate consequence of the bijection between the signature and log-signature, and the invariance of the signature (Lemma A.3). Re-parameterizing a path does not change its (log)-signature. In <ref type="figure" target="#fig_1">Figure 2</ref>, speed changes result in different time series representation but the same (log)signature feature. The (log)-signature feature can remove the redundancy caused by the speed of traversing the path, which brings massive dimension reduction benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Missing Data, variable length and unequally time spacing</head><p>The (log)-signature feature set can both deal with time series of variable length and unequal time spacing. No matter the length of the time series is and how the time spacing is, the (log)-signature feature transformation provides a fixed dimension descriptor for any d-dimensional time series. Compared with the signature, the log-signature is empirically more robust to missing data ( <ref type="figure" target="#fig_13">Figure 5</ref>). signature log-signature uniqueness of a path invariance of time parameterization the universality no redundancy <ref type="table">Table 1</ref>: Comparison of Signature and Log-signature <ref type="bibr" target="#b6">7</ref> The equivalent statement is that the log signature is a Lie series, which is defined in Definition A. <ref type="bibr">4.</ref> In contrast to signature, the log-signature does not have universality, and thus it needs be combined with non-linear models for learning. We summarize the comparison of the signature and log-signature in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Logsig-RNN Network</head><p>Consider a discrete d-dimensional time series xD " px ti q n i"1 over time interval J. The lifted path associated with xD is the piecewise linear interpolation of xD. Let D be a coarser time partition of J such that D :" pu k q N k"0 ĂD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Log-Signature Layer</head><p>We propose the Log-Signature (Sequence) Layer, which transforms an input xD to a sequence of the log signature of xD over a coarser time partition D .</p><p>Definition 3.1 (Log-Signature (Sequence) Layer). Given D andD, a Log-Signature Layer of degree M is a mapping from R dˆn to R d lsˆN , which computes pl k q N´1 k"0 as an output for any x D , where l k is the truncated log signature of x over the time interval ru k , u k`1 s of degree M as follows:</p><formula xml:id="formula_12">l k " lS M pxq ru k ,u k`1 s ,<label>(9)</label></formula><p>where k P t0, 1,¨¨¨N´1u and d ls is the dimension of the truncated log-signature.</p><p>It is noted that the Log-Signature Layer does not have any weights. In addition, the input dimension of Log-signature layer is pd, nq and the output dimension is pN, d ls q where N ď n and d ls ě d. The Log-Signature Layer potentially shrinks the time dimension effectively by using the more informative spatial features of a higher dimension.</p><p>Backpropogation Let us consider the derivative of the scalar function F on pl k q N k"1 with respect to path xD, given the derivatives of F with respect to pl k q N k"1 . By the Chain rule, it holds that</p><formula xml:id="formula_13">BF ppl 1 ,¨¨¨, l N qq Bx ti " N ÿ k"1 BF pl 1 ,¨¨¨, l N q Bl k Bl k Bx ti .<label>(10)</label></formula><p>where k P t1,¨¨¨, N u and i P t0, 1,¨¨¨, nu.</p><formula xml:id="formula_14">If t i R ru k´1 , u k s, Bl k Bxt i " 0; otherwise Bl k Bxt i</formula><p>is the derivative of the single log-signature l k with respect to path x u k´1 ,u k where t i P D X ru k´1 , u k s. The log signature lSpxDq with respect to x ti is proved differentiable and the algorithm of computing the derivatives is given in <ref type="bibr" target="#b16">[17]</ref>, denoted by xt i LSpxDq. This is a special case for our log-signature layer when N " 1. In general, for any N P Z`, it holds that @i P t0, 1,¨¨¨, nu and k P t1,¨¨¨, N u,</p><formula xml:id="formula_15">Bl k Bx ti " 1 tiPru k´1 ,u k s xt i LSpx u k´1 ,u k q,<label>(11)</label></formula><p>Thus the backpropogation algorithm of the Log-Signature Layer can be implemented using Equation <ref type="formula" target="#formula_0">(10)</ref> and <ref type="formula" target="#formula_0">(11)</ref>. <ref type="bibr" target="#b7">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Logsig-RNN Network</head><p>Before proceeding to the Logsig-RNN network, we introduce the conventional recurrent neural network(RNN). RNN is composed with three types of layers, i.e. the input layer px t q t , the hidden layer ph t q t and the output layer po t q t . RNN takes the sequence of d-dimensional vectors px 1 , x 2 ,¨¨¨, x T q as an input and compute the output po t q T t"1 P R eˆT using Equation <ref type="formula" target="#formula_0">(12)</ref>:</p><formula xml:id="formula_16">h t " σpU x t`W h t´1 q, o t " qpV h t q,<label>(12)</label></formula><p>where U , W and V are model parameters, and σ and q are two activation functions in the hidden layer and output layer respectively. Let R σ ppx t q t |Θq denote the RNN model with px t q t as the input, σ and linear function q as the activation functions of the hidden layer and output layer respectively and Θ :" tU, W, V u is the parameter set of the RNN model.</p><p>We propose the Logsig-RNN model by incorporating the log-signature layer to the RNN. We defer the motivation of the Logsig-RNN Network to next section.  • The output layer is computed by R σ ppl k q N´1 k"0 |Θq, where R σ is a RNN network with certain activation function σ. Remark 3.2. The sampling time partition of raw dataD can be potentially much higher than D used in Logsig-RNN model. The higher frequency of input data would not increase the dimension of the log-signature layer, but it makes the computation of l k more accurate.</p><p>The Logsig-RNN model (depicted in <ref type="figure" target="#fig_8">Figure 1</ref>) can be served as an alternative to the RNN model and its variants of RNNs, e.g. LSTM, GRU. One main advantage of our method is to reduce the time dimension of the RNN model significantly while using the log-signature as an effective representation of data stream over sub-time interval. It leads to higher accuracy and efficiency compared with the standard RNN model. Compared with Sig-OLR ( <ref type="bibr" target="#b19">[20]</ref>) our approach achieves better accuracy via dimension reduction by using the log signature sequence of lower degree to represent the signature of high degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Path Transformation Layers</head><p>To efficiently and effectively exploit the path, we further propose two transformation layers accompanied with Log-Signature Layer. The overall model, namely PT-Logsig-RNN, is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Embedding Layer In many real-world applications, the input path dimension is large and the dimension of the truncated log-signature grows fast w.r.t. the path dimension. To reduce the high path dimension, we add a linear Embedding Layer before the Log-Signature Layer. The mapping L, implemented by the embedding layer, translates the input sequence pX ti q n i"1 into real vectors pLX ti q n i"1 , where LX ti P R d 1 and d 1 ă d. The weights in this layer are trainable and are learned from data. The embedding layer leads to significant spatial dimension reduction of rear layers.</p><p>Accumulative Layer The Accumulative Layer maps the input sequence pX ti q n i"1 to its partial sum sequence Y ti , where Y ti " ř i j"1 X tj , and i " 1,¨¨¨, n. One advantage of using the Accumulative Layer along with Log-Signature Layer is to extract the quadratic variation and other higher order statistics of an input path X effectively <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time-incorporated Layer</head><p>The Time-incorporated Layer is to add the time dimension to the input sequence pX ti q n i"1 ; in formula, the output is pt i , X ti q n i"1 . The log-signature of the time-incorporated transformation of a path is proved to fully recover the path by Lemma A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Universality of Logsig-RNN Network</head><p>In this section, we prove the universality of Logsig-RNN network to approximate a solution to any controlled differential equation under mild conditions. The motivation of the Logsig-RNN network comes naturally from the numerical approximation theory of the SDEs.</p><p>Let pX t q tPr0,T s and pY t q tPr0,T s be two stochastic processes under the probability space pΩ, F, P q such that Y is the solution to Equation (13) driven by the path X,</p><formula xml:id="formula_17">dY t " f pY t qdX t , Y S " ξ,<label>(13)</label></formula><p>where X has finite p-variation a.s., and f : R Ñ LpE, Rq is a smooth vector field satisfying certain regularity condition. Let I f denote the solution map which maps X J to Y T . The goal is to learn the solution map I f from the input-output pairs pX </p><p>where f˝m : R Ñ LpE bm , Rq is defined recursively by</p><formula xml:id="formula_19">f˝1 " f ; f˝k`1 " Dpf˝kqf,<label>(15)</label></formula><p>where Dpgq denotes the differential of the function g.</p><p>We paste the local Taylor approximation together to estimate for the solution on the whole time interval J. The strategy is outlined as follows. Fix a time partition D " pu k q N k"0 of J. We define the estimator pŶ D,M u k q k given by M -step Taylor expansion associated with D in the following recursive way: for k P t0,¨¨¨, N´1u,</p><formula xml:id="formula_20">Y D,M u0 " y 0 , Y D,M u k`1 "Ŷ D,M u k`M ÿ j"1 f˝jpŶ D,M u k qX j u k ,u k`1<label>(16)</label></formula><p>:</p><formula xml:id="formula_21">" g f D,M pl k ,Ŷ D,M u k q,</formula><p>where M is the degree of log-signature and l k is defined in Equation <ref type="formula">(</ref> k"0 as an input (see <ref type="figure" target="#fig_16">Figure 7</ref>). It motivates us to propose the Logsig-RNN Network (Model 3.1) to approximate the solution to any SDE under the regularity condition. This idea is natural if one thinks that g f D,M can be universally approximated by neural network. We establish the universality theorem of the Logsig-RNN network as follows. The proof can be found in the appendix. . For any ε ą 0, there exist the constants C 1 :" C 1 pp, γ, f, Kq and C 2 :" C 2 pf, Kq such that M ą tpu and ∆D ď minpε p{M`1´p C 1 , εC 2 q, l k is defined in Equation <ref type="bibr" target="#b8">(9)</ref>. Then there exists a RNN R σ p|Θq with some Θ, s.t.</p><formula xml:id="formula_22">sup SpXqPK ||Y T´Rσ ppl k q N k"1 |Θq|| ď ε.<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical Experiments</head><p>We demonstrate the performance of the Logsig-RNN algorithm on both synthetic SDE data and empirical data, including NTU RGB+D 120 action data and Chalearn 2013 gesture data in terms of the accuracy, efficiency and robustness. <ref type="bibr" target="#b9">10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Data Generated by a SDE</head><p>As an example of high frequency data, we simulate the solution Y T to the SDE of Example 5.1 using Milstein's method with the time step T 50000 for T " 10. An input path is the discretized Brownian motion WD, whereD " D 50001 11 . We simulate 2000 samples of pXD, Y T q, which is split to 80% for the training and the rest for the testing. Here we benchmark our approach with (1) RNN 0 : the conventional RNN model, (2) Sig-OLR: the linear model on the signature, (3) Sig-RNN: the RNN model with the signature sequence. Example 5.1. Suppose Y t satisfies the following SDE:  <ref type="table">Table 2</ref>: Comparison of methods on the SDEs data.</p><formula xml:id="formula_23">dY t " p´πY t`s inpπtqqdX p1q t`Yt dX p2q t , Y 0 " 0,<label>(18)</label></formula><formula xml:id="formula_24">where X t " pX p1q t , X p2q t q " pt, W t q, W t is</formula><p>As shown in <ref type="table">Table 2</ref>, we apply the above four methods for three kinds of inputs (1) XD (high frequency); (2) downsampling XD to 1k time steps (downsampling); (3) randomly throw away 5% points of 1k down sampled data (missing data). We compare the accuracy and training time of the Logsig-RNN algorithm. The training time is the first time of the loss function of the model to reach the error tolerance level 2˚10´6 before 25k epochs in the train set and the MSE is chosen as performance metric. First of all, <ref type="table">Table 2</ref> shows that the Logsig-RNN achieves the best accuracy for all three cases among all the methods. In particular, it is the most robust to missing data. Moreover, it reduces the time dimension of RNN from 50k{1k to 4, and thus significantly save the training time from 50930s to 343s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Action Recognition: NTU RGB+D 120 Data</head><p>NTU RGB+D 120 <ref type="bibr" target="#b22">[23]</ref> is a large-scale benchmark dataset for 3D action recognition, which consists of 114, 480 RGB+D video samples that are captured from 106 distinct human subjects for 120 action classes.</p><p>We use the network constructed by the path transformation layers following by the Logsig-RNN model (i.e. PT-Logsig-RNN shown in <ref type="figure" target="#fig_2">Figure 3</ref>) and apply it to the skeleton data of the NTU RGB+D 120 data (150 dimensional data streams) for action classification. The network configurations are given in appendix. <ref type="bibr" target="#b8">9</ref> f P C 8 b means f is infinitely differentiable and all the derivatives are bounded. This regularity condition can be weaken. <ref type="bibr" target="#b9">10</ref> We implement all algorithms in Tensorflow. It runs on a computer equipped with GeForce RTX 2080 Ti GPU. <ref type="bibr" target="#b10">11</ref> Dn denotes an equally spaced partition of r0, T s of n steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>X-Subject(%) X-Setup(%) Dynamic Skeleton <ref type="bibr" target="#b15">[16]</ref> 50.8 54.7 ST LSTM <ref type="bibr" target="#b25">[26]</ref> 55.7 57.9 FSNet <ref type="bibr" target="#b23">[24]</ref> 59.9 62.4 TS Attention LSTM <ref type="bibr" target="#b26">[27]</ref> 61.2 63.3 MT-CNN + RotClips <ref type="bibr" target="#b18">[19]</ref> 62.2 61.8 Pose Evolution Map <ref type="bibr" target="#b27">[28]</ref> 64.6 66.9 LSTM (baseline) 61.6 58.5 PT + LSTM 62.0 60.5 PT + Logsig +LSTM 65.7 64.5 <ref type="table">Table 3</ref>: Comparison of methods on the NTU RGB+D 120.</p><p>As shown in <ref type="table">Table 3</ref>, we subsequently add the Path Transformation Layers (PT) and the Log-signature layer (Logsig) to the baseline LSTM to validate the performance of each model. For X-Subject task, adding PT Layer results in a 0.4 percentage points (pp) gain over the baseline and the Logsig layer further gives a 3.7 pp gain. For X-Subject protocol, our method outperforms other methods. For X-Setup, our method is only beaten by <ref type="bibr" target="#b27">[28]</ref>. The latter leverages the informative pose estimation maps as additional clues. Notice that our PT-Logsig-LSTM is flexible enough to allow incorporating other advanced techniques (e.g. data augmentation and attention module) or combining multimodal clues (e.g. pose confidence score) to achieve further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Gesture Recognition: Chalearn 2013 data</head><p>The Chalearn 2013 dataset <ref type="bibr" target="#b6">[7]</ref> is a public available dataset for gesture recognition, which contains 20 Italian gestures performed by 27 subjects. It provides Kinect data, which contains RGB, depth, foreground segmentation and skeletons.</p><p>Here, we only use skeleton data (20 3D joints) for the gesture recognition (see appendix for more details).</p><p>We compare our method ( <ref type="figure" target="#fig_2">Figure 3)</ref> with several state-of-the-art methods <ref type="bibr" target="#b21">[22]</ref>. <ref type="table" target="#tab_2">Table 4</ref> shows that the PT-Logsig-RNN algorithm with M " 2 and N " 4 outperforms other methods in terms of the accuracy. We present both the results with/without the data-augmentation. With augmentation, our results significantly outperform others to achieve the state-of-the-art result.</p><p>Methods Accuracy(%) Data Aug. Deep LSTM <ref type="bibr" target="#b38">[39]</ref> 87.10T wo-stream LSTM <ref type="bibr" target="#b40">[41]</ref> 91.70 ' ST-LSTM + Trust Gate <ref type="bibr" target="#b24">[25]</ref> 92.00 ' 3s net TTM <ref type="bibr" target="#b21">[22]</ref> 92  Regarding to the robustness to missing data, we randomly set a certain percentage of frames (r) by all-zeros for each sample in the validation set, and evaluate the trained models of our method and RNN 0 to the new validation data. <ref type="table" target="#tab_4">Table 5</ref> shows that logsig-RNN model with M " 2 consistently beats the baseline RNN for different r, which validates the robustness of our method comparing with the benchmark.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The Logsig-RNN model, inspired from the numerical approximation theory of SDEs, provides an accurate, efficient and robust algorithm to learn a functional on streamed data. Numerical results show that it improves the performance of LSTM significantly on both synthetic data and empirical data. In ChaLearn2013 gesture data, PT-Logsig-RNN achieves the state-of-the-art classification accuracy. It is noted that the gesture or action data is naturally one kind of enormous continuous data streams in real world. When devices make higher frequency sampled data available, the proposed algorithm can be very suited in related tasks, while conventional downsampling-based RNNs probably fail.</p><p>The signature of X arises naturally as the basis function to represent the solution to linear controlled differential equation based on the Picard's iteration ( <ref type="bibr" target="#b29">[30]</ref>). It plays the role of non-commutative monomials on the path space. In particular, if X is a one dimensional path, the k th level of the signature of X can be computed explicitly by induction as follows that for every k P N,</p><formula xml:id="formula_25">X k J " pX T´XS q k k! .<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Multiplicative Property</head><p>The signature of paths of finite 1´variation has the multiplicative property, also called Chen's identity. Definition A.2. Let X : r0, ss Ñ E and Y : rs, ts Ñ E be two continuous paths. Their concatenation is the path denoted by X˚Y : r0, ts Ñ E defined by pX˚Y q u " " X u , u P r0, ss, Y u´Ys`Xs , u P rs, ts. Chen's identity asserts that the signature is a homomorphism between the path space and the signature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Calculation of the signature</head><p>In this subsection, we explain how to compute the truncated signature of a piecewise linear path. Let us start with a d-dimensional linear path. Lemma A.1. Let X : rS, T s Ñ E be a linear path. Then S n pXq " pX T´XS q bn n! .</p><p>Equivalently speaking, for any multi-index I " pi 1 ,¨¨¨, i n q,</p><formula xml:id="formula_27">S I " ś n j"1 pX pij q T q n!<label>(23)</label></formula><p>Chen's identity is a useful tool to enable compute the signature of the piecewise linear path numerically. Lemma A.2. Let X be a E-valued piecewise linear path, i.e.Xis the concatenation of a finite number of linear paths, and in other words there exists a positive integer land linear pathsX 1 , X 2 , ..., X l such that X " X 1˚X2˚˚Xl . Then</p><p>SpXq " b l i"1 exppX i q. Intuitively a tree-like path is a trajectory in which a section where the path exactly retraces itself. The tree-like equivalence is defined as follows: we say that two paths X and Y are the same up to the tree-like equivalence if and only if the concatenation of X and the inverse of Y is tree-like. Now we are ready to characterize the kernel of the signature transformation. Theorem A.2 shows that the signature of the path can recover the path trajectory under a mild condition. The uniqueness of the signature is important, as it ensures itself to be a discriminative feature set of un-parameterized streamed data. Remark A.1. A simple sufficient condition for the uniqueness of the signature of a path of finite length is that one component of X is monotone. Thus the signature of the time-joint path determines its trajectory (see <ref type="bibr" target="#b19">[20]</ref> </p><formula xml:id="formula_28">SpXq J " SpXq J .<label>(25)</label></formula><p>Re-parameterizing a path inside the interval does not change its signature. In <ref type="figure" target="#fig_1">Figure 2</ref>, speed changes result in different time series representation but the same signature feature. It means that signature feature can reduce dimension massively by removing the redundancy caused by the speed of traversing the path. It is very useful for the applications where the output is invariant w.r.t. the speed of an input path, e.g. online handwritten character recognition and video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.5 Shuffle Product Property</head><p>We introduce a special class of linear forms on T ppEqq; Suppose pe1 ,¨¨¨, ed ,¨¨¨q are elements of E˚. We can introduce coordinate iterated integrals by setting X piq u :" xei , X u y, and rewriting xei 1 b¨¨¨b ei n , SpXqy as the scalar iterated integral of coordinate projection. In this way, we realize n th degree coordinate iterated integrals as the restrictions of linear functionals in E bn to the space of signatures of paths. If pe 1 ,¨¨¨, e d q is a basis for a finite dimensional space E, and pe1 ,¨¨¨, ed q is a basis for the dual E˚. Therefore, it follows that</p><formula xml:id="formula_29">X J " ÿ kě0 i1,¨¨¨,i k tP1,2,¨¨¨,du ż . . . ż u1ă¨¨¨ău k u 1 ,...,u k PJ dX pi1q u1 b¨¨¨b dX pi k q u k e 1 b¨¨¨b e k .</formula><p>Theorem A.3 (Shuffle Algebra). The linear forms on T ppEqq induced by T pE˚q, when restricted to the range SpV p pr0, T s, Eq of the signature, form an algebra of real valued functions for p ă 2.</p><p>The proof can be found in page 35 in <ref type="bibr" target="#b29">[30]</ref>. The proof is based on the Fubini theorem, and it is to show that for any e˚, f˚P T pE˚q, such that for all a P SpV p pr0, T s, Eq, e˚paqf˚paq " pe˚¡ f˚qpaq</p><p>A.1.6 Universality of the signature Any functional on the path can be rewritten as a function on the signature based on the uniqueness of the signature (Theorem A.2). The signature of the path has the universality, i.e. that any continuous functional on the signature can be well approximated by the linear functional on the signature (Theorem A.4) <ref type="bibr" target="#b19">[20]</ref>.</p><p>Theorem A.4 (Signature Approximation Theorem). Suppose f : S 1 Ñ R is a continuous function, where S 1 is a compact subset of SpV p pJ, Eqq 13 . Then @ε ą 0, there exists a linear functional L P T ppEqq˚such that sup aPS1 ||f paq´Lpaq|| ď ε.</p><p>Proof. It can be proved by the shuffle product property of the signature and the Stone-Weierstrass Theorem.</p><p>A.2 The log-signature of a path</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Lie algebra and Lie series</head><p>If F 1 and F 2 are two linear subspaces of T ppEqq, let us denote by rF 1 , F 2 s the linear span of all the elements of the form ra, bs, where a P F 1 and b P F 2 . Consider the sequence pL n q ně0 be the subspace of T ppEqq defined recursively as follows: L 0 " 0; @n ě 1, L n " rE, L n´1 s.</p><p>Definition A.4. The space of Lie formal series over E, denoted as LppEqq is defined as the following subspace of T ppEqq:</p><p>LppEqq " tl " pl 0 ,¨¨¨, l n ,¨¨¨q|@n ě 0, l n P L n u.</p><p>Theorem 2.1 can be rewritten in the following form.</p><p>Theorem A.5 (Theorem 2.23 <ref type="bibr" target="#b29">[30]</ref>). Let X be a path of finite 1-variation. Then the log-signature of X is a Lie series in LppEqq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 The bijection between the signature and log-signature</head><p>Similar to the way of defining the logarithm of a tensor series, we have the exponential mapping of the element in T ppEqq defined in a power series form.</p><p>Definition A.5 (Exponential map). Let a " pa 0 , a 1 ,¨¨¨q P T ppEqq. Define the exponential map denoted by exp as follows:</p><p>exppaq " </p><p>Lemma A.4. The inverse of the logarithm on the domain ta P T ppEqq|a 0 ‰ 0u is the exponential map.</p><p>Theorem A.6. The dimension of the space of the truncated log signature of d-dimensional path up to degree n over d letters is given by:</p><formula xml:id="formula_35">DL n " 1 n ÿ d|n µpdqq n|d</formula><p>where µ is the Mobius function, which maps n to $ &amp; % 0, if n has one or more repeated prime factors 1,</p><p>if n " 1 p´1q k if n is the product of k distinct prime numbers</p><p>The proof can be found in Corollary 4.14 p. 96 of <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Calculation of the log-signature</head><p>Let's start with a linear path. The log signature of a linear path X J is nothing else, but the increment of the path X T´XS .</p><p>Baker-Cambpell-Hausdorff formula gives a general method to compute the log-signature of the concatenation of two paths, which uses the multiplicativity of the signature and the free Lie algebra. It provides a way to compute the log-signature of the piecewise linear path by induction.</p><p>Theorem A.7. For any S 1 , S 2 P LppEqq Z " logpe S1 e S2 q " ÿ ně1 p1,...,pně0 q1,....qně0 pi`qią0 p´1q n`1 n</p><formula xml:id="formula_36">1 p 1 !q 1 !...p n !q n ! rpS p1 1 S q1 2 ...S pn 1 S qn 2 q<label>(31)</label></formula><p>where r : A˚Ñ A˚is the right-Lie-bracketing operator, such that for any word w " a 1 ...a n rpwq " ra 1 , ..., ra n´1 , a n s...s.</p><p>This version of BCH is sometimes called the Dynkin's formula.</p><p>Proof. See remark of appendix 3.5.4 p. 81 in <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.4 Uniqueness of the log-signature</head><p>Like the signature, the log-signature has the uniqueness stated in the following theorem.</p><p>Theorem A.8 (Uniqueness of the log-signature). Let X P V p pJ, Eq . Then lSpXq determines X up to the tree-like equivalence defined in Definition A.3.</p><p>Theorem A.8 shows that the signature of the path can recover the path trajectory under a mild condition.</p><p>Lemma A.5. A simple sufficient condition for the uniqueness of the log-signature of a path of finite length is that one component of X is monotone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Comparison of the Signature and Log-signature</head><p>Both the signature and log-signature take the functional view on the discrete time series data, which allows a unified way to treat time series of variable length and missing data. For example, we chose one pen-digit data of length 53 and simulate 100 samples of modified pen trajectories by dropping at most 16 points from it, to mimic the missing data of variable length case (See one sample in <ref type="figure" target="#fig_12">Figure 4</ref>). <ref type="figure" target="#fig_13">Figure 5</ref> shows that the mean absolute relative error of the signature and log-signature of the missing data is no more than 6%. Besides the log-signature feature is more robust to missing data and of lower dimension compared with the signature feature.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Rough Path and the Extension Theory</head><p>Let C 0,p p∆ T , T tpu pEqq be the space of all continuous functions from the simplex ∆ T :" tps, tq|0 ď s ď t ď T u into the truncated tensor algebra T tpu pEq. We define the p-variation metric on this linear space as follows: for X, Y P C 0,p p∆ T , T tpu pEqq, set</p><formula xml:id="formula_37">d p pX, Y q " max 1ďiďtpu˜s up DĂr0,T s ÿ D ||X i t l´1 ,t l´Y i t l´1 ,t l || p i¸1 p .<label>(32)</label></formula><p>Definition A.6. Let p ě 1 be a real number and n ě 1 be an integer. Let ω : r0, T s Ñ r0, 8s be a control. Let X : ∆ T Ñ T pnq be a multiplicative functional. We say that X has finite p-variation on ∆ T controlled by ω if</p><formula xml:id="formula_38">||X i s,t || ď ωps, tq i p βp i p q! , @ps, tq P ∆ T .<label>(33)</label></formula><p>In general, we say that X has finite p-variation if there exists a control such that the conditions above are satisfied.</p><p>The concept of the rough path theory is a generalization of the signature of a path of finite 1-variation. Definition A.7 (Rough path). Let E be a Banach space. Let p ě 1 be a real number. A p-rough path in E is a multiplicative functional of degree tpu in E with finite p-variation. The space of p-rough paths is denoted by Ω p pV q. Definition A.8 (Geometric rough path). A geometric p-rough path is a p-rough path that can be expressed as a limit of 1-rough path in the p-variation distance defined above. The space of geometric p-rough path in E is denoted by GΩ p pEq.</p><p>Theorem A.9 (Extension theorem). Let X and Y be two multiplicative functional in T pnq pV q of finite p-variation, n ě tpu controlled by ω. Suppose that for some ε P p0, 1q,</p><formula xml:id="formula_39">||X i s,t´Y i s,t || ď ε ωps, tq i p β´i p¯! ,<label>(34)</label></formula><p>for i " 1,¨¨¨, n and for all ps, tq P ∆ T . Then provided β is chosen such that</p><formula xml:id="formula_40">β ě 2p 2¨1`8 ÿ r"3ˆ2 r´2˙t pu p‚ .<label>(35)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Proof of Global Approximation Theorem</head><p>In this section, we prove that the global approximation theorem of the high order Taylor expansion of the solution to the controlled differential equations. </p><formula xml:id="formula_41">||Y T´Ŷ D,M u N || ď ε,<label>(36)</label></formula><p>where SpXq P K, K is a compact set SpV p pJ, Eqq andC is a constant depending p, γ, the norm of f and the radius of K defined in Equation <ref type="bibr" target="#b40">(41)</ref>.</p><p>Before proceeding to the proof of the above theorem, we need the following auxiliary lemma and classical results on the numerical approximation of the SDEs (Theorem B.2). Lemma B.1. Let K be a compact set of GΩ p pJ, Eq for some p ě 1. J 1 is a compact sub-time interval of J. Then the mapping F : K Ñ T tpu pEq, i.e. x Þ Ñ logpx J 1 q is continuous. The image of K under the function F is compact.</p><p>Theorem B.2.</p><p>[10] Assume that X " p1, X 1 , . . . , X tpu q is a p-geometric rough path <ref type="bibr" target="#b13">14</ref> . Let f be a Lippγq vector field where γ ą p. Then there exists C :" Cpp, γq such that</p><formula xml:id="formula_42">||Y T´Ŷ D,M u N || ď C N ÿ k"1 |f | tγu`1 γ ||X|| tγu`1 p´var;rt k´1 ,t k s .<label>(37)</label></formula><p>Now we are ready to prove Theorem B.1.</p><p>Proof. According to Theorem B.2, there exists C :" Cpp, γq such that</p><formula xml:id="formula_43">||Y T´Ŷ D,M u N || ď C N ÿ k"1 |f | tγu`1 γ ||X|| tγu`1 p´var;rt k´1 ,t k s .<label>(38)</label></formula><p>It implies that the estimation error is of order ∆ tγu`1 p . As SpXq P K, then it exists a constant C 1 ą 0 s.t.</p><p>sup SpXqPK ||X|| p´var,J ď C 1 .</p><p>Equation <ref type="bibr" target="#b37">(38)</ref> implies that</p><formula xml:id="formula_45">||Y T´Ŷ D,M u N || ďC∆D tγu`1 p´1 .<label>(40)</label></formula><p>where C :" Cpp, γq is given in Theorem B.2 and</p><formula xml:id="formula_46">C " C N max k"1´| |f || |γ|`1 γ¯C p 1 .<label>(41)</label></formula><p>Appendix C Proof of Universality of the Logsig-RNN model</p><p>In this section, we prove that the universality of the Logsig-RNN model (Theorem 4.1 of our paper). Firstly, we introduce the auxiliary lemmas and then complete the proof of Theorem 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Auxiliary Lemmas</head><p>In the following, we use the uniform norm of a functionf : K Ñ R d , i.e. ||f || 8,K :" sup xPK |f pxq|.</p><p>The following lemma on the universality of shallow neural network was proved by Funahshi (1989). where C is a constant depending on the f and N , i.e.</p><p>C " Proof. As f andf are continuous functions and K is compact, then the image of G f and Gf for any px 1 ,¨¨¨, x N q P K are compact. Let ph i q N i"1 and ph i q N i"1 denote G f and Gf evaluated at px 1 , x 2 ,¨¨¨, x N q respectively. Then we have</p><formula xml:id="formula_47"># C N 1´1 C1´1 , if C 1 ‰ 1; N, if C 1 " 1.<label>(42)</label></formula><formula xml:id="formula_48">C 1 :" sup px1,¨¨¨,x N qPK N max k"1 || o f px k , o k q||.</formula><formula xml:id="formula_49">h i`1 " f px i`1 , h i q andh i`1 "f px i`1 ,h i q.</formula><p>Then it follows that</p><formula xml:id="formula_50">||h i`1´hi`1 || " ||f px i`1 , h i q´f px i`1 ,h i q|| ď ||f px i`1 , h i q´f px i`1 ,h i q||| |f px i`1 ,h i q´f px i`1 ,h i q|| ď ||f px i`1 , h i q´f px i`1 ,h i q||s up xPK ||Df px, hq||||h i´hi ||,</formula><p>which shows the recursive relation of ||h i´hi ||.</p><p>It is easy to check that if a i`1 ď C 0`C1 a i with a 0 " 0, it implies that</p><formula xml:id="formula_51">a i ď # C i 1´1 C1´1 C 0 , if C 1 ‰ 1 iC 0 , if C 1 " 1.</formula><p>Therefore using the above inequality when a i " ||h i´hi ||, C 0 " max xPK ||f´f ||, it follows that</p><formula xml:id="formula_52">||h i´hi || ď C||f´f || 8 ,</formula><p>and so does</p><formula xml:id="formula_53">||G f´Gf || 8,K ď C||f´f || 8,K ,</formula><p>where C is defined by Equation <ref type="formula" target="#formula_1">(42)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Theorem 4.1</head><p>First of all, let us give the intuition of the proof of Theorem 4.1. There is a remarkable similarity between the recursive structure of an RNN R σ and the one defined by the numerical Taylor approximation to solutions of SDEsŶ u N ( <ref type="figure" target="#fig_16">Figure  7)</ref>. It is noted that the numerical approximation of the solutionŶ D,M u k`1 represented in Equation <ref type="formula" target="#formula_0">(16)</ref>  we use G to denote the common recursive structure between those two. Specifically, for any given functionf : R d`e Ñ R e , define Gf :" Gf ,o1,N : R Nˆd Ñ R Nˆe as follows:</p><formula xml:id="formula_54">px 1 ,¨¨¨, x N q Þ Ñ po 1 ,¨¨¨, o N q,</formula><p>where o t`1 "f px t`1 , o t q, @t P t1,¨¨¨, N´1u. As o 1 is set to be the same and N is fixed in Theorem 4.1 , we skip the subscript o 1 and N in the notation Gf .</p><p>On the one hand, whenf px, sq :" AσpU x`W sq, where A is a matrix of dimension dˆe, x P R d and s P R e , then Gf is the RNN equipped with the activation function σ, denoted by Rp|Θq; on the other hand, the numerical solution</p><formula xml:id="formula_55">to SDEŶ u N is G g f D,M</formula><p>. Therefore the error E 2 is the norm of the difference between G AσpU x`W q and G g f</p><formula xml:id="formula_56">D,M , i.e. Y u N´R σ ppl k q N k"1 "´G g f D,M´G AσpU x`W q¯p pl k q N k"1 q (43)</formula><p>Proof. By the triangle inequality, it holds that</p><formula xml:id="formula_57">||Y T´Rσ ppl k q N k"1 |Θq|| ď ||Y T´Ŷu N || loooooomoooooon E1`| |Ŷ u N´R σ ppl k q N k"1 |Θq|| loooooooooooooomoooooooooooooon E2 .<label>(44)</label></formula><p>By Global Approximation Theorem (Theorem B.1), E 1 can be arbitrarily small by setting ∆D sufficiently small and truncation degree of the log-signature M sufficiently large. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Numerical Examples</head><p>In this section, we add one more empirical data experiment on the UCI pen-digit recognition data. Moreover, we provide the implementation details of our method for the NTURGB+D 120 action data and Chalearn 2013 gesture data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 UCI Pen-Digit Data</head><p>In this subsection, we apply the Logsig-RNN algorithm on the UCI sequential pen-digit data <ref type="bibr" target="#b14">15</ref> . In <ref type="table">Table 6</ref>  Robustness to missing data and change of sampling frequency To mimic the missing data case, we randomly throw a certain portion of points for each sample, and evaluated the trained models of Logsig-RNN and RNN 0 to the new testing data. <ref type="table">Table 6</ref> shows that our proposed method outperforms the other methods significantly for the missing data case. <ref type="figure">Figure 9</ref> shows the robustness of trained Logsig-RNN model for the down-sampled test data. Here the test data is down-sampled to that of length 8 provided in UCI data. For M " 4, the accuracy of testing data is above 80%, which is about 4 times of that of the baseline methods.  <ref type="table">Table 6</ref>: The accuracy of the modified testing set using different missing data rate (r). Here N " 4. <ref type="figure">Figure 9</ref>: Validation of the trained models on the down-sampled dataset. The accuracy of RNN D is below 13.5%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Action Recognition: NTU RGB+D 120 Data</head><p>In this subsection, we provide the network architecture and implementation details of the LP-Logsig-RNN model for the NTU RGB+D 120 dataset. In Tabel 7, it displays the path transform layers is mainly composed with two Conv2D layers in <ref type="bibr" target="#b43">[44]</ref> followed by one Conv1D layer. Before the LSTM layer, the starting frame of each segment of Conv1D output is incorporated into the output of Logsig layer. DropOut layer is applied after the LSTM layer with dropout rate 0.8. The number of hidden neurons of LSTM layer is 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Gesture Recognition: Chalearn 2013</head><p>We provide the implementation details of the PT-Logsig-RNN model depicted in <ref type="figure" target="#fig_2">Figure 3</ref> for the Chalearn 2013 data. The skeletons are pre-processed by first subtracting the central joint, which is the average position of all joints in one sample. Then we normalize the data and sample all clips to 39 frames by linear interpolation and uniform sampling. The path transform layers are composed of two Conv2D layers followed by a Conv1D layer, a Time-incorporated layer and an Accumulative Layer followed by the log-signature transformation with d 1 " 30. We add DropOut layer to both of the embedding layer and the LSTM layer to avoid over-fitting, where the two dropout rate are 0.3 and 0.5 resp. The number of hidden neurons of LSTM layer is 128. To make fair comparison, a DropOut layer is added to the benchmark RNN 0 . Three methods of data augmentation are used in the experiments. The first one is rotating coordinates along x, y, z axis in range of r´π{36, π{36s, r´π{18, π{18s and r´π{36, π{36s respectively. The second one is randomly shifting the frame temporally in range of r´5, 5s. The last one is adding Gaussian noise with standard deviation 0.001 to joints coordinates.  <ref type="table">Table 7</ref>: Architecture of the action recognition model</p><p>Proof. Suppose that Y andỸ be the solution to the following equation driven by X, i.e. Let Y t be the solution to the following equation</p><formula xml:id="formula_58">dY t " LpY t qdX t , Y 0 " y 0 dỸ t "LpỸ t qdX t ,Ỹ 0 "ỹ 0 ,<label>(46)</label></formula><p>where L andL are two linear vector fields. which can be rewritten as follows: there exist l 1 , l 2 P LpW, LpE, W qq, such that dpY piq tỸ pjq t q " xwj , l 1 pỸ t b Y t qdX t y`xwi , l 2 pỸ t b Y t qdX t y.</p><p>Therefore, xwi , Y t yxwj ,Ỹ t y can be rewritten as a linear functional on Z t " Y t bỸ t , which is the solution to a linear controlled differential equation driven by X, i.e.</p><formula xml:id="formula_59">dZ t " L Y,Ỹ pZ t qdX t ,</formula><p>where L Y,Ỹ : W b W Ñ LpE, W b W q.</p><p>Let C 1 pV 1 pJ, Eq, W q denote the space of continuous functionals on V 1 pJ, Eq taking values in W , which are invariant w.r.t time parameterization. Theorem E.2 (Universality of the linear controlled differential equations). The linear functionals on the solution map on the path space defined in Equation (46) are dense in the space C 1 pV 1 pJ, Eq, W q.</p><p>Proof. Let L be a trivial vector field and y 0 " 1. Then I V " 1. By Theorem E.1, the linear functions on the solution map form an algebra under the multiplication. By the Stone-Weierstrass Theorem, the proof is complete.</p><p>The set of the solution map driven by potential non-linear vector fields include all the solution driven by linear vector fields. Therefore we establish the universality of linear functionals on solution to controlled differential equations provided vector fields under the regularity condition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 : 1 .</head><label>11</label><figDesc>Comparison of Logsig-RNN and RNN. The Logsig-RNN network has the following advantages: Time dimension reduction: The Log-Signature Layer transforms a high frequency sampled time series to a sequence of the log-signatures over a potentially much coarser time partition. It reduces the time dimension of RNN significantly and thus speed up training time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The top-left figure represents the trajectory of the digit 2, and the rest of figures plot the coordinates of the pen location via different speed respectively, which share the same signature and log signature given in the first subplot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of PT-Logsig-RNN Model. It consists with the first Path Transformation Layers, the Log-Signature (Sequence) Layer, the RNN-type layer and the last fully connected layer. It is used for both action and gesture recognition in our experimental section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Model 3 . 1 (</head><label>31</label><figDesc>Logsig-RNN Network). Given D :" pu k q N k"0 , a Logsig-RNN network computes a mapping from an input path x D to an output defined as follows:• Compute pl k q N´1 k"0 as the output of the Log-Signature Layer of degree M for an input pxDq by Definition 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Remark 3 . 1 (</head><label>31</label><figDesc>Link between RNN model and Logsig-RNN model). For M " 1, Logsig-RNN network is reduced to the RNN model with px u k`1´x u k q N k"1 as an input. When D coincides withD, the Logsig-RNN Model is the RNN model with increment of raw data input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Ns i"1 , which are iid samples of pX J , Y T q. Classical numerical schemes of simulation of the solution to Equation (13) are mainly composed with two steps: 1. Local Approximation of Y t´Ys when t is close to s. 2. Paste the local approximation together to get the global approximation for Y T . Let us start with the local approximation of the solution to Equation (13) using step M -Taylor Expansion, i.e. dX t1 b¨¨¨b dX s k ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>9).Ŷ D,M u N converges to Y T when ∆D tends to 0 provided f is smooth enough (see Theorem B.1). Equation (16) exploits a remarkable similarity between the recursive structure of an RNN R σ and the one defined by the numerical Taylor approximation to solutions of SDEsŶ u N . It is noted that in Equation (16)Ŷ D,M u k`1 depends on Y D,M u k and the log-signature l k . In this way,Ŷ D,M u k plays a role similar to the hidden neurons of the RNN model with pl k q N´1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem 4 . 1 (</head><label>41</label><figDesc>Universality of Logsig-RNN Model). Let Y denote the solution of a SDE of form (13) under the previous regularity condition of Theorem B.2. Let K be any a compact set SpV p pJ, Eqq. Assume f P C 8 b 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Theorem A. 1 (</head><label>1</label><figDesc>Chen's identity). . Let X : r0, ss Ñ E and Y : rs, ts Ñ E be two continuous paths of bounded one-variation. ThenSpX˚Y q " SpXq b SpY q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( 24 )</head><label>24</label><figDesc>A.1.3 Uniqueness of the signature Let us start with introducing the definition of the tree-like path. Definition A.3 (Tree-like Path). A path X : J " rS, T s Ñ E is tree-like if there exists a continuous function h : J Ñ r0,`8q such that hpSq " hpT q " 0 and such that, for all s, t P J with s ď t, ||X t´Xs || ď hpsq`hptq´2 inf uPrs,ts hpuq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Theorem A. 2 (</head><label>2</label><figDesc>Uniqueness of the signature). Let X P V p pJ, Eq . Then SpXq determines X up to the tree-like equivalence defined in Definition A.3.<ref type="bibr" target="#b14">[15]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 :</head><label>4</label><figDesc>(Left) The chosen pen trajectory of digit 9. (Right) The simulated path by randomly dropping at most 16 points of the pen trajectory on the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 5 :</head><label>5</label><figDesc>Signature and Log-Signature Comparison for the missing data case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 :</head><label>6</label><figDesc>Signature and Log-Signature Comparison for the missing data case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Theorem B. 1 (</head><label>1</label><figDesc>Global Approximation Theorem). LetŶ D,M u N be defined as previously. For any ε ą 0, when ∆D ď ε C¯p {ptγu`1´pq and M ě tγu, thenŶ D,M u N satisfies that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 7 :</head><label>7</label><figDesc>The shared recursive structure of numerical approximation of the solutionŶ u N and the RNN R σ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>depends onŶ D,M u k and the log-signature l k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>, the Logsig-RNN with M " 4 and N " 4 achieves the accuracy 97.88% in the testing data compared with 95.80% of RNN 0 . In addition, the training time of the Logsig-RNN takes 30% of RNN D and 3% of the training time of RNN 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 8 :</head><label>8</label><figDesc>The accuracy comparison of Logsig-RNN in the testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>For</head><label></label><figDesc>any two basis wi and wj of W˚,xwi , Y t yxwj ,Ỹ t y " Y tL pỸ t qdX t y`xwi ,L pjq t LpY t qdX t y,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparison of methods on the Chalearn 2013 data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The accuracy (%) of the testing set with missing data with different dropping ratio (r). Here N " 4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Lemma C.1. Let σpxq be a sigmoid function (i.e. a non-constant, increasing, and bounded continuous function on R). Let K be any compact subset of R n , and f : K Ñ R e be a continuous function mapping. Then for an arbitrary ε ą 0, there exist an integer N ą 0, an mˆN matrix A and an N dimensional vector θ such that max xPK |f pxq´AσpBx`θq| ă , holds where σ : R N Þ Ñ R N is a sigmoid mapping defined by σp 1 pu 1 ,¨¨¨, u N qq " 1 pσpu 1 q,¨¨¨, σpu N qq.Lemma C.2. Let K be any compact subset of R d . Let f andf be two continuously differentiable functions on R d`e . Then it follows that ||G f´Gf || 8,K ă C||f´f || 8,K ,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The universality of the Logsig-RNN model is reduced to control the error E 2 , which is the difference between G AσpU x`W q and G g fD,M (Equation 43). Lemma C.1 ensures that the shallow neural network can approximate any continuous function uniformly well while Lemma C.2 demonstrates the continuity of the mapf Þ Ñ Gf . Combining both lemmas we are able to show that E 2 can be arbitrarily small provided that ∆D sufficiently small and degree M sufficiently large.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>0% 97.83% 97.83% 97.88% 97.40% 97.68% 95.80% 10% 97.63% 97.77% 97.14% 96.88% 97.60% 40.91% 20% 96.74% 97.06% 96.68% 95.85% 97.06% 37.28% 30% 95.99% 95.40% 95.17% 95.03% 95.77% 32.65%</figDesc><table><row><cell>r</cell><cell>M</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>RNN 0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">pendigit demo.ipdb can be found via the github link: https://github.com/logsigRNN/learn_sde/blob/ master/Pen-digit_learning/pendigit_demo.ipynb.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">In iisignature python package<ref type="bibr" target="#b17">[18]</ref>, logsigbackprop(deriv, path, s, Method = None) returns the derivatives of some scalar function F with respect to path, given the derivatives of F with respect to logsig(path, s, methods). Our implementation of the back-propogation algorithm of the log-signature layer uses logigbackprop() provided in iisignature.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">pendigit demo.ipynb is provided in the gitub via https://github.com/logsigRNN/learn_sde/blob/master/ Pen-digit_learning/pendigit_demo.ipynb</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">SpVppJ, Eqq denotes the range of the signature of x P VppJ, Eq.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">A geometric p-rough path is the limit of the sequence of the signature of paths of finite 1-variation in the p-variation distance. A discrete time series of finite length is an example of geometric p-rough path @p ě 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Appendix E The Universality of Controlled Differential Equations Definition E.1. Let Y t be the solution to the following equation</p><p>where X : r0, T s Ñ" E of finite 1-variation, and V P C 8 b and Y : r0, T s Ñ W :" R o . Let I V denote the solution map, i.e. for any X is in the admissible set,</p><p>Theorem E.1. The linear functional on the solution map on the path space defined in Equation (46) restricted to the linear vector fields forms an algebra with componentwise multiplication.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Parameter estimation in stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bishwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The pricing of options and corporate liabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scholes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of political economy</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="637" to="654" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The valuation of options for alternative stochastic processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of financial economics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="145" to="166" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-dimensional data analysis: The curses and blessings of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMS math challenges lecture</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A proposal on machine learning via dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. in Math. and Stat</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-modal gesture recognition challenge 2013: dataset and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An optimal polynomial approximation of brownian motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06998</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Physical brownian motion in a magnetic field as a rough path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Friz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gassiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7939" to="7955" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Friz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Victoir</surname></persName>
		</author>
		<title level="m">Multidimensional Stochastic Processes as Rough Paths: Theory and Applications. Cambridge Studies in Advanced Mathematics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximation of dynamical systems by continuous time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-I</forename><surname>Funahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Handbook of stochastic methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Gardiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sparse arrays of signatures for online character recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0371</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Extracting information from the signature of a financial data stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Gyurkó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kontkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Field</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.7244</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Uniqueness for the signature of a path of bounded variation and the reduced path group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hambly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2186" to="2200" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Iterated-Integral Signatures in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jeremy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The iisignature library: efficient calculation of iterated-integral signatures and log signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jeremy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Benjamin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08252.</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning from the past, predicting the statistics for the future, learning an evolving system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.0260</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lpsnet: a novel log path signature feature based hand gesture recognition framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="631" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Skeleton-based gesture recognition using several fully connected layers with path signature features and temporal transformer module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Skeleton-based online action prediction using scale selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page" from="2" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using spatio-temporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3007" to="3021" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global context-aware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10121</idno>
		<title level="m">Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Differential Equation driven by Rough Paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lévy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A feature set for streams and an application to high-frequency financial tick data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Big Data Science and Computing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Differential equations driven by rough signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Revista Matemática Iberoamericana</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="310" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Theory of rational option pricing. Theory of Valuation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Merton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="page" from="229" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Functional data analysis for volatility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Stadtmüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="245" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A multi-dimensional stream and its signature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.03346</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Parameter estimation for rough differential equations. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papavasiliou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ladroue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2047" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The iisignature library: efficient calculation of iterated-integral signatures and log signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08252</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Free lie algebras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reutenauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of algebra</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="6" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Smoothed functional principal components analysis by choice of norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning spatial-semantic context with fully convolutional recurrent network for online handwritten chinese text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1903" to="1917" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Leveraging the path signature for skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03993</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
