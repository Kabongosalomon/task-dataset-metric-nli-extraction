<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Gradients for Stochastic Differential Equations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research University of Toronto University of Toronto Vector Institute University of Toronto Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Kam</forename><forename type="middle">Leonard</forename><surname>Wong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research University of Toronto University of Toronto Vector Institute University of Toronto Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research University of Toronto University of Toronto Vector Institute University of Toronto Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research University of Toronto University of Toronto Vector Institute University of Toronto Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Gradients for Stochastic Differential Equations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The adjoint sensitivity method scalably computes gradients of solutions to ordinary differential equations. We generalize this method to stochastic differential equations, allowing time-efficient and constant-memory computation of gradients with high-order adaptive solvers. Specifically, we derive a stochastic differential equation whose solution is the gradient, a memory-efficient algorithm for caching noise, and conditions under which numerical solutions converge. In addition, we combine our method with gradient-based stochastic variational inference for latent stochastic differential equations. We use our method to fit stochastic dynamics defined by neural networks, achieving competitive performance on a 50-dimensional motion capture dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deterministic dynamical systems can often be modeled by ordinary differential equations (ODEs). The adjoint sensitivity method can efficiently compute gradients of ODE solutions with constant memory cost. This method was well-known in the physics, numerical analysis, and control communities for decades <ref type="bibr">[3,</ref><ref type="bibr">4,</ref><ref type="bibr">60,</ref><ref type="bibr">65]</ref>. Recently, it was combined with modern reverse-mode automatic differentiation packages, enabling ODEs with millions of parameters to be fit to data <ref type="bibr">[12]</ref> and allowing more flexible density estimation and time series models [23, 32, 72].</p><p>Stochastic differential equations (SDEs) generalize ODEs, adding instantaneous noise to their dynamics <ref type="bibr">[55,</ref><ref type="bibr">77,</ref><ref type="bibr">78]</ref>. They are a natural model for phenomena governed by many small and unobserved interactions, such as motion of molecules in a liquid <ref type="bibr">[8]</ref>, * Member of AI Residency program. Proceedings of the 23 rd International Conference on Artificial Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. PMLR: Volume 108. Copyright 2020 by the author(s).</p><p>allele frequencies in a gene pool <ref type="bibr">[15]</ref>, or prices in a market <ref type="bibr">[79]</ref>. Previous attempts on fitting SDEs mostly relied on methods with poor scaling properties. The pathwise approach <ref type="bibr">[22,</ref><ref type="bibr">89]</ref>, a form of forward-mode automatic differentiation, scales poorly in time with the number of parameters and states in the model. On the other hand, simply differentiating through the operations of an SDE solver <ref type="bibr">[19]</ref> scales poorly in memory.</p><p>In this work, we generalize the adjoint method to stochastic dynamics defined by SDEs. We give a simple and practical algorithm for fitting SDEs with tens of thousands of parameters, while allowing the use of high-order adaptive time-stepping SDE solvers. We call this approach the stochastic adjoint sensitivity method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Memory Time  There are two main difficulties in generalizing the adjoint formulation for ODEs to SDEs. The first is mathematical: SDEs are defined using nonstandard integrals that usually rely on Itô calculus. The adjoint method requires solving the dynamics backwards in time from the end state. However, it is not clear exactly what "running the SDE backwards" means in the context of stochastic calculus, and when it correctly reconstructs the forward trajectory. We address this problem in Section 3, deriving a backward Stratonovich SDE whose dynamics compute the necessary gradient.</p><p>The second difficulty is computational: To retrace the steps, one needs to reconstruct the noise sampled on the forward pass, ideally without storing it. In Section 4, we give an algorithm that allows querying a Brownian motion sample at any time point arbitrarily-precisely, arXiv:2001.01328v6 [cs.</p><p>LG] 18 Oct 2020 while only storing a single random seed.</p><p>We combine our adjoint approach with a gradientbased stochastic variational inference scheme for efficiently marginalizing over latent SDE models with arbitrary differentiable likelihoods. This model family generalizes several existing families such as latent ODEs <ref type="bibr">[12,</ref><ref type="bibr">72]</ref>, Gaussian state-space models <ref type="bibr">[36,</ref><ref type="bibr">81]</ref>, and deep Kalman filters <ref type="bibr">[40]</ref>, and can naturally handle irregularly-sampled times series and missing observations. We train latent SDEs on toy and real datasets, demonstrating competitive performance compared to existing approaches for dynamics modeling.</p><p>2 Background: Stochastic Flows</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adjoint Sensitivity Method</head><p>The adjoint sensitivity method is an efficient approach to solve control problems relying on the adjoint (costate) system <ref type="bibr">[65]</ref>. Chen et al. <ref type="bibr">[12]</ref> used this method to compute the gradient with respect to parameters of a neural ODE, which is a particular model among many others inspired by the theory of dynamical systems <ref type="bibr">[10,</ref><ref type="bibr">11,</ref><ref type="bibr">26,</ref><ref type="bibr">44,</ref><ref type="bibr">46,</ref><ref type="bibr">74,</ref><ref type="bibr">86]</ref>. The method, shown in Algorithm 1, is scalable, since the most costly computation is a vector-Jacobian product defining its backwards dynamics. In addition, since the gradient is obtained by solving another ODE, no intermediate computation is stored as in the case of regular backpropagation <ref type="bibr">[73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stochastic Differential Equations</head><p>Consider a filtered probability space (Ω, F, {F t } t∈T , P ) on which an m-dimensional adapted Wiener process (or Brownian motion) {W t } t∈T is defined. For a fixed terminal time T &gt; 0, we denote by T = [0, T ] the time horizon. We denote the ith component of W t by W (i) t . Due to space constraint, we refer the read to Appendix 9.1 for more on notation.</p><p>A stochastic process {Z t } t∈T can be defined by an Itô SDE</p><formula xml:id="formula_0">Z T = z 0 + T 0 b(Z t , t) dt+ m i=1 T 0 σ i (Z t , t) dW (i) t ,<label>(1)</label></formula><p>where z 0 ∈ R d is the starting state, and b : R d ×R → R d and σ i : R d × R → R d are the drift and diffusion functions, respectively. For ease of presentation, we let m = 1 in the following unless otherwise stated. Our contributions can be easily generalized to cases where m &gt; 1. Here, the second integral on the right hand side of (1) is the Itô stochastic integral <ref type="bibr">[55]</ref>. When the coefficients are globally Lipschitz in both the state and time, there exists a unique strong solution to the SDE [55].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Neural Stochastic Differential Equations</head><p>Similar to neural ODEs, one can consider drift and diffusion functions defined by neural networks, a model known as the neural SDE <ref type="bibr">[32,</ref><ref type="bibr">45,</ref><ref type="bibr">82,</ref><ref type="bibr">83</ref>].</p><p>Among works on neural SDEs, none has enabled an efficient training framework. In particular, Tzen and Raginsky <ref type="bibr">[82]</ref> and Liu et al. <ref type="bibr">[45]</ref> considered computing the gradient by simulating the forward dynamics of an explicit Jacobian matrix. This Jacobian has size of either the square of the number of parameters, or the number of parameters times the number of states, building on the pathwise approach <ref type="bibr">[22,</ref><ref type="bibr">89]</ref>. In contrast, our approach only requires a small number of cheap vector-Jacobian products, independent of the dimension of the parameter and state vectors. These vector-Jacobian products have the same asymptotic time cost as evaluating the drift and diffusion functions, and can be easily computed by modern automatic differentiation libraries <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">16,</ref><ref type="bibr">49,</ref><ref type="bibr">59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Backward Stratonovich Integral</head><p>Our stochastic adjoint sensitivity method involves stochastic processes running both forward and backward in time. The Stratonovich stochastic integral, due to its symmetry, gives nice expressions for the backward dynamics and is more convenient for our purpose. Our results can also be straightforwardly applied to Itô SDEs, relying on a simple conversion rule (see e.g. [64, Sec. 2]).</p><p>Following the treatment of Kunita [41], we introduce the forward and backward Stratonovich integrals. Let {F s,t } s≤t;s,t∈T be a two-sided filtration, where F s,t is the σ-algebra generated by</p><formula xml:id="formula_1">{W v − W u : s ≤ u ≤ v ≤ t} for s, t ∈ T such that s ≤ t. For a continuous semi- martingale {Y t } t∈T adapted to the forward filtration {F 0,t } t∈T , the Stratonovich stochastic integral is T 0 Y t • dW t = lim |Π|→0 N k=1 (Yt k +Yt k−1 ) 2 W t k − W t k−1 ,</formula><p>where Π = {0 = t 0 &lt; · · · &lt; t N = T } is a partition of the interval T = [0, T ], |Π| = max k t k −t k−1 denotes the size of largest segment of the partition, and the limit is to be interpreted in the L 2 sense. The Itô integral uses instead the left endpoint Y t k rather than the average. In general, the Itô and Stratonovich integrals differ by a term of finite variation.</p><p>To define the backward Stratonovich integral, we consider the backward Wiener process { W t } t∈T defined as W t = W t − W T for all t ∈ T that is adapted to the backward filtration {F t,T } t∈T . For a continuous semimartingale Y t adapted to the backward filtration, Algorithm 1 ODE Adjoint Sensitivity Input: Parameters θ, start time t 0 , stop time t 1 , final state z t1 , loss gradient ∂L/z t1 , dynamics f (z, t, θ).</p><formula xml:id="formula_2">def f ([z t , a t , ·], t, θ): Augmented dynamics v = f (z t , −t, θ) return [−v, a t ∂v/∂z, a t ∂v/∂θ ]   z t0 ∂L/∂z t0 ∂L/∂θ   = odeint     z t1 ∂L/∂z t1 0 p   , f , −t 1 , −t 0   return ∂L/∂z t0 , ∂L/∂θ</formula><p>Algorithm 2 SDE Adjoint Sensitivity (Ours) Input: Parameters θ, start time t 0 , stop time t 1 , final state z t1 , loss gradient ∂L/z t1 , drift f (z, t, θ), diffusion σ(z, t, θ), Wiener process sample w(t). the backward Stratonovich integral is</p><formula xml:id="formula_3">def f ([z t , a t , ·], t, θ): Augmented drift v = f (z t , −t, θ) return [−v, a t ∂v/∂z, a t ∂v/∂θ] def σ([z t , a t , ·], t, θ): Augmented diffusion v = σ(z t , −t, θ) return [−v, a t ∂v/∂z, a t ∂v/∂θ] def w(t): Replicated noise return [−w(−t), −w(−t), −w(−t)]   z t0 ∂L/∂z t0 ∂L/∂θ   = sdeint     z t1 ∂L/∂z t1 0 p   , f , σ, w, −t 1 , −t 0   return ∂L/∂z t0 , ∂L/∂θ</formula><formula xml:id="formula_4">T s Y t • d W t = lim |Π|→0 N k=1 Y t k + Y t k−1 2 W t k−1 − W t k ,</formula><p>where Π = {0 = t N &lt; · · · &lt; t 0 = T } is the partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Stochastic Flow of Diffeomorphisms</head><p>It is well known that an ODE defines a flow of diffeomorphisms [6]. Here we consider the stochastic analog for the Stratonovich SDE</p><formula xml:id="formula_5">Z T = z 0 + T 0 b(Z t , t) dt + T 0 σ(Z t , t) • dW t . (2)</formula><p>Throughout the paper, we assume that both b and σ have infinitely many bounded derivatives w.r.t. the state, and bounded first derivatives w.</p><formula xml:id="formula_6">r.t. time, i.e. b, σ ∈ C ∞,1 b</formula><p>, and thus the SDE has a unique strong solution. Let Φ s,t (z) := Z s,z t be the solution at time t when the process is started at z at time s. Given a realization of the Wiener process, this defines a collection of continuous maps S = {Φ s,t } s≤t;s,t∈T from R d to itself.</p><p>The following theorem shows that these maps are diffeomorphisms (after choosing a suitable modification) and that they satisfy backward SDEs. </p><formula xml:id="formula_7">Φ s,t (z) = Φ u,t (Φ s,u (z)), s ≤ u ≤ t, z ∈ R d .</formula><p>Moreover, each Φ s,t is a smooth diffeomorphism from R d to itself. We thus call S the stochastic flow of diffeomorphisms generated by the SDE (2).</p><p>(b) The backward flow Ψ s,t := Φ −1 s,t satisfies the backward SDE:</p><formula xml:id="formula_8">Ψ s,t (z) = z− t s b( Ψ u,t (z), u) du− t s σ( Ψ u,t (z), u) • d W u , (3)</formula><p>for all z ∈ R d and s, t ∈ T such that s ≤ t.</p><p>The coefficients in (2) and (3) differ by only a negative sign. This symmetry is due to our use of the Stratonovich integral (see <ref type="figure" target="#fig_2">Figure 2</ref>). We present our main contribution: a stochastic analog of the adjoint sensitivity method for SDEs. We use (3) to derive another backward Stratonovich SDE, which we call the stochastic adjoint process. The direct implication is a gradient computation algorithm that works by solving a set of dynamics in reverse time, and relies on cheap vector-Jacobian products without storing any intermediate quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stochastic Adjoint Process</head><p>The goal is to derive a stochastic adjoint process {∂L/∂Z t } t∈T that can be simulated by evaluating only vector-Jacobian products, where L = L(Z T ) is a scalar loss of the terminal state from the forward flow</p><formula xml:id="formula_9">Z T = Φ 0,T (z 0 ).</formula><p>We first derive a backward SDE for the process {∂Z T /∂Z t } t∈T , assuming that Z t = Ψ t,T (Z T ) follows the inverse flow from a deterministic end state Z T ∈ R d that does not depend on the realized Wiener process (Lemma 3.1). We then extend to the case where Z T = Φ 0,T (z 0 ) is obtained by the forward flow starting from a deterministic initial state z 0 ∈ R d (Theorem 3.2). This latter part is unconventional, and the resulting value cannot be interpreted as the solution to a backward SDE anymore due to loss of adaptedness. Instead, we will formulate the result with the Itô map [69]. Finally, it is straightforward to extend the state Z t to include parameters of the drift and diffusion functions such that the desired gradient can be obtained for stochastic optimization; we comment on this step in Section 3.3.</p><p>We first present the SDE for the Jacobian matrix of the backward flow.</p><p>Lemma 3.1 (Dynamics of ∂Z T /∂Z t ). Consider the stochastic flow generated by the backward SDE (3) as in</p><formula xml:id="formula_10">Theorem 2.1(b). Letting J s,t (z) := ∇ Ψ s,t (z), we have J s,t (z) = I d − t s ∇b( Ψ r,t (z), r)J r,t (z) dr− t s ∇σ( Ψ r,t (z), r)J r,t (z) • d W r , (4) for all s ≤ t and z ∈ R d . Furthermore, letting K s,t (z) = [J s,t (z)] −1 , we have K s,t (z) = I d + t s K r,t (z)∇b( Ψ r,t (z), r) dr+ t s K r,t (z)∇σ( Ψ r,t (z), r) • d W r , (5) for all s ≤ t and z ∈ R d .</formula><p>The proof included in Appendix 9.2 relies on Itô's lemma in the Stratonovich form [41, Theorem 2.4.1]. We stress that this lemma considers only the case where the endpoint z is fixed and deterministic. Now, we extend to the case where the endpoint is not deterministic, but rather computed from the forward flow. To achieve this, we compose the state process and the loss function. Consider A s,t (z) = ∂L(Φ s,t (z))/∂z.</p><formula xml:id="formula_11">The chain rule gives A s,t (z) = ∇L(Φ s,t (z))∇Φ s,t (z). Let A s,t (z) :=A s,t ( Ψ s,t (z)) = (6) ∇L(z)∇Φ s,t ( Ψ s,t (z)) = ∇L(z)K s,t (z). Note that A s,t (z) = A s,t (Φ s,t (z)). Since ∇L(z) is a constant, ( A s,t (z), Ψ s,t (z)) satisfies the augmented backward SDE system A s,t (z) =∇L(z) + t s A r,t (z)∇b( Ψ r,t (z), r) dr+ t s A r,t (z)∇σ( Ψ r,t (z), r) • d W r , Ψ s,t (z) =z − t s b( Ψ r,t (z), r) dr− t s σ( Ψ r,t (z), r) • d W r .<label>(7)</label></formula><p>Since the drift and diffusion functions of this augmented system are C ∞,1 b</p><p>, the system has a unique strong solution. Let s = 0 and t = T . Since (7) admits a strong solution, we may write</p><formula xml:id="formula_12">A 0,T (z) = F(z, W · ),<label>(8)</label></formula><p>where W · = {W t } 0≤t≤T denotes the path of the Wiener process and</p><formula xml:id="formula_13">F : R d × C([0, 1], R m ) → R d</formula><p>is a deterministic measurable function (the Itô map) [69, Chapter V, Definition 10.9]. Intuitively, F can be thought as a black box that computes the solution to the backward SDE system (7) given the position z at time T and the realized Wiener process sample. Similarly, we let G be the solution map for the forward flow (2). The next theorem follows immediately from (6) and the definition of F.</p><p>Theorem 3.2. For P -almost all ω ∈ Ω, we have</p><formula xml:id="formula_14">A 0,T (z) = A 0,T (G(z, W · )) = F(G(z, W · ), W · ),</formula><p>where G(z, W · ) = Φ 0,T (z).</p><p>Proof. This is a consequence of composing A 0,T (z) = A 0,T (Φ 0,T (z)) and (8).</p><p>This shows that one can obtain the gradient by "composing" the backward SDE system (7) with the original forward SDE (2) and ends our continuous-time analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Numerical Approximation</head><p>In practice, we compute solutions to SDEs with numerical solvers F h and G h , where h = T /L denotes the mesh size of a fixed grid. The approximate algorithm thus outputs F h (G h (z, W · ), W · ). The following theorem provides sufficient conditions for convergence.</p><p>Theorem 3.3. Suppose the schemes F h and G h satisfy the following conditions:</p><formula xml:id="formula_15">(i) F h (z, W · ) → F(z, W · ) and G h (z, W · ) → G(z, W · ) in probability as h → 0, and (ii) for any M &gt; 0, we have sup |z|≤M |F h (z, W · ) − F(z, W · )| → 0 in probability as h → 0.</formula><p>Then, for any starting point z of the forward flow, we have</p><formula xml:id="formula_16">F h (G h (z, W · ), W · ) → F(G(z, W · ), W · ) = A 0,T (z)</formula><p>in probability as h → 0.</p><p>See Appendix 9.3 for the proof. Usual schemes such as the Euler-Maruyama scheme (more generally Itô-Taylor schemes) converge pathwise (i.e. almost surely) from any fixed starting point <ref type="bibr">[38]</ref> and satisfies (i). While (ii) is strong, we note that the SDEs considered here have smooth coefficients, and thus their solutions enjoy nice regularity properties in the starting position. Therefore, it is reasonable to expect that the corresponding numerical schemes to also behave nicely as a function of both the mesh size and the starting position. To the best of our knowledge, this property is not considered at all in the literature on numerical methods for SDEs (where the initial position is fixed), but is crucial in the proof of Theorem 3.3. In Appendix 9.4, we prove that condition (ii) holds for the Euler-Maruyama scheme. Detailed analysis for other schemes is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Algorithm</head><p>So far we have derived the gradient of the loss with respect to the initial state. We can extend these results to give gradients with respect to parameters of the drift and diffusion functions by treating them as an additional part of the state whose dynamics has zero drift and diffusion. We summarize this in Algorithm 2, assuming access only to a black-box solver sdeint. All terms in the augmented dynamics, such as a t ∂f /∂θ and a t ∂σ/∂θ can be cheaply evaluated by calling vjp(a t , f, θ) and vjp(a t , σ, θ), respectively.</p><p>Difficulties with non-diagonal diffusion. In principle, we can simulate the forward and backward adjoint dynamics with any high-order solver of choice. However, for general matrix-valued diffusion functions σ, to obtain a numerical solution with strong order 1 beyond 1/2, we need to simulate multiple integrals of the Wiener process such as</p><formula xml:id="formula_17">t 0 s 0 dW (i) u dW (j) s , i, j ∈ [m], i = j.</formula><p>These random variables are difficult to simulate and costly to approximate <ref type="bibr">[87]</ref>.</p><p>Fortunately, if we restrict our SDE to have diagonal noise, then even though the backward SDE for the stochastic adjoint will not in general have diagonal noise, it will satisfy a commutativity property <ref type="bibr">[70]</ref>. In that case, we can safely adopt certain numerical schemes of strong order 1.0 (e.g. Milstein [52] and stochastic Runge-Kutta [71]) without approximating multiple integrals or the Lévy area during simulation. We formally show this in Appendix 9.5.</p><p>One may also consider numerical schemes with high weak order <ref type="bibr">[39]</ref>. However, analysis of this scenario is beyond the current scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Software and Implementation</head><p>We have implemented several common SDE solvers in PyTorch [59] with adaptive timestepping using a PI controller [9, 30]. Following torchdiffeq [12], we have created a user-friendly subclass of torch.autograd.Function that facilitates gradient computation using our stochastic adjoint framework for SDEs that are subclasses of torch.nn.Module. We include a short code snippet covering the main idea of the stochastic adjoint in Appendix 9.13. The complete codebase can be found at https://github.com/google-research/torchsde.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Virtual Brownian Tree</head><p>Our formulation of the adjoint can be numerically integrated efficiently, since simulating its dynamics only requires evaluating cheap vector-Jacobian products, as opposed to whole Jacobians. However, the backward-in-time nature introduces a new difficulty: The same Wiener process sample path used in the forward pass must be queried again during the backward pass. Naïvely storing Brownian motion increments implies a large memory consumption and complicates the usage of adaptive time-stepping integrators, where the evaluation times in the backward pass may be different from those in the forward pass.</p><p>To overcome this issue, we combine Brownian trees with splittable pseudorandom number generators (PRNGs) to give an algorithm that can query values of a Wiener process sample path at arbitrary times. This algorithm, which we call the virtual Brownian tree, has O(1) memory cost, and time cost logarithmic with respect to the inverse error tolerance. <ref type="figure">Figure 3</ref>: Evaluating a Brownian motion sample at time t q using a virtual Brownian tree. Our algorithm repeatedly bisects the interval, sampling from a Brownian bridge at each halving to determine intermediate values. Each call to the random number generator uses a unique key whose value depends on the path taken to reach it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Brownian Bridges and Brownian Trees</head><p>Lévy's Brownian bridge [67] states that given a start time t s and end time t e along with their respective Wiener process values w s and w e , the marginal of the process at time t ∈ (t s , t e ) is a normal distribution:</p><formula xml:id="formula_18">N (t e − t)w s + (t − t s )w e t e − t s , (t e − t)(t − t s ) t e − t s I d . (9)</formula><p>We can recursively apply this formula to evaluate the process at the midpoint of any two distinct timestamps where the values are already known. Constructing the whole sample path of a Wiener process in this manner results in what is known as the Brownian tree <ref type="bibr">[17]</ref>. Storing this tree would be memory-intensive, but we show how to reconstruct any node in this tree as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Brownian Trees using Splittable Seeds</head><p>We assume access to a splittable PRNG <ref type="bibr">[14]</ref>, which has an operation split that deterministically generates two keys from an existing key. Given a key, the function BrownianBridge samples deterministically from (9). To obtain the Wiener process value at a specific time, we must first know or sample the values at the initial and terminal times. Then, the virtual Brownian tree recursively samples from the midpoint of Brownian bridges, each sample using a key split from that of its parent node. The algorithm terminates when the most recently sampled time is close enough to the desired time. We outline the full procedure in Algorithm 3. This algorithm has constant memory cost. For a fixed-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Virtual Brownian Tree</head><p>Input: Seed s, query time t, error tolerance , start time t s , start state w s , end time t e , end state w e .</p><formula xml:id="formula_19">t m = (t s + t e )/2 s m , s l , s r = split(s, children=3) w m = BrownianBridge(t s , w s , t e , w e , t m , s m ) while |t − t m | &gt; do if t &lt; t m then t e , x e , s = t m , w m , s l else t s , x s , s = t m , w m , s r end if t m = (t s + t e )/2 s m , s l , s r = split(s, children=3) w m = BrownianBridge(t s , w s , t e , w e , t m , s m ) end while return w m</formula><p>step-size solver taking L steps, the tolerance that the tree will need to be queried at scales as 1/L. Thus the per-step time complexity scales as log L. Our implementation uses an efficient count-based PRNG [76] which avoids passing large random states, and instead simply passes integers. <ref type="table" target="#tab_0">Table 1</ref> compares the asymptotic time complexity of this approach against existing alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Latent Stochastic Differential Equations</head><p>The algorithms presented in Sections 3 and 4 allow us to efficiently compute gradients of scalar objectives with respect to SDE parameters, letting us fit SDEs to data. This raises the question: Which loss to optimize?</p><p>Simply fitting SDE parameters to maximize likelihood will in general cause overfitting, and will result in the diffusion function going to zero. In this section, we show how to do efficient variational inference in SDE models, and optimize the marginal log-likelihood to fit both prior (hyper-)parameters and the parameters of a tractable approximate posterior over functions.</p><p>In particular, we can parameterize both a prior over functions and an approximate posterior using SDEs:</p><formula xml:id="formula_20">dZ t = h θ (Z t , t) dt + σ(Z t , t) dW t ,<label>(prior)</label></formula><formula xml:id="formula_21">dZ t = h φ (Z t , t) dt + σ(Z t , t) dW t , (approx. post.)</formula><p>where h θ , h φ , and σ are Lipschitz in both arguments, and both processes have the same starting value:</p><formula xml:id="formula_22">Z 0 = Z 0 = z 0 ∈ R d .</formula><p>If both processes share the same diffusion function σ, then the KL divergence between them is finite (under additional mild regularity conditions; see Appendix 9.7), and can be estimated by sampling paths from the approximate posterior process. Then, the evidence lower bound (ELBO) can be written as:</p><formula xml:id="formula_23">z 0 z t 1 z t 2 z t n . . . w(·) SDESolve θ x t 1 x t 2 x t n . . . (a) Generation z 0 z t 1 z t 2 z t n . . . w(·) SDESolve φ x t 1 x t 2 x t n . . . (b) Recognition</formula><formula xml:id="formula_24">log p(x 1 , x 2 , . . . , x N |θ) ≥ (10) E Zt N i=1 log p(x ti |z ti ) − T 0 1 2 |u(z t , t)| 2 dt , where u : R d × [0, T ] → R m satisfies σ(z, t)u(z, t) = h φ (z, t) − h θ (z, t),</formula><p>and the expectation is taken over the approximate posterior process defined by (approx. post.). The likelihoods of observations x 1 , . . . , x N at times t 1 , . . . , t N depend only on latent states z t at corresponding times.</p><p>To compute the gradient with respect to prior parameters θ and variational parameters φ, we need only augment the forward SDE with an extra scalar variable whose drift is 1 2 |u(Z t , t)| 2 and diffusion is zero. The backward dynamics can be derived analogously using (7). We include a detailed derivation in Appendix 9.7. Thus, a stochastic estimate of the gradients of the loss w.r.t. all parameters can be computed in a single pair of forward and backward SDE solves.</p><p>The variational parameters φ can either be optimized individually for each sequence, or if multiple time series are sharing parameters, then an encoder network can be trained to input the observations and output φ. This architecture, shown in <ref type="figure" target="#fig_4">Figure 4</ref>, can be viewed as an infinite-dimensional Variational AutoEncoder (VAE) <ref type="bibr">[35,</ref><ref type="bibr">68]</ref>, whose latent is an SDE-induced stochastic process. We may generalize the above to cases where the diffusion is parameterized, which is then analogous to learning the prior of the latent code in VAEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Sensitivity Analysis for SDEs. Gradient computation is closely related to sensitivity analysis. Computing gradients with respect to parameters of vector fields of an SDE has been extensively studied in the stochastic control literature <ref type="bibr">[42]</ref>. In particular, for low dimensional problems, this is done effectively using dynamic programming <ref type="bibr">[</ref> They named this method the adjoint approach, which, by modern standards, is a form of "backpropagation through the operations of a numerical solver". This approach, widely adopted in the field of finance for  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>The aim of this section is threefold. We first empirically verify our theory by comparing the gradients obtained by our stochastic adjoint framework against analytically derived gradients for problems having closed-form solutions. We then fit latent SDE models with our framework on two synthetic datasets, verifying that the variational inference framework allows learning a generative model of time series. Finally, we learn dynamics parameterized by neural networks with a latent SDE from a motion capture dataset, demonstrating competitive performance compared to existing approaches.</p><p>We report results based on an implementation of Brownian motion that stores all intermediate queries. The virtual Brownian tree allowed training with much larger batch sizes on GPUs, but was not necessary for our small-scale experiments. Notably, our adjoint approach, even when combined with the Brownian motion implementation that stores noise, was able to reduce the memory usage by 1/2-1/3 compared to directly backpropagating through solver operations on the tasks we considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Numerical Studies</head><p>We consider three test problems (examples 1-3 from [66]; details in Appendix 9.8), all of which have closedform solutions. We compare the gradient computed from simulating our stochastic adjoint process using the Milstein scheme against the exact gradient. <ref type="figure">Figure 5</ref>(a) shows that for test example 2, the error between the adjoint gradient and analytical gradient decreases with step size.</p><p>For all three test problems, the mean squared error across dimensions tends to be smaller as the absolute tolerance of the adaptive solver is reduced (e.g. see <ref type="figure">Fig.  5 (b)</ref>). However, the Number of Function Evaluations (NFEs) tends to be much larger than that in the ODE case <ref type="bibr">[12]</ref>.</p><p>Additionally, for two out of three test problems, we found that our adjoint approach with the Milstein scheme and fixed step size can be much more timeefficient than regular backpropagation through operations of the Milstein and Euler schemes (see e.g. <ref type="figure">Fig.  5(c)</ref>). Backpropagating through the Euler scheme gives gradients of higher error compared to the Milstein method. On the other hand, directly backpropagating through the Milstein solve requires evaluating highorder derivatives and can be costly.</p><p>Results for examples 1 and 3 are in Appendix 9.9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Synthetic Datasets</head><p>We trained latent SDEs with our adjoint framework to recover (1) a 1D Geometric Brownian motion, and (2) a 3D stochastic Lorenz attractor process. The main objective is to verify that the learned posterior can reconstruct the training data, and that the learned priors are not deterministic. We jointly optimize the evidence lower bound (10) with respect to parameters of the prior and posterior distributions at the initial latent state z 0 , the prior and posterior drift, the diffusion function, the encoder, and the decoder. We include the details of datasets and architectures in Appendix 9.10.</p><p>For the stochastic Lorenz attractor, not only is the model able to reconstruct the data well, but also the learned prior process can produce bimodal samples in both data and latent space. This is showcased in the last row of <ref type="figure" target="#fig_6">Figure 6</ref> where the latent and data space samples cluster around two modes. This is hard to achieve using a latent ODE with a unimodal Gaussian initial approximate posterior. We include additional visualizations in Appendix 9.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Motion Capture Dataset</head><p>To demonstrate that latent SDEs can learn complex dynamics from real-world datasets, we evaluated their predictive performance on a 50-dimensional motion capture dataset.  We presented a generalization of the adjoint sensitivity method to compute gradients through solutions of SDEs. In contrast to existing approaches, this method has nearly the same time and memory complexity as simply solving the SDE. We showed how our stochastic adjoint framework can be combined with a gradientbased stochastic variational inference scheme for training latent SDEs.</p><p>It is worthwhile to mention that SDEs and the commonly used GP models define two distinct classes of stochastic processes, albeit having a nonempty intersection (e.g. Ornstein-Uhlenbeck processes fall under both). Computationally, the cost of fitting GPs lies in the matrix inversion, whereas the computational bottleneck of training SDEs is the sequential numerical solve. Empirically, another avenue of research is to reduce the variance of gradient estimates. In the future, we may adopt techniques such as control variates or antithetic paths.</p><p>On the application side, our method opens up a broad set of opportunities for fitting any differentiable SDE model, such as Wright-Fisher models with selection and mutation parameters [15], derivative pricing models in finance, or infinitely-deep Bayesian neural networks [61]. In addition, the latent SDE model enabled by our framework can be extended to include domain knowledge and structural or stationarity constraints [48] in the prior process for specific applications.</p><p>On the theory side, there remain fundamental questions to be answered. Convergence rates of numerical gradients estimated with general schemes are unknown. Additionally, since our analyses are based on strong orders of schemes, it is natural to question whether convergence results still hold when we consider weak errors, and moreover if the method could be reformulated more coherently with rough paths theory <ref type="bibr">[47]</ref>. machine learning. In 12th Symposium on Operating Systems Design and Implementation, pages 265-283, 2016.</p><p>[2] R Adams. Sobolev Spaces. Academic Press, 1975.</p><p>[3] Joel Andersson. A general-purpose software framework for dynamic optimization. PhD thesis, Arenberg Doctoral School, KU Leuven, 2013.</p><p>[4] Joel Andersson, Joris Gillis, Greg Horn, James B Rawlings, and Moritz Diehl. CasADi: a software framework for nonlinear optimization and optimal control. Mathematical Programming Computation, 11( [61] Stefano Peluchetti and Stefano Favaro. Neural stochastic differential equations. arXiv preprint arXiv:1904.01681, 2019.</p><p>[62] Shige Peng. A general stochastic maximum principle for optimal control problems. SIAM Journal on Control and Optimization, 28(4):966-979, 1990.</p><p>[63] Shige Peng and Zhen Wu. Fully coupled forwardbackward stochastic differential equations and applications to optimal control. SIAM Journal on Control and Optimization, 37(3):825-843, 1999.</p><p>[64] Eckhard Platen. An introduction to numerical methods for stochastic differential equations. [80] Evangelos Theodorou. Nonlinear stochastic control and information theoretic dualities: Connections, interdependencies and thermodynamic interpretations. Entropy, 17(5):3352-3375, 2015.</p><p>[81] Ryan Turner, Marc Deisenroth, and Carl Rasmussen. State-space inference and learning with gaussian processes. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 868-875, 2010.</p><p>[82] Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit. arXiv preprint arXiv:1905.09883, 2019.</p><p>[83] Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diffusions. Proceeings of the Conference on Learning Theory, 2019.</p><p>[84] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.</p><p>[85] Jack M Wang, David J Fleet, and Aaron Hertzmann. Gaussian process dynamical models for human motion. [90] Çağatay Yıldız, Markus Heinonen, and Harri Lähdesmäki. Ode2vae: Deep generative second order odes with bayesian neural networks. arXiv preprint arXiv:1905.10994, 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Notation</head><p>For a fixed terminal time T &gt; 0, we denote by T = [0, T ] ⊆ R the time horizon. Let C ∞ be the class of infinitely differentiable functions from R d to itself. Let C p,q be the class of functions from R d × T to R d that are p and q times continuously differentiable in the first and second input, respectively. Let C p,q b ⊆ C p,q be the subclass with bounded derivatives of all possible orders. For a positive integer m, we adopt the shorthand [m] = {1, 2, . . . , m}. We denote the Euclidean norm of a vector v by |v|. For f ∈ C p,q , we denote its Jacobian with respect to the first input by ∇f . We denote the concatenation of two vectors u ∈ R d1 and v ∈ R d2 by the simplified notation (u, v), as opposed to the slightly lengthy notation (u , v ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Proof of Theorem 3.1</head><p>Proof of Theorem 3.1. We have J s,t (z) = ∇ Ψ s,t (z), where Ψ s,t (z) is defined in (3). Now we take the gradient with respect to z on both sides. The solution is differentiable with respect to z and we may differentiate under the stochastic integral [41, Proposition 2.4.3]. Theorem 3.4.3 [41] is sufficient for the regularity conditions required. Since K s,t (z) = J s,t (z) −1 , applying the Stratonovich version of Itô's formula to (4), we have (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Proof of Theorem 3.3</head><p>Proof of Theorem 3.3. By the triangle inequality,</p><formula xml:id="formula_25">|F(G(z, W · ), W · ) − F h (G h (z, W · ), W · )| ≤ |F(G(z, W · ), W · ) − F(G h (z, W · ), W · )| I (1) h + |F(G h (z, W · ), W · ) − F h (G h (z, W · ), W · )| I (2) h .</formula><p>We show that both I <ref type="bibr" target="#b0">(1)</ref> h and I</p><p>(2) h converge to 0 in probability as h → 0. For simplicity, we suppress z and W · . Bounding I <ref type="bibr" target="#b0">(1)</ref> h . Let &gt; 0 be given. Since G h → G in probability, there exist M 1 &gt; 0 and h 0 &gt; 0 such that P(|G| &gt; M 1 ) &lt; , P(|G h | &gt; 2M 1 ) &lt; , for all h ≤ h 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By Lemma 2.1 (iv) of Ocone and Pardoux [54]</head><p>, which can be easily adapted to our context, there exists a positive random variable C 1 , finite almost surely, such that sup |z|≤2M1 |∇ z F| ≤ C 1 , and there exists M 2 &gt; 0 such that P(|C 1 | &gt; M 2 ) &lt; . Given M 2 , there exists h 1 &gt; 0 such that</p><formula xml:id="formula_26">P |G − G h | &gt; M 2 &lt; , for all h ≤ h 1 .</formula><p>Now, suppose h ≤ min{h 0 , h 1 }. Then, by the union bound, with probability at least 1 − 4 , we have</p><formula xml:id="formula_27">|G| ≤ M 1 , |G h | ≤ 2M 1 , |C 1 | ≤ M 2 , |G − G h | ≤ M 2 .</formula><p>On this event, we have I</p><formula xml:id="formula_28">(1) h = |F(G) − F(G h )| ≤ C 1 |G − G h | ≤ M 2 M 2 = .</formula><p>Thus, we have shown that I</p><p>h converges to 0 in probability as h → 0.</p><p>Bounding I</p><p>(2) h . The idea is similar. By condition (ii), we have</p><formula xml:id="formula_30">lim h→0 sup |z T |≤M |F h (z T ) − F(z T )| = 0</formula><p>in probability. Using this and condition (i), for given &gt; 0, there exist M &gt; 0 and h 2 &gt; 0 such that for all h ≤ h 2 , we have |G h | ≤ M and sup</p><formula xml:id="formula_31">|z T |≤M |F h (z T ) − F(z T )| &lt;</formula><p>with probability at least 1 − . On this event, we have</p><formula xml:id="formula_32">|F(G h ) − F h (G h )| ≤ sup |z T |≤M |F h (z T ) − F(z T )| &lt; .</formula><p>Thus I</p><p>(2)</p><p>h also converges to 0 in probability as h → 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Euler-Maruyama Scheme Satisfies Local Uniform Convergence</head><p>Here we verify that the Euler-Maruyama scheme satisfies condition (ii) when d = 1. Our proof can be extended to the case where d &gt; 1 assuming an L p estimate of the error; see the discussion after the proof of Proposition 9.1.</p><p>Proposition 9.1. Let F h (z) be the Euler-Maruyama discretization of a 1-dimensional SDE with mesh size h of F(z). Then, for any compact A ⊂ R, we have</p><formula xml:id="formula_33">plim h→0 sup z∈A |F h (z) − F(z)| = 0.</formula><p>Usual convergence results in stochastic numerics only control the error for a single fixed starting point. Here, we strengthen the result to local uniform convergence. Our main idea is to apply a Sobolev inequality argument [54, Part II]. To do so, we need some preliminary results about the Euler-Maruyama discretization of the original SDE and its derivative. We first recall a theorem characterizing the expected squared error for general schemes. . Furthermore, suppose that the numerical scheme has order of accuracy p 1 for the expectation of deviation and order of accuracy p 2 for the mean-square deviation. If p 1 ≥ p 2 + 1/2 and p 2 ≥ 1/2, then, for any N ∈ N,</p><formula xml:id="formula_34">k ∈ [N ], and z ∈ R d E |Z z t k −Z z k | 2 ≤ C 1 + |z| 2 h 2p2−1 ,</formula><p>for a constant C that does not depend on h or z.</p><p>We refer the reader to [51] for the precise definitions of orders of accuracy and the proof. Given this theorem, we establish an estimate regarding errors of the discretization and its derivative with respect to the initial position.</p><p>Lemma 9.3. We have</p><formula xml:id="formula_35">E |F(z) − F h (z)| 2 + |∇ z F(z) − ∇ z F h (z)| 2 ≤C 1 (1 + |z| 2 )h,</formula><p>where C 1 is a constant independent of z and h.</p><p>Proof of Lemma 9.3. Since the coefficients of the SDE are of class C ∞,1 b , we may differentiate the SDE in z to get the SDE for the derivative</p><formula xml:id="formula_36">∇ z Z z t [41]. Specifically, letting Y z t = ∇ z Z z t , we have Y z t = I d + t 0 ∇b(Z z s , s)Y z s ds + t 0 ∇σ(Z z s , s)Y z s dW s .</formula><p>Note that the augmented process (F(z), ∇ z F(z)) satisfies an SDE with C ∞,1 b coefficients. By the chain rule, one can easily show that the derivative of the Euler-Maruyama discretization F h (z) is the discretization of the derivative process Y z t . Thus, (F h (z), ∇ z F h (z)) is simply the discretization of (F(z), ∇ z F(z)). Since the Euler-Maruyama scheme has orders of accuracy (p 1 , p 2 ) = (1.5, 1.0) [51, Section 1.1.5], by Theorem 9.2, we have</p><formula xml:id="formula_37">E |F(z) − F h (z)| 2 + |∇ z F(z) − ∇ z F h (z)| 2 ≤ C 1 (1 + |z| 2 )h, z ∈ R d</formula><p>for some constant C 1 that does not depend on z or h.</p><p>We also recall a variant of the Sobolev inequality which we will apply for d = 1.</p><p>Theorem 9.4 (Sobolev inequality [2, Theorem 5.4.1.c]). For any p &gt; d, there exists a universal constant c p such that</p><formula xml:id="formula_38">sup x∈R d |f (x)| ≤ c p f 1,p , where f p 1,p := R d |f (x)| p dx + R d |∇ x f (x)| p dx,</formula><p>for all continuously differentiable f :</p><formula xml:id="formula_39">R d → R. Proof of Proposition 9.1. Define H α h : Ω × R → R, regarded as a random function H α h (ω) : R → R, by H α h (z) = F(z) − F h (z) (1 + |z| 2 ) 1/2+α ,</formula><p>where α &gt; 1/2 is a fixed constant. Since H α h is continuously differentiable a.s., by Theorem 9.4,</p><formula xml:id="formula_40">|F(z) − F h (z)| ≤ c 2 (1 + |z| 2 ) 1/2+α H α h 1,2 , for all z ∈ R a.s.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Without loss of generality, we may let the compact set be</head><formula xml:id="formula_41">A = {z : |z| ≤ M } where M &gt; 0. Then, sup |z|≤M |F(z) − F h (z)| ≤ c 2 (1 + M 2 ) 1/2+α H α h 1,2 , a.s.<label>(11)</label></formula><p>It remains to estimate H α h 1,2 . Starting from the definition of · 1,p , a standard estimation yields</p><formula xml:id="formula_42">H α h 2 1,2 ≤C 2 R |F(z) − F h (z)| 2 + |∇ z F(z) − ∇ z F h (z)| 2 (1 + |z| 2 ) 1+2α dz,</formula><p>where C 2 is a deterministic constant depending only on α (but not z and h).</p><p>Now we take expectation on both sides. By Lemma 9.3, we have</p><formula xml:id="formula_43">E H α h 2 1,2 ≤C 2 R E[|F(z) − F h (z)| 2 + |∇ z F(z) − ∇ z F h (z)| 2 ] (1 + |z| 2 ) 1+2α dz, ≤C 1 C 2 h R 1 (1 + |z| 2 ) 2α dz,</formula><p>where the last integral is finite since α &gt; 1/2.</p><p>We have shown that E H α h 2 1,2 = O(h). Thus H α h 1,2 → 0 in L 2 , and hence also in probability, as h → 0. From equation 11, we have that sup z∈A |F h (z) − F(z)| converges to 0 in probability as h → 0.</p><p>It is clear from the above proof that we may generalize to the case where d &gt; 1 and other numerical schemes if we can bound the expected W 1,p -norm of F h − F in terms of z and h, for p &gt; d, where W 1,p here denotes the Sobolev space consisting of all real-valued functions on R d whose weak derivatives are functions in L p . For the Euler scheme and d &gt; 1, we need only bound the L p norm of the discretization error in terms of z and h for general p. To achieve this, we would need to make explicit the dependence on z for existing estimates (see e.g. [39, Chapter 10]).</p><p>Generically extending the argument to other numerical schemes, however, is technically non-trivial. We plan to address this question in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">Stochastic Adjoint has Commutative Noise when Original SDE has Diagonal Noise</head><p>Recall the Stratonovich SDE (2) with drift and diffusion functions b, σ 1 , . . . , σ m ∈ R d × R → R d being governed by a set of parameters θ ∈ R p . Consider the augmented state composed of the state and parameters Y t = (Z t , θ). The augmented state satisfies a Stratonovich SDE with the drift function f (y, t) = (b(z, t), 0 p ) and diffusion functions g i (y, t) = (σ i (z, t), 0 p ) for i ∈ [m]. We have omitted the dependence on parameter θ to reduce notational clutter. By (5) and (6), the adjoint process of the augmented state follows the backward Stratonovich SDE:</p><formula xml:id="formula_44">A y t = A y T + T t A y s ∇f ( Y s , s) ds + m i=1 T t A y s ∇g i ( Y s , s) • d W (i) s .</formula><p>By definitions of f and g i , the Jacobian matrices ∇f (x, s) and ∇g i (x, s) have the forms</p><formula xml:id="formula_45">∇f (y, s) = ∇b(z, s) 0 d×p 0 p×d 0 p×p ∈ R (d+p)×(d+p) , ∇g i (y, s) = ∇σ i (z, s) 0 d×p 0 p×d 0 p×p ∈ R (d+p)×(d+p) .</formula><p>Thus, the backward Stratonovich SDEs for the adjoint processes of the state and parameters have the forms </p><formula xml:id="formula_46">1 2 |u(Z s , s)| 2 ds − t 0 u(Z s , s) dW s , 0 ≤ t ≤ T,</formula><formula xml:id="formula_47">dZ t = h θ (Z t , t) dt + σ(Z t , t) d W t , Z 0 = z 0 ,<label>(17)</label></formula><p>we conclude that the Q-law of (17) (or equivalently (16)) is the same as the P -law of the prior process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6.1">Deriving the Variational Bound</head><p>Let x t1 , . . . , x t N be observed data at times t 1 , . . . , t N , whose conditionals only depend on the respective latent states z t1 , . . . , z t N . Since the Q-law of the approximate posterior is the same as the P -law of the prior,</p><formula xml:id="formula_48">log p(x t1 , . . . , x t N ) = log E P N i=1 p(x ti |z ti ) = log E Q N i=1 p(x ti |z ti ) = log E P N i=1 p(x ti |z ti )M T ≥E P N i=1 log p(x ti |z ti ) + log M T =E P N i=1 log p(x ti |z ti ) − T 0 1 2 |u(Z t , t)| 2 dt − T 0 u(Z t ) dW t =E P N i=1 log p(x ti |z ti ) − T 0 1 2 |u(Z t , t)| 2 dt ,</formula><p>where the second line follows from the definition of Q and third line follows from Jensen's inequality. In the last equality we used the fact that the Itô integral · 0 u(Z t ) dW t is a martingale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.7">Stochastic Adjoint for Latent SDE</head><p>To simulate the variational lower bound (10) with Monte Carlo in the forward pass along with the original dynamics, we need only extend the original augmented state with an extra variable L t such that the new drift and diffusion functions for the new augmented state Y t = (Z t , θ, L t ) are</p><formula xml:id="formula_49">f (x, t) =   b(z, t) 0 p 1 2 |u(z, t)| 2 2   ∈ R d+p+1 , g i (x, t) =   σ i (z, t) 0 p 0   ∈ R d+p+1 , i ∈ [m].</formula><p>By <ref type="formula" target="#formula_11">(7)</ref>, the backward SDEs of the adjoint processes become</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.8">Test Problems</head><p>In the following, α, β, and p are parameters of SDEs, and x 0 is a fixed initial value.</p><formula xml:id="formula_50">Example 1. dX t = αX t dt + βX t dW t , X 0 = x 0 .</formula><p>Analytical solution:</p><formula xml:id="formula_51">X t = X 0 e β− α 2 2 t+αWt .</formula><p>Example 2.</p><p>dX t = − p 2 2 sin (X t ) cos 3 (X t ) dt + p cos 2 (X t ) dW t , X 0 = x 0 .</p><p>Analytical solution:</p><formula xml:id="formula_52">X t = arctan (pW t + tan (X 0 )) .</formula><p>Example 3.</p><formula xml:id="formula_53">dX t = β √ 1 + t − 1 2(1 + t) X t dt + αβ √ 1 + t dW t , X 0 = x 0 .</formula><p>Analytical solution:</p><formula xml:id="formula_54">X t = 1 √ 1 + t X 0 + β √ 1 + t (t + αW t ) .</formula><p>In each numerical experiment, we duplicate the equation 10 times to obtain a system of SDEs where each dimension had their own parameter values sampled from the standard Gaussian distribution and then passed through a sigmoid to ensure positivity. Moreover, we also sample the initial value for each dimension from a Gaussian distribution. dX t = µX t dt + σX t dW t , X 0 = x 0 .</p><p>We use µ = 1, σ = 0.5, and x 0 = 0.1 + as the ground-truth model, where ∼ N (0, 0.03 2 ). We sample 1024 time series, each of which is observed at intervals of 0.02 from time 0 to time 1. We corrupt this data using Gaussian noise with mean zero and standard deviation 0.01.</p><p>To recover the dynamics, we use a GRU-based [13] latent SDE model where the GRU has 1 layer and 100 hidden units, the prior and posterior drift functions are MLPs with 1 hidden layer of 100 units, and the diffusion function is an MLP with 1 hidden layer of 100 hidden units and the sigmoid activation applied at the end. The drift function in the posterior is time-inhomogenous in the sense that it takes in a context vector of size 1 at each observation that is output by the GRU from running backwards after processing all future observations. The decoder is a linear mapping from a 4 dimensional latent space to observation space. For all nonlinearities, we use the softplus function. We fix the observation model to be Gaussian with noise standard deviation 0.01.</p><p>We optimize the model jointly with respect to the parameters of a Gaussian distribution for initial latent state distribution, the prior and posterior drift functions, the diffusion function, the GRU encoder, and the decoder. We use a fixed discretization with step size of 0.01 in both the forward and backward pass. We use the Adam optimizer [34] with an initial learning rate of 0.01 that is decay by a factor of 0.999 after each iteration. We use a linear KL annealing schedule over the first 50 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.10.2">Stochastic Lorenz Attractor</head><p>Consider a stochastic Lorenz attractor SDE with diagonal noise:</p><formula xml:id="formula_55">dX t =σ (Y t − X t ) dt + α x dW t , X 0 = x 0 , dY t = (X t (ρ − Z t ) − Y t ) dt + α y dW t , Y 0 = y 0 , dZ t = (X t Y t − βZ t ) dt + α z dW t , Z 0 = z 0 .</formula><p>We use σ = 10, ρ = 28, β = 8/3, (α x , α y , α z ) = (.15, .15., .15), and (x 0 , y 0 , z 0 ) sampled from the standard Gaussian distribution as the ground-truth model. We sample 1024 time series, each of which is observed at intervals of 0.025 from time 0 to time 1. We normalize these samples by their mean and standard deviation across each dimension and corrupt this data by Gaussian noise with mean zero and standard deviation 0.01.</p><p>We use the same architecture and training procedure for the latent SDE model as in the geometric Brownian motion section, except that the diffusion function consists of four small neural networks, each for a single dimension of the latent SDE.   See <ref type="figure" target="#fig_12">Figure 8</ref> for additional visualization on the synthetic Lorenz attractor dataset. See <ref type="figure">Figure 9</ref> for visualization on the synthetic geometric Brownian motion dataset. We comment that for the second example, the posterior reconstructs the data well, and the prior process exhibit behavior of the data. However, from the third row, we can observe that the prior process is learned such that most of the uncertainty is account for in the initial latent state. We leave the investigation of more interpretable prior process for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.11">Additional Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.12">Model Architecture for Learning from Motion Capture Dataset</head><p>We use a latent SDE model with an MLP encoder which takes in the first three frames and outputs the mean and log-variance of the variational distribution of the initial latent state and a context vector. The decoder has a similar architecture as that for the ODE 2 VAE model [90] and projects the 6-dimensional latent state into the 50-dimensional observation space. The posterior drift function takes in a 3-dimensional context vector output by the encoder and the current state and time, whereas the prior drift only takes in the current state and time. The diffusion function is composed of multiple small neural nets, each producing a scalar for the corresponding <ref type="figure">Figure 9</ref>: Visualizations of learned posterior and prior dynamics on the synthetic geometric Brownian motion dataset. First row displays the true data and posterior reconstructions. Orange contour covers 95% of 512 samples. Second row displays samples with initial latent state for each trajectory is sampled independently. Third row displays samples with initial latent state sampled and fixed to be the same for different trajectories.</p><p>dimension such that the posterior SDE has diagonal noise. We use the same observation likelihood as that of the ODE 2 VAE model <ref type="bibr">[90]</ref>. We comment that the overall parameter count of our model (11605) is smaller than that of ODE 2 VAE for the same task (12157).</p><p>The latent ODE baseline was implemented with a similar architecture, except is does not have the diffusion and prior drift components, and its vector field defining the ODE does not take in a context vector. Therefore, the model has slightly fewer parameters (10573) than the latent SDE model. See <ref type="figure" target="#fig_1">Figure 10</ref> for overall details of the architecture.</p><p>The main hyperparameter we tuned was the coefficient for reweighting the KL. For both the latent ODE and SDE, we considered training the model with a reweighting coefficient in {1, 0.1, 0.01, 0.001}, either with or without a linear KL annealing schedule that increased from 0 to the prescribed value over the first 200 iterations of training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.13">Stochastic Adjoint Implementation</head><p>We include the core implementation of the stochastic adjoint, assuming access to a callable Brownian motion bm, an Euler-Maruyama integrator ito_int_diag for diagonal noise SDEs, and several helper functions whose purposes can be inferred from their names. # Take the result at the end time. adj_y = tuple(adj_y_ <ref type="bibr" target="#b0">[1]</ref> for adj_y_ in adj_y) adj_params_f, adj_params_g = adj_params_f <ref type="bibr" target="#b0">[1]</ref>, adj_params_g <ref type="bibr" target="#b0">[1]</ref> # Accumulate gradients at intermediate points. adj_y = _sequence_add( adj_y, tuple(grad_outputs_[i -1] for grad_outputs_ in grad_outputs) ) return (*adj_y, None, None, None, adj_params_f, adj_params_g, None, None)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Forward</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Pseudocode of the (ODE) adjoint sensitivity method (left), and our generalization to Stratonovich SDEs (right). Differences are highlighted in blue. Square brackets denote vector concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Negating the drift and diffusion functions for an Itô SDE and simulating backwards from the end state gives the wrong reconstruction. Negating the drift and diffusion functions for the converted Stratonovich SDE gives the same path when simulated backwards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1</head><label></label><figDesc>A numerical scheme is of strong order p if E [|XT − XNη|] ≤ Cη p for all T &gt; 0, where Xt and XNη are respectively the coupled true solution and numerical solution, N and η are respectively the iteration index and step size such that N η = T , and C is independent of η.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Graphical models for the generative process (decoder) and recognition network (encoder) of the latent stochastic differential equation model. This model can be viewed as a variational autoencoder with infinite-dimensional noise. Red circles represent entire function draws from Brownian motion. Given the initial state z 0 and a Brownian motion sample path w(·), the intermediate states z t1 , . . . , z tn are deterministically approximated by a numerical SDE solver.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>dt=0.001) Milstein backprop (dt=0.01) Milstein adjoint (dt=0.01) (c) Efficiency ComparisonFigure 5: (a) Same fixed step size used in both forward and reverse simulation. Boxplot generated by repeating the experiment with different Brownian motion sample paths 64 times. (b) Colors of dots represent tolerance levels and correspond to the colorbar on the right. Only atol was varied and rtol was set to 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Learned posterior and prior dynamics on data from a stochastic Lorenz attractor. All samples from our model are continuous-time paths, and form a multi-modal, non-Gaussian distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(2):283-298, 2007. [86] E Weinan. A proposal on machine learning via dynamical systems. Communications in Mathematics and Statistics, 5(1):1-11, 2017. [87] Magnus Wiktorsson et al. Joint characteristic function and simultaneous simulation of iterated itô integrals for multiple independent brownian motions. The Annals of Applied Probability, 11(2): 470-487, 2001. [88] Ronald J Williams. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229-256, 1992. [89] Jichuan Yang and Harold J Kushner. A monte carlo method for sensitivity analysis and parametric optimization of nonlinear stochastic systems. SIAM Journal on Control and Optimization, 29 (5):1216-1249, 1991.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Theorem 9.2 (Mean-square order of convergence [51, Theorem 1.1]). Let {Z z t } t≥0 be the solution to an Itô SDE, and {Z z k } k∈N be a numerical discretization with fixed step size h, both of which are started at z ∈ R d and defined on the same probability space. Let the coefficients of the SDE be C 1,∞ b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>is a P -martingale. By Girsanov Theorem II [56, Theorem 8.6.4], the processW t = t 0 u(Z s , s) ds + W t , 0 ≤ t ≤ Tis a Wiener process under the probability measure Q defined by dQ = M T dP, Moreover, since a simple rewrite shows that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>(a-c) Example 1. (d-f) Example 3. 9.9 Results for Example 1 and 3 9.10 Toy Datasets Configuration 9.10.1 Geometric Brownian Motion Consider a geometric Brownian motion SDE:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Additional visualizations of learned posterior and prior dynamics on the synthetic stochastic Lorenz attractor dataset. First row displays the true data and posterior reconstructions. Second row displays samples with initial latent state for each trajectory is sampled independently. Third row displays samples with initial latent state sampled and fixed to be the same for different trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Architecture specifics for the latent SDE model used to train on the mocap dataset. First row from left to right are the encoder and decoder. Second row from left to right are the prior drift, posterior drift, and diffusion functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>class _SdeintAdjointMethod(torch.autograd.Function): @staticmethod def forward(ctx, *args): (y0, f, g, ts, flat_params_f, flat_params_g, dt, bm) = ( args[:-8], args[-7], args[-6], args[-5], args[-4], args[-3], args[-2], args[-1]) ctx.f, ctx.g, ctx.dt, ctx.bm = f, g, dt, bm def g_prod(t, y, noise): g_eval = g(t=t, y=y) g_prod_eval = tuple( g_eval_i * noise_i for g_eval_i, noise_i in _zip(g_eval, noise)) return g_prod_eval with torch.no_grad(): ans = ito_int_diag(f, g_prod, y0, ts, dt, bm) ctx.save_for_backward(ts, flat_params_f, flat_params_g, *ans) return ans @staticmethod def backward(ctx, *grad_outputs): ts, flat_params_f, flat_params_g, *ans = ctx.saved_tensors f, g, dt, bm = ctx.f, ctx.g, ctx.dt, ctx.bm f_params, g_params = tuple(f.parameters()), tuple(g.parameters()) n_tensors = len(ans) def aug_f(t, y_aug): y, adj_y = y_aug[:n_tensors], y_aug[n_tensors:2 * n_tensors] with torch.enable_grad(): y = tuple(y_.detach().requires_grad_(True) for y_ in y) adj_y = tuple(adj_y_.detach() for adj_y_ in adj_y) g_eval = g(t=-t, y=y) gdg = torch.autograd.grad( outputs=g_eval, inputs=y, grad_outputs=g_eval, create_graph=True) f_eval = f(t=-t, y=y) f_eval = _sequence_subtract(gdg, f_eval) # -f + gdg. vjp_y_and_params = torch.autograd.grad( outputs=f_eval, inputs=y + f_params + g_params, grad_outputs=tuple(-adj_y_ for adj_y_ in adj_y), retain_graph=True, allow_unused=True) vjp_y = vjp_y_and_params[:n_tensors] vjp_f = vjp_y_and_params[-len(f_params + g_params):-len(g_params)] vjp_g = vjp_y_and_params[-len(g_params):] vjp_y = tuple(torch.zeros_like(y_) if vjp_y_ is None else vjp_y_ for vjp_y_, y_ in zip(vjp_y, y)) adj_times_dgdx = torch.autograd.grad( outputs=g_eval, inputs=y, grad_outputs=adj_y, create_graph=True) extra_vjp_y_and_params = torch.autograd.grad( outputs=g_eval, inputs=y + f_params + g_params, grad_outputs=adj_times_dgdx, allow_unused=True) extra_vjp_y = extra_vjp_y_and_params[:n_tensors] extra_vjp_f = extra_vjp_y_and_params[-len(f_params + g_params):-len(g_params)] extra_vjp_g = extra_vjp_y_and_params[-len(g_params):] extra_vjp_y = tuple( torch.zeros_like(y_) if extra_vjp_y_ is None else extra_vjp_y_ for extra_vjp_y_, y_ in zip(extra_vjp_y, y)) vjp_y = _sequence_add(vjp_y, extra_vjp_y) vjp_f = vjp_f + extra_vjp_f vjp_g = vjp_g + extra_vjp_g return (*f_eval, *vjp_y, vjp_f, vjp_g) def aug_g_prod(t, y_aug, noise): y, adj_y = y_aug[:n_tensors], y_aug[n_tensors:2 * n_tensors] with torch.enable_grad(): y = tuple(y_.detach().requires_grad_(True) for y_ in y) adj_y = tuple(adj_y_.detach() for adj_y_ in adj_y) g_eval = tuple(-g_ for g_ in g(t=-t, y=y)) vjp_y_and_params = torch.autograd.grad( outputs=g_eval, inputs=y + f_params + g_params, grad_outputs=tuple(-noise_ * adj_y_ for noise_, adj_y_ in zip(noise, adj_y)), allow_unused=True) vjp_y = vjp_y_and_params[:n_tensors] vjp_f = vjp_y_and_params[-len(f_params + g_params):-len(g_params)] vjp_g = vjp_y_and_params[-len(g_params):] vjp_y = tuple( torch.zeros_like(y_) if vjp_y_ is None else vjp_y_ for vjp_y_, y_ in zip(vjp_y, y) ) g_prod_eval = _sequence_multiply(g_eval, noise) return (*g_prod_eval, *vjp_y, vjp_f, vjp_g) def aug_bm(t): return tuple(-bmi for bmi in bm(-t)) T = ans[0].size(0) with torch.no_grad(): adj_y = tuple(grad_outputs_[-1] for grad_outputs_ in grad_outputs) adj_params_f = torch.zeros_like(flat_params_f) adj_params_g = torch.zeros_like(flat_params_g) for i in range(T -1, 0, -1): ans_i = tuple(ans_[i] for ans_ in ans) aug_y0 = (*ans_i, *adj_y, adj_params_f, adj_params_g) aug_ans = ito_int_diag( f=aug_f, g_prod=aug_g_prod, y0=aug_y0, ts=torch.tensor([-ts[i], -ts[i -1]]).to(ts), dt=dt, bm=aug_bm) adj_y = aug_ans[n_tensors:2 * n_tensors] adj_params_f, adj_params_g = aug_ans[-2], aug_ans[-1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Asymptotic complexity comparison. L is the number of steps used in a fixed-step solve, and D is the number of state and parameters. Both memory and time are expressed in units of the cost of evaluating the drift and diffusion functions once each.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>7] and finite differences [20, 43]. However, both approaches scale poorly with the dimensionality of the parameter vector. Analogous to REINFORCE (or the score-function estimator) [21, 37, 88], Yang and Kushner [89] considered deriving the gradient as ∇E [L(Z T )] = E [L(Z T )H] for some random variable H. However, H usually depends on the density of Z T with respect to the Lebesgue measure which can be difficult to compute. Gobet and Munos [22] extended this approach by weakening a nondegeneracy condition using Mallianvin calculus [53]. Closely related to the current approach is the pathwise method [89], which is also a continuous-time analog of the reparameterization trick [35, 68]. Existing methods in this regime [22, 45, 82] all require simulating a (forward) SDE where each step requires computing entire Jacobian matrices. This computational cost is prohibitive for high-dimensional systems with a large number of parameters. Based on the Euler discretization, Giles and Glasserman [19] considered simply performing reverse-mode automatic differentiation through all intermediate steps.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Ryder et al.[75]  perform variational inference over the state and parameters for Euler-discretized latent SDEs and optimize the model with backpropagation. This approach should not be confused with the formulation of variational inference for non-discretized SDEs presented in previous works [25, 57, 82] and our work, as it is unclear whether the limit of their discretization corresponds to that obtained by operating with continuous-time SDEs using Girsanov's theorem.Backward SDEs. Our stochastic adjoint process relies on the notion of backward SDEs devised by Kunita[41], which is based on two-sided filtrations. This is different from the more traditional notion of backward SDEs where only a single filtration is defined[58, 62]. Based on the latter notion, forward-backward SDEs (FBSDEs) have been proposed to solve stochastic optimal control problems [63]. However, simulating FBS-DEs is costly due to the need to estimate conditional expectations in the backward pass[58].Bayesian Learning of SDEs. Recent works considered the problem of inferring an approximate posterior SDE given observed data under a prior SDE with the same diffusion coefficient [25, 57, 82]. The special case</figDesc><table><row><cell>calibrating market models [19], has high memory cost,</cell></row><row><cell>and relies on a fixed Euler-Maruyama discretization.</cell></row><row><cell>Recently, this approach was also used by Hegde et al.</cell></row><row><cell>[27] to learn parameterized drift and diffusion functions</cell></row><row><cell>of an SDE. In scientific computing, Innes et al. [31] con-</cell></row><row><cell>sidered backpropagating through high-order implicit</cell></row><row><cell>SDE solvers.</cell></row></table><note>with constant diffusion coefficients was considered more than a decade ago [5]. Notably, computing the KL di- vergence between two SDEs over a finite time horizon was well-explored in the control literature [33, 80]. We include background on this topic in Appendix 9.6. Bayesian learning and parameter estimation for SDEs have a long history [24]. Techniques which don't re- quire positing a variational family such as the extended Kalman filter and Markov chain Monte Carlo have been considered in the literature [50].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Test MSE on 297 future frames averaged over 50 samples. 95% confidence interval reported based on t-statistic. † results from [90].</figDesc><table><row><cell>Method</cell><cell>Test MSE</cell></row><row><cell>DTSBN-S [18]</cell><cell>34.86 ± 0.02  †</cell></row><row><cell>npODE [28]</cell><cell>22.96</cell></row></table><note>† NeuralODE [12] 22.49 ± 0.88† ODE 2 VAE [90] 10.06 ± 1.4† ODE 2 VAE-KL [90] 8.09 ± 1.95† Latent ODE [12, 72] 5.98 ± 0.28 Latent SDE (this work) 4.03 ± 0.20</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>In ACM SIGPLAN Notices, volume 48, pages 47-58. ACM, 2013. Deep learning with differential gaussian process flows. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1812-1821, 2019.</figDesc><table><row><cell>volume 23. Springer Science &amp; Business Media,</cell><cell>method to universality, volume 192. Cambridge</cell></row><row><cell>2013.</cell><cell>University Press, 2012.</cell></row><row><cell>[15] Warren J Ewens. Mathematical population genetics [40] Rahul G Krishnan, Uri Shalit, and David Sontag.</cell><cell>[54] Daniel Ocone and Étienne Pardoux. A general-</cell></row><row><cell>1: theoretical introduction, volume 27. Springer Structured inference networks for nonlinear state</cell><cell>[28] Markus Heinonen, Cagatay Yildiz, Henrik Man-ized itô-ventzell formula. application to a class of</cell></row><row><cell>Science &amp; Business Media, 2012. space models. In Thirty-First AAAI Conference</cell><cell>nerström, Jukka Intosalmi, and Harri Lähdesmäki. anticipating stochastic differential equations. 25</cell></row><row><cell>on Artificial Intelligence, 2017. [16] Roy Frostig, Matthew James Johnson, and Chris Leary. Compiling machine learning programs via high-level tracing, 2018. [17] Jessica G Gaines and Terry J Lyons. Variable step size control in the numerical solution of stochastic differential equations. SIAM Journal on Applied Mathematics, 57(5):1455-1484, 1997. [18] Zhe Gan, Chunyuan Li, Ricardo Henao, David E Carlson, and Lawrence Carin. Deep temporal sig-moid belief networks for sequence modeling. In Advances in Neural Information Processing Sys-tems, pages 2467-2475, 2015. [19] Mike Giles and Paul Glasserman. Smoking ad-joints: Fast Monte Carlo greeks. Risk, 19(1):88-92, 2006. Management Science, 38(6):884-908, 1992. [41] Hiroshi Kunita. Stochastic Flows and Jump-Diffusions. Springer, 2019. [42] Harold Kushner and Paul G Dupuis. Numerical methods for stochastic control problems in continu-ous time, volume 24. Springer Science &amp; Business Media, 2013. [43] Pierre L'Ecuyer and Gaétan Perron. On the con-vergence rates of ipa and fdc derivative estimators. Operations Research, 42(4):643-656, 1994. [44] Qianxiao Li, Long Chen, Cheng Tai, and E Weinan. Maximum principle based algorithms for deep learning. The Journal of Machine Learning Re-search, 18(1):5998-6026, 2017. [45] Xuanqing Liu, Si Si, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh. Neural sde: Stabilizing neural ode networks with stochastic noise. arXiv preprint arXiv:1906.02355, 2019. [21] Peter W Glynn. Likelihood ratio gradient estima-tion for stochastic systems. Communications of the ACM, 33(10):75-84, 1990. [22] Emmanuel Gobet and Rémi Munos. Sensitivity FFJORD: Free-form continuous dynamics for scal-[24] Narendra Gupta and Raman Mehra. Computa-tional aspects of maximum likelihood estimation and reduction in sensitivity function calculations. Automatic Machine Learning, 2015. entiation of native python. In ICML workshop on and RP Adams. Autograd: Reverse-mode differ-Conference on Learning Representations, 2019. [49] Dougal Maclaurin, David Duvenaud, M Johnson, able reversible generative models. International pages 2917-2925, 2015. vances in Neural Information Processing Systems, tencourt, Ilya Sutskever, and David Duvenaud. plete recipe for stochastic gradient mcmc. In Ad-[23] Will Grathwohl, Ricky T. Q. Chen, Jesse Bet-[48] Yi-An Ma, Tianqi Chen, and Emily Fox. A com-1676-1713, 2005. 14(2):215-310, 1998. SIAM Journal on control and optimization, 43(5): rough signals. Revista Matemática Iberoamericana, gales, and application to stochastic optimal control. [47] Terry J Lyons. Differential equations driven by analysis using Itô-Malliavin calculus and martin-[46] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural networks: Bridg-ing deep architectures and numerical differential equations. arXiv preprint arXiv:1710.10121, 2017.</cell><cell>1):1-36, 2019. [5] Cédric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, and John S Shawe-Taylor. Varia-tional inference for diffusion processes. In Advances in Neural Information Processing Systems, pages 17-24, 2008. The Philosophical Magazine, 4(21):161-173, 1828. [9] Pamela M Burrage, R Herdiana, and Kevin Bur-rage. Adaptive stepsize based on control theory for stochastic differential equations. Journal of Computational and Applied Mathematics, 170(2): 317-336, 2004. [10] Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert. Multi-level residual networks from dynamical systems view. arXiv preprint arXiv:1710.10348, 2017. [11] Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible architectures for arbitrarily deep residual neural networks. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018. Learning unknown ode models with gaussian pro-(1):39-71, 1989. cesses. arXiv preprint arXiv:1803.04303, 2018. [55] Bernt Øksendal. Stochastic Differential Equations. [29] Irina Higgins, Loic Matthey, Arka Pal, Christo-Springer, 2003. pher Burgess, Xavier Glorot, Matthew Botvinick, [56] Bernt Oksendal. Stochastic differential equations: Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a con-strained variational framework. ICLR, 2(5):6, 2017. [30] Silvana Ilie, Kenneth R Jackson, and Wayne H Enright. Adaptive time-stepping for the strong nu-merical solution of stochastic differential equations. Numerical Algorithms, 68(4):791-812, 2015. [31] Mike Innes, Alan Edelman, Keno Fischer, Chris Rackauckus, Elliot Saba, Viral B Shah, and Will an introduction with applications. Springer Science &amp; Business Media, 2013. [57] Manfred Opper. Variational inference for stochas-tic differential equations. Annalen der Physik, 531 (3):1800233, 2019. [58] Etienne Pardoux and Shige Peng. Backward stochastic differential equations and quasilinear parabolic partial differential equations. In Stochas-tic Partial Differential Equations and Their Ap-plications, pages 200-217. Springer, 1992. Tebbutt. Zygote: A differentiable programming system to bridge machine learning and scien-tific computing. arXiv preprint arXiv:1907.07587, 2019. [59] Adam Paszke, Sam Gross, Soumith Chintala, Gre-gory Chanan, Edward Yang, Zachary DeVito, Zem-ing Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. Analysis of Time Series, pages 55-65. Springer, sian state space modeling. In Smoothness Priors [36] Genshiro Kitagawa and Will Gersch. Linear gaus-arXiv:1312.6114, 2013. encoding variational bayes. arXiv preprint [35] Diederik P Kingma and Max Welling. Auto-arXiv:1412.6980, 2014. method for stochastic optimization. arXiv preprint [34] Diederik P Kingma and Jimmy Ba. Adam: A 1244-1266, 2016. ference. Journal of Statistical Physics, 162(5): [33] Hilbert Johan Kappen and Hans Christian Ruiz. Adaptive importance sampling for control and in-[60] Barak A Pearlmutter. Gradient calculations for dy-namic recurrent neural networks: A survey. IEEE Transactions on Neural networks, 6(5):1212-1228, 1995.</cell></row><row><cell>IEEE transactions on automatic control, 19(6): [50] Isambi S Mbalawata, Simo Särkkä, and Heikki 774-783, 1974. [25] Jung-Su Ha, Young-Jin Park, Hyeok-Joo Chae, Soon-Seo Park, and Han-Lim Choi. Adaptive path-integral autoencoders: Representation learning Haario. Parameter estimation in stochastic differ-ential equations with markov chain monte carlo and non-linear kalman filtering. Computational Statistics, 28(3):1195-1223, 2013.</cell><cell>1996. [12] Ricky Tian Qi Chen, Yulia Rubanova, Jesse Bet-tencourt, and David K Duvenaud. Neural ordinary differential equations. In Advances in neural in-formation processing systems, pages 6571-6583, 2018. [37] Jack PC Kleijnen and Reuven Y Rubinstein. Op-timization and sensitivity analysis of computer simulation models by the score function method. European Journal of Operational Research, 88(3):</cell></row><row><cell>and planning for dynamical systems. In Advances [51] Grigori Noah Milstein and Michael V Tretyakov. in Neural Information Processing Systems, pages 8927-8938, 2018. Stochastic Numerics for Mathematical Physics. Springer Science &amp; Business Media, 2013.</cell><cell>413-427, 1996. [13] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning [38] Peter E Kloeden and Andreas Neuenkirch. The pathwise convergence of approximation schemes</cell></row><row><cell></cell><cell>phrase representations using rnn encoder-decoder for stochastic differential equations. LMS jour-</cell></row><row><cell></cell><cell>for statistical machine translation. arXiv preprint nal of Computation and Mathematics, 10:235-253,</cell></row><row><cell></cell><cell>arXiv:1406.1078, 2014. 2007.</cell></row><row><cell>'s</cell><cell></cell></row></table><note>[6] VI Arnold. Ordinary Differential Equations. The MIT Press, 1978.[7] Jonathan Baxter and Peter L Bartlett. Infinite- horizon gradient-based policy search. 2001.[8] Robert Brown. ... microscopical observations ... on the particles contained in the pollen of plants.[14] Koen Claessen and Michał H Pałka. Splittable pseudorandom number generators using crypto- graphic hashing.[20] Paul Glasserman and David D Yao. Some guide- lines and guarantees for common random numbers.[26] Eldad Haber and Lars Ruthotto. Stable architec- tures for deep neural networks. Inverse Problems, 34(1):014004, 2017.[27] Pashupati Hegde, Markus Heinonen, Harri Lähdesmäki, and Samuel Kaski.[32] Junteng Jia and Austin R. Benson. Neural Jump Stochastic Differential Equations. arXiv e-prints, art. arXiv:1905.10403, May 2019.[39] Peter E Kloeden and Eckhard Platen. Numer- ical solution of stochastic differential equations,[52] Grigorii Noikhovich Milstein. Numerical integra- tion of stochastic differential equations, volume 313. Springer Science &amp; Business Media, 1994.[53] Ivan Nourdin and Giovanni Peccati. Normal ap- proximations with Malliavin calculus: from Stein</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>68] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014. [69] L Chris G Rogers and David Williams.</figDesc><table><row><cell></cell><cell>Diffusions,</cell><cell></cell></row><row><cell cols="2">Markov Processes and Martingales: Volume 2, Itô</cell><cell></cell></row><row><cell cols="2">Calculus, volume 2. Cambridge University Press,</cell><cell></cell></row><row><cell>2000.</cell><cell></cell><cell></cell></row><row><cell>[70] Andreas Rößler.</cell><cell>Runge-Kutta methods for</cell><cell></cell></row><row><cell cols="2">stratonovich stochastic differential equation sys-</cell><cell></cell></row><row><cell cols="2">tems with commutative noise. Journal of Com-</cell><cell></cell></row><row><cell cols="2">putational and Applied mathematics, 164:613-627,</cell><cell></cell></row><row><cell>2004.</cell><cell></cell><cell></cell></row><row><cell cols="2">[71] Andreas Rößler. Runge-Kutta methods for the</cell><cell></cell></row><row><cell cols="2">strong approximation of solutions of stochastic</cell><cell></cell></row><row><cell cols="2">differential equations. SIAM Journal on Numerical</cell><cell></cell></row><row><cell cols="2">Analysis, 48(3):922-952, 2010.</cell><cell></cell></row><row><cell cols="2">[72] Yulia Rubanova, Ricky TQ Chen, and David Du-</cell><cell></cell></row><row><cell cols="2">venaud. Latent odes for irregularly-sampled time</cell><cell></cell></row><row><cell cols="2">series. Neural Information Processing Systems,</cell><cell></cell></row><row><cell>2019.</cell><cell></cell><cell></cell></row><row><cell cols="2">[73] David E Rumelhart, Geoffrey E Hinton, Ronald J</cell><cell></cell></row><row><cell cols="2">Williams, et al. Learning representations by back-</cell><cell></cell></row><row><cell cols="2">propagating errors. Cognitive Modeling, 5(3):1,</cell><cell></cell></row><row><cell>1988.</cell><cell></cell><cell></cell></row><row><cell cols="2">[74] Lars Ruthotto and Eldad Haber. Deep neural net-</cell><cell></cell></row><row><cell cols="2">works motivated by partial differential equations.</cell><cell></cell></row><row><cell cols="2">arXiv preprint arXiv:1804.04272, 2018.</cell><cell></cell></row><row><cell cols="2">[75] Thomas Ryder, Andrew Golightly, A Stephen Mc-</cell><cell></cell></row><row><cell cols="2">Gough, and Dennis Prangle. Black-box varia-</cell><cell></cell></row><row><cell cols="2">tional inference for stochastic differential equa-</cell><cell></cell></row><row><cell cols="2">tions. arXiv preprint arXiv:1802.03335, 2018.</cell><cell></cell></row><row><cell cols="2">[76] John K Salmon, Mark A Moraes, Ron O Dror,</cell><cell></cell></row><row><cell cols="2">and David E Shaw. Parallel random numbers: as</cell><cell></cell></row><row><cell cols="2">easy as 1, 2, 3. In Proceedings of 2011 Interna-</cell><cell></cell></row><row><cell cols="2">tional Conference for High Performance Comput-</cell><cell></cell></row><row><cell cols="2">ing, Networking, Storage and Analysis, page 16.</cell><cell></cell></row><row><cell>ACM, 2011.</cell><cell></cell><cell></cell></row><row><cell cols="2">[77] Simo Särkkä. Bayesian filtering and smoothing,</cell><cell></cell></row><row><cell cols="2">volume 3. Cambridge University Press, 2013.</cell><cell></cell></row><row><cell cols="2">[78] Simo Särkkä and Arno Solin. Applied stochas-tic differential equations, volume 10. Cambridge</cell><cell>numerica, 8:197-246, 1999.</cell><cell>Acta</cell></row><row><cell cols="2">University Press, 2019.</cell><cell cols="2">[65] Lev Semenovich Pontryagin. Mathematical Theory</cell></row><row><cell cols="2">[79] Steven E Shreve. Stochastic calculus for finance</cell><cell>of Optimal Processes. Routledge, 2018.</cell></row><row><cell cols="2">II: Continuous-time models, volume 11. Springer Science &amp; Business Media, 2004.</cell><cell cols="2">[66] Christopher Rackauckas and Qing Nie. Adaptive methods for stochastic differential equations via</cell></row><row><cell></cell><cell></cell><cell cols="2">natural embeddings and rejection sampling with</cell></row><row><cell></cell><cell></cell><cell cols="2">memory. Discrete and Continuous Dynamical Sys-</cell></row><row><cell></cell><cell></cell><cell>tems. Series B, 22(7):2731, 2017.</cell></row></table><note>[67] Daniel Revuz and Marc Yor. Continuous martin- gales and Brownian motion, volume 293. Springer Science &amp; Business Media, 2013.[</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Yulia Rubanova, Danijar Hafner, Mufan Li, Shengyang Sun, Kenneth R. Jackson, Simo Särkkä, Daniel Lacker, and Philippe Casgrain for helpful discussions. We thank Çağatay Yıldız for helpful discussions regarding evaluation settings of the mocap task. We also thank Guodong Zhang, Kevin Swersky, Chris Rackauckas, and members of the Vector Institute for helpful comments on an early draft of this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Now assume the original SDE has diagonal noise. Then, m = d and Jacobian matrix ∇σ i (z) has the form</p><p>Consider the adjoint process for the augmented state along with the backward flow of the backward Stratonovich SDE (3), whose overall state we denote by X t = ( Z t , A z t , A θ t ). By (12) and (13), { X t } t∈T satisfies a backward Stratonovich SDE with a diffusion of the form</p><p>where x = (z, a z , a θ ), and the subscript indexes the dimension. Recall, for an SDE with diffusion function </p><p>where the Brownian motion increment</p><p>can be easily sampled.</p><p>We show the diffusion function (14) satisfies the commutativity condition (15) with a proof by exhaustion:</p><p>Case 1: k = 1, . . . , d. Both LHS and RHS are zero unless j 1 = j 2 = k, since for Σ i,j2 (x) ∂Σ k,j 1 (x) ∂xi to be non-zero, i = j 1 = j 2 = k.</p><p>Case 2: k = d + 1 . . . , 2d. Similar to the case above.</p><p>Case 3: k = 2d + 1 . . . , 2d + p. Write k = 2d + l, where l ∈ [p]. Both LHS and RHS are zero unless j 1 = j 2 = l, since for Σ i,j2 (x)</p><p>to be non-zero i = l or i = d + l and j 1 = j 2 = l.</p><p>This concludes that the commutativity condition holds. Finally, we comment that the Milstein scheme for the stochastic adjoint of diagonal noise SDEs can be implemented such that during each iteration of the backward solve, vjp is only called a number of times independent of the dimensionality of the original SDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6">Background on Latent SDE</head><p>Consider a filtered probability space (Ω, F, {F t } 0≤t≤T , P ), where T = [0, T ] is a finite time horizon.</p><p>Recall the approximate posterior process that we intend to learn is governed by the SDE:</p><p>Suppose there exists a measurable function u(z, t) such that</p><p>Novikov's condition ensures that the process </p><p>In this case, neither does one need to simulate the backward SDE of the extra variable nor does one need to simulate its adjoint. Moreover, when considered as a single system for the augmented adjoint state, the diffusion function of the backward SDE (18) satisfies the commutativity property (15).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<title level="m">A system for large-scale</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
