<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Autoencoder for Combined Human Pose Estimation and Body Model Upscaling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Creative Intelligence Lab</orgName>
								<address>
									<country>Adobe Research</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Autoencoder for Combined Human Pose Estimation and Body Model Upscaling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Pose Estimation</term>
					<term>Multiple Viewpoint Video</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method for simultaneously estimating 3D human pose and body shape from a sparse set of wide-baseline camera views. We train a symmetric convolutional autoencoder with a dual loss that enforces learning of a latent representation that encodes skeletal joint positions, and at the same time learns a deep representation of volumetric body shape. We harness the latter to up-scale input volumetric data by a factor of 4×, whilst recovering a 3D estimate of joint positions with equal or greater accuracy than the state of the art. Inference runs in real-time (25 fps) and has the potential for passive human behavior monitoring where there is a requirement for high fidelity estimation of human body shape and pose. Fig. 1. Simultaneous estimation of 3D human pose and 4× upscaled volumetric body shape, from coarse visual hull data derived from a sparse set of wide-baseline views.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multiple viewpoint video of open spaces (e. g. for sports or surveillance) is often captured using a sparse set of wide-baseline static cameras, in which human subjects are relatively small (tens of pixels in height) due to their physical distance. Nevertheless, it is useful to infer human behavioural data from this limited knowledge for performance analytics or security. In this paper, we explore the possibility of using a deeply learned prior inferring high fidelity three-dimensional (3D) body shape and skeletal pose data from a coarse (low-resolution) volumetric estimate of body shape estimated across a sparse set of camera views <ref type="figure">(Fig. 1)</ref>.</p><p>The technical contribution of this paper is to explore the possibility of learning a deep representation for volumetric (3D) human body shape driven by a latent encoding for skeletal pose that can, in turn, be inferred from coarse volumetric shape data. Specifically, we investigate whether convolutional autoencoder architectures, commonly applied to 2D visual content for de-noising and up-scaling (super-resolution), may be adapted to up-scale volumetric 3D human shape whilst simultaneously providing high-level information on the 3D human pose from the bottle-neck (latent) representation of the autoencoder. We propose a symmetric autoencoder with 3D convolutional stages capable of refining a probabilistic visual hull (PVH) <ref type="bibr" target="#b0">[1]</ref> i. e. voxel occupancy data derived at very coarse scale (grid resolution 32 × 32 × 32 encompassing the subject). We demonstrate that our autoencoder is able to estimate an up-scaled body shape volume at up to 128 × 128 × 128 resolution, whilst able to estimate the skeleton joint positions of the subject to equal or better accuracy than the current state of the art methods due to deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work makes a dual contribution to two long-standing Computer Vision problems: super-resolution (SR) and human pose estimation (HPE).</p><p>Super-resolution: Data-driven approaches to image SR integrate pixel data e. g. from auxiliary images <ref type="bibr" target="#b1">[2]</ref>, or from a single image <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>) to perform image up-scaling or restoration. Model based approaches learn appearance priors from training images, applying these as optimization constraints to solve for SR content <ref type="bibr" target="#b4">[5]</ref>. A wide variety of machine learning approaches have been applied to the latter e. g. sparse coding <ref type="bibr" target="#b5">[6]</ref>, regression trees <ref type="bibr" target="#b6">[7]</ref>, and stacked autoencoders <ref type="bibr" target="#b7">[8]</ref>; many such approaches are surveyed in <ref type="bibr" target="#b8">[9]</ref>. Deep learning has more recently applied convolutional autoencoders for up-scaling of images <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> and video <ref type="bibr" target="#b12">[13]</ref>; our work follows suit, extending symmetric autoencoders commonly used for image restoration to volumetric data using 3D (up-)convolutional layers <ref type="bibr" target="#b13">[14]</ref>. Our work is not the first to propose volumetric super-resolution. Data-driven volumetric SR has been explored using multiple image fusion across the depth of field in <ref type="bibr" target="#b14">[15]</ref> and across multiple spectral channels in <ref type="bibr" target="#b5">[6]</ref>. Very recent work by Brock et al. explores deep variational auto-encoders for volumetric SR of objects <ref type="bibr" target="#b15">[16]</ref>. However, our work is unique in its ability to upscale to 4× whilst simultaneously estimating human pose to a high accuracy, exploiting a learned latent representation encoding joint positions.</p><p>Human pose estimation has been classically approached through top-down fitting of models such as Pictorial structures <ref type="bibr" target="#b16">[17]</ref>, fused with Ada-Boost shape classification in <ref type="bibr" target="#b17">[18]</ref>. Conditional dependencies between parts (limbs) during model fitting were explored in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Huang <ref type="bibr" target="#b20">[21]</ref> tracked 3D mesh deformation over time and attach a skeleton to tracked vertices. The SMPL body model <ref type="bibr" target="#b21">[22]</ref> provides a rich statistical body model that can be fitted to (possibly incomplete) visual data. Marcard <ref type="bibr" target="#b22">[23]</ref> explored the orthogonal modality of IMU measurements using SMPL for HPE without visual data. Malleson <ref type="bibr" target="#b23">[24]</ref> used IMUs with a full <ref type="figure">Fig. 2</ref>. Overview of the proposed method. A coarse PVH is estimated as input volumetric data (32 3 voxels) and up-scaled via tricubic interpolation to a (32n) 3 voxel grid (where n = {1, 2, 4}). The input PVH is deeply encoded to the latent feature representation (3D joint positions). Non-linear decoding of the feature via successive up-convolutional layers yields a higher fidelity PVH of (32n) 3 voxels. kinematic solve to estimate 3D pose. SMPL was recently applied to a deep encoder-decoder network to estimate 3D pose from 2D images <ref type="bibr" target="#b24">[25]</ref>. Several deep approaches estimate 2D pose or infer 3D pose from intermediate 2D estimations. DeepPose <ref type="bibr" target="#b25">[26]</ref> applies a convolutional neural network (CNN) cascade. Descriptors learned via CNN have been used in 2D pose estimation from low-resolution 2D images <ref type="bibr" target="#b26">[27]</ref> and real-time multi-subject 2D pose estimates were demonstrated by cao <ref type="bibr" target="#b27">[28]</ref>. Sanzari <ref type="bibr" target="#b28">[29]</ref> estimates the location of 2D joints, before predicting 3D pose using appearance and probable 3D pose of parts. Zhou <ref type="bibr" target="#b29">[30]</ref> integrates 2D, 3D and temporal information to account for uncertainties in the data.</p><p>The challenge of estimating 3D human pose from volumetric data is more sparsely explored. Trumble <ref type="bibr" target="#b30">[31]</ref> used a spherical histogram and later voxel input to regress a pose estimate using a CNN <ref type="bibr" target="#b31">[32]</ref>. Pavlakos <ref type="bibr" target="#b32">[33]</ref> used a simple volumetric representation in a 3D convnet for pose estimation. While Tekin <ref type="bibr" target="#b33">[34]</ref> included a pretrained autoencoder within the network to enforce structural constraints. Our work also trains an autoencoder for HPE but simultaneously infers a high resolution body model via a dual loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Estimating Human Pose and Body Shape</head><p>Our method accepts a coarse resolution volumetric reconstruction of a subject as input, and in a single inference step estimates both the skeletal joint positions and a higher resolution (up-scaled) volumetric reconstruction of that subject <ref type="figure">(Fig. 2</ref>). Sec. 3.1 first describes how the input volumetric reconstruction is formed, through a simplified form of Graumann 's probabilistic visual hull (PVH) <ref type="bibr" target="#b0">[1]</ref>. The architecture of our 3D convolutional autoencoder is then described in Sec. 3.2 including the dual loss function necessary to learn a deep representation of body shape and the latent pose representation. Finally, Sec. 3.3 describes the data augmentation and methodology for training the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Volumetric Representation</head><p>The capture volume V ∈ R 3 containing the subject is observed by a set of C calibrated cameras c = [1, C] for which camera world position T c and orientation R c (both matrices in homogeneous form) are known as are intrinsics: camera focal length (f c ) and optical center [o x c , o y c ]. An external process (e. g. a person tracker) is assumed to isolate the bounding sub-volume X I ∈ V corresponding to, and centered upon, a single subject of interest, and which is decimated to a coarse voxel grid <ref type="bibr">32 3</ref> ] where V denotes the coarse voxel volume passed as input to the network in Sec 3.2. Each voxel v i ∈ V projects to coordinates (x[v i ], y[v i ]) in each camera view c derived in homogeneous form via pin-hole projection:</p><formula xml:id="formula_0">V = {v i x , v i y , v i z } for i = [1, ...,</formula><formula xml:id="formula_1">  αx[v i ] αy[v i ] α   =   f c 0 o x c 0 0 f c o y c 0 0 0 1 0   −R −1 c T c     v i x v i y v i z 1     .<label>(1)</label></formula><p>Given a soft matte I c obtained, for example by background (clean-plate) subtraction, the probability of the voxel being part of the performer in a given view c is:</p><formula xml:id="formula_2">p(v i |c) = I c (x[v i ], y[v i ]).<label>(2)</label></formula><p>The overall probability of occupancy for a given voxel p(v i ) is:</p><formula xml:id="formula_3">p(v i ) = C i=1 1/(1 + e p(v i |c) ).<label>(3)</label></formula><p>For all voxels v i ∈ V we compute p(v i ) to form the coarse input PVH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual Loss Convolutional Autoencoder</head><p>We use a convolutional autoencoder with a symmetrical 'hourglass' (encoderdecoder) architecture. The goal of the network is learn a deep representation given an input tensor V I ∈ R N ×N ×N ×1 encoding the coarse PVH, V at a given resolution N = (32n) 3 , where n = {1, 2, 4} is a configuration parameter determining the degree of up-scaling required from the network (1×, 2×, 4×) respectively. The coarse PVH input V is scaled via tri-cubic interpolation to fit V I . We train the deep representation to solve the prediction problem</p><formula xml:id="formula_4">V H = F(V I ) for similarly encoded output tensor V O , where V O = F(V I ) = D(E(V I ))<label>(4)</label></formula><p>for the end to end trained encoder (E) and decoder (D) functions The encoder yields a latent feature representation via a series of 3D convolutions, max-pooling and fully-connected layers. We enforce J(V I ) = E(V I ) where J(V I ) is a skeletal pose vector corresponding the input PVH; specifically a 78-D vector concatenation of 26× 3D Cartesian joint coordinates in {x, y, z}. The decoder half of the network inverts this process to output tensor V O matching the input resolution but with higher fidelity content. <ref type="figure" target="#fig_0">Fig. 3</ref> illustrates our architecture which During inference these are passed through an LSTM to enhance temporal consistency to produce the joint position skeleton estimate. Architecture pictured here is for 2× scaleup -in order to accommodate different receptive field sizes for VI /VO (de-)convolutional layer count is adjusted -see Tbl. 1.</p><p>incorporates two skip connections bypassing the network bottleneck to allow the output from a convolutional layer in the encoder to feed into the corresponding up-convolution layer in the decoder. Activations from the preceding layer in the main network and skip connection data are combined via mean average rather than channel augmentation/residuals. Tbl. 1 describes the parameters (filter count and size) of each layer. We report experiments up-scaling to n = {1, 2, 4} requiring varying sizes of receptive field to accommodate V I and V O . For each step up in scale, we add a single additional convolutional layer to the encoder, and two additional de-convolutional layers to the decoder. Max-pooling occurs always at the fourth convolutional layer, and the filter size is 3 × 3 × 3 except for the first two and last two layers, where the filter size is 5 × 5 × 5 .</p><p>Learning the end-to-end mapping from coarse PVH to both an up-scaled PVH and accurate 3D joint positions requires estimation of the weights φ in F represented by the convolutional and deconvolutional kernels.</p><p>Specifically, given a collection of M training triplets {V I ,V O ,Ĵ}, where p i ∈ V I is voxel data from a coarse (input) PVH, q i ∈V O is voxel data of an ideal up-scaled PVH, and j is a vector of ideal joint positions for the given volume. We minimize the Mean Squared Error (MSE) at the outputs of the bottleneck and decoder stages across M = N × N × N voxels:</p><formula xml:id="formula_5">L(φ) = 1 M M i=1 F(p i : φ) − q i 2 2 + λ E(V I : φ) − j 2 2 .<label>(5)</label></formula><p>These training triplets are formed by extracting voxel volumes from exemplar multi-view video footage at resolution N × N × N (yieldingV O and the artificially down-sampling to 32 × 32 × 32 to yield V (from which V I is up-sampled via tri-cubic interpolation). Human pose (joint positions) corresponding to the  multi-view video frame is acquired using a commercial (Vicon Blade) human performance capture system run in parallel with video acquisition (such annotations are provided with the TotalCapture and Human3.6M datasets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Methodology</head><p>To train F we use Adadelta <ref type="bibr" target="#b34">[35]</ref> an extension of Adagrad, with the pose term of the dual loss (eq. 5) scaled by a factor of λ. We found the approach insensitive to this parameter up to an order of magnitude setting λ = 10 −3 for all experiments. Below 10 −3 , the bottleneck convergences to a semantic representation of the pose that is stable but does not resemble joint angles -above 10 −2 the network will not converge. Data is augmented during training with a random rotation around the central vertical axis of the PVH. Before full network training, the encoder stage is trained separately, purely as a pose regression task, up to the latent representation layer. These trained weights initialize the encoder stage to help constrain the latent representation during full, dual-loss network training. Training typically converges within 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Enforcing Temporal Consistency</head><p>Given the rich temporal nature of the pose sequences, it is prudent to exploit and enforce the temporal consistency of the otherwise detection based human joint estimation. By enforcing temporal consistency it is possible to smooth noise in individual joint detections that otherwise would cause large estimation errors. To learn a model of the motion over time we employ Long Short Term Memory (LSTM) layers <ref type="bibr" target="#b35">[36]</ref>, they have been heavily utilized in applications where long term temporal correlation can be exploited such as e.g. speech recognition <ref type="bibr" target="#b36">[37]</ref>, video description <ref type="bibr" target="#b37">[38]</ref>, and pose estimation <ref type="bibr" target="#b38">[39]</ref>. LSTM layers are based on a recurrent neural network (RNN). They can store and access information over long periods of time but are able to mitigate the vanishing gradient problem common in RNNs through a specialized gating mechanism. The input vector from the encoder J i (t) = E(V I ) at time t consisting of concatenated joint spatial coordinates is passed through a series of gates resulting in an output joint vector J o (t). The aim is to learn the function that minimizes the loss between the input vector and the output vector J o = o t • tanh(c t ) (• denotes the Hadamard product) where o t is the output gate, and c t is the memory cell, a combination of the previous memory c t−1 multiplied by a decay based forget gate, and the input gate. Thus, intuitively the LSTM result is the combination of the previous memory and the new input vector. In the implementation, our model consists of two LSTM layers both with 1024 memory cells, using a look back of f = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Discussion</head><p>To quantify the improvement in both the upscaling of low resolution volumetric representations and human pose estimation, we evaluate over three public multiview video datasets of human actions. For Human 3.6M <ref type="bibr" target="#b39">[40]</ref> we estimate the 3D human pose, and examine the performance of the skeleton estimation and volume upscaling in the TotalCapture <ref type="bibr" target="#b31">[32]</ref> dataset. Finally, we visualize the results of the skeleton estimation and upscaling on the dataset TotalCaptureOutdoor [41], a challenging collection of multi-view human actions shot outdoors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Human 3.6M evaluation</head><p>The 3D human pose estimation dataset Human3.6M <ref type="bibr" target="#b39">[40]</ref> is a 4 camera view dataset of 10 subjects performing 210 actions at 50Hz in a 360 • arrangement. A 3D ground truth for joint positions (key points) are available via annotation using a commercial marker-based motion capture system, allowing quantification of error. The dataset consists of 3.6 million video frames, balanced over 5 female and 6 male subjects. They perform common activities such as posing, sitting and  <ref type="table">Table 2</ref>. A Comparison of our approach to other works on the Human 3.6m dataset giving directions. To allow comparison to other approaches we follow the same data partition protocol as in previous works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>, and we use the publicly released foreground mattes. The training data consists of subjects S1, S5, S6, S7, S8 and it is tested on unseen subjects S9, S11. We compare our approach to many previously published state of the art methods, using 3D Euclidean (L 2 ) error to compute accuracy. Error is measured between each ground truth and estimated 3D joint position and is averaged over all frames and all 17 joints in millimeters (mm). The results of our approach are evaluated qualitatively in <ref type="figure" target="#fig_2">Fig 4</ref> and quantitatively in Tbl. 2, drawing comparison to state of the art approaches.</p><p>Our approach outperforms with the lowest mean joint error on the challenging Human3.6M dataset, slightly reduced over the state of the art approach by Martinez <ref type="bibr" target="#b45">[46]</ref>, with a similar mean joint error of just over 6cm. This is averaged over both test subjects and the 59 sequences. The error decrease over the other approaches is possible due to the dual loss formulation ensuring that the skeleton is kept bounded by realistic 3D volume representations after the decoder. Our approach struggles with the actions Sit Down and Photo, the action sit down contains a chair and given the already poor quality of the PVH it is likely that such incorrect joint estimations occur. In the sequences of photo the hands of the subject are close the subject head and it is likely the PVH volume doesn't contain enough discriminative information to correctly estimate their location. However, despite these two sequences, all others have a low error score and are smooth and qualitatively realistic. We show qualitative comparisons with respect to the ground truth in <ref type="figure" target="#fig_2">Fig. 4</ref>. To illustrate the stability of our approach across different test subjects we performed five rounds of cross-validation using multiple pairs of test subjects with the remaining subjects held out for training the model. <ref type="table">Table 3</ref> shows the standard test on S9 and S11 (mean accuracy of 62.5mm) from <ref type="table">Table 2</ref> against the mean and standard deviation from our cross-validation experiment. The mean performance across random pairs of test subjects is similar to that of  <ref type="table">Table 3</ref>. A Comparison of testing on subjects S9 and S11 against a five-fold cross validation of other subject pairs on the Human 3.6m dataset the official S9/S11 test split, and the σ is low. Thus they serve to show stability of the approach across different test subject pairings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TotalCapture evaluation</head><p>Approach SeenSubjects(S1,2,3) UnseenSubjects(S4,5) Mean W2 FS3 A3 W2 FS3 A3 Tri-CPM-LSTM <ref type="bibr" target="#b27">[28]</ref>   <ref type="table">Table 4</ref>. Comparison of our approach on TotalCapture to other human pose estimation approaches, expressed as average per joint error (mm).</p><p>In addition, we evaluate our approach on the TotalCapture dataset <ref type="bibr" target="#b31">[32]</ref>. This is also a 3D human pose estimation dataset with the ground truth joint position provided by Vicon markers. It is also captured indoors in a volume roughly measuring 8x4m with 8 calibration HD video cameras at 60Hz in a 360 • . There are a total of 5 subjects performing 4 actions with 3 repetitions at 60Hz in a 360 • arrangement. There are publicly released foreground mattes that we use as the input to our approach. Note to provide the Vicon groundtruth the subjects in both TotalCapture and Human3.6M are wearing dots visible to infrared cameras. However these dots are not used explicitly by our algorithm, and their size is negligible compared to the performance volume. There are five subjects in the dataset, four male, and one female, each performs four diverse performances, that are repeated 3 times: ROM, Walking, Acting, and Freestyle. The length of each sequence is between 3000-5000 frames, this results in a total of ∼ 1.9M frames of synchronized groundtruth video data. Especially within the acting and freestyle sequences, there is great diversity in the actions performed, as illustrated in the qualitative results in <ref type="figure" target="#fig_3">Fig. 5</ref>. To allow for comparison between seen and unseen subjects in the test evaluation, the test consists of sequences Freestyle3 (FS3), Acting (A3) and Walking2 (W2) on subjects 1,2,3,4 and 5. While the training is performed using the sequences of ROM1,2,3; Walking1,3; Freestyle1,2 and Acting1,2 on subjects 1, 2 and 3. We compare the pose estimation error for a number of upscale models; x1, x2, and x4 upscaling of the input PVH. Thus at the largest upscaling the PVH volume vector is v ∈ R 128×128×128 . Tbl. 4 shows the results of the different upscaling models against the previous state of the art for the dataset.</p><p>All three learnt upscaling models reduce the mean error of the joints by over 50% compared to previously published works for this dataset, with the error for some subjects sequences being reduced by an order of magnitude. <ref type="figure" target="#fig_3">Figure 5</ref> provides some examples of the actions performed by the subjects and the excellent ability of the approach to estimate the pose.</p><p>Also, the table presents the AutoEnc-FrontHalf results, this is shows initial convolutional encoder, without the decoder loss constraints. It provides a far higher error measure, indicating the importance of the dual loss constraining the skeleton pose space during training and inference. It is possible to examine the per frame error for subject 3, sequence Acting3, in <ref type="figure" target="#fig_4">Fig 6.</ref> This figure shows how consistently low the error is across the full sequence. despite a number of challenging poses being performed by the actor. There are a few error peaks, especially at the center point, and these are generally caused by a failure in the background segmentation from which the input PVH is generated, resulting in, for example, missing or weakly defined limb extremities.This data is underrepresented within the training data. However otherwise error is low. Use of the symmetrical network and dual loss has provided a large reduction in joint error for the skeleton it is also possible to upscale the initially very coarse and small volume at up to 4× times. <ref type="figure" target="#fig_5">Figure 7</ref> displays the initial volume estimate, the 4x upscaled volume and the skeleton estimate for 1x, 2x and 4x for a selection of example frames on the TotalCapture dataset. The pose estimate for each upscaled model (1×,2× and 4×) is nearly identical as born out by the results previously presented in Tbl. 4. however, the volume enhancement from the 4× upscaling is impressive allowing for greater details to be hallucinated without noise or degeneration. Tbl. 5 compares the input and output PVH volumes against a groundtruth high resolution volume generated directly from the camera views. The input volume is a naive tricubic upsampled volume and the error metric is MSE. The table shows that an order of magnitude improvement occurs using Approach SeenSubjects(S1,2,3) UnseenSubjects(S4,5) Mean W2 FS3 A3 W2 FS3 A3 AutoEnc-x2 input 9. <ref type="bibr" target="#b26">27</ref>   <ref type="table">Table 5</ref>. Accuracy of generated volumes compared to tri-cubic upsampled input, over TotalCapture dataset. Expressed as mean voxel squared error ×10 −3 from ground truth high resolution volume our proposed method against a naive tricubic up-sampling method. Comparing the x2 and x4 outputs, the MSE increases only slightly despite the generative doubling of the actor volume. An illustration of the upscaling performance is shown in <ref type="figure">Figure 8</ref>, where the input and output volumes at up to x4 upscaling are shown for the TotalCapture dataset.</p><p>Despite the initial block low-res PVH, we are able to accurately generate a hi-res PVHs at up to 4 times the size, that compare favorably to a natively generated (i. e. R 128×128×128 ) PVH. we are able to maintain extremity and no phantom volumes are formed in the upscaling process. <ref type="figure" target="#fig_6">Figure 9</ref> shows the per frame MSE over a sequence, for x2 and x4 upscaling. There is little difference between the two scales despite the greatly increased volume. <ref type="table">Table 6</ref> shows the training and inference times (the latter near real-time) of our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Outdoor footage evaluation</head><p>To further demonstrate the flexibility of our upscaling and pose estimation approach, we test on a recent challenging dataset, TotalCaptureOutdoor <ref type="bibr" target="#b23">[24]</ref>. This is an multi-view video dataset captured outdoors in a challenging uncontrolled conditions with a moving and varying background of trees and differing illumination. There are 6 video cameras placed in a 120 • arrangement around the subject, with a large 10x10m capture volume used. This large capture volume means the subjects are small in the scene as shown in <ref type="figure" target="#fig_7">Figure 10</ref> below. For this dataset there are no released mattes, therefore we background subtraction was performed as a per pixel difference in HSV colour space to provide robust invariance against illumination change. There is no groundtruth annotation available for TotalCaptureOutdoor, however, we are present several illustrative results on two sequences: Subject1, Freestyle, and Acting1. Given the small size of the subjects, a traditional 3D pose estimation or volume reconstruction would be challenging. However as shown in <ref type="figure" target="#fig_8">Figure 11</ref> we are able to use a small blocky low resolution PVH volume, that is upscaled by a factor of ×4 to produce a smooth approximation of the distant subject together with an accurate estimation of  <ref type="table">x1  50  34  20  71  15  x2  42  32  40  58  21  x4  13  43  23  180  313  Table 6</ref>. Computational cost of model training and inference (TotalCapture dataset) <ref type="figure">Fig. 8</ref>. Illustration of the upscaling ability of our approach on the TotalCapture dataset together with the native 128x128x128 groundtruth PVH their joints. Furthermore, despite the camera being arranged in a 120 • arc, we are able to simulate novel viewpoints of the upscaled full volume as shown in <ref type="figure">Figure 12</ref>, where complete 360 views are possible. This upscaling enables a future avenue of work, creating a 3D model of the upscaled volume to produce VR/AR compositions or for film/ sports post-production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a deep representation for volumetric (3D) human body shape driven by a latent encoding for the skeletal pose that can, in turn, be inferred from very coarse (R 32×32×32 ) volumetric shape data. In a single inference pass our convolutional autoencoder both up-scales up the provided volumetric data (demonstrated to a factor of 4×) and predicts 3D human pose (joint positions) with greater or equal accuracy to state of the art deep human pose estimation approaches.   Future work could explore the end-to-end integration of the LSTM to the autoencoder during training since the latter currently learns no temporal prior to aid pose or volume regression. Nevertheless, we achieve state of the art results on very low resolution volumetric input, indicating the technique has potential to enable behavioural analytics using multi-view video footage shot at a distance. <ref type="figure">Fig. 12</ref>. Visualising the upscaled volumes from novel viewpoints. 3D reconstruction is of high quality despite the input PVH being captured from just two viewpoints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Proposed convolutional autoencoder structure. The coarse input PVH is encoded into a latent feature representation via 3D (C)onvolutional, (M)ax-(P)ooling and (F)ully-(C)onnected layers. The decoder uses the latent representation to synthesize an upscaled PVH via (D)e-(C)onvolutional layers. Two skip connections bridge the latent representation which is constrained during training to encode Cartesian joint positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Convolution layer parameters for the encoder (En), bottleneck (B), and decoder (Dn) stages for n = {1, 2, 4)×. Suffix −M indicates max-pooling. All En and Dn layers learn 3 × 3 × 3 filters, except where indicated by * filters are 5 × 5 × 5. All B layers are fully-connected including the latent representation (3D joint positions) suffixed −J.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Representative visual results for pose estimation on Human 3.6M across four test sequences (source footage from four views and inferred 3D skeletal pose).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Representative visual results from TotalCapture showing 3D pose estimation (×2 up-scaling). See Tbl. 4 for quantitative results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Per frame skeletal error millimetres (mm) per joint on subject S3 A3 in the TotalCapture test sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Results illustrating the ×1, ×2, ×4 upscaled volume for a representative coarse PVH alongside upscaling inferred skeletons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Plotting volumetric reconstruction error per frame (MSE/voxel) on unseen subject S4 A3 of the TotalCapture test sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>TotalCaptureOutdoor dataset; red box indicates the person in the scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Representative TotalCaptureOutdoor results showing the low-res input PVH, and resulting skeleton and upscaled volumes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Sit. Sit D Smke Wait W.Dog walk W. toget. Mean CrossVal mean 64.9 108.3 68.9 63.0 63.6 57.4</figDesc><table><row><cell>Approach</cell><cell cols="8">Direct. Discus Eat Greet. Phone Photo Pose Purch.</cell></row><row><cell cols="7">CrossVal mean 52.2 49.8 53.0 63.1 61.4 76.8</cell><cell>63.2</cell><cell>59.3</cell></row><row><cell>CrossVal sd</cell><cell>7.6</cell><cell>5.1</cell><cell>9.1</cell><cell>5.8</cell><cell>3.9</cell><cell>4.7</cell><cell>10.4</cell><cell>6.9</cell></row><row><cell>Proposed</cell><cell cols="6">41.7 43.2 52.9 70.0 64.9 83.0</cell><cell>57.3</cell><cell>63.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.0</cell><cell>70.2</cell></row><row><cell>CrossVal sd</cell><cell>5.2</cell><cell cols="2">15.8 5.7</cell><cell>3.2</cell><cell>6.9</cell><cell>5.2</cell><cell>3.0</cell><cell>3.3</cell></row><row><cell>Proposed</cell><cell cols="6">61.0 95.0 70.0 62.3 66.2 53.7</cell><cell>52.4</cell><cell>62.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work was supported by an EPSRC doctoral bursary and InnovateUK via the TotalCapture project, grant agreement 102685. The work was supported in part through the donation of GPU hardware by the NVidia corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A bayesian approach to image-based visual hull reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIG-GRAPH</title>
		<meeting>ACM SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Computer Vision (ICCV)</title>
		<meeting>Intl. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single image super-resolution using deformable patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comp. Vision and Pattern Recognition (CVPR)</title>
		<meeting>Comp. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2917" to="2924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Example based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comp. Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Volumetric super-resolution of multispectral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05745v1</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Corr</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascades of regression tree fields for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="689" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Intl. Conf. Machine Learning (ICML)</title>
		<meeting>Intl. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Super-resolution via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayat</surname></persName>
		</author>
		<idno>abs/1706.09077</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Processing Systems (NIPS</title>
		<meeting>Neural Inf. essing Systems (NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="350" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep networks for image superresolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Computer Vision (ICCV</title>
		<meeting>Intl. Conf. Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient subpixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comp. Vision and Pattern Recognition (CVPR)</title>
		<meeting>Comp. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Processing Systems (NIPS)</title>
		<meeting>Neural Inf. essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multifocus structured illumination microscopy for fast volumetric super-resolution imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abrahamsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Optics Express</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4135" to="4140" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Weston</surname></persName>
		</author>
		<title level="m">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pictorial structures for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pictoral structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond trees: common-factor model for 2d human pose recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Computer Vision</title>
		<meeting>Intl. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="470" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human pose estimation using consistent max-covering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hybrid skeletal-surface motion graphs for character animation from 4d performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tejera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse inertial poser: Automatic 3d human pose estimation from sparse imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Conference of the European Association for Computer Graphics (Eurographics</title>
		<meeting>the 38th Annual Conference of the European Association for Computer Graphics (Eurographics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Real-time fullbody motion capture from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Indirect deep structured learning for 3d human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep pose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with tiny synthetic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHA-LEARN Workshop on Looking at People</title>
		<meeting>CHA-LEARN Workshop on Looking at People</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian image based 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ntouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="566" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep convolutional networks for marker-less human pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Visual Media Production</title>
		<meeting>the 13th European Conference on Visual Media Production</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 28th British Machine Vision Conference</title>
		<meeting>28th British Machine Vision Conference</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06316</idno>
		<title level="m">Lstm pose machines</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Realtime full-body motion capture from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05708</idno>
		<title level="m">Fusing 2d uncertainty and 3d cues for monocular body pose estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00295</idno>
		<title level="m">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L K W</forename><surname>Liang Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
