<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Class Multi-Object Tracking using Changing Point Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungjae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enkhbayar</forename><surname>Erdenee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songguo</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phill</forename><forename type="middle">Kyu</forename><surname>Rhee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Class Multi-Object Tracking using Changing Point Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-class and multi-object tracking</term>
					<term>changing point de- tection</term>
					<term>entity transition</term>
					<term>object detection from video</term>
					<term>convolutional neu- ral network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a robust multi-class multi-object tracking (MCMOT) formulated by a Bayesian filtering framework. Multiobject tracking for unlimited object classes is conducted by combining detection responses and changing point detection (CPD) algorithm. The CPD model is used to observe abrupt or abnormal changes due to a drift and an occlusion based spatiotemporal characteristics of track states. The ensemble of convolutional neural network (CNN) based object detector and Lucas-Kanede Tracker (KLT) based motion detector is employed to compute the likelihoods of foreground regions as the detection responses of different object classes. Extensive experiments are performed using lately introduced challenging benchmark videos; ImageNet VID and MOT benchmark dataset. The comparison to state-of-the-art video tracking techniques shows very encouraging results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-object tracking (MOT) is emerging technology employed in many realworld applications such as video security, gesture recognition, robot vision, and human robot interaction <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. The challenge is drifts of tracking points due to appearance variations caused by noises, illumination, pose, cluttered background, interactions, occlusion, and camera movement. Most MOT methods are suffered from varying numbers of objects, and leading to performance degradation and tracking accuracy impairments in cluttered backgrounds. However, most of them only focus on a limited categories, usually people or vehicle tracking. MOT with unlimited classes of objects has been rarely studied due to very complex and high computation requirements.</p><p>The Bayesian filter consists of the motion dynamics and observation models which estimates posterior likelihoods. One of the Bayesian filter based object tracking methods is Markov chain Monte Carlo (MCMC)-based method <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, which can handle various object moves and interactions of multiple objects. Most MCMC based methods assume that the number of objects would not change over time, which is not acceptable in a real world applications. Reversible jump MCMC (RJMCMC) was proposed by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, where a variable number of objects arXiv:1608.08434v1 [cs.CV] 30 Aug 2016 with different motion changes, such as update, swap, birth, and death moves. They start a new track by initializing a new object or terminates currently tracked object by eliminating the object.</p><p>Even MCMC based MOT approaches were successful to some extent, computational overheads are very high due to a high-dimensional state space. The variations in appearances, the interaction and occlusions and changing number of moving objects are challenging, which require high computation overheads. Saka et. al. <ref type="bibr" target="#b0">[1]</ref> proposes a MCMC sampling with low computation overhead by separating motion dynamics into birth and death moves and the iteration loop of the Markov chain for motion moves of update and swap. If the moves of birth and death are determined inside of the MCMC chain, it requires the dimension changes in the MCMC sampling approaches as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Since the Markov chain has no dimension variation in the iteration loop by separating the moves of birth and death, it can reach to stationary states with less computation overhead <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. However, such a simple approach for separating birth and death dynamics cannot deal with complex situations that occur in MOT. Many of them are suffered from track drifts due to appearance variations.</p><p>In this paper, we propose a robust multi-class multi-object tracking (MC-MOT) that conducts unlimited object classes by combining detection responses and changing point detection (CPD) algorithm. With advances of deep learning based object detection technology such as Faster R-CNN <ref type="bibr" target="#b27">[28]</ref>, and ResNet <ref type="bibr" target="#b28">[29]</ref>, it becomes feasible to adopt a detector ensemble with unlimited classes of objects. The detector ensemble combines the model based detector implemented by Faster R-CNN <ref type="bibr" target="#b27">[28]</ref> and the motion detector by Lucas-Kanade Tracker (KLT) algorithm <ref type="bibr" target="#b25">[26]</ref>. The method separates the motion dynamic model of Bayesian filter into the entity transitions and motion moves. The entity transitions are modeled as the birth and death events. Observation likelihood is calculated by more sophisticated deep learning based data-driven algorithm. Drift problem which is one of the most cumbersome problems in object tracking is attacked by a CPD algorithm similarly to <ref type="bibr" target="#b23">[24]</ref>. Assuming the smoothness of motion dynamics, the abrupt changes of the observation are dealt with the CPD algorithm, whereas the abrupt changes are associated illuminations, cluttered backgrounds, poses, and scales. The main contributions of the paper are below:</p><p>• MCMOT can track varying number of objects with unlimited classes which is formulated as a way to estimate a likelihood of foreground regions with optimal smoothness. Departing from the likelihood estimation only belong to limited type of objects, such as pedestrian or vehicles, efficient convolutional neural network (CNN) based multi-class object detector is employed to compute the likelihoods of multiple object classes. • Changing point detection is proposed for a tracking failure assessment by exploiting static observations as well as dynamic ones. Drifts in MCMOT are investigated by detecting such abrupt change points between stationary time series that represent track segment.</p><p>This paper is organized as follows. We review related work in Section 2. In Section 3, the outline of MCMOT is discussed. Section 4 introduces our proposed tracking method. Section 5 describes the experiments, and concluding remarks and future directions are discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi Object Tracking</head><p>Recent research in MOT has focused on the tracking-by-detection principal to perform data association based on linking object detections through a video sequence. Majority of the batch methods formulates MOT with future frame's information to get better data association via hierarchical tracks association <ref type="bibr" target="#b12">[13]</ref>, network flows <ref type="bibr" target="#b11">[12]</ref>, and global trajectory optimization <ref type="bibr" target="#b10">[11]</ref>. However, batch methods have higher computational cost relatively. Whereas online methods only consider past and current frame's information to solve the data association problem. Online methods are more suitable for real-time application, but those are likely to drift since objects in a video show significant variations in appearances due to noises, illuminations, poses, viewing angles, occlusions, and shadows, some objects enters or leaves the scene, and sometimes show sharp turns and abrupt stops. Dynamically varying number of objects is difficult to handle, especially when track crowded or high traffic objects in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>. Most MOT methods relying on the observation of different features are prone to result in drifts. Against this nonstationarity and nonlinearity, stochastic-based tracking <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> appear superior to deterministic based tracking such as Kalman filter <ref type="bibr" target="#b32">[33]</ref> or particle filter <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolutional Neural Network</head><p>In the last few years, considerable improvements have been appeared in the computer vision task using CNN. One of the particularly remarkable studies is R-CNN <ref type="bibr" target="#b33">[34]</ref>. They transferred CNN based image classification task to CNN based object detection task using region-based approach. SPPnet <ref type="bibr" target="#b34">[35]</ref> and Fast R-CNN <ref type="bibr" target="#b35">[36]</ref> extend R-CNN by pooling convolutional features from a shared convolutional feature map. More recently, RPN <ref type="bibr" target="#b27">[28]</ref> is suggested to generate region proposals within R-CNN framework using RPN. Those region-based CNN pipelines outperform all the previous works by a significant margin. Despite such great success of CNNs, only a few number of MOT algorithms using the representations from CNNs have been proposed <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. In <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, they proposed a CNN based framework with simple object tracking algorithm for MOT task in ImageNet VID. In <ref type="bibr" target="#b21">[22]</ref>, they used CNN based object detector for MOT Challenge <ref type="bibr" target="#b31">[32]</ref>. Our experiment adopts this paradigm of region based CNN to build observation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Outline of MCMOT</head><p>We propose an efficient multi-class multi-object tracker, called MCMOT that can deal with object birth, death, occlusion, interaction, and drift efficiently. MCMOT may fail due to the miscalculations of the observation likelihood, interaction model, entry model, and motion model. The objective of MCMOT is to stop the tracking as quick as possible if a drift occurs, recover from the wrong decisions, and to continue tracking. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the main concept of our framework.</p><p>In MCMOT, objects are denoted by bounding boxes which are tracked by a tracking algorithm. In the tracking algorithm, if a possible interaction or occlusion is detected, the trajectory is split into several parts, called track segments. The combination of track segments is controlled by CPD. Considering fallible decision tracker points, CPD monitors a drift due to abnormal events, abrupt changing environments by comparing the localized bounding boxes by the observations within the segment. The motion-based tracking component facilitates KLT <ref type="bibr" target="#b25">[26]</ref> adaptive for predicting the region of a next tracking point. The model-based component consists of the global object detector and adaptive local detector. We use a deep feature based multi-class object detector <ref type="bibr" target="#b27">[28]</ref> as the global and local object detector. One can notice that the number of object categories can be readily extended depending on object detector capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-Class Multi-Object Tracking</head><p>MCMOT employs an data-driven approach which investigates the events caused by object-level events, object birth and death, inter-object level events, i.e., interaction and occlusion between objects, and tracking level events, e.g. track birth, update, and death. Possible drifts due to the observation failures are dealt with the abnormality detection method based on the changing point detection.</p><p>We define track segments using the birth and death detection. Only visible objects are tracked, the holistic trajectory divided into several track segments, if an occlusion happens as in <ref type="bibr" target="#b15">[16]</ref>. If the object becomes ambiguous due to occlusion or noise, the track segment is terminated (associated object death), and the tracker will restart tracking (associated object birth) nearby the terminated tracking point if the same object reoccurs, and the track segment is continuously built, if it is required, or a new track segment is started and merged later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Observation Model</head><p>We define observation model (observation likelihood) P (z t |x t ) in this section. The observation likelihood for tracked objects need to estimate both the object class and accurate location. MCMOT ensembles object detectors with different characteristics to calculate the observation likelihood accurately. Since the dimensionality of the scene state is allowed to be varied, the measure is defined as the ratio of the likelihoods of the existence and non-existence. As the likelihood of the non-existence set cannot be measured, we adopt a soft max f ( · ) of the likelihood model, as in <ref type="bibr" target="#b17">[18]</ref>.</p><formula xml:id="formula_0">P (õ t |o id,t ) P (õ t | o id,t ) = exp e f (λ e log e (õ t |o id,t )<label>(1)</label></formula><p>where o id,t indicates the non-existence of object id, f soft max function, λ e the weight of object detector e. The approach is expected to be robust to sporadic noises since each detector has its own pros and cons. We employ ensemble object detectors: deep feature based global object detector (GT), deep feature based local object detector (LT), color detector (CT), and motion detector (MT):</p><p>• Global object detector (GT): Deep feature based object detector <ref type="bibr" target="#b27">[28]</ref> in terms of hierarchical data model (HDM) <ref type="bibr" target="#b43">[44]</ref> is used. • Local object detector (LT): By fine-tuning deep feature based object detector using confident track segments, issues due to false negatives can be minimized. Deep feature based object detector <ref type="bibr" target="#b27">[28]</ref> is used for the local object detector. • Color detector (CT): Similarity score between the observed appearance model and the reference target is calculated through Bhattacharyya distance <ref type="bibr" target="#b16">[17]</ref> using RGB color histogram of the bounding box. • Motion detector (MT): The presence of an object is checked by using KLT based motion detector <ref type="bibr" target="#b25">[26]</ref> which detects the presence of motion in a scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Track Segment Creation</head><p>The MCMOT models the tracking problem to determine optimal scene particles in a given video sequence. MCMOT can be thought as reallocation steps of objects from the current scene state to the next scene state repeatedly. First, the birth and death allocations are performed in the entity status transition step. Second, the intermediate track segments are built using the data-driven MCMC sampling step with the assumption that the appearances and positions of track segments change smoothly. In the final step, the detection of a track drift is conducted by a changing point detection algorithm to prevent possible drifts. Change point denotes a time step where the data attributes abruptly change <ref type="bibr" target="#b23">[24]</ref> which is expected to be a drift starting point with high probability. We discuss the detail of the data-driven MCMC sampling, and entity status transition in follows.</p><p>Date-Driven MCMC Sampling In a MCMC based sampling, the efficiency of the proposal density function is important since it affects much in constructing a Markov chain with stationary distribution, and thus affects much on tracking performance in practice. The proposal density function should be measurable and can be sampled efficiently from the proposal distribution <ref type="bibr" target="#b1">[2]</ref>, which is proportional to a desired target distribution. We employ "one object at a time" strategy, whereas one object state is modified at a time, as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. Given a particle x t at time t, the distribution of current proposal density function π(x ; x t ) is used to suggest for the next particle. In MCMOT, we assume that the distribution of the proposal density follows the pure motion model for the MCMC sampling, i.e., π(x ; x t ) ≈ P (x t+1 |x t ), as in <ref type="bibr" target="#b1">[2]</ref>. Given a scene particle, i.e., a set of object states x t , a candidate scene particle x t is suggested by randomly selecting object o id,t , and then determines the proposed state x t relying the object o id,t with uniform probability assumption. In this paper, a strategy of data-driven proposal density <ref type="bibr" target="#b2">[3]</ref> is employed to make the Markov chain has a better acceptance rate. MCMOT proposes a new state o id,t according to the informed proposal density with a mixture of the state moves to ensure motion smoothness as in <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_1">π(o id,t ; x t ) = λ 1 1 N s p(o id,t |o (s) id,t−1 ) + λ 2 p(o id,t |D id,t )<label>(2)</label></formula><p>where λ 1 + λ 2 = 1. The first term is from the motion model and the second term from the detector ensemble and using the closest result from the all detection of object id.</p><p>Remind that the posterior probability for time-step t-1 is assumed to be represented by a set of N samples (scene particles). Given observations from the initial time to the current time t, the calculation of the current posterior is done by MCMC sampling using N samples. We use B samples as burn-in samples <ref type="bibr" target="#b5">[6]</ref>. B burn-in samples are used initially and eliminated for the efficient convergence to a stationary state distribution. More details and other practical considerations about MCMC can be found in <ref type="bibr" target="#b41">[42]</ref>.</p><p>Estimation of entity status transition The entity status is estimated by two binomial probabilities of the birth status and death status according to the entry  Similarly, ES d id,t (x, y) = ν denotes death status. The posterior probability of entry status is defined at time t as follows:</p><formula xml:id="formula_2">P ES (o id,t |o id,t−1 ) ≈        P b = p(ES b</formula><p>id,t (x, y) = 1|o id,1:t ) , if object id exists time t and not t−1 P d = p(ES d id,t (x, y) = 1|o id,1:t ) , if object id exists at time t−1 and not t P a = 1 − P d , if object id exists at time t−1 and t P ∅ = 1 − P b , if object id exists neither time t nor t−1</p><p>If a new object id is observed by the observation likelihood mode at time t in position (x,y) which did not exist (detected) in time t-1, the birth status of object id ES b id,t (x, y)is set to 1, otherwise, it is set to 0. If an object id is not observed by the detector ensemble at time t in position (x,y) which existed in time t-1, the death status of object id, i.e.,ES b id,t (x, y)is set to 1, otherwise, it is set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Changing Point Detection</head><p>MCMOT may fail to track an object if it is occluded or confused by a cluttered background. MCMOT would determine whether or not a track is terminated or continues tracking. Drifts in MCMOT are investigated by detecting such abrupt change points between stationary time series that represent track segment. A higher response indicates a higher uncertainty with high possibility of a drift occurrence <ref type="bibr" target="#b24">[25]</ref>. Two-stage time-series learning algorithm is used as in <ref type="bibr" target="#b23">[24]</ref>, where a possible track drift is determined by a change point detection method <ref type="bibr" target="#b23">[24]</ref> as follows. The 2 nd level time series is built using the scanned average responses to reduce outliers in the times series. The procedure to prevent drift is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>If high CPD response is detected on track segment, the forward-backward error (FB error) validation <ref type="bibr" target="#b6">[7]</ref> is defined to estimate the confidence of a track segment by tracking in reverse sequence of the track segments. A given video, the confidence of track segment τ t is to be estimated. Let τ r t denotes the reverse sequential states, i.e., o id,t:1 = {ô id,t , . . . ,ô id,1 }. The backward track is a random trajectory that is expected to be similar to the correct forward track. The confidence of a track segments is defined as the distance between these two track segments: Conf(τ t |τ r t ) = distance(τ t , τ r t ). We use the Euclidean distance between the initial point and the end point of the validation trajectory as distance(τ t , τ r t ) = ||o id,1:t − o id,t:1 ||. The MCMOT algorithm is summarized in the followings: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Results</head><p>We describe the details about MCMOT experiment setting, and demonstrate the performance of MCMOT compared to the state-of-the-art methods in challenging video sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>To build global and local object detector, we use publicly available sixteen-layer VGG-Net <ref type="bibr" target="#b18">[19]</ref> and ResNet <ref type="bibr" target="#b28">[29]</ref> which are pre-trained on an ImageNet classification dataset. We fine-tune an initial model using ImageNet Challenge Detection dataset (ImageNet DET) with 280K iterations at a learning rate of 0.001. After 280K iterations, the learning rate is decreased by a factor of 10 for fine-tuning with 70K iteration. For region proposal generation, RPN <ref type="bibr" target="#b27">[28]</ref> is employed because it is fast and provides accurate region proposals in end-to-end manner by sharing convolutional features. After building initial model, we perform domainadaptation for each dataset by fine-tuning with similar step described beforehand. Changing point detection algorithms used a two-stage time-series learning algorithm <ref type="bibr" target="#b23">[24]</ref> which is computationally effective and achieves high detection accuracy. We consider as change point when change point score is greater than change point threshold. Change point threshold is empirically set as 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset</head><p>There are a few benchmark datasets available for multi-class multi-object tracking <ref type="bibr" target="#b42">[43]</ref>. Since they deal with only two or three classes, we used benchmark datasets, ImageNet VID <ref type="bibr" target="#b30">[31]</ref> and MOT 2016 <ref type="bibr" target="#b31">[32]</ref>, where the former has 30 object classes and the latter is an up-to-date multiple object tracking benchmark. We compare its performance with state-of-the-arts on the ImageNet VID and MOT Benchmark 2016.</p><p>ImageNet VID We demonstrate our proposed algorithm using ImageNet object detection from video (VID) task dataset <ref type="bibr" target="#b30">[31]</ref>. ImageNet VID task is originally used to evaluate performance of object detection from video. Nevertheless, this dataset can be used to evaluate MCMOT because this challenging dataset consists of the video sequences recorded with a moving camera in real-world scenes with 30 object categories and the number of targets in the scene is changing over time. Object categories in these scenes take on different viewpoints and are subject to various degrees of occlusions. To ease the comparison with other state-of-the-arts, the performance of MCMOT on this dataset is primarily measured by mean average precision (mAP) which is used in ImageNet VID Challenge <ref type="bibr" target="#b30">[31]</ref>. We use the initial release of ImageNet VID dataset, which consists of three splits which are train, validation, and test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOT Benchmark 2016</head><p>We evaluate our tracking framework on the MOT Benchmark <ref type="bibr" target="#b31">[32]</ref>. The MOT Benchmark is an up-to-date multiple object tracking benchmark. The MOT Benchmark collects some new challenging sequences and widely used video sequences in the MOT community. MOT 2016 consists of a total of 14 sequences in unconstrained environments filmed with both static and moving cameras. All the sequences contain only pedestrians. These challenging sequences are composed with various configurations such as different viewpoints, and different weather condition. Therefore, tracking algorithms which are tuned for specific scenario or scene could not perform well. We adopt the CLEAR MOT tracking metrics <ref type="bibr" target="#b22">[23]</ref> using MOT Benchmark Development Kit <ref type="bibr" target="#b31">[32]</ref> for the evaluation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MCMOT CPD Analysis</head><p>In order to investigate the proposed MCMOT changing point detection component, we select two sequences, MOT16-02 and MOT16-09 from the MOT 2016 training set. For change point detection, we assign a change point if change point score is larger than 0.3. <ref type="figure" target="#fig_4">Fig. 3</ref> illustrates the observation likelihood and detected change point of the segment. A low likelihood or rapid change in likelihood is an important factor for detecting potential changing point. In the tracking result of MOT16-02 sequence in <ref type="figure" target="#fig_4">Fig. 3</ref>, unstable likelihood is observed until frame 438, where a motion-blurred half-body person moves. Tracking is drifted because occluded person appears at similar position with previous tracked point at frame 440. After several frames, the target is swapped to another person at frame 444. In this case, bounding boxes within drift area are unstable, which observed strong fluctuation of likelihood. Changing point detection algorithm produces high change point score at frame 440 by detecting this fluctuation. In the tracking result of MOT16-09 sequence in <ref type="figure" target="#fig_4">Fig. 3</ref> also illustrates similar situation explained before. As we can see, a possible track drift is implicitly handled by the change point detection method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">ImageNet VID Evaluation</head><p>Since the official ImageNet Challenge test server is primarily used for annual competition and has limited number of usage, we evaluate the performance of the proposed method on the validation set instead of the test set as a practical convention <ref type="bibr" target="#b19">[20]</ref> for ImageNet VID task. For the ImageNet VID train/validation experiment, all the training and testing images are scaled by 600 pixel to be the length of image's shortest side. This value was selected so that VGG16 or ResNet fits in GPU memory during fine-tuning <ref type="bibr" target="#b27">[28]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the effect of different components of MCMOT. Each method is distinguished in terms of MCMOT with CPD algorithm (MCMOT CPD), and MCMOT using CPD with forward-backward validation (MCMOT CPD FB). In the following evaluations, we filter out segments that have an average observation score lower than 0.3. As shown in the table 1, significant improvement can be achieved with 9.8% from detection baseline by adapting MCMOT CPD, and  reached to 71.1%. After the adaptation of the FB validation, an overall 74.5% mAP was achieved on the ImageNet VID validation set. <ref type="table">Table 2</ref> summarizes the evaluation accuracy of MCMOT and the comparison with the other state-ofthe-art algorithms on the whole 281 validation video sequences. Our MCMOT is achieved overall 74.5% mAP on the ImageNet VID validation set, which is higher than state-of-the-art methods such as T-CNN <ref type="bibr" target="#b19">[20]</ref>. This result is mainly due to the MCMOT approach of constructing a highly accurate segments by using CPD. As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, unlimited number of classes are successfully tracked with high localization accuracy using MCMOT.  Each frame is sampled every 100 frames (these are not curated). The color of the boxes represents the identity of the targets. The figure is best shown in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">MOT Benchmark 2016 Evaluation</head><p>We evaluate MCMOT on the MOT Challenge 2016 benchmark to compare our approach with other state-of-the-art algorithms. For the MOT 2016 experiment, all the training and testing images are scaled by 800 pixel to be the length of image's shortest side. This larger value is selected because pedestrian bounding box size is smaller than ImageNet VID. In MCMOT, we also implement hierarchical data model (HDM) <ref type="bibr" target="#b43">[44]</ref> which is CNN based object detector. The timing excludes detection time. <ref type="table" target="#tab_2">Table 3</ref> summarizes the evaluation metrics of MCMOT and the other stateof-the-arts on the test video sequences. <ref type="figure" target="#fig_6">Fig. 5</ref> visualizes examples of MCMOT tracking results on the test sequences. As shown in the table 3, MCMOT outperforms the previously published state-of-the-art methods on overall performance evaluation metric which is called multi object tracking accuracy (MOTA). We also achieved much smaller numbers of mostly lost targets (ML) by a signifi-cant margin. Even though our method outperforms most of the metrics, tracker speed in frames per second (HZ) is faster than other tracking methods. This is thanks to the simple MCMC tracking structure with entity status transition, and selective FB error validation with CPD, which is boosted tracking speed on a multi-object tracking task. However, high identity switch (IDS) and high fragmentation (FRAG) are observed because of the lack of identity mapping between track segments. More importantly, MCMOT achieves state-of-the-art performance in two different datasets, we demonstrate the general multi-class multi-obejct tracking applicability to any kind of situation with unlimited number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented a novel multi-class multi-object tracking framework. The framework surpassed the performance of state-of-the-art results on ImageNet VID and MOT benchmark 2016. MCMOT that cunducted unlimited object class association based on detection responses. The CPD model was used to observe abrupt or abnormal changes due to a drift. The ensemble of KLT based motion detector and CNN based object detector was employed to compute the likelihoods. A future research direction is to deal with the optimization problem of MCMOT structure and identity mapping problem between track segments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>MCMOT framework has four major steps: (a) Likelihood calculation based on observation models, (b) Track segment creation, (c) Changing point detection, and (d) Trajectory combination. The drifts in segments are effectively controlled by changing point detection algorithm with forward-backward validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of CPD. A change point score is calculated by the changing point detection algorithm. If the high change point score is detected, forward-backward error is checked from the detected change point. FB error checks whether the segment is drifted. A possible track drift is determined effectively by the change point detection method with forward-backward validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>model at time step t and t-1. Let ES b id,t (x, y) = ν (ν ∈ {1, 0}) denote the birth status with ν=1 indicating true, ν= 0 false of an object id in the potion(x, y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Change points obtained from the segment in MOT16-02 and MOT16-09 sequence. A higher change point response indicates a higher uncertainty with high possibility of a drift occurrence. Notice that our method can effectively detect drifts in challenging situations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>MCMOT tracking results on the validation sequences in the ImageNet VID dataset. Each bounding box is labeled with the identity, the predicted class, and the confidence score of the segment. Viewing digitally with zoom is recommended.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>MCMOT tracking results on the test sequences in the MOT Benchmark 2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>MCMOT using CPD Detect changing points for all track segmentsStep 4. Do forward-backward validation for the track segments with detected changing pointsStep 5. Generate resulting trajectories by combining the track segments</figDesc><table><row><cell>Input : Motion model, entry model</cell></row><row><cell>Output: Confident track segments</cell></row><row><cell>Step 1. Calculate the posterior P (x t |z t 1:t )</cell></row><row><cell>Step 2. Generate track segments</cell></row><row><cell>Step 3.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Effect of different components on the ImageNet VID validation set</figDesc><table><row><cell></cell><cell cols="3">aero antelope bear</cell><cell>bike bird bus</cell><cell>car</cell><cell cols="2">cattle dog</cell><cell>cat</cell><cell>elephant</cell></row><row><cell cols="2">Detection baseline 84.6</cell><cell>75.8</cell><cell>77.2</cell><cell>57.2 60.8 84.6</cell><cell>62.4</cell><cell>66.3</cell><cell>57.7</cell><cell>62.3</cell><cell>74.0</cell></row><row><cell>MCMOT CPD</cell><cell>87.1</cell><cell>81.2</cell><cell>83.2</cell><cell>76.6 64.3 86.1</cell><cell>64.4</cell><cell>79.4</cell><cell>69.4</cell><cell>74.4</cell><cell>77.4</cell></row><row><cell cols="2">MCMOT CPD FB 86.3</cell><cell>83.4</cell><cell>88.2</cell><cell>78.9 65.9 90.6</cell><cell>66.3</cell><cell>81.5</cell><cell>72.1</cell><cell>76.8</cell><cell>82.4</cell></row><row><cell></cell><cell cols="9">fox g panda hamster horse lion lizard monkey m-bike rabbit r panda sheep</cell></row><row><cell cols="2">Detection baseline 79.6</cell><cell>89.9</cell><cell>80.0</cell><cell>58.7 15.5 70.0</cell><cell>45.5</cell><cell>78.1</cell><cell>67.5</cell><cell>51.2</cell><cell>30.7</cell></row><row><cell>MCMOT CPD</cell><cell>87.3</cell><cell>90.2</cell><cell>85.3</cell><cell>63.3 31.7 74.8</cell><cell>52.6</cell><cell>86.9</cell><cell>74.7</cell><cell>75.2</cell><cell>30.5</cell></row><row><cell cols="2">MCMOT CPD FB 88.9</cell><cell>91.3</cell><cell>89.3</cell><cell>66.5 38.0 77.1</cell><cell>57.3</cell><cell>88.8</cell><cell>78.2</cell><cell>77.7</cell><cell>40.6</cell></row><row><cell></cell><cell cols="6">snake squirrel tiger train turtle boat whale zebra</cell><cell cols="3">mean AP (%)</cell></row><row><cell cols="2">Detection baseline 50.7</cell><cell>29.0</cell><cell>79.5</cell><cell>71.5 68.9 77.0</cell><cell>57.9</cell><cell>77.9</cell><cell></cell><cell>64.7</cell><cell></cell></row><row><cell>MCMOT CPD</cell><cell>43.7</cell><cell>39.0</cell><cell>87.4</cell><cell>75.1 67.0 80.2</cell><cell>59.7</cell><cell>84.1</cell><cell></cell><cell>71.1</cell><cell></cell></row><row><cell cols="2">MCMOT CPD FB 50.3</cell><cell>44.3</cell><cell>91.8</cell><cell>78.2 75.1 81.7</cell><cell>63.1</cell><cell>85.2</cell><cell></cell><cell>74.5</cell><cell></cell></row><row><cell cols="10">Table 2. Tracking performance comparison on the ImageNet VID validation set</cell></row><row><cell></cell><cell cols="3">aero antelope bear</cell><cell>bike bird bus</cell><cell>car</cell><cell cols="2">cattle dog</cell><cell>cat</cell><cell>elephant</cell></row><row><cell>TCN [21]</cell><cell>72.7</cell><cell>75.5</cell><cell>42.2</cell><cell>39.5 25.0 64.1</cell><cell>36.3</cell><cell>51.1</cell><cell>24.4</cell><cell>48.6</cell><cell>65.6</cell></row><row><cell>ITLab VID-Inha</cell><cell>78.5</cell><cell>68.5</cell><cell>76.5</cell><cell>61.4 43.1 72.9</cell><cell>61.6</cell><cell>61.1</cell><cell>52.2</cell><cell>56.6</cell><cell>74.0</cell></row><row><cell>T-CNN [20]</cell><cell>83.7</cell><cell>85.7</cell><cell>84.4</cell><cell>74.5 73.8 75.7</cell><cell>57.1</cell><cell>58.7</cell><cell>72.3</cell><cell>69.2</cell><cell>80.2</cell></row><row><cell cols="2">MCMOT (Ours) 86.3</cell><cell>83.4</cell><cell>88.2</cell><cell>78.9 65.9 90.6</cell><cell>66.3</cell><cell>81.5</cell><cell>72.1</cell><cell>76.8</cell><cell>82.4</cell></row><row><cell></cell><cell cols="9">fox g panda hamster horse lion lizard monkey m-bike rabbit r panda sheep</cell></row><row><cell>TCN [21]</cell><cell>73.9</cell><cell>61.7</cell><cell>82.4</cell><cell>30.8 34.4 54.2</cell><cell>1.6</cell><cell>61.0</cell><cell>36.6</cell><cell>19.7</cell><cell>55.0</cell></row><row><cell>ITLab VID-Inha</cell><cell>72.5</cell><cell>85.5</cell><cell>67.5</cell><cell>64.7 5.7 54.3</cell><cell>34.7</cell><cell>77.6</cell><cell>53.5</cell><cell>40.8</cell><cell>34.3</cell></row><row><cell>T-CNN [20]</cell><cell>83.4</cell><cell>80.5</cell><cell>93.1</cell><cell>84.2 67.8 80.3</cell><cell>54.8</cell><cell>80.6</cell><cell>63.7</cell><cell>85.7</cell><cell>60.5</cell></row><row><cell cols="2">MCMOT (Ours) 88.9</cell><cell>91.3</cell><cell>89.3</cell><cell>66.5 38.0 77.1</cell><cell>57.3</cell><cell>88.8</cell><cell>78.2</cell><cell>77.7</cell><cell>40.6</cell></row><row><cell></cell><cell cols="6">snake squirrel tiger train turtle boat whale zebra</cell><cell></cell><cell cols="2">mean AP (%)</cell></row><row><cell>TCN [21]</cell><cell>38.9</cell><cell>2.6</cell><cell>42.8</cell><cell>54.6 66.1 69.2</cell><cell>26.5</cell><cell>68.6</cell><cell></cell><cell>47.5</cell><cell></cell></row><row><cell>ITLab VID-Inha</cell><cell>18.1</cell><cell>23.4</cell><cell>69.6</cell><cell>53.4 61.6 78.0</cell><cell>33.2</cell><cell>77.7</cell><cell></cell><cell>57.1</cell><cell></cell></row><row><cell>T-CNN [20]</cell><cell>72.9</cell><cell>52.7</cell><cell>89.7</cell><cell>81.3 73.7 69.5</cell><cell>33.5</cell><cell>90.2</cell><cell></cell><cell>73.8</cell><cell></cell></row><row><cell cols="2">MCMOT (Ours) 50.3</cell><cell>44.3</cell><cell>91.8</cell><cell>78.2 75.1 81.7</cell><cell>63.1</cell><cell>85.2</cell><cell></cell><cell>74.5</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Tracking performances comparison on the MOT benchmark 2016 (results on 7/14/2016). The symbol ↑ denotes higher scores indicate better performance. The symbol ↓ means lower scores indicate better performance</figDesc><table><row><cell>Method</cell><cell cols="4">MOTA↑ MOTP↑ FAF↓ MT↑ ML↓ FP↓</cell><cell cols="3">FN↓ ID Sw↓ Frag↓ Hz↑</cell></row><row><cell>GRIM</cell><cell cols="7">-14.5% 73.0% 10.0 9.9% 49.5% 59,040 147,908 1,869 2,454 10.0</cell></row><row><cell>JPDA m [41]</cell><cell>26.2%</cell><cell>76.3%</cell><cell>0.6</cell><cell cols="3">4.1% 67.5% 3,689 130,549 365</cell><cell>638 22.2</cell></row><row><cell>SMOT [40]</cell><cell>29.7%</cell><cell>75.2%</cell><cell>2.9</cell><cell cols="4">5.3% 47.7% 17,426 107,552 3,108 4,483 0.2</cell></row><row><cell>DP NMS [39]</cell><cell>32.2%</cell><cell>76.4%</cell><cell cols="4">0.2 5.4% 62.1% 1,123 121,579 972</cell><cell>944 212.6</cell></row><row><cell>CEM [38]</cell><cell>33.2%</cell><cell>75.8%</cell><cell>1.2</cell><cell cols="3">7.8% 54.4% 6,837 114,322 642</cell><cell>731</cell><cell>0.3</cell></row><row><cell>TBD [37]</cell><cell>33.7%</cell><cell>76.5%</cell><cell>1.0</cell><cell cols="4">7.2% 54.2% 5,804 112,587 2,418 2,252 1.3</cell></row><row><cell>LINF1</cell><cell>41.0%</cell><cell>74.8%</cell><cell cols="3">1.3 11.6% 51.3% 7,896 99,224</cell><cell>430</cell><cell>963</cell><cell>1.1</cell></row><row><cell>olCF</cell><cell>43.2%</cell><cell>74.3%</cell><cell cols="3">1.1 11.3% 48.5% 6,651 96,515</cell><cell cols="2">381 1,404 0.4</cell></row><row><cell>NOMT [22]</cell><cell>46.4%</cell><cell>76.6%</cell><cell cols="3">1.6 18.3% 41.4% 9,753 87,565</cell><cell>359</cell><cell>504</cell><cell>2.6</cell></row><row><cell>AMPL</cell><cell>50.9%</cell><cell>77.0%</cell><cell cols="3">0.5 16.7% 40.8% 3,229 86,123</cell><cell>196</cell><cell>639</cell><cell>1.5</cell></row><row><cell>NOMTwSDP16 [22]</cell><cell cols="5">62.2% 79.6% 0.9 32.5% 31.1% 5,119 63,352</cell><cell>406</cell><cell>642</cell><cell>3.1</cell></row><row><cell cols="3">MCMOT HDM (Ours) 62.4% 78.3%</cell><cell cols="5">1.7 31.5% 24.2% 9,855 57,257 1,394 1,318 34.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by an Inha University research grant. A GPU used in this research was generously donated by NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video-based tracking, learning, and recognition method for multiple moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakaino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. on circuits and systems for video technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MCMC-based particle filtering for tracking a variable number of interacting targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1805" to="1819" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segmentation and tracking of multiple humans in crowded environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1198" to="1211" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MCMC data association and sparse factorization updating for real time multitarget tracking with merged and multiple measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1960" to="1972" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trans-dimensional markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oxford Statistical Science Series</title>
		<imprint>
			<biblScope unit="page" from="179" to="198" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Track Creation and Deletion Framework for Long-Term Online Multiface Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="272" to="285" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Forward-backward error: Automatic detection of tracking failures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<editor>ICPR.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scene-Adaptive Hierarchical Data Association for Multiple Objects Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="697" to="701" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple player tracking in sports video: A dualmode two-way bayesian inference approach with progressive observation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1652" to="1667" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple object tracking using kshortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1806" to="1819" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust people tracking with global trajectory optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Robust object tracking by hierarchical association of detection responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online multiperson tracking-by-detection from a single, uncalibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1820" to="1833" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical data association and depth-invariant appearance model for indoor multiple objects tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collaborative Tracking for Multiple Objects in the Presence of Inter-Occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oussalah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="304" to="318" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On a measure of divergence between two multinomial populations. Sankhy: the indian journal of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1946" />
			<biblScope unit="page" from="401" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A general framework for tracking multiple people from a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1577" to="1591" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">T-cnn: Tubelets with convolutional neural networks for object detection from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02532</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04053</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the CLEAR MOT metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A unifying framework for detecting outliers and change points from time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamanishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="482" to="492" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kitagawa</surname></persName>
		</author>
		<title level="m">Practices in Time Series Analysis I,II. Asakura Shoten</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detection and tracking of point features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pittsburgh: School of Computer Science</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon Univ</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Object detection via a multi-region and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv pre-print</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">MOT16: A Benchmark for Multi-Object Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tracking multiple vehicles using foreground, background and motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Magee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d traffic scene understanding from movable platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1012" to="1025" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Continuous energy minimization for multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="58" to="72" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The way they move: Tracking multiple targets with similar appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dicle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">I</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Joint probabilistic data association revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Introducing markov chain monte carlo. Markov chain Monte Carlo in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tracking Interacting Objects Using Intertwined Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient object detection using convolutional neural network-based hierarchical feature modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal, Image and Video Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
