<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Perfect Match: 3D Point Cloud Matching with Smoothed Densities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<title level="a" type="main">The Perfect Match: 3D Point Cloud Matching with Smoothed Densities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose 3DSmoothNet, a full workflow to match 3D point clouds with a siamese deep learning architecture and fully convolutional layers using a voxelized smoothed density value (SDV) representation. The latter is computed per interest point and aligned to the local reference frame (LRF) to achieve rotation invariance. Our compact, learned, rotation invariant 3D point cloud descriptor achieves 94.9% average recall on the 3DMatch benchmark data set <ref type="bibr" target="#b49">[50]</ref>, outperforming the state-of-the-art by more than 20 percent points with only 32 output dimensions. This very low output dimension allows for near realtime correspondence search with 0.1 ms per feature point on a standard PC. Our approach is sensor-and sceneagnostic because of SDV, LRF and learning highly descriptive features with fully convolutional layers. We show that 3DSmoothNet trained only on RGB-D indoor scenes of buildings achieves 79.0% average recall on laser scans of outdoor vegetation, more than double the performance of our closest, learning-based competitors <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>. Code, data and pre-trained models are available online at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D point cloud matching is necessary to combine multiple overlapping scans of a scene (e.g., acquired using an RGB-D sensor or a laser scanner) into a single representation for further processing like 3D reconstruction or semantic segmentation. Individual parts of the scene are usually captured from different viewpoints with a relatively low overlap. A prerequisite for further processing is thus aligning these individual point cloud fragments in a common coordinate system, to obtain one large point cloud of the complete scene.</p><p>Although some works aim to register 3D point clouds based on geometric constraints (e.g., <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b35">36]</ref>), most approaches match corresponding 3D feature descriptors that are custom-tailored for 3D point clouds and usually de- scribe point neighborhoods with histograms of point distributions or local surface normals (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref>). Since the comeback of deep learning, research on 3D local descriptors has followed the general trend in the vision community and shifted towards learning-based approaches and more specifically deep neural networks <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b3">4]</ref>. Although the field has seen significant progress in the last three years, most learned 3D feature descriptors are either not rotation invariant <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b45">46]</ref>, need very high output dimensions to be successful <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b3">4]</ref> or can hardly generalize to new domains <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b17">18]</ref>. In this paper, we propose 3DSmoothNet, a deep learning approach for 3D point cloud matching, which has low output dimension (16 or 32) for very fast correspondence search, high descriptiveness (outperforms all state-of-the-art approaches by more than 20 percent points), is rotation invariant, and does generalize across sensor modalities and from indoor scenes of buildings to natural outdoor scenes.</p><p>Contributions We propose a new compact learned local feature descriptors for 3D point cloud matching that is efficient to compute and outperforms all existing methods significantly. A major technical novelty of our paper is the smoothed density value (SDV) voxelization as a new input data representation that is amenable to fully convolutional layers of standard deep learning libraries. The gain of SDV is twofold. On the one hand, it reduces the sparsity of the input voxel grid, which enables better gradient flow during backpropagation, while reducing the boundary effects, as well as smoothing out small miss-alignments due to errors in the estimation of the local reference frame (LRF). On the other hand, we assume that it explicitly models the smoothing that deep networks typically learn in the first layers, thus saving network capacity for learning highly descriptive features. Second, we present a Siamese network architecture with fully convolutional layers that learns a very compact, rotation invariant 3D local feature descriptor. This approach generates low-dimensional, highly descriptive features that generalize across different sensor modalities and from indoor to outdoor scenes. Moreover, we demonstrate that our low-dimensional feature descriptor (only 16 or 32 output dimensions) greatly speeds up correspondence search, which allows real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section reviews advances in 3D local feature descriptors, starting from the early hand-crafted feature descriptors and progressing to the more recent approaches that apply deep learning.</p><p>Hand-crafted 3D Local Descriptors Pioneer works on hand-crafted 3D local feature descriptors were usually inspired by their 2D counterparts. Two basic strategies exist depending on how rotation invariance is established. Many approaches including SHOT <ref type="bibr" target="#b38">[39]</ref>, RoPS <ref type="bibr" target="#b11">[12]</ref>, USC <ref type="bibr" target="#b37">[38]</ref> and TOLDI <ref type="bibr" target="#b42">[43]</ref> try to first estimate a unique local reference frame (LRF), which is typically based on the eigenvalue decomposition of the sample covariance matrix of the points in neighborhood of the interest point. This LRF is then used to transform the local neighborhood of the interest point to its canonical representation in which the geometric peculiarities, e.g. orientation of the normal vectors or local point density are analyzed. On the other hand, several approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b1">2]</ref> resort to a LRF-free representation based on intrinsically invariant features (e.g., point pair features). Despite significant progress, hand-crafted 3D local descriptors never reached the performance of hand-crafted 2D descriptors. In fact, they still fail to handle point cloud resolution changes, noisy data, occlusions and clutter <ref type="bibr" target="#b10">[11]</ref>.</p><p>Learned 3D Local Descriptors The success of deeplearning methods in image processing also inspired various approaches for learning geometric representations of 3D data. Due to the sparse and unstructured nature of raw point clouds, several parallel tracks regarding the representation of the input data have emerged.</p><p>One idea is projecting 3D point clouds to images and then inferring local feature descriptors by drawing from the rich library of well-established 2D CNNs developed for image interpretation. For example, <ref type="bibr" target="#b5">[6]</ref> project 3D point clouds to depth maps and extract features using an auto-encoder.</p><p>[14] use a 2D CNN to combine the rendered views of feature points at multiple scales into a single local feature descriptor. Another possibility are dense 3D voxel grids either in the form of binary occupancy grids <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26]</ref> or an alternative encoding <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b44">45]</ref>. For example, 3DMatch <ref type="bibr" target="#b49">[50]</ref>, one of the pioneer works in learning 3D local descriptors, uses a volumetric grid of truncated distance functions to represent the raw point clouds in a structured manner. Another option is estimating the LRF (or a local reference axis) for extracting canonical, high-dimensional, but hand-crafted features and using a neural network solely for a dimensionality reduction. Even-though these methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref> manage to learn a non-linear embedding that outperforms the initial representations, their performance is still bounded by the descriptiveness of the initial handcrafted features.</p><p>PointNet <ref type="bibr" target="#b24">[25]</ref> and PointNet++ <ref type="bibr" target="#b26">[27]</ref> are seminal works that introduced a new paradigm by directly working on raw unstructured point-clouds. They have shown that a permutation invariance of the network, which is important for learning on unordered sets, can be accomplished by using symmetric functions. Albeit, successful in segmentation and classification tasks, they do not manage to encapsulate the local geometric information in a satisfactory manner, largely because they are unable to use convolutional layers in their network design <ref type="bibr" target="#b4">[5]</ref>. Nevertheless, PointNet offers a base for PPFNet <ref type="bibr" target="#b4">[5]</ref>, which augments raw point coordinates with point-pair features and incorporates global context during learning to improve the feature representation. However, PPFNet is not fully rotation invariant. PPF-FoldNet addresses the rotation invariance problem of PPFNet by solely using point-pair features as input. It is based on the architectures of the PointNet <ref type="bibr" target="#b24">[25]</ref> and Fold-ingNet <ref type="bibr" target="#b43">[44]</ref> and is trained in a self-supervised manner. The recent work of <ref type="bibr" target="#b45">[46]</ref> is based on PointNet, too, but deviates from the common approach of learning only the feature descriptor. It follows the idea of <ref type="bibr" target="#b46">[47]</ref> in trying to fuse the learning of the keypoint detector and descriptor in a single network in a weakly-supervised way using GPS/INS tagged 3D point clouds. <ref type="bibr" target="#b45">[46]</ref> does not achieve rotation invariance of the descriptor and is limited to smaller point cloud sizes due to using PointNet as a backbone.</p><p>Arguably, training a network directly from raw point clouds fulfills the end-to-end learning paradigm. On the downside, it does significantly hamper the use of convolutional layers, which are crucial to fully capture local geometry. We thus resort to a hybrid strategy that, first, transforms point neighborhoods into LRFs, second, encodes unstructured 3D point clouds as SDV grids amenable to convolutional layers and third, learns descriptive features with a siamese CNN. This strategy does not only establish rotation invariance but also allows good performance with low output dimensions, which speeds up correspondence search. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In a nutshell, our workflow is as follows ( <ref type="figure" target="#fig_1">Fig. 2 &amp; 3</ref>): (i) given two raw point clouds, (ii) compute the LRF of the spherical neighborhood around the randomly selected interest points, (iii) transform the neighborhoods to their canonical representations, (iv) voxelize them with the help of Gaussian smoothing, (v) infer the per point local feature descriptors using 3DSmoothNet and, for example, use them as input to a RANSAC-based robust point cloud registration pipeline.</p><p>More formally, consider two overlapping point cloud sets P and Q represented in matrix form as P ∈ R n×3 and Q ∈ R m×3 . Let (P) i =: p i represent the coordinate vector of an individual point of point cloud P located in the overlapping region. A bijective function maps point p i to its corresponding (but initially unknown) point (Q) j =: q j in the second point cloud. Under the assumption of a static scene and rigid point clouds (and neglecting noise and differing point cloud resolutions), this bijective function can be described with the transformation parameters of the congruent transformation</p><formula xml:id="formula_0">q j = Rp i + t,<label>(1)</label></formula><p>where R ∈ SO(3) denotes the rotation matrix and t ∈ R 3 the translation vector. With point subsets P c and Q c for which correspondences exist, the mapping function can be written as</p><formula xml:id="formula_1">Q c = KP c R T + 1 ⊗ t T ,<label>(2)</label></formula><p>where K ∈ P |Q | denotes a permutation matrix whose entries k ij = 1 if p i corresponds to q j and 0 otherwise and 1 is a vector of ones. In our setting both, the permutation matrix K and the transformation parameters R and t are unknown initially. Simultaneously solving for all is hard as the problem is non-convex with binary entries in K <ref type="bibr" target="#b20">[21]</ref>. However, if we find a way to determine K, the estimation of the transformation parameters is straightforward. This boils down to learning a function that maps point p i to a higher dimensional feature space in which we can determine its corresponding point q j . Once we have established correspondence, we can solve for R and t. Computing a rich feature representation across the neighborhood of p i ensures robustness against noise and facilitates high descriptiveness. Our main objective is a fully rotation invariant local feature descriptor that generalizes well across a large variety of scene layouts and point cloud matching settings. We choose a data-driven approach for this task and learn a compact local feature descriptor from raw point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Input parameterization</head><p>A core requirement for a generally applicable local feature descriptor is its invariance under isometry of the Euclidian space. Since achieving the rotation invariance in practice is non-trivial, several recent works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b45">46]</ref> choose to ignore it and thus do not generalize to rigidly transformed scenes <ref type="bibr" target="#b3">[4]</ref>. One strategy to make a feature descriptor rotation invariant is regressing the canonical orientation of a local 3D patch around a point as an integral part of a deep neural network <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b45">46]</ref> inspired by recent work in 2D image processing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48]</ref>. However, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> find that this strategy often fails for 3D point clouds. We therefore choose a different approach and explicitly estimate LRFs by adapting the method of <ref type="bibr" target="#b42">[43]</ref>. An overview of our method is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> and is described in the following.</p><p>Local reference frame Given a point p in point cloud P, we select its local spherical support S ⊂ P such that S = {p i : ||p i − p|| 2 ≤ r LRF } where r LRF denotes the radius of the local neighborhood used for estimating the LRF. In contrast to <ref type="bibr" target="#b42">[43]</ref>, where only points within the distance <ref type="bibr">1 3</ref> r LRF are used, we approximate the sample covariance matrixΣ S using all the points p i ∈ S. Moreover, we replace the centroid with the interest point p to reduce computational complexity. We compute the LRF via the eigendecomposition ofΣ S : We select the z-axisẑ p to be collinear to the estimated normal vectorn p obtained as the eigenvector corresponding to the smallest eigenvalue ofΣ S . We solve for the sign ambiguity of the normal vectorẑ p bŷ</p><formula xml:id="formula_2">Σ S = 1 |S| pi∈S (p i − p)(p i − p) T<label>(3)</label></formula><formula xml:id="formula_3">z p =   n p , if pi∈S n p , p i p ≥ 0 −n p , otherwise<label>(4)</label></formula><p>The x-axisx p is computed as the weighted vector sum</p><formula xml:id="formula_4">x p = 1 || pi∈S α i β i v i || 2 pi∈S α i β i v i<label>(5)</label></formula><p>where v i = pp i − pp i ,ẑ p ẑ p is the projection of the vector pp i to a plane orthogonal toẑ p and α i and β i are weights related to the norm and the scalar projection of the vector pp i to the vectorẑ p computed as</p><formula xml:id="formula_5">α i = (r LRF − ||p − p i || 2 ) 2 β i = pp i ,ẑ p 2<label>(6)</label></formula><p>Intuitively, the weight α i favors points lying close to the interest point thus making the estimation ofx p more robust against clutter and occlusions. β i gives more weight to points with a large scalar projection, which are likely to contribute significant evidence particularly in planar areas <ref type="bibr" target="#b42">[43]</ref>. Finally, the y-axisŷ p completes the left-handed LRF and is computed asŷ p =x p ×ẑ p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smoothed density value (SDV) voxelization</head><p>Once points in the local neighborhood p i ∈ S have been trans-formed to their canonical representation p i ∈ S ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>), we use them to describe the transformed local neighborhood of the interest points. We represent points in a SDV voxel grid, centered on the interest point p and aligned with the LRF. We write the SDV voxel grid as a three dimensional matrix X SDV ∈ R W ×H×D whose elements (X SDV ) jkl =: x jkl represent the SDV of the corresponding voxel computed using the Gaussian smoothing kernel with bandwidth h:</p><formula xml:id="formula_6">x jkl = 1 n jkl n jkl i=1 1 √ 2πh exp −||c jkl − p i || 2 2 2h 2 s.t. ||c jkl − p i || 2 &lt; 3h<label>(7)</label></formula><p>where n jkl denotes the number of points p i ∈ S that lie within the distance 3h of the voxel centroid c jkl (see <ref type="figure" target="#fig_1">Fig. 2(d)</ref>). Further, all values of X SDV are normalized such that they sum up to 1 in order to achieve invariance with respect to varying point cloud densities. For ease of notation we omit the superscript SDV in X SDV in all following equations. The proposed SDV voxel grid representation has several advantages over the traditional binaryoccupancy grid <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41]</ref>, the truncated distance function <ref type="bibr" target="#b49">[50]</ref> or hand-crafted feature representations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>. First, we mitigate the impact of boundary effects and noise of binary-occupancy grids and truncated distance functions by smoothing density values over voxels. Second, compared to the binary occupancy grid we reduce the sparsity of the representation on the fragments of the test part of 3DMatch data set by more than 30 percent points (from about 90% to about 57%), which enables better gradient flow during backpropagation. Third, the SDV representation helps our method to achieve better generalization as we do not overfit exact data cues during training. Finally, in contrast to hand-crafted feature representations, SDV voxel grid representation provides input with a geometrically informed structure, which enables us to exploit convolutional layers that are crucial to capture the local geometric characteristics of point clouds ( <ref type="figure" target="#fig_4">Fig. 5</ref>).</p><p>Network architecture Our network architecture ( <ref type="figure" target="#fig_2">Fig. 3)</ref> is loosely inspired by L2Net <ref type="bibr" target="#b36">[37]</ref>, a state-of-the-art learned local image descriptor. 3DSmoothNet consists of stacked convolutional layers that applies strides of 2 (instead of max-pooling) in some convolutional layers to down-sample the input <ref type="bibr" target="#b33">[34]</ref>. All convolutional layers, except the final one, are followed by batch normalization <ref type="bibr" target="#b14">[15]</ref> and use the ReLU activation function <ref type="bibr" target="#b22">[23]</ref>. In our implementation, we follow <ref type="bibr" target="#b36">[37]</ref> and fix the affine parameters of the batch normalization layer to 1 and 0 and we do not train them during the training of the network. To avoid over-fitting the network, we add dropout regularization <ref type="bibr" target="#b34">[35]</ref> with a 0.3 dropout rate before the last convolutional layer. The output of the last convolutional layer is fed to a batch normalization layer followed by an l2 normalization to produce unit length local feature descriptors.</p><p>Training We train 3DSmoothNet ( <ref type="figure" target="#fig_2">Fig. 3</ref>) on point cloud fragments from the 3DMatch data set <ref type="bibr" target="#b49">[50]</ref>. This is an RGB-D data set consisting of 62 real-world indoor scenes, ranging from offices and hotel rooms to tabletops and restrooms. Point clouds obtained from a pool of data sets <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b2">3]</ref> are split into 54 scenes for training and 8 scenes for testing. Each scene is split into several partially overlapping fragments with their ground truth transformation parameters T . Consider two fragments F i and F j , which have more than 30% overlap. To generate training examples, we start by randomly sampling 300 anchor points p a from the overlapping region of fragment F i . After applying the ground truth transformation parameters T j () the positive sample p p is then represented as the nearest-neighbor p p =: nn(p a ) ∈ T j (F j ), where nn() denotes the nearest neighbor search in the Euclidean space based on the l2 distance. We refrain from pre-sampling the negative examples and instead use the hardest-in-batch method <ref type="bibr" target="#b12">[13]</ref> for sampling negative samples on the fly. During training we aim to minimize the soft margin Batch Hard (BH) loss function</p><formula xml:id="formula_7">L BH (θ, X ) = 1 |X | |X | i=1 ln 1 + exp ||f θ (X a i ) − f θ (X p i )|| 2 − min j=1...|X | j =i ||f θ (X a i ) − f θ (X p j )|| 2<label>(8)</label></formula><p>The BH loss is defined for a mini-batch X , where X a i and X p i represent the SDV voxel grids of the anchor and positive input samples, respectively. The negative samples are retrieved as the hardest non-corresponding positive samples in the mini-batch (c.f. Eq. 8). Hardest-in-batch sampling ensures that negative samples are neither too easy (i.e, non-informative) nor exceptionally hard, thus preventing the model to learn normal data associations <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Implementation details Our 3DSmoothNet approach is implemented in C++ (input parametrization) using the PCL <ref type="bibr" target="#b30">[31]</ref> and in Python (CNN part) using Tensorflow <ref type="bibr" target="#b0">[1]</ref>. During training we extract SDV voxel grids of size W = H = D = 0.3 m (corresponding to <ref type="bibr" target="#b49">[50]</ref>), centered at each interest point and aligned with the LRF. We use r LRF = √ 3W to extract the spherical support S and estimate the LRF. We obtain the circumscribed sphere of our voxel grid and use the points transformed to the canonical frame to extract the SDV voxel grid. We split each SDV voxel grid into 16 3 voxels with an edge w = W 16 and use a Gaussian smoothing kernel with an empirically determined optimal width h = 1.75w 2 . All the parameters wew slected on the validation data set. We train the network with mini-batches of size 256 and optimize the parameters with the ADAM optimizer <ref type="bibr" target="#b18">[19]</ref>, using an initial learning rate of 0.001 that is exponentially decayed every 5000 iterations.</p><p>Weights are initialized orthogonally <ref type="bibr" target="#b31">[32]</ref> with 0.6 gain, and biases are set to 0.01. We train the network for 20 epochs.</p><p>We evaluate the performance of 3DSmoothNet for correspondence search on the 3DMatch data set <ref type="bibr" target="#b49">[50]</ref> and compare against the state-of-the-art. In addition, we evaluate its generalization capability to a different sensor modality (laser scans) and different scenes (e.g., forests) on the Challenging data sets for point cloud registration algorithms data set <ref type="bibr" target="#b23">[24]</ref> denoted as ETH data set.</p><p>Comparison to state-of-the-art We adopt the commonly used hand-crafted 3D local feature descriptors FPFH <ref type="bibr" target="#b28">[29]</ref> (33 dimensions) and SHOT <ref type="bibr" target="#b38">[39]</ref> (352 dimensions) as baselines and run implementations provided in PCL <ref type="bibr" target="#b30">[31]</ref> for both approaches. We compare against the current stateof-the-art in learned 3D feature descriptors: 3DMatch <ref type="bibr" target="#b49">[50]</ref> (512 dimensions), CGF <ref type="bibr" target="#b17">[18]</ref> (32 dimensions), PPFNet <ref type="bibr" target="#b4">[5]</ref> (64 dimensions), and PPF-FoldNet <ref type="bibr" target="#b3">[4]</ref> (512 dimensions). In case of 3DMatch and CGF we use the implementations provided by the authors in combination with the given pretrained weights. Because source-code of PPFNet and PPF-FoldNet is not publicly available, we report the results presented in the original papers. For all descriptors based on the normal vectors, we ensure a consistent orientation of the normal vectors across the fragments. To allow for a fair evaluation, we use exactly the same interest points (provided by the authors of the data set) for all descriptors. In case of descriptors that are based on spherical neighborhoods, we use a radius that yields a sphere with the same volume as our voxel. All exact parameter settings, further implementation details etc. used for these experiments are available in supplementary material.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on the 3DMatch data set</head><p>Setup The test part of the 3DMatch data set consists of 8 indoor scenes split into several partially overlapping fragments. For each fragment, the authors provide indices of 5000 randomly sampled feature points. We use these feature points for all descriptors. The results of PPFNet and PPF-FoldNet are based on a spherical neighborhood with a diameter of 0.6m. Furthermore, due to its memory bottleneck, PPFNet is limited to 2048 interest points per fragment. We adopt the evaluation metric of <ref type="bibr" target="#b4">[5]</ref> (see supplementary material). It is based on the theoretical analysis of the number of iterations needed by a robust registration pipeline, e.g. RANSAC, to find the correct set of transformation parameters between two fragments. As done in <ref type="bibr" target="#b4">[5]</ref>, we set the threshold τ 1 = 0.1m on the l2 distance between corresponding points in the Euclidean space and τ 2 = 0.05 to threshold the inlier ratio of the correspondences at 5%.</p><p>Output dimensionality of 3DSmoothNet A general goal is achieving the highest matching performance with the lowest output dimensionality (i.e., filter number in the last convolutional layer of 3DSmoothNet) to decrease run-time and to save memory. Thus, we first run trials to find a good compromise between matching performance and efficiency for the 3DSmoothNet descriptors <ref type="bibr" target="#b0">1</ref> . We find that the performance of 3DSmoothNet quickly starts to saturate with increasing output dimensions <ref type="figure" target="#fig_5">(Fig. 6</ref>). There is only marginal improvement (if any) when using more than 64 dimensions. We thus decide to process all further experiments only for 16 and 32 output dimensions of 3DSmoothNet.</p><p>Comparison to state-of-the-art Results of experimental evaluation on the 3DMatch data set are summarized in Tab. 1 (left) and two hard cases are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>  <ref type="figure">Figure 7</ref>: Recall in relation to inlier ratio threshold. Recall of 3DSmoothNet on the 3DMatch data set remains high even when the inlier threshold ratio is increased. <ref type="bibr" target="#b15">(16)</ref> and Ours (32) achieve an average recall of 92.8% and 94.7%, respectively, which is close to solving the 3DMatch data set. 3DSmoothNet outperforms all state-ofthe-art 3D local feature descriptors with a significant margin on all scenes. Remarkably, Ours (16) improves average recall over all scenes by almost 20 percent points with only 16 output dimensions compared to 512 dimensions of PPF-FoldNet and 352 of SHOT. Furthermore, Ours <ref type="bibr" target="#b15">(16)</ref> and Ours (32) show a much smaller recall standard deviation (STD), which indicates robustness of 3DSmoothNet to scene changes and hints at good generalization ability. The inlier ratio threshold τ 2 = 0.05 as chosen by <ref type="bibr" target="#b4">[5]</ref> results in ≈ 55k iterations to find at least 3 correspondences (with 99.9% probability) with the common RANSAC approach. Increasing the inlier ratio to τ 2 = 0.2 would decrease RANSAC iterations significantly to ≈ 850, which would speed up processing massively. We thus evaluate how gradually increasing the inlier ratio changes performance of 3DSmoothNet in comparison to all other tested approaches ( <ref type="figure">Fig. 7)</ref>. While the average recall of all other methods drops below 30% for τ 2 = 0.2, recall of Ours (16) (blue) and Ours (32) (orange) remains high at 62% and 72%, respectively. This indicates that any descriptorbased point cloud registration pipeline can be made more efficient by just replacing the existing descriptor with our 3DSmoothNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation invariance</head><p>We take a similar approach as <ref type="bibr" target="#b3">[4]</ref> to validate rotation invariance of 3DSmoothNet by rotating all fragments of 3DMatch data set (we name it 3DRotat-edMatch) around all three axis and evaluating the performance of the selected descriptors on these rotated versions. Individual rotation angles are sampled arbitrarily between [0, 2π] and the same indices of points for evaluation are used as in the previous section. Results of Ours <ref type="bibr" target="#b15">(16)</ref> and Ours (32) remain basically unchanged (Tab 1 (right)) compared to the non-rotated variant (Tab 1 (left)), which confirms rotation invariance of 3DSmoothNet (due to estimating LRF). Because performance of all other rotation invari- Ablation study To get a better understanding of the reasons for the very good performance of 3DSmoothNet, we analyze the contribution of individual modules with an ablation study on 3DMatch and 3DRotatedMatch data sets. Along with the original 3DSmoothNet, we consider versions without SDV (we use a simple binary occupancy grid), without LRF and finally without both, LRF and SDV. All networks are trained using the same parameters and for the same number of epochs. Results of this ablation study are summarized in Tab. 2. It turns out that the version without LRF performs best on 3DMatch because most fragments are already oriented in the same way and the original data set version is tailored for descriptors that are not rotation invariant. Inferior performance of the full pipeline on this data set is most likely due to a few wrongly estimated LRF, which reduces performance on already oriented data sets (but allows generalizing to the more realistic, rotated cases). Unsurprisingly, 3DSmoothNet without LRF fails on 3DRotatedMatch because the network cannot learn rotation invariance from the data. A significant performance gain of up to more than 9 percent points can be attributed to using an a SDV voxel grid instead of the traditional binary occupancy grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalizability across modalities and scenes</head><p>We evaluate how 3DSmoothNet generalizes to outdoor scenes obtained using a laser scanner <ref type="figure" target="#fig_0">(Fig. 1</ref>   we use models Ours <ref type="bibr" target="#b15">(16)</ref> and Ours (32) trained on 3DMatch (RGB-D images of indoor scenes) and test on four outdoor laser scan data sets Gazebo-Summer, Gazebo-Winter, Wood-Autumn and Wood-Summer that are part of the ETH data set <ref type="bibr" target="#b23">[24]</ref>. All acquisitions contain several partially overlapping scans of sparse and dense vegetation (e.g., trees and bushes). Accurate ground-truth transformation matrices are available through extrinsic measurements of the scanner position with a total-station. We start our evaluation by down-sampling the laser scans using a voxel grid filter of size 0.02m. We randomly sample 5000 points in each point cloud and follow the same evaluation procedure as in Sec 4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Computation time</head><p>We compare average run-time of our approach per interest point on 3DMatch test fragments to <ref type="bibr" target="#b49">[50]</ref> in Tab. 4 (ran on the same PC with Intel Xeon E5-1650, 32 GB of ram and NVIDIA GeForce GTX1080). Note that input preparation (Input prep.) and inference of <ref type="bibr" target="#b49">[50]</ref> are processed on the GPU, while our approach does input preparation on CPU in its current state. For both methods, we run nearest neighbor correspondence search on the CPU. Naturally, input preparation of 3DSmoothNet on the CPU takes considerably longer (4.2 ms versus 0.5 ms), but still the overall computation time is slightly shorter (4.6 ms versus 5.0 ms). Main drivers for performance are inference (0.3 ms versus 3.7 ms) and nearest neighbor correspondence search (0.1 ms versus 0.8 ms). This indicates that it is worth investing computational resources into custom-tailored data preparation because it significantly speeds up all later tasks. The bigger gap between Ours(16 dim) and Ours(32 dim), is a result of the lower capacity and hence lower descriptiveness of the 16-dimensional descriptor, which becomes more apparent on the harder ETH data set, but can also be seen in additional experiments in supplementary material. Supplementary material also contains additional experiments, which show the invariance of the proposed descriptor to changes in point cloud density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented 3DSmoothNet, a deep learning approach with fully convolutional layers for 3D point cloud matching that outperforms all state-of-the-art by more than 20 percent points. It allows very efficient correspondence search due to low output dimensions <ref type="bibr">(16 or 32)</ref>, and a model trained on indoor RGB-D scenes generalizes well to terrestrial laser scans of outdoor vegetation. Our method is rotation invariant and achieves 94.9% average recall on the 3DMatch benchmark data set, which is close to solving it. To the best of our knowledge, this is the first learned, universal point cloud matching method that allows transferring trained models between modalities. It takes our field one step closer to the utopian vision of a single trained model that can be used for matching any kind of point cloud regardless of scene content or sensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary Material</head><p>In this supplementary material we provide additional information about the evaluation experiments (Sec. 6.1, 6.2 and 6.3) along with the detailed per-scene results (Sec. 6.4) and some further visualizations ( <ref type="figure" target="#fig_6">Fig. 8 and 9</ref>). The source code and all the data needed for comparison are publicly available at https://github.com/zgojcic/ 3DSmoothNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Evaluation metric</head><p>This section provides a detailed explanation of the evaluation metric adopted from <ref type="bibr" target="#b4">[5]</ref> and used for all evaluation experiments throughout the paper.</p><p>Consider two point cloud fragments P and Q, which have more than 30% overlap under ground-truth alignment. Furthermore, let all such pairs form a set of fragment pairs F = {(P, Q)}. For each fragment pair the set of correspondences obtained in the feature space is then defined as</p><formula xml:id="formula_8">C = {{p i ∈ P, q j ∈ Q}, f (p i ) = nn(f (q j ), f (P))∧ f (q j ) = nn(f (p i ), f (Q))}<label>(9)</label></formula><p>where f (p) denotes a non-linear function that maps the feature point p to its local feature descriptor and nn() denotes the nearest neighbor search based on the l2 distance. Finally, the quality of the correspondences in terms of average recall R per scene is computed as</p><formula xml:id="formula_9">R = 1 |F | |F | f =1 1 1 |Cf | i,j∈Cs 1 ||p i − T f (q j )|| 2 &lt; τ 1 &gt; τ 2<label>(</label></formula><p>10) where T f denotes the ground-truth transformation alignment of the fragment pair f ∈ F. τ 1 is the threshold on the Euclidean distance between the correspondence pair (i, j) found in the feature space and τ 2 is a threshold on the inlier ratio of the correspondences <ref type="bibr" target="#b4">[5]</ref>. Following <ref type="bibr" target="#b4">[5]</ref> we set τ 1 = 0.1m and τ 2 = 0.05 for both, the 3DMatch <ref type="bibr" target="#b49">[50]</ref> as well as the ETH <ref type="bibr" target="#b23">[24]</ref> data set. The evaluation metric is based on the theoretical analysis of the number of iterations k needed by RANSAC <ref type="bibr" target="#b7">[8]</ref> to find at least n = 3 corresponding points with the probability of success p = 99.9%. Considering, τ 2 = 0.05 and the relation</p><formula xml:id="formula_10">k = log(1 − p) log(1 − τ n 2 ) ,<label>(11)</label></formula><p>the number of iterations equals k ≈ 55000 and can be greatly reduced if the number of inliers τ 2 can be increased (e.g. k = 860 if τ 2 = 0.2). <ref type="bibr" target="#b2">3</ref> Larger voxel grid width used due to the memory restrictions. <ref type="bibr" target="#b3">4</ref> Used to avoid the excessive binning near the center, see <ref type="bibr" target="#b17">[18]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Baseline Parameters</head><p>In order to perform the comparison with the state-of-theart methods, several parameters have to be set. To ensure a fair comparison we set all the parameters relative to our voxel grid width W which we set as W 3DMatch = 0.3m and W ETH = 1m for 3DMatch and ETH data sets respectively. More specific, for the descriptors based on the spherical support we use a feature radius r f = 3 3 4π W that yields a sphere with the same volume as our voxel grid and for all voxel-based descriptors we use the same voxel grid width W . For descriptors that require, along with the coordinates also the normal vectors, we use the point cloud library (PCL) built-in function for normal vector computation, using all the points in the spherical support with the radius r n = r f 2 . Tab. 5 provides all the parameters that were used for the evaluation. If some parameters are not listed in Tab 5 we use the original values set by the authors. For the handcrafted descriptors, FPFH <ref type="bibr" target="#b28">[29]</ref> and SHOT <ref type="bibr" target="#b38">[39]</ref> we use the implementation provided by the original authors as a part of the PCL 5 . We use the PCL version 1.8.1 x64 on Windows 10 and use the parallel programming implementations (omp) of both descriptors. For 3DMatch <ref type="bibr" target="#b49">[50]</ref> we use the implementation provided by the authors 6 on Ubuntu 16.04 in combination with the CUDA 8.0 and cuDNN 5.1. Finally, for CGF <ref type="bibr" target="#b17">[18]</ref> we use the implementation provided by the authors 7 on a PC running Windows 10. Note that we report the results of PPFNet <ref type="bibr" target="#b4">[5]</ref> and PPF-FoldNet <ref type="bibr" target="#b3">[4]</ref> as reported by the authors in the original papers, because the source code is not publicly available. Nevertheless, for the sake of completeness we report the feature radius r f and the k-nearest neighbors k n used for the normal vector computation, which were used by the authors in the original works.  For the 3DRotatedMatch and 3DSparseMatch data sets we use the same parameters as for the3DMatch data set.</p><p>Performance of the 3DMatch descriptor The authors of the 3DMatch descriptor provide along with the source code and the trained model also the precomputed truncated distance function (TDF) representation and inferred descriptors for the 3DMatch data set. We use this descriptors directly for all evaluations on the original 3DMatch data set. For the evaluations on the 3DRotatedMatch, 3DSparse-Match and ETH data sets we use their source code in combination with the pretrained model to infer the descriptors. When analyzing the 3DSparseMatch data set results, we noticed a discrepancy. The descriptors inferred by us achieve better performance than the provided ones. We analyzed this further and determined that the TDF representation (i.e. the input to the CNN) is identical and the difference stems from the inference using their provided weights. In the paper this is marked by a footnote in the results section. For the sake of consistency, we report in this Supplementary material all results for 3DMatch data set using the precoumpted descriptors and the results on all other data set using the descriptors inferred by us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Preprocessing of the benchmark data sets</head><p>3DMatch data set The authors of 3DMatch data set provide along with the point cloud fragments and the groundtruth transformation parameters also the indices of the interest points and the ground-truth overlap for all fragments.</p><p>To make the results comparable to previous works, we use these indices and overlap information for all descriptors and perform no preprocessing of the data.</p><p>3DSparseMatch data set In order to test the robustness of our approach to variations in point density we create a new data set, denoted as 3DSparseMatch, using the point cloud fragments from the 3DMatch data set. Specifically, we first extract the indices of the interest points provided by the authors of the 3DMatch data set and then randomly downsample the remaining points, keeping 50%, 25% and 12.5% of the points. We consider two scenarios in the evaluation. In the first scenario we use one of the fragments to be registered with the full and the other one with the reduced point cloud density (Mixed), while in the second scenario we evaluate the descriptors on the fragments with the same level of sparsity (Both).</p><p>ETH data set For the ETH data set we use the point clouds and the ground-truth transformation parameters provided by the authors of the data set. We start by downsampling the point clouds using a voxel grid filter with the voxel size equal to 0.02m. The authors of the data set also provide the ground-truth overlap information, but due to the downsampling step we opt to compute the overlap on our own as follows. Let p i ∈ P and q i ∈ Q denote points in the point clouds P and Q, which are part of the same scene of ETH data set, respectively. Given the ground-truth transformation T Q P that aligns the point cloud Q with the point cloud P, we compute the overlap ψ P,Q relative to point cloud P as   where nn denotes the nearest neighbor search based on the l2 distance in the Euclidean space and τ ψ thresholds the distance between the nearest neighbors. In our evaluation experiments, we select τ ψ = 0.06m, which equals three times the resolution of the point clouds after the voxel grid downsampling, and consider only the point cloud pairs for which both ψ P,Q and ψ Q,P are bigger than 0.3. Because no indices of the interest points are provided we randomly sample 5000 interest points that have more than 10 neighbor points in a sphere with a radius r = 0.5m in every point cloud. The condition of minimum ten neighbors close to the interest point is enforced in order to avoid the problems with the normal vector computation.</p><formula xml:id="formula_11">ψ P,Q = 1 |P| |P| i=1 1 ||p i − nn(p i , T P,Q (Q)|| 2 &lt; τ ψ<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Detailed results</head><p>3DMatch data set Detailed per scene results on the 3DMatch data set are reported in Tab. 6. Ours (32 dim) consistently outperforms all state-of-the-art by a significant margin and achieves a recall higher than 89% on all of the scenes. However, the difference between the performance of individual descriptors is somewhat masked by the selected low value of τ 2 , e.g. same average recall on Hotel 3 scene achieved by Ours (16 dim) and Ours (32 dim). Therefore, we additionally perform a more direct evaluation of the quality of found correspondences, by computing the average number of correct correspondences established by each individual descriptor (Tab 7). Where the term correct correspondences, denotes the correspondences for which the distance between the points in the coordinate space after the ground-truth alignment is smaller than 0.1m. Results in Tab. 7 again show the dominant performance of the 3DSmoothNet compared to the other state-of-the-art but also highlight the difference between Ours (32 dim) and Ours (16 dim). Remarkably, Ours (32 dim) can establish almost two times more correspondences than the closest competitor.</p><p>3DRotatedMatch data set We additionally report the detailed results on the 3DRotatedMatch data set in Tab 8.</p><p>Again, 3DSmoothNet outperforms all other descriptor on all the scenes and maintains a similar performance as on the 3DMatch data set. As expected the performance of the rotational invariant descriptors <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4]</ref> is not affected by the rotations of the fragments, whereas the performance of the descriptors, which are not rotational invariant <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b4">5]</ref> drops to almost zero. This greatly reduces the applicability of such descriptors for general use, where one considers the point cloud, which are not represented in their canonical representation.</p><p>3DSparseMatch data set Tab 9 shows the results on the three different density levels (50%, 25% and 12, 5%) of the 3DSparseMatch data set. Generally, all descriptors perform better when the point density of only one fragments is reduced, compared to when both fragments are downsampled. In both scenarios, the recall of our approach drops marginally by max 1 percent point and remains more than 20 percent points above any other competing method. Therefore, 3DSmoothNet can be labeled as invariant to point density changes.  <ref type="table">Table 9</ref>: Results on the 3DSparseMatch data set. 'Mixed' denotes Scenario 1 in which only one of the fragments was downsampled and 'Both' denotes that both fragments were downsampled. We report average recall in percent over all scenes. Best performance is shown in bold.  : Qualitative results of the 3DSmoothNet on the ETH data set. 3DSmoothNet trainined only on the indoor reconstructions from RGB-D images can generalize to outdoor natural scenes, which consist of high level of noise and predominantly unstructured vegetation. The data set is made even harder by the introduced dynamic between the epochs (e.g. walking persons)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>3DSmoothNet generalization ability: our descriptor, trained solely on indoor scenes (top) can seamlessly generalize to outdoor scenes (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Input parameterization: (a) We extract the spherical support S of the interest point p, which is used (b) to estimate a unique LRF. (c) Each data cube is transformed to its canonical representation and (d) voxelized using a Gaussian smoothing kernel. (e) The normalized 3D SDV voxel grid is used as input to our siamese 3DSmoothNet architecture. Note that (d) and (e) show 2D slices of 3D cubes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>3DSmoothNet network architecture: We extract interest points in the overlapping region of two fragments. The cubic patches (bounding box is color coded to the interest points), centered at the interest point and aligned with the estimate LRF are converted to the SDV voxel grid and fed to the network. 3DSmoothNet consists of convolutional (green rectangles with number of filters and filter size respectively), batch-normalization (orange), ReLU activation function (blue) and an l2-normalization (magenta) layer. Both branches share all the parameters. The anchor f θ (X a ), positive f θ (X p ) and negative f θ (X n ) arguments of the batch hard loss are color coded according to the interest points. Negative examples are sampled on the fly from all the positive examples of the mini-batch (denoted with the four voxel grids).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Results on the 3DMatch data set after RANSAC: 3DSmoothNet generates reliable correspondences for pairs with low overlap (32% (top), 48% (bottom)) and predominantly planar regions (top row) or hard cases with vegetation and repetitive geometries (Christmas tree, windows in bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>3DSmoothNet descriptors are geometrically informed: Embedding in 3D space with PCA (first three components RGB color-coded). Planar regions lie in the blue-green, edges and corners in the orange-pink and spherical surfaces in the yellow color spectrum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Recall in relation to 3DSmoothNet output dimensions. Values in brackets denote average recall over all scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Additional qualitative results of 3DSmoothNet on the 3DMatch data set. First three rows show hard examples for which the 3DSmoothNet succeeds, whereas the last three rows show some of the failure cases. 3DMatch and CGF fail for all these examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9</head><label>9</label><figDesc>Figure 9: Qualitative results of the 3DSmoothNet on the ETH data set. 3DSmoothNet trainined only on the indoor reconstructions from RGB-D images can generalize to outdoor natural scenes, which consist of high level of noise and predominantly unstructured vegetation. The data set is made even harder by the introduced dynamic between the epochs (e.g. walking persons)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). To this end, 3DMatch data set Original Rotated τ 2 = 0.05 τ 2 = 0.2 τ 2 = 0.05 τ 2 = 0.2</figDesc><table><row><cell>All together</cell><cell>94.7</cell><cell>72.7</cell><cell>94.9</cell><cell>72.8</cell></row><row><cell>W/o SDV</cell><cell>92.5</cell><cell>63.5</cell><cell>92.5</cell><cell>63.6</cell></row><row><cell>W/o LRF</cell><cell>96.3</cell><cell>81.6</cell><cell>11.6</cell><cell>2.7</cell></row><row><cell>W/o SDV &amp; LRF</cell><cell>95.6</cell><cell>78.6</cell><cell>9.7</cell><cell>2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of 3DSmoothNet on 3DMatch and 3DRotatedMatch data sets. We report average recall over all overlapping fragment pairs. Best performance is shown in bold.</figDesc><table><row><cell></cell><cell cols="2">Gazebo</cell><cell>Wood</cell></row><row><cell></cell><cell cols="3">Sum. Wint. Aut. Sum. Average</cell></row><row><cell>FPFH [29]</cell><cell cols="3">38.6 14.2 14.8 20.8</cell><cell>22.1</cell></row><row><cell>SHOT [39]</cell><cell cols="3">73.9 45.7 60.9 64.0</cell><cell>61.1</cell></row><row><cell cols="2">3DMatch [50] 22.8</cell><cell cols="2">8.3 13.9 22.4</cell><cell>16.9</cell></row><row><cell>CGF [18]</cell><cell cols="3">37.5 13.8 10.4 19.2</cell><cell>20.2</cell></row><row><cell cols="4">Ours ( 16 dim) 76.1 47.7 31.3 37.6</cell><cell>48.2</cell></row><row><cell cols="4">Ours ( 32 dim) 91.3 84.1 67.8 72.8</cell><cell>79.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on the ETHdata set. We report average recall in percent per scene as well as across the whole data set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1, again considering only point clouds with more than 30% overlap. More details on sampling of the feature points and computation of the point cloud overlaps are available in the supplementary material. Due to the lower resolution of the point clouds, we now use a larger value of W = 1 m for the SDV voxel grid (consequently the radius for the descriptors based on the spherical neighborhood is also increased). A voxel grid with an edge equal to 1.5 m is used for 3DMatch because of memory restrictions. Results on the ETH data set are reported in Tab 3. 3DSmoothNet achieves best performance on average (right column), Ours (32) with 79.0% average recall clearly outperforming Ours</figDesc><table><row><cell></cell><cell cols="4">Input prep. Inference NN search Total</cell></row><row><cell></cell><cell>[ms]</cell><cell>[ms]</cell><cell>[ms]</cell><cell>[ms]</cell></row><row><cell>3DMatch</cell><cell>0.5</cell><cell>3.7</cell><cell>0.8</cell><cell>5.0</cell></row><row><cell>3DSmoothNet</cell><cell>4.2</cell><cell>0.3</cell><cell>0.1</cell><cell>4.6</cell></row></table><note>(16) with 48.2% due to its larger output dimension. Ours (32) beats runner-up (unsupervised) SHOT by more than 15 percent points whereas all state-of-the-art methods stay sig- nificantly below 30%. In fact, Ours (32) applied to outdoor</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Average run-time per feature-point on test fragments of 3DMatch data set. laser scans still outperforms all competitors that are trained and tested on the 3DMatch data set (cf. Tab. 3 with Tab. 1).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Parameters used for the state-of-the-art methods in the evaluation experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Detailed quantitative results on the 3DMatch dataset. For each scene we report the average recall in percent over all overlapping fragment pairs. Best performance is shown in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Average number of correct correspondences on 3DMatch data set. We report the average number of correct correspondences over all overlapping fragments of individual scenes.FPFH<ref type="bibr" target="#b28">[29]</ref> SHOT<ref type="bibr" target="#b38">[39]</ref> 3DMatch<ref type="bibr" target="#b49">[50]</ref> CGF<ref type="bibr" target="#b17">[18]</ref> PPFNet<ref type="bibr" target="#b4">[5]</ref> PPF-FoldNet<ref type="bibr" target="#b3">[4]</ref> </figDesc><table><row><cell>Ours</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Detailed quantitative results on the 3DRotatedMatch data set. For each scene we report the average recall in percent over all overlapping fragment pairs. Best performance is shown in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Ours (16 dim) 92.5 92.3 91.3 92.7 91.7 90.5 Ours (32 dim) 95.0 94.5 94.1 95.0 94.5 93.7</figDesc><table><row><cell></cell><cell cols="2">3DSparseMatch data set</cell></row><row><cell></cell><cell>Mixed</cell><cell>Both</cell></row><row><cell></cell><cell cols="2">50% 25% 12.5% 50% 25% 12.5%</cell></row><row><cell>FPFH [29]</cell><cell>54.4 52.0 48.3</cell><cell>52.2 49.7 41.5</cell></row><row><cell>SHOT [39]</cell><cell>71.1 69.8 69.8</cell><cell>70.8 68.4 66.4</cell></row><row><cell cols="2">3DMatch [50] 73.0 72.7 70.2</cell><cell>73.8 72.8 72.8</cell></row><row><cell>CGF [18]</cell><cell>54.2 49.0 37.5</cell><cell>50.3 38.3 24.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Recall that for correspondence search, the brute-force implementation of nearest-neighbor search scales with O(DN 2 ), where D denotes the dimension and N the number of data points. The time complexity can be reduced to O(DN log N ) using tree-based methods, but still becomes inefficient if D grows large ("curse of dimensionality").</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Using the precomputed feature descriptors provided by the authors. For more results see the Supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/PointCloudLibrary/pcl 6 https://github.com/andyzeng/3dmatch-toolbox 7 https://github.com/marckhoury/CGF</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensor-Flow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 5</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Point Pair Features Based Object Detection and Pose Estimation Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BundleFusion: Real-time Globally Consistent 3D Reconstruction using On-the-fly Surface Re-integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>2017. 5</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ppfnet: Global context aware local features for robust 3d point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d point cloud registration for localization using a deep neural network autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning SO(3) Equivariant Representations with Spherical CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Thrift: Local 3D structure recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Flint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hangel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th Biennial Conference of the Australian Pattern Recognition Society on Digital Image Computing Techniques and Applications</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="182" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learned compact local feature descriptor for tls-based geodetic monitoring of natural outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Annals of Photogrammetry, Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Comprehensive Performance Evaluation of 3D Local Feature Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="89" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rotational projection statistics for 3d local surface description and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Local Shape Descriptors from Part Correspondences with Multiview Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: a Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for 3d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The 3D-3D registration problem revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Challenging data sets for point cloud registration algorithms. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vosselman. An integrated approach for modelling and global registration of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dijkman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Den Heuvel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="355" to="370" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (FPFH) for 3D registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3D is here: Point Cloud Library (PCL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Striving for Simplicity: The All Convolutional Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICLR) -workshop track</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Globally consistent registration of terrestrial laser scans via graph optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Theiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="126" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">L2-Net: Deep Learning of Discriminative Patch Descriptor in Euclidean Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno>2017. 5</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unique shape context for 3D data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM workshop on 3D object retrieval</title>
		<meeting>the ACM workshop on 3D object retrieval</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to Navigate the Energy Landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05772</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">TOLDI: An effective and robust approach for 3D local shape description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Geometric features for voxel-based surface recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarotsky</surname></persName>
		</author>
		<idno>abs/1701.04249</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3dfeat-net: Weakly supervised local 3d features for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to assign orientations to feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic registration of rgb-d scans via salient directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Köser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2808" to="2815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Local Geometric Descriptors from RGB-D Reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
