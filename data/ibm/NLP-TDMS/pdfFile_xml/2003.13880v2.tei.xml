<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MUXConv: Information Multiplexing in Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><forename type="middle">Lu</forename><surname>Kalyanmoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Vishnu</surname></persName>
							<email>vishnu@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Boddeti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MUXConv: Information Multiplexing in Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks have witnessed remarkable improvements in computational efficiency in recent years. A key driving force has been the idea of tradingoff model expressivity and efficiency through a combination of 1×1 and depth-wise separable convolutions in lieu of a standard convolutional layer. The price of the efficiency, however, is the sub-optimal flow of information across space and channels in the network. To overcome this limitation, we present MUXConv, a layer that is designed to increase the flow of information by progressively multiplexing channel and spatial information in the network, while mitigating computational complexity. Furthermore, to demonstrate the effectiveness of MUXConv, we integrate it within an efficient multi-objective evolutionary algorithm to search for the optimal model hyper-parameters while simultaneously optimizing accuracy, compactness, and computational efficiency. On ImageNet, the resulting models, dubbed MUXNets, match the performance (75.3% top-1 accuracy) and multiply-add operations (218M) of Mo-bileNetV3 while being 1.6× more compact, and outperform other mobile models in all the three criteria. MUXNet also performs well under transfer learning and when adapted to object detection. On the ChestX-Ray 14 benchmark, its accuracy is comparable to the state-of-the-art while being 3.3× more compact and 14× more efficient. Similarly, detection on PASCAL VOC 2007 is 1.2% more accurate, 28% faster and 6% more compact compared to MobileNetV2. The code is available from https://github.com/ human-analysis/MUXConv.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the span of the last decade, convolutional neural networks (CNNs) have undergone a dramatic transformation in terms of predictive performance, compactness and computational efficiency. The development largely happened in two phases. Starting from AlexNet <ref type="bibr" target="#b19">[21]</ref>, the focus of the first wave of models was on improving the predictive accuracy of CNNs including VGG <ref type="bibr" target="#b35">[37]</ref>, GoogleNet <ref type="bibr" target="#b37">[39]</ref>, ResNet <ref type="bibr" target="#b10">[11]</ref>, ResNeXt <ref type="bibr" target="#b43">[45]</ref>, DenseNet <ref type="bibr" target="#b15">[17]</ref> etc. These models progressively increased the contribution of 3×3 convolutions, both in model size as well as multiply-add operations (MAdds). The focus of the second wave of models was on improving their computational efficiency while trading-off accuracy to a small extent. Models in this category include ShuffleNet <ref type="bibr" target="#b25">[27]</ref>, MobileNetV2 <ref type="bibr" target="#b32">[34]</ref>, MnasNet <ref type="bibr" target="#b38">[40]</ref> and Mo-bileNetV3 <ref type="bibr" target="#b12">[13]</ref>. Such solutions sought to improve computational efficiency by progressively replacing the parameter and compute intensive standard convolutions by a combination of 1×1 convolutions and depth-wise separable 3×3 convolutions. <ref type="figure" target="#fig_1">Figure 2</ref> depicts the trend in the relative contributions of different layers in terms of parameters and MAdds.</p><p>Depth-wise separable convolutions <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b3">4]</ref> offer significant computational benefits, both from the perspective of number of parameters as well as computational complexity. A salient feature of these layers is the lack of interac- tions between information in the channels. This limitation is overcome through 1×1 convolution, a layer which allows for interactions and information flow across the channels.</p><p>The combination of depth-wise separable and 1×1 convolution fully decouples the task of spatial and channel information flow, respectively, into two independent and efficient layers. On the other hand, a standard convolutional layer couples the spatial and channel information flow into a single, yet, computationally inefficient layer. Therefore, the former replaced the latter as the workhorse of CNN designs.</p><p>In this paper, we seek an alternative approach to trade-off the expressivity and efficiency of convolutional layers. We introduce MUXConv, a layer that leverages the efficiency of depth-wise or group-wise convolutional layers along with a mechanism to enhance the flow of information in the network. MUXConv achieves this through two components, spatial multiplexing and channel multiplexing. Spatial multiplexing extracts feature information at multiple scales via spatial shuffling, processes such information through depthwise or group-wise convolutions and then unshuffles them back together. Channel multiplexing is inspired by Shuf-fleNet <ref type="bibr" target="#b25">[27]</ref> and is designed to address the limitation of depth-wise/group convolutions, namely the lack of information flow across channels/groups of channels, by shuffling the channels. The shuffling procedure and the operations we perform on the shuffled channels are motivated by computational efficiency and differ significantly from ShuffleNet. Collectively, these two components increase the flow of information, both spatially and across channels, while mitigating the computational burden of the layer.</p><p>To further realize the full potential of MUXConv in trading-off accuracy and computational efficiency, we propose a population based evolutionary algorithm to efficiently search for the hyperparameters of each MUXConv layer in the network. The search simultaneously optimizes three objectives, namely, prediction accuracy, model compactness and model efficiency in terms of MAdds. To improve the efficiency of the search process we decompose the multi-objective optimization problem into a collection of single-objective optimization sub-problems, that are in turn optimized simultaneously and cooperatively. We refer to the resulting family of CNNs as MUXNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>We first develop a new layer, called MUX-Conv, that multiplexes information flow spatially and across channels while improving the computational efficiency of equivalent combination of depth-wise separable and 1×1 convolutions. Then, we develop the first multi-objective neural architecture search (NAS) algorithm to simultaneously optimize compactness, efficiency, and accuracy of MUXNets designed with MUXConv as the basic building block. We present thorough experimental evaluation demonstrating the efficacy and value of each component of MUXNet across multiple tasks including image classification (ImageNet), object detection (PASCAL VOC 2007) and transfer learning (CIFAR-10, CIFAR-100, ChestX-Ray14). Our results indicate that, unlike the conventional wisdom in all existing solutions, it is feasible to design CNNs that do not sacrifice compactness for efficiency or vice versa in the quest for better predictive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related-work</head><p>Many CNN architectures have been developed by optimizing different objectives, such as, model compactness, computational efficiency, or predictive performance. Below, we categorize the solutions into a few major themes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Scale and Shuffling:</head><p>The notion of multi-scale processing in CNNs has been utilized in different forms and in a variety of contexts. These include explicit processing of multi-resolution feature maps for object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">22]</ref> and image classification [15] and computational blocks with built-in multi-scale processing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>. The focus of these methods is predictive performance and hence towards large scale models. In contrast, multi-scale processing in MUX-Conv is motivated by enhancing information flow in small scale models deployed in resource constrained environments. Notably, MUXConv scales the feature maps through a pixel shuffling operation that is similar to subpixel convolution in <ref type="bibr" target="#b33">[35]</ref>. The channel shuffling component of MUX-Conv is motivated by <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b25">27]</ref>.</p><p>Mobile Architectures: A number of CNN architectures have been developed for mobile settings. These include SqueezeNet <ref type="bibr" target="#b17">[19]</ref>, MobileNet <ref type="bibr" target="#b13">[14]</ref>, MobileNetV2 <ref type="bibr" target="#b32">[34]</ref>, Mo-bileNetV3 <ref type="bibr" target="#b12">[13]</ref>, ShuffleNet <ref type="bibr" target="#b47">[49]</ref>, ShuffleNetV2 <ref type="bibr" target="#b25">[27]</ref> and CondenseNet <ref type="bibr" target="#b14">[16]</ref>. The focus of this body of work has largely been to optimize two objectives, either accuracy and compactness or accuracy and efficiency, thereby resulting in models that are either efficient or compact but not both. In contrast, MUXNets are designed to simultaneously optimize all three objectives, compactness, efficiency and accuracy, and therefore leads to models that are both compact and efficient at the same time.</p><p>Neural Architecture Search: Automated approaches to search for good neural architectures have proven to be very effective in finding computational blocks that not only exhibit high predictive performance but also generalize and transfer to other tasks. Majority of the approaches including, NasNet <ref type="bibr" target="#b51">[53]</ref>, PNAS <ref type="bibr" target="#b21">[23]</ref>, DARTS <ref type="bibr" target="#b22">[24]</ref>, Amoe-baNet <ref type="bibr" target="#b29">[31]</ref> and MixNet <ref type="bibr" target="#b40">[42]</ref>, are optimized against a single objective, namely predictive performance. A couple of recent approaches, LEMONADE <ref type="bibr" target="#b6">[7]</ref>, NSGANet <ref type="bibr" target="#b24">[26]</ref>, simultaneously optimize the networks against multiple objectives, including parameters, MAdds, latency, and accuracy. However, only results on small-scale datasets like CIFAR-10 are demonstrated in both approaches. Concurrently, a number of CNN architectures, such as ProxylessNAS <ref type="bibr" target="#b0">[1]</ref>, MnasNet <ref type="bibr" target="#b38">[40]</ref>, ChamNet <ref type="bibr" target="#b4">[5]</ref> and FBNet <ref type="bibr" target="#b4">[5]</ref>, have been designed to target specific computing platforms such as mobile, CPU, and GPU. In contrast to the aforementioned NAS approaches, we adopt a hybrid search strategy where the basic computational block, MUXConv, is hand-designed while the hyper-parameters of each MUXConv layer in the network are searched through a population based evolutionary algorithm directly on a large scale dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multiplexed Convolutions</head><p>The multiplexed convolution layer, called MUXConv, is a combination of two components: (1) spatial multiplexing which enhances the expressivity and predictive performance of the network, and (2) channel multiplexing which aids in reducing the computational complexity of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial Multiplexing</head><p>The expressivity of a standard convolutional layer stems from the flow of information spatially and across the channels. Spatial multiplexing is designed to mimic this property while mitigating its computational complexity. The key idea is to map spatial information at multiple scales into channels and vice versa. Specifically, given a feature map x ∈ R C×H×W , where C is the number of channels, H is the height and W is the width of the feature map, the channels are grouped into three groups of (C 1 , C 2 , C 3 ) channels such that C = C 1 + C 2 + C 3 . The first and third group of channels are subjected to a subpixel and superpixel multiplexing operation, respectively. The multiplexed channels are then processed through a group-wise convolution operation defined over each of the three groups. The output feature maps from the group convolutions are mapped back to the same dimensions as the input feature maps by reversing the respective subpixel and superpixel operations. An illustration of this process is shown in <ref type="figure" target="#fig_2">Fig. 3a</ref>. Collectively, the subpixel and superpixel operations allow multi-scale spatial information to flow across channels. We note that the standard idea of multi-scale processing in existing approaches, multi-scale feature representations or kernels with larger receptive fields, is typically across different layers. In contrast, MUXConv seeks to exploit multi-scale information within a layer through pixel manipulation. As we show in Section 6, this operation significantly improves network accuracy especially as they get more compact. We parameterize the subpixel multiplexing operation (see <ref type="figure" target="#fig_2">Fig. 3b</ref>) by r and define a window and stride of size r×r. The features in the windows are mapped to r 2 channels, with each window corresponding to a unique feature location in the channels. On the whole, the subpixel operation maps the first group of channel features of size C 1 × H × W to features of size r 2 C 1 × H r × W r . Therefore, the subpixel operation enables down-scaled spatial information to be multiplexed with channel information and processed jointly by a standard convolution over the group. The combination of the two operations effectively increases the receptive field of the convolution by a factor of r.</p><p>We define the superpixel multiplexing operation (see <ref type="figure" target="#fig_2">Fig. 3c</ref>) as an inverse of subpixel multiplexing. It is parameterized by r 2 which corresponds to the number of channels that will be multiplexed spatially into a single channel. The feature values at a particular location from the r 2 channels are mapped to a unique window in the output feature map. On the whole, the superpixel operation maps the third group of channels features of size C 3 × H × W to features of size ↓ Reduction Block ↓ copy leave out copy leave out <ref type="figure">Figure 4</ref>: Illustration of two channel multiplexing layers. In each layer, half the channels are propagated as is while the other half are processed through the spatial multiplexing operation. The channels from the two groups are then interleaved as denoted by the indices. Color intensity denotes number of times that channel is processed.</p><p>volution over the group. The combination of the two operations effectively decreases the receptive field of the convolution by a factor of r. Our superpixel operation bears similarity to the concept of tiled convolution <ref type="bibr" target="#b26">[28]</ref>, a particular realization of locally connected layers. This idea has also been particularly effective for image super-resolution <ref type="bibr" target="#b33">[35]</ref> in the form of "subpixel" convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Channel Multiplexing</head><p>While the spatial multiplexing operation described above is effective, it still suffers from some limitations. Firstly, the group convolutions in spatial multiplexing are more computationally expensive than depth-wise separable convolutions that they replace. Secondly, the decoupled nature of the group convolutions does not allow for flow of information across the groups. The channel multiplexing operation is designed to mitigate these drawbacks by reducing the computational burden of spatial multiplexing and further enhancing the flow of information across the feature map channels. This is achieved in two stages, selective processing and channel shuffling. A illustration of the whole operation is shown in <ref type="figure">Fig. 4</ref>. Overall, the channel multiplexing operation is similar in spirit to ShuffleNet <ref type="bibr" target="#b47">[49]</ref> and Shuf-fleNetV2 <ref type="bibr" target="#b25">[27]</ref> but with notable variations; (1) ShuffleNet uses shuffling to share channel information that are processed in different groups, while we use shuffling to blend the raw and processed channel information., (2) While ShuffleNetV2 always splits the input channels in half, we treat it as a hyperparameter that is searched for each layer, and (3) Shuffled channels are processed through an inverted residual bottleneck block in ShuffleNetV2 as opposed to spatial multiplexing in our case.</p><p>Selective Processing: We process only a part of the input channels by the spatial multiplexing block. Specifically, the C channels in the input feature maps are split into two groups with C 1 and C 2 channels, such that C = C 1 + C 2 . The first group of channels are propagated as is while the second group are processed through spatial multiplexing. This scheme immediately increases the compactness and ef-ficiency by a factor of C C2 2 , which can compensate for the computational burden of grouped as opposed to depth-wise separable convolutions.</p><p>Channel Shuffling: After the selective processing operation, we shuffle the channels of the output feature map in a fixed pattern. Alternative channels selected from the unprocessed and processed channels are interleaved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Tri-Objective Hyperparameter Search</head><p>Designing a CNN typically involves many hyperparameters that critically impact the performance of the models. In order to realize the full potential of MUXNet we seek to search for the optimal hyperparameters in each layer of the network. Since the primary design motive of MUXConv is to increase model expressivity while mitigating computational complexity, we propose a multi-objective hyperparameter search algorithm to simultaneously optimize for accuracy, compactness and efficiency. This can be stated as,</p><formula xml:id="formula_0">minimize F(x) = f 1 (x), · · · , f m (x) T , subject to x ∈ Ω,<label>(1)</label></formula><p>where in our context</p><formula xml:id="formula_1">Ω = Π n i=1 [a i , b i ] ⊆ R n is the hy- perparameter decision space, where a i , b i</formula><p>are the lower and upper bounds, x = (x 1 , . . . , x n ) T ∈ Ω is a candidate hyperparameter setting, F : Ω → R m constitutes m competing objectives, i.e. predictive error, model size, model inefficiency, etc., and R m is the objective space.</p><p>As the number of objectives increases, the number of solutions needed to approximate the entire Pareto surface grows exponentially <ref type="bibr" target="#b5">[6]</ref>, rendering a global search impractical in most cases. To overcome this challenge we propose a reference guided hyperparameter search. Instead of spanning the entire search space, we focus the hyperparameter search to a neighborhood around few desired user-defined preferences. An illustration of this concept is shown in <ref type="figure">Fig. 5a</ref>. For instance, in our context, this could correspond to different desired accuracy targets and hardware specifications. This idea enables us to decompose the tri-objective problem into multiple single objective subproblems. We adopt the penalty-based boundary intersection (PBI) method <ref type="bibr" target="#b46">[48]</ref> to scalarize multiple objectives into a single objective,</p><formula xml:id="formula_2">minimize g pbi (x|w, z * ) = d 1 + θd 2 subject to x ∈ Ω,<label>(2)</label></formula><p>where </p><formula xml:id="formula_3">d 2 = F(x) − z * + d 1 w ||w|| , d 1 = ||(F(x)−z * ) T w|| ||w|| , z * = (z * 1 , . . . , z * m ) T is the ideal objective vector with z * i &lt; min x∈Ω f i (x) i ∈ {1, . . . , m}. θ ≥ 0 is a</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region of Interest</head><p>(b) <ref type="figure">Figure 5</ref>: Tri-Objective Search: (a) We leverage user-defined preferences to decompose the tri-objective problem into multiple single-objective subproblems. By focusing on sub-regions as opposed to the entire Pareto surface, our approach is more efficient. (b) The reference direction is formed by joining the ideal point and user supplied reference targets. The PBI method is used to scalarize the objectives based on the projected distance d 2 to the reference target w, and the distance d 1 to the ideal point.</p><p>trade-off hyperparameter that is set to 5 and w is the reference direction obtained by connecting the ideal solution to the desired reference target. Conceptually, the PBI method constructs a composite measure of the convergence (d 1 ) of the solution to the given reference targets and diversity (d 2 ) of the solutions itself. See <ref type="figure">Fig.5b</ref> for an illustration. In our context, d 1 (distance between current projected solution and ideal solution) seeks to push the solution to the boundary of attainable objective space and d 2 measures how close the solution is to the user's preference. Finally, we adopt a multi-objective evolutionary algorithm based on decomposition (MOEA/D <ref type="bibr" target="#b46">[48]</ref>), to simultaneously solve the decomposed sub-problems while optimizing the scalarized objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the efficacy of MUXNets on three tasks; image classification, object detection, and transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Hyperparameter Search Details</head><p>Search Space: To compensate for the extra hyperparameters introduced by spatial and channel multiplexing, we constrain the commonly adopted layer-wise search space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b12">13</ref>] to a stage-wise search space, where layers within the same stage share the same hyperparameters. MUXNets consist of four stages, where each stage begins with a reduction block and is followed by a series of normal blocks. In each stage, we search for kernel size, expansion ratio, repetitions of normal blocks, leave-out ratio for channel multiplexing and the spatial multiplexing settings (see <ref type="figure" target="#fig_0">Fig. 10</ref> in Appendix A). To further reduce the search space, we always adopt squeeze-and-excitation <ref type="bibr" target="#b17">[19]</ref> and use swish <ref type="bibr" target="#b28">[30]</ref> nonlinearity for activation at each stage except the first stage, where a ReLU is used. Search: Following previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">40]</ref>, we conduct the search directly on ImageNet and estimate model accuracy on a subset consisting of 50K randomly sampled images from the training set. As a common practice, during search, the number of training epochs are reduced to 5. We select four reference points with preferences on model size ranging from 1.5M to 5M, MAdds ranging from 60M to 300M, and predictive accuracy fixed at 1. The compactness and efficiency objectives are normalized between [0, 1] before aggregation. Search is initialized with a global population size of 40 and evolved for 100 iterations, which takes about 11 days on sixteen 2080Ti GPUs. At the end of evolution, we pick the top 5 (based on PBI aggregated function values) models from each of the four subproblems, and retrain them thoroughly from scratch on ImageNet. The four resulting models are named as MUXNet-xs/s/m/l. Architectural details can be found in Appendix A <ref type="figure" target="#fig_0">(Fig. 11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ImageNet Classification</head><p>For training on ImageNet, we follow the procedure outlined in <ref type="bibr" target="#b38">[40]</ref>. Specifically, we adopt Inception preprocessing with image size 224×224 <ref type="bibr" target="#b36">[38]</ref>, batch size of 256, RMSProp optimizer with decay 0.9, momentum 0.9, and weight decay 1e-5. A Dropout layer of rate 0.2 is added before the last linear layer. Learning rate is linearly increased to 0.016 in the initial 5 epochs <ref type="bibr" target="#b9">[10]</ref>, it then decays every 3 epochs at a rate of 0.03. We further complement the training with exponential moving average with decay rate of 0.9998. <ref type="table" target="#tab_1">Table 1</ref> shows the performance of baselines and MUXNets on ImageNet 2012 benchmark <ref type="bibr" target="#b31">[33]</ref>. We compare them in terms of accuracy on validation set, model compactness (parameter size), model efficiency (MAdds) and inference latency on CPU and GPU. Overall, MUXNets consistently either match or outperform other models across different accuracy levels. In particular, MUXNet-m achieves 75.3% accuracy with 3.4M parameters and 218M MAdds, which is 1.4× more efficient and 1.6× more compact when compared to MnasNet-A1 <ref type="bibr" target="#b38">[40]</ref> and MobileNetV3 <ref type="bibr" target="#b12">[13]</ref>, respectively. <ref type="figure" target="#fig_0">Figures 1 and 6</ref> visualize the trade-off obtained by MUXNet and previous models. In terms of accuracy and compactness, MUXNet clearly dominates all previous models including MnasNet <ref type="bibr" target="#b38">[40]</ref>, FBNet <ref type="bibr" target="#b42">[44]</ref>, Mo-bileNetV3 <ref type="bibr" target="#b12">[13]</ref>, and MixNet <ref type="bibr" target="#b40">[42]</ref>. In terms of accuracy and efficiency, MUXNets are on par with current state-of-the-art models, i.e. MobileNetV3 and MixNet.</p><p>In terms of latency, the performance of MUXNet models is mixed since they, (i) use non-standard primitives that do not have readily available efficient low-level implementations, and (ii) are not explicitly optimized for latency. Compared to methods that use optimized convolutional primitives but do not directly optimize for latency (Efficient-Net/MixNet), MUXNet's latency is competitive despite using unoptimized spatial and channel multiplexing primitives. MUXNet's limitations due to unoptimized implementation can be offset, to an extent, by its inherent FLOPs  <ref type="figure">Figure 6</ref>: The trade-off between model complexity and top-1 accuracy on ImageNet. This allows us to compare models designed for different computation requirements in number of parameters or number of multi-adds. All our models use input resolution of 224 × 224. We use dash line to denote models from channel width multipliers or with different input resolutions. and parameter efficiency. MUXNet is not as competitive as methods that directly use CPU or GPU latency on Pixel phones as a search objective (MobileNetV3, MnasNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Object Detection</head><p>We evaluate and compare the generalization ability of MUXNet and other peer models on the PASCAL VOC detection benchmark <ref type="bibr" target="#b7">[8]</ref>. Our experiments use both the Single Shot Detector (SSD) <ref type="bibr" target="#b23">[25]</ref> and the Single Shot Detector Lite (SSDLite) <ref type="bibr" target="#b32">[34]</ref> as the detection frameworks, with MUXNet as the feature extraction backbone. We follow the procedure in <ref type="bibr" target="#b32">[34]</ref> to setup the additional prediction layers, i.e. location of detection heads in the backbone, size of corresponding   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Transfer Learning</head><p>To further explore the efficacy of MUXNet we evaluate it under the transfer learning setup in <ref type="bibr" target="#b18">[20]</ref> on three different datasets; CIFAR-10, CIFAR-100 and ChestX-Ray14 <ref type="bibr" target="#b41">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">CIFAR-10 and CIFAR-100</head><p>Both CIFAR-10 and -100 datasets have 50,000 and 10,000 images for training and testing, respectively. CIFAR-100 extends CIFAR-10 by adding 90 more classes resulting in 10× fewer training examples per class. For training on both datasets, the models are initialized with weights pre-trained on ImageNet. The model is then fine-tuned using SGD with momemtum 0.9, weight decay 4e-5 and gradients clipped to a magnitude of 5. Learning rate is set to 0.01 with cosine annealing to 0.0 in 150 epochs. For data augmentation, images are up-sampled via bicubic interpolation to 224×224 and horizontally fliped at random. <ref type="table" target="#tab_4">Table 3</ref> and <ref type="figure" target="#fig_3">Figure 7</ref> reports the accuracy, compactness and efficiency of MUXNet and other baselines. Overall, MUXNet significantly outperforms previous methods on both CIFAR-10 and -100 datasets, indicating that our models also transfer well to other similar tasks. In particular, MUXNet-m achieves 1% higher accuracy than NASNet-A mobile with 3× fewer parameters while being 2× more efficient in MAdds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">ChestX-Ray14</head><p>The ChestX-Ray14 benchmark was recently introduced in <ref type="bibr" target="#b41">[43]</ref>. The dataset consists of 112,120 high resolution frontal-view chest X-ray images from 30,805 patients. Each image is labeled with one or multiple common thorax diseases, or "Normal", otherwise. Due to the multi-label nature of the dataset, we use a multitask learning setup where each disease is treated as an individual binary classification  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Study</head><p>Spatial Multiplexing: We incorporate the spatial multiplexing operation within the 3×3 depth-wise separable convolution layers of MobileNetV2. As we do in our main experiments, we do not apply spatial multiplexing to the reduction blocks. We manually fix the multiplexing hyperparameters to C 1 = C 3 = C 4 , C 2 = C 2 i.e., 1/4 channels are processed by subpixeling, 1/4 of the channels are processed by superpixeling, and the remaining channels are processed without modification. <ref type="figure" target="#fig_5">Figure 8a</ref> shows the effect of spatial  multiplexing on MobileNetV2 <ref type="bibr" target="#b32">[34]</ref> at different width multipliers. Spatial multiplexing consistently improves accuracy over the original depth-wise separable convolution at fixed spatial resolution. In particular, spatial multiplexing boosts accuracy by 5.8% in low MAdds regime. The results suggest that per MAdd, spatial multiplexing (groups+full conv) has better information flow than dep-sep+1 × 1 conv. This is more apparent in small models which have less channels, so 1 × 1 conv cannot effectively mix channel information.</p><p>Channel Multiplexing: To make models more efficient, methods such as scaling down the number of channels by a factor (named width multiplier), or scaling down the input resolution have been proposed. Here we investigate the impact of channel multiplexing as an alternative to reduce model complexity. To be consistent with the main experiments we only apply channel multiplexing to the normal blocks. In MobileNetV2 <ref type="bibr" target="#b32">[34]</ref> we gradually increase the number of input channels that are left unprocessed in each normal block. We use l to denote the leave-out ratio, where a high value corresponds to less channels being processed and hence more efficiency. The resulting trade-off with accuracy is shown in <ref type="figure" target="#fig_5">Figure 8b</ref>. Evidently, reducing the resolutions of input images provides a better trade-off between accuracy and MAdds than reducing the channels. However, reducing the input resolution provides no benefit to model size. On the other hand, channel multiplexing offers competitive trade-off in both cases; MAdds and model size. In particular, leaving out 25% of the input channels at every normal block appears to affect the predictive accuracy minimally, while simultaneously saving 13% in parameters and 20% in multiply-adds.</p><p>Search Efficiency: To thoroughly and efficiently evaluate the effectiveness of the PBI decomposition technique and the search efficiency of our proposed NAS algorithm, we adopt the NASBench101 <ref type="bibr" target="#b45">[47]</ref> benchmark. It contains more than 400K unique models pre-trained on CIFAR-10, whose Pareto-optimal solutions and predictive performance are readily available without expensive training. In this case, we aim to minimize the number of parameters, the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attainable models</head><p>Reference Point-1 Reference Point-2 Reference Point-3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours Regularized Evolution Ours</head><p>Regularized Evolution <ref type="figure">Figure 9</ref>: Performance comparison between our approach and regularized evolution (RE) <ref type="bibr" target="#b29">[31]</ref> on NASBench101 <ref type="bibr" target="#b45">[47]</ref>. Both methods are subject to the same search budget of 1,000 maximum models sampled. We distribute the search budget across three executions of RE for each one of the three reference points. Our approach simultaneously targets all three reference points in one run using all available budget.</p><p>training time and maximize the accuracy. We also adopt the regularized evolution <ref type="bibr" target="#b29">[31]</ref> approach as a baseline for comparison. <ref type="figure">Figure 9</ref> shows the search effectiveness for three reference points under a fixed computational budget. The PBI scalarization is effective in directing the search towards pre-defined target regions as the obtained solutions from both methods are centered around the three provided target points. In addition, we observe that by collectively solving the sub-problems, we achieve better results under the same search budget as opposed to solving the sub-problem one at a time, as in case of regularized evolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper introduced MUXConv, an efficient alternative to a standard convolutional layer that is designed to progressively multiplex channel and spatial information in the network. Furthermore, we coupled it with an efficient multi-objective evolutionary algorithm based hyperparameter search to trade-off predictive accuracy, model compactness and computational efficiency. Experimental results on image classification, object detection and transfer learning suggest that MUXNets are able to match the predictive accuracy and efficiency of current state-of-the-art models while be more compact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>In this Appendix we include (1) MUXNet hyperparameter search space in Section A, (2) computational complexity of MUXNet and comparison to a combination of 1 × 1 + 3 × 3 in Section B, (3) effectiveness of MUXNet as a backbone semantic segmentation in Section C.1, and (4) evaluation of generalization and robustness properties of MUXNet in Section C.2. Finally <ref type="figure" target="#fig_0">Fig. 16</ref> shows some qualitative object detection results on PASCAL VOC 2007, and <ref type="figure" target="#fig_0">Fig. 17</ref> shows gradCam results on the ChestX-Ray14 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Search Space</head><formula xml:id="formula_4">K E N K E G L K E N K E G L S 0 1 − 1 0 1 2 0 + 0 1 − 1 0 1 2 0 3 ⋯</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reduction Normal Reduction Normal</head><p>Stage 1 Stage 2 <ref type="figure" target="#fig_0">Figure 10</ref>: Search Space Encoding: Each stage is encoded as an integer string. Genetic operations are performed on such encoding. See <ref type="table" target="#tab_1">Table 1</ref> for full details on the options.</p><p>To encode the hyperparameter settings for a model, we first divide the model architectures into four stages, based on the spatial resolution of each layer's output feature map. In each stage spatial resolution does not change. The first layer in each stage reduces the feature map size by half. For each stage, we search for kernel size (K) and expansion ratio (E). In addition, from second layer in each stage, we search for # of repetitions (N), # of input channels to compute convolution (G), leave-out ratio in channel multiplexing (L) and the spatial multiplexing setting (S) (see <ref type="figure" target="#fig_0">Fig. 10</ref>). <ref type="table">Table 5</ref> summarizes the hyperparameters and available options for each stage. The obtained hyperparameters for our MUXNets are visualized in <ref type="figure" target="#fig_0">Figure 11</ref>. The total volume of the search space is approximately 14 12 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Computational Complexity</head><p>In this section, we analytically compare the computational complexity of our MUXConv block <ref type="figure" target="#fig_0">(Figure 12b</ref>) with the widely-used MobileNet block <ref type="bibr" target="#b32">[34]</ref>. For simplicity, we ignore the computation induced by the normalization and activation layers and we assume that for both blocks the number of input and output channels is the same i.e., C channels.  <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>  <ref type="table">Table 5</ref>: Hyperparameter search space summary. The searched hyperparameters depend on both the block type-i.e. normal or reduction block, and the stages. In case of spatial mutiplexing, option "-1" means subpixel multiplexing, "1" means superpixel multiplexing, and "0" means no spatial multiplexing. For instance, "[-1, 0, 1]" means applying subpixel to 1/3 of the input channels, superpixel to another 1/3 of the input channels, and the remaining 1/3 are processed at the original resolution. And we only apply spatial mutiplexing in stages two and three. For the kernel size options in case of reduction blocks, we allow multiple parallel kernels to down-sample the resolution, for example, " <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>" means three parallel convolutions with kernel size of 3, 5, and 7.</p><p>The Mobilenet block consist of a 1 × 1 convolution to expand the input channels, followed by a 3 × 3 depth-wise separable convolution and another 1×1 convolution to compress the channels (see <ref type="figure" target="#fig_0">Figure 12a</ref>). We use E to denote expansion rate. Then the total number of parameters and floating point operations are:</p><formula xml:id="formula_5">Params = C · EC 1 × 1 conv + EC · 3 · 3 3 × 3 d.w. conv + EC · C 1 × 1 conv FLOPs = H · W · C · EC· 1 × 1 conv + EC · 3 · 3 3 × 3 d.w. conv + EC · C 1 × 1 conv</formula><p>On the other hand, our MUXConv block first select a subset of the input channels to be processed, and the remaining portion is directly propagated to the output. We use L to denote the ratio of the leave-out un-processed channels. Then we use a 1 × 1 convolution to expand, followed by a group-wise convolution <ref type="bibr" target="#b43">[45]</ref> and another 1 × 1 convolution to compress (see <ref type="figure" target="#fig_0">Figure 12b</ref>). And we use G to denote the group factor, which indicates the # of input channels used for computing each output channel. For instance, setting G equal to 1 is equivalent as using a depth-wise separable convolution. The resulting number of parameters and the floating point operations associated with our MUXConv block is: <ref type="figure" target="#fig_0">Figure 13</ref> provides an visual comparison showing the ratio of the number of parameters between our MUXConv block and Mobilenet block as the group factor (G) and leave-out ratio (L) vary. The choice of G and L hyperparameters we consider in our search space (see <ref type="table">Table 5</ref>) corresponds to computational complexity that is less than the Mobilenet block (ratio ≤ 1, i.e. red color in <ref type="figure" target="#fig_0">Fig.13</ref>).    <ref type="table" target="#tab_1">Table 1</ref> (main paper). All architectures share the same hyperparameter settings (except # of output channels) for the blocks colored in yellow and they are fixed manually. The Dash lines indicate down-sampling points and we divide the architectures into four main stages. We use E, K, G, and L to denote expansion rate, kernel size, number of channels per group and leave-out ratio, respectively. Blocks colored in green use the inverted bottleneck structure proposed in <ref type="bibr" target="#b32">[34]</ref>. Blocks colored in pink use both spatial and channel multiplexing and blocks colored in blue only use channel multiplexing.  </p><formula xml:id="formula_6">C = (1 − L) · C Params =Ĉ · EĈ 1 × 1 conv + G · EĈ · 3 · 3 3 × 3 group conv + EĈ ·Ĉ 1 × 1 conv FLOPs = H · W · Ĉ · EĈ· 1 × 1 conv + G · EĈ · 3 · 3 3 × 3 group conv + EĈ ·Ĉ 1 × 1 conv</formula><formula xml:id="formula_7">−−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − ↓ STEM ↓ DSConv_K3 ↓ IB_E4_K3.5 ↓ E6_K3_G1_L0.5 ↓ IB_E4_K3.5.7 ↓ E6_K5_G1_L0.5 ↓ IB_E4_K3.</formula><formula xml:id="formula_8">(a) MUXNet-xs −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − ↓ STEM ↓ DSConv_K3 ↓ IB_E4_K3 ↓ IB_E4_K3 ↓ E4_K3_G2_L0.5 ↓ IB_E4_K3 ↓ E4_K5_G2_L0.5 ↓ IB_E4_K3 ↓ E4_K5_G2_L0.5 ↓ IB_E6_K3 ↓ E6_K5_G2_L0.5 ↓ IB_E6_K3 ↓ 3×224×224 16×112×112 16×112×112 24×56×56 32×28×28 32×28×28 64×14×14 64×14×14 96×14×14 96×14×14 112×7×7 112×7×7 160×7×7 ×2 ×2 ×2 ×2 (b) MUXNet-s ↓ STEM ↓ DSConv_K3 ↓ IB_E4_K3 ↓ E4_K3_G2_L0.5 ↓ IB_E4_K3.5.7 ↓ E6_K3_G2_L0.5 ↓ IB_E4_K3.5.7.9 ↓ E6_K5_G2_L0.5 ↓ IB_E6_K5 ↓ E6_K5_G2_L0.5 ↓ IB_E4_K3.5.7.9.11 ↓ E6_K5_G2_L0.5 ↓ IB_E6_K3 ↓ 3×224×224 24×112×112 24×112×112 24×56×56 24×56×56 40×28×28 40×28×28 80×14×14 80×14×14 112×14×14 112×14×14 160×7×7 160×7×7 200×7×7 ×2 ×2 ×2 ×2 ×3 −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − (c) MUXNet-m −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − −−−−−−−−−−−−−−−−−− − ↓ STEM ↓ DSConv_K3 ↓ IB_E6_K3 ↓ E6_K3_G2_L0.5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Semantic Segmentation</head><p>We further evaluate the effectiveness of our models as backbones for the task of mobile semantic segmentation. We compare MUXNet-m with both MobileNetV2 <ref type="bibr" target="#b32">[34]</ref> and ResNet18 <ref type="bibr" target="#b10">[11]</ref> on ADE20K <ref type="bibr" target="#b50">[52]</ref> benchmark. Additionally, we also compare two different segmentation heads. The first one, referred as C1, only uses one convolution module. And the other one, Pyramid Pooling Module (PPM), was proposed in <ref type="bibr" target="#b48">[50]</ref>. All models are trained under the same setup: we use SGD optimizer with initial learning rate 0.02, momentum 0.9, weight decay 1e-4 for 20 epochs. <ref type="table">Table 6</ref> reports the mean IoU (mIoU) and pixel accuracy on the ADE20K validation set. MUXNet-m performs comparably with MobileNetV2 when paired with PPM, while being 1.5× more efficient in MAdds. We also provide qualitative visualization of semantic segmentation examples in <ref type="figure" target="#fig_0">Figure 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Generalization and Robustness</head><p>To further evaluate the generalization performance of our proposed models, we compare on a recently proposed benchmark dataset, ImageNetV2 <ref type="bibr" target="#b30">[32]</ref>, complementary to the original Im-ageNet 2012. We use the MatchedFrequency version of the ImageNet-V2. <ref type="figure" target="#fig_0">Figure 15a</ref> reports the top-5 accuracy compari-  <ref type="figure" target="#fig_0">Figure 13</ref>: Ratio of #Params between our MUXConv block and Mobilenet block <ref type="bibr" target="#b32">[34]</ref>. The search space that we consider for these two hyperparameters is highlighted by a black box.  <ref type="table">Table 6</ref>: ADE20K [52] Semantic Segmentation Results. Since networks in each section use the same segmentation head, we report the #MAdds and #Params on the backbone models only. mIoU is the mean IoU and Acc is the pixel accuracy. C1 use one convolution module as segmentation head and PPM use the Pyramid Pooling Module from <ref type="bibr" target="#b48">[50]</ref>.</p><p>son between our MUXNets and a wide-range of previous models. Even though there is a significant accuracy drop of 8% to 10% on ImageNet-V2 across models, the relative rank-order of accuracy on the original ImageNet validation set translates well to the new ImageNet-V2. And our MUXNet performs competitively on ImageNet-V2 as compared to other mobile models, such as Shuf-fleNetV2 <ref type="bibr" target="#b25">[27]</ref>, MobileNetV2 <ref type="bibr" target="#b32">[34]</ref> and MnasNet-A1 <ref type="bibr" target="#b38">[40]</ref>. The vulnerability to small changes in query images has always been a concern for designing better models. Hendrycks and Dietterich <ref type="bibr" target="#b11">[12]</ref> recently introduced a new dataset, ImageNet-C, by applying commonly observable corruptions (e.g., noise, weather, compression, etc.) to the clean images from the original ImageNet dataset. The new dataset contains images perturbed by 19 different types of corruption at five different levels of severity. And we leverage this dataset to evaluate the robustness of our proposed models. <ref type="figure" target="#fig_0">Figure 15b</ref> compares Top-5 accuracy between our MUXNet-m and four other representative models, designed both manual and automatically. MUXNet-m performs favourably on ImageNet-C, achieving better accuracy on 18 out of 19 corruption types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>MUXNet-m +PPM  (b) ImageNet-C <ref type="bibr" target="#b11">[12]</ref> Figure 15: (a) Generalization performance on ImageNet-V2 (MatchedFrequency) <ref type="bibr" target="#b30">[32]</ref>. Numbers in the boxes indicate the drop in accuracy. (b) Robustness performance on ImageNet-C <ref type="bibr" target="#b11">[12]</ref>, which consist of ImageNet validation images corrupted by 19 commonly observable corruptions. Following the original paper that proposed ImageNet-C, we normalized the top-5 accuracy by AlexNet's Top-5 accuracy. DARTS is from the author's public Github repository.</p><p>All other compared models are from Pytorch repository https://pytorch.org/docs/stable/torchvision/models.html.</p><p>[15] G Huang, D Che, T Li, F Wu, L van der Maaten, and K Weinberger. Multi-scale dense networks for resource effi-cient image classification. In International Conference on Learning Representations (ICLR), 2018. 2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Atelectasis</head><p>Cardiomegaly Effusion Infiltrate Pneumonia Pneumothorax <ref type="figure" target="#fig_0">Figure 17</ref>: Examples of class activation map <ref type="bibr" target="#b49">[51]</ref> of MUXNet-m on ChestX-Ray14 <ref type="bibr" target="#b41">[43]</ref>, highlighting the class-specific discriminative regions. The ground truth bounding boxes are plotted over the heatmaps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Accuracy vs. Compactness vs. Efficiency: Existing networks outperform each other in at most two criteria. MUXNet models are, however, dominant in all three objectives under mobile settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Relative contribution of different layers in CNN designs in terms of parameters (top) and MAdds (bottom). Initial models largely relied on standard convolutional layers. More recent networks, on the other hand, largely rely on 1×1 convolutions and linear layers. In contrast, MUXNets reverse this trend to an extent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Overview of spatial multiplexing operation. (b) Subpixel operation multiplexes spatial information into channels. (c) Superpixel operation multiplexes channels into spatial information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Transfer Learning on CIFAR: Trade-off between Top-1 accuracy and #Params / #MAdds. learning rate of 0.01 with cosine annealing to 0.0 in 200 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Multiplexed Convolution Ablation Study: (a) Results correspond to width multiplier of 0.1, 0.25, 0.5, 0.75, and 1.0. (b) w, r and l are width multiplier, input resolution and leave-out ratio, respectively. When l = 0.25, 75% of the input information is processed at each normal block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>The architectures of MUXNet-xs/s/m/l in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>The visualization of the Mobilenet block (a) and our MUX-Conv block (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Examples from ADE20K validation set showing the ground truth (2nd row) and the scene parsing result (3rd row) from MUXNet-m. Color encoding of semantic categories is available from here. S h u ff le N e t V 2 R e s N e t 1 8 M U X N e t s ( o u r s ) G o o g L e N e t M o b il e N e t V 2 D A R T S M n a s N e t A 1 N A S N e t A m o b il e M U X N e t m ( o u r s ) M U X N e t l ( o u r s ) D e n s e N e t 1 tr a s t d e fo c u s _ b lu r e la s ti c _ tr a n s fo rm fo g fr o s t g a u s s ia n _ b lu r g a u s s ia n _ n o is e g la s s _ b lu r im p u ls e _ n o is e jp e g _ c o m p re s s io n m o ti o n _ b lu r p ix e la te s a tu ra te s h o t_ n o is e s n o w s p a tt e r s p e c k le _ n o is e z o o m _ b lu r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Examples visualizing the detection performance of MUXNet-m on PASCAL VOC 2007 [8].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>ImageNet Classification<ref type="bibr" target="#b31">[33]</ref>: MUXNet comparison with manual and automated design of efficient convolutional neural networks. Models are grouped into sections for better visualization. Our results are underlined and the best result in each section is in bold. CPU latency (batchsize=1) is measured on Intel i7-8700K and GPU latency (batchsize=64) is measured on 1080Ti. ‡ indicates the objective (in addition to predictive performance) that the method explicitly optimizes through NAS.</figDesc><table><row><cell cols="2">Model</cell><cell>Type</cell><cell cols="2">#MAdds</cell><cell>Ratio</cell><cell></cell><cell>#Params</cell><cell cols="2">Ratio</cell><cell cols="6">CPU(ms) GPU(ms)</cell><cell cols="2">Top-1 (%)</cell><cell></cell><cell cols="3">Top-5 (%)</cell></row><row><cell cols="2">MUXNet-xs (ours)</cell><cell>auto</cell><cell cols="2">66M  ‡</cell><cell>1.0x</cell><cell></cell><cell>1.8M  ‡</cell><cell cols="2">1.0x</cell><cell></cell><cell>6.8</cell><cell></cell><cell></cell><cell></cell><cell>18</cell><cell>66.7</cell><cell></cell><cell></cell><cell cols="2">86.8</cell></row><row><cell cols="2">MobileNetV2_0.5 [34]</cell><cell>manual</cell><cell cols="2">97M</cell><cell>1.5x</cell><cell></cell><cell>2.0M</cell><cell cols="2">1.1x</cell><cell></cell><cell>6.2</cell><cell></cell><cell></cell><cell></cell><cell>17</cell><cell>65.4</cell><cell></cell><cell></cell><cell cols="2">86.4</cell></row><row><cell cols="2">MobileNetV3 small [13]</cell><cell>combined</cell><cell cols="2">66M</cell><cell>1.0x</cell><cell></cell><cell>2.9M</cell><cell cols="2">1.6x</cell><cell></cell><cell>6.2  ‡</cell><cell></cell><cell></cell><cell></cell><cell>14</cell><cell>67.4</cell><cell></cell><cell></cell><cell>-</cell><cell></cell></row><row><cell cols="2">MUXNet-s (ours)</cell><cell>auto</cell><cell cols="2">117M  ‡</cell><cell>1.0x</cell><cell></cell><cell>2.4M  ‡</cell><cell cols="2">1.0x</cell><cell></cell><cell>9.5</cell><cell></cell><cell></cell><cell></cell><cell>25</cell><cell>71.6</cell><cell></cell><cell></cell><cell cols="2">90.3</cell></row><row><cell cols="2">MobileNetV1 [14]</cell><cell>manual</cell><cell cols="2">575M</cell><cell>4.9x</cell><cell></cell><cell>4.2M</cell><cell cols="2">1.8x</cell><cell></cell><cell>7.3</cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell>70.6</cell><cell></cell><cell></cell><cell cols="2">89.5</cell></row><row><cell cols="2">ShuffleNetV2 [27]</cell><cell>manual</cell><cell cols="2">146M</cell><cell>1.3x</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>6.8</cell><cell></cell><cell></cell><cell></cell><cell>11  ‡</cell><cell>69.4</cell><cell></cell><cell></cell><cell>-</cell><cell></cell></row><row><cell cols="2">ChamNet-C [5]</cell><cell>auto</cell><cell cols="2">212M</cell><cell>1.8x</cell><cell></cell><cell>3.4M</cell><cell cols="2">1.4x</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>71.6</cell><cell></cell><cell></cell><cell>-</cell><cell></cell></row><row><cell cols="2">MUXNet-m (ours)</cell><cell>auto</cell><cell cols="2">218M  ‡</cell><cell>1.0x</cell><cell></cell><cell>3.4M  ‡</cell><cell cols="2">1.0x</cell><cell></cell><cell cols="2">14.7</cell><cell></cell><cell></cell><cell>42</cell><cell>75.3</cell><cell></cell><cell></cell><cell cols="2">92.5</cell></row><row><cell cols="2">MobileNetV2 [34]</cell><cell>manual</cell><cell cols="2">300M</cell><cell>1.4x</cell><cell></cell><cell>3.4M</cell><cell cols="2">1.0x</cell><cell></cell><cell>8.3  ‡</cell><cell></cell><cell></cell><cell></cell><cell>23</cell><cell>72.0</cell><cell></cell><cell></cell><cell cols="2">91.0</cell></row><row><cell cols="2">ShuffleNetV2 2× [27]</cell><cell>manual</cell><cell cols="2">591M</cell><cell>2.7x</cell><cell></cell><cell>7.4M</cell><cell cols="2">2.2x</cell><cell></cell><cell cols="2">11.0</cell><cell></cell><cell></cell><cell>22  ‡</cell><cell>74.9</cell><cell></cell><cell></cell><cell>-</cell><cell></cell></row><row><cell cols="2">MnasNet-A1 [40]</cell><cell>auto</cell><cell cols="2">312M</cell><cell>1.4x</cell><cell></cell><cell>3.9M</cell><cell cols="2">1.1x</cell><cell></cell><cell>9.3  ‡</cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell>75.2</cell><cell></cell><cell></cell><cell cols="2">92.5</cell></row><row><cell cols="2">MobileNetV3 large [13]</cell><cell>combined</cell><cell cols="2">219M</cell><cell>1.0x</cell><cell></cell><cell>5.4M</cell><cell cols="2">1.6x</cell><cell></cell><cell cols="2">10.0  ‡</cell><cell></cell><cell></cell><cell>33</cell><cell>75.2</cell><cell></cell><cell></cell><cell>-</cell><cell></cell></row><row><cell cols="2">MUXNet-l (ours)</cell><cell>auto</cell><cell cols="2">318M  ‡</cell><cell>1.0x</cell><cell></cell><cell>4.0M  ‡</cell><cell cols="2">1.0x</cell><cell></cell><cell cols="2">19.2</cell><cell></cell><cell></cell><cell>74</cell><cell>76.6</cell><cell></cell><cell></cell><cell cols="2">93.2</cell></row><row><cell cols="2">MnasNet-A2 [40]</cell><cell>auto</cell><cell cols="2">340M</cell><cell>1.1x</cell><cell></cell><cell>4.8M</cell><cell cols="2">1.2x</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>75.6</cell><cell></cell><cell></cell><cell cols="2">92.7</cell></row><row><cell cols="2">FBNet-C [44]</cell><cell>auto</cell><cell cols="2">375M</cell><cell>1.2x</cell><cell></cell><cell>5.5M</cell><cell cols="2">1.4x</cell><cell></cell><cell>9.1  ‡</cell><cell></cell><cell></cell><cell></cell><cell>31</cell><cell>74.9</cell><cell></cell><cell></cell><cell>-</cell><cell></cell></row><row><cell cols="2">EfficientNet-B0 [41]</cell><cell>auto</cell><cell cols="2">390M  ‡</cell><cell>1.2x</cell><cell></cell><cell>5.3M</cell><cell cols="2">1.3x</cell><cell></cell><cell cols="2">14.4</cell><cell></cell><cell></cell><cell>46</cell><cell>76.3</cell><cell></cell><cell></cell><cell cols="2">93.2</cell></row><row><cell cols="2">MixNet-M [42]</cell><cell>auto</cell><cell cols="2">360M  ‡</cell><cell>1.1x</cell><cell></cell><cell>5.0M</cell><cell cols="2">1.2x</cell><cell></cell><cell cols="2">24.3</cell><cell></cell><cell></cell><cell>79</cell><cell>77.0</cell><cell></cell><cell></cell><cell cols="2">93.3</cell></row><row><cell></cell><cell>MUXNet</cell><cell cols="2">MobileNetV2</cell><cell></cell><cell cols="3">MobileNetV3large</cell><cell cols="4">MobileNetV3small</cell><cell></cell><cell></cell><cell cols="2">MnasNet</cell><cell cols="3">MixNet</cell><cell></cell><cell></cell></row><row><cell></cell><cell>FBNet</cell><cell>ChamNet</cell><cell></cell><cell></cell><cell cols="3">ProxylessNASGPU</cell><cell cols="2">NASNetA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AmoebaNetA</cell><cell cols="3">DARTS</cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top1accuracy(%)</cell><cell>64 66 68 70 72 74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64 66 68 70 72 74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>100</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell cols="3">NumberofParameters(Millions)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">NumberofMAdds(Millions)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>PASCAL</figDesc><table><row><cell></cell><cell cols="2">VOC2007 [8] Detection</cell><cell></cell></row><row><cell>Network</cell><cell cols="3">#MAdds #Params mAP (%)</cell></row><row><cell>VGG16 + SSD [25]</cell><cell>35B</cell><cell>26.3M</cell><cell>74.3</cell></row><row><cell>MobileNet + SSD [18]</cell><cell>1.6B</cell><cell>9.5M</cell><cell>67.6</cell></row><row><cell>MobileNetV2 + SSDLite [34]</cell><cell>0.7B</cell><cell>3.4M</cell><cell>67.4</cell></row><row><cell>MobileNetV2 + SSD [34]</cell><cell>1.4B</cell><cell>8.9M</cell><cell>73.2</cell></row><row><cell>MUXNet-m + SSDLite (ours)</cell><cell>0.5B</cell><cell>3.2M</cell><cell>68.6</cell></row><row><cell>MUXNet-l + SSD (ours)</cell><cell>1.4B</cell><cell>9.9M</cell><cell>73.8</cell></row></table><note>boxes, etc. The combined trainval sets of PASCAL VOC 2007 and 2012 are used for training. Other details include, SGD optimizer with momentum 0.9 and weight decay 5e- 4, batch size of 32, input image resized to 300×300 and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>reports the mean Average Precision (mAP)</cell></row><row><cell>on the PASCAL VOC 2007 test set. When paired with the</cell></row><row><cell>same detector framework SSDLite, our MUXNet-m model</cell></row><row><cell>achieves 1.2% higher mAP than MobileNetV2 [34] while</cell></row><row><cell>being 6% more compact and 1.4× more efficient.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">Transfer Learning: Top-1 accuracy on CIFAR-10 (C-10) and</cell></row><row><cell cols="5">CIFAR-100 (C-100). ResNet, DenseNet, MobileNetV2, and NASNet-A</cell></row><row><cell>results are from [20].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">#MAdds #Params C-10 (%) C-100 (%)</cell></row><row><cell>ResNet-50 [11]</cell><cell>4.1B</cell><cell>23.5M</cell><cell>96.77</cell><cell>84.50</cell></row><row><cell>DenseNet-169 [17]</cell><cell>3.4B</cell><cell>12.5M</cell><cell>97.40</cell><cell>85.00</cell></row><row><cell>MobileNetV2 [34]</cell><cell>0.3B</cell><cell>2.2M</cell><cell>95.74</cell><cell>80.80</cell></row><row><cell>NASNet-A mobile [53]</cell><cell>0.6B</cell><cell>4.2M</cell><cell>96.83</cell><cell>83.90</cell></row><row><cell>EfficientNet-B0 [41]</cell><cell>0.4B</cell><cell>4.0M</cell><cell>98.10</cell><cell>88.10</cell></row><row><cell>MixNet-M [42]</cell><cell>0.4B</cell><cell>3.5M</cell><cell>97.92</cell><cell>-</cell></row><row><cell>MUXNet-m (ours)</cell><cell>0.2B</cell><cell>2.1M</cell><cell>98.00</cell><cell>86.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Transfer Learning on ChestX-Ray14<ref type="bibr" target="#b41">[43]</ref> </figDesc><table><row><cell>Method</cell><cell cols="3">#MAdds #Params Test AUROC (%)</cell></row><row><cell>Wang et al. (2017) [43]</cell><cell>-</cell><cell>-</cell><cell>73.8</cell></row><row><cell>Yao et al. (2017) [46]</cell><cell>-</cell><cell>-</cell><cell>79.8</cell></row><row><cell>CheXNet (2017) [29]</cell><cell>2.8B</cell><cell>7.0M</cell><cell>84.4</cell></row><row><cell>MUXNet-m (ours)</cell><cell>0.2B</cell><cell>2.1M</cell><cell>84.1</cell></row><row><cell cols="4">problem. We define a 14-dimensional label vector of binary</cell></row><row><cell cols="4">values indicating the presence of one or more diseases, and</cell></row><row><cell cols="4">optimize a regression loss as opposed to cross-entropy in</cell></row><row><cell cols="4">single-label cases. The training procedure is similar to the</cell></row><row><cell cols="4">CIFAR experiments for transfering pre-trained models. Ta-</cell></row><row><cell cols="4">ble 4 compares the performance of MUXNet-m with previ-</cell></row><row><cell cols="4">ous approaches, including CheXNet [29] which represents</cell></row><row><cell cols="4">the state-of-the-art on this dataset. Evidently, MUXNet-m's</cell></row><row><cell cols="4">performance in terms of area under the receiver operating</cell></row><row><cell cols="4">characteristic (AUROC) curve on the test set is comparable</cell></row><row><cell cols="4">(84.1% vs 84.4%) to CheXNet while being 3× more com-</cell></row><row><cell cols="2">pact and 14× more efficient.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We gratefully acknowledge Dr. Erik Goodman and Dr. Wolfgang Banzhaf for partially supporting the computational requirements of this work. Vishnu Naresh Boddeti was partially supported by the Ford-MSU Alliance.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rogerio S Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Big-little net: An efficient multi-scale feature representation for visual and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mallinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards efficient network design through platform-aware model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multi-objective optimization using evolutionary algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Chichester: Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient multi-objective neural architecture search via lamarckian evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Condensenet: An efficient densenet using learned group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nsga-net: Neural architecture search using multi-objective genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic and Evolutionary Computation Conference (GECCO)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tiled convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hershel</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Bagul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Shpanskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<title level="m">Radiologistlevel pneumonia detection on chest x-rays with deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Ph. D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mixed depthwise convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning to diagnose from scratch by exploiting dependencies among labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Poblenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Dagunts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lyman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10501</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Moea/d: A multiobjective evolutionary algorithm based on decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingfu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
