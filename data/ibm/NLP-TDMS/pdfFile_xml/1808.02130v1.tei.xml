<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CPlaNet: Enhancing Image Geolocalization by Combinatorial Partitioning of Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of CSE</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
							<email>jacksim@google.combhhan@snu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Dept. of ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CPlaNet: Enhancing Image Geolocalization by Combinatorial Partitioning of Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image geolocalization</term>
					<term>combinatorial partitioning</term>
					<term>fine-grained classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image geolocalization is the task of identifying the location depicted in a photo based only on its visual information. This task is inherently challenging since many photos have only few, possibly ambiguous cues to their geolocation. Recent work has cast this task as a classification problem by partitioning the earth into a set of discrete cells that correspond to geographic regions. The granularity of this partitioning presents a critical trade-off; using fewer but larger cells results in lower location accuracy while using more but smaller cells reduces the number of training examples per class and increases model size, making the model prone to overfitting. To tackle this issue, we propose a simple but effective algorithm, combinatorial partitioning, which generates a large number of fine-grained output classes by intersecting multiple coarse-grained partitionings of the earth. Each classifier votes for the fine-grained classes that overlap with their respective coarse-grained ones. This technique allows us to predict locations at a fine scale while maintaining sufficient training examples per class. Our algorithm achieves the state-of-the-art performance in location recognition on multiple benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image geolocalization is the task of predicting the geographic location of an image based only on its pixels without any meta-information. As the geolocation is an important attribute of an image by itself, it also plays as a proxy to other location attributes such as elevation, weather, and distance to a particular point of interest. However, geolocalizing images is a challenging task since input images often contain limited visual information representative of their locations. To handle this issue effectively, the model is required to capture and maintain visual cues of the globe comprehensively.</p><p>There exist two main streams to address this task: retrieval and classification based approaches. The former searches for nearest neighbors in a database of geotagged images by matching their feature representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Visual appearance of an image at a certain geolocation is estimated using the representations arXiv:1808.02130v1 [cs.CV] 6 Aug 2018 <ref type="bibr" target="#b0">( 1,</ref><ref type="bibr" target="#b0">1)</ref> ( <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2)</ref> ( Predicted scores over geoclass set Predicted scores over geoclass set <ref type="figure">Fig. 1</ref>: Visualization of combinatorial partitioning. Two coarse-grained class sets, P = {p 1 , p 2 , . . . , p 5 } and Q = {q 1 , q 2 , . . . , q 5 } in the map on the left, are merged to construct a fine-grained partition as shown in the map on the right by a combination of geoclasses in the two class sets. Each resulting fine-grained class is represented by a tuple (p i , q j ), and is constructed by identifying partially overlapping partitions in P and Q.</p><p>of the geotagged images in database. The latter treats the task as a classification problem by dividing the map into multiple discrete classes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Thanks to recent advances in deep learning, simple classification techniques based on convolutional neural networks handle such complex visual understanding problems effectively.</p><p>There are several advantages of formulating the task as classification instead of retrieval. First, classification-based approaches save memory and disk space to store information for geolocalization; they just need to store a set of model parameters learned from training images whereas all geotagged images in the database should be embedded and indexed to build retrieval-based systems. In addition to space complexity, inference of classification-based approaches is faster because a result is given by a simple forward pass computation of a deep neural network while retrieval-based methods undergo significant overhead for online search from a large index given a query image. Finally, classification-based algorithms provide multiple hypotheses of geolocation with no additional cost by presenting multi-modal answer distributions.</p><p>On the other hand, the standard classification-based approaches have a few critical limitations. They typically ignore correlation of spatially adjacent or proximate classes. For instance, assigning a photo of Bronx to Queens, which are both within New York city, is treated equally wrong as assigning it to Seoul. Another drawback comes from artificially converting continuous geographic space into discrete class representations. Such an attempt may incur various artifacts since images near class boundaries are not discriminative enough compared to data variations within classes; training converges slowly and performance is affected substantially by subtle changes in map partitioning. This limitation can be alleviated by increasing the number of classes and reducing the area of the region corresponding to each class. However, this strategy increases the number of parameters while decreasing the size of the training dataset per class.</p><p>To overcome such limitations, we propose a novel algorithm that enhances the resolution of geoclasses and avoids the training data deficiency issue. This is achieved by combinatorial partitioning, which is a simple technique to generate spatially fine-grained classes through a combination of the multiple configurations of classes. This idea has analogy to product quantization <ref type="bibr" target="#b4">[5]</ref> since they both construct a lot of quantized regions using relatively few model parameters through a combination of low-bit subspace encodings or coarse spatial quantizations. Our combinatorial partitioning allows the model to be trained with more data per class by considering a relatively small number of classes at a time. <ref type="figure">Figure 1</ref> illustrates an example of combinatorial partitioning, which enables generating more classes with minimal increase of model size and learning individual classifiers reliably without losing training data per class. Combinatorial partitioning is applied to an existing classification-based image geolocalization technique, PlaNet <ref type="bibr" target="#b3">[4]</ref>, and our algorithm is referred to as CPlaNet hereafter. Our contribution is threefold:</p><p>â€¢ We introduce a novel classification-based model for image geolocalization using combinatorial partitioning, which defines a fine-grained class configuration by combining multiple heterogeneous geoclass sets in coarse levels. â€¢ We propose a technique that generates multiple geoclass sets by varying parameters, and design an efficient inference technique to combine prediction results from multiple classifiers with proper normalization. â€¢ The proposed algorithm outperforms the existing techniques in multiple benchmark datasets, especially at fine scales.</p><p>The rest of this paper is organized as follows. We review the related work in Section 2, and describe combinatorial partitioning for image geolocalization in Section 3. The details about training and inference procedures are discussed in Section 4. We present experimental results of our algorithm in Section 5, and conclude our work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The most common approach of image geolocalization is based on the image retrieval pipeline. Im2GPS <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and its derivative <ref type="bibr" target="#b2">[3]</ref> perform image retrieval in a database of geotagged images using global image descriptors. Various visual features can be applied to the image retrieval step. NetVLAD <ref type="bibr" target="#b5">[6]</ref> is a global image descriptor trained end-to-end for place recognition on street view data using a ranking loss. Kim et al. <ref type="bibr" target="#b6">[7]</ref> learn a weighting mask for the NetVLAD descriptor to focus on image regions containing location cues. While global features have the benefit to retrieve diverse natural scene images based on ambient information, local image features yield higher precision in retrieving structured objects such as buildings and are thus more frequently used <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. DELF <ref type="bibr" target="#b16">[17]</ref> is a deeply learned local image feature detector and descriptor with attention for image retrieval.</p><p>On the other hand, classification-based image geolocalization formulates the problem as a classification task. In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, a classifier is trained to predict the geolocation of an input image. Since the geolocation is represented in a continuous space, classification-based approaches quantize the map of the entire earth into a set of geoclasses corresponding to partitioned regions. Note that training images are labeled into the corresponding geoclasses based on their GPS tags. At test time, the center of the geoclass with the highest score is returned as the predicted geolocation of an input image. This method is lightweight in terms of space and time complexity compared to retrieval-based methods, but its prediction accuracy highly depends on how the geoclass set is generated. Since every image that belongs to the same geoclass has an identical predicted geolocation, more fine-grained partitioning is preferable to obtain precise predictions. However, it is not always straightforward to increase the number of geoclasses as it linearly increases the number of parameters and makes the network prone to overfitting to training data.</p><p>Pose estimation approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> match query images against 3D models of an area, and employ 2D-3D feature correspondences to identify 6-DOF query poses. Instead of directly matching against a 3D model, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref> first perform image retrieval to obtain coarse locations and then estimate poses using the retrieved images. PoseNet <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> treats pose estimation as a regression problem based on a convolutional neural network. The accuracy of PoseNet is improved by introducing an intermediate LSTM layer for dimensionality reduction <ref type="bibr" target="#b26">[27]</ref>.</p><p>A related line of research is landmark recognition, where images are clustered by their geolocations and visual similarity to construct a database of popular landmarks. The database serves as the index of an image retrieval system <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> or the training data of a landmark classifier <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. Cross-view geolocation recognition makes additional use of satellite or aerial imagery to determine query locations <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Geolocalization using Multiple Classifiers</head><p>Unlike existing classification-based methods <ref type="bibr" target="#b3">[4]</ref>, CPlaNet relies on multiple classifiers that are all trained with unique geoclass sets. The proposed model predicts more fine-grained geoclasses, which are given by combinatorial partitioning of multiple geoclass sets. Since our method requires a distinct geoclass set for each classifier, we also propose a way to generate multiple geoclass sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Combinatorial Partitioning</head><p>Our primary goal is to establish fine-grained geoclasses through a combination of multiple coarse geoclass sets and exploit benefits from both coarse-and finegrained geolocalization-by-classification approaches. In our model, there are multiple unique geoclass sets represented by partitions P = {p 1 , p 2 , . . . , p 5 } and Q = {q 1 , q 2 , . . . , q 5 } as illustrated on the left side of <ref type="figure">Figure 1</ref>. Since the region boundaries in these geoclass sets are unique, overlapping the two maps constructs a set of fine-grained subregions. This procedure, referred to as combinatorial partitioning, is identical to the Cartesian product of the two sets, but disregards the tuples given by two spatially disjoint regions in the map. For instance, combining two aforementioned geoclass sets in <ref type="figure">Figure 1</ref>, we obtain fine-grained partitions defined by a tuple (p i , q j ) as depicted by the map on the right of the figure, while the tuples made by two disjoint regions, e.g., (p 1 , q 5 ), are not considered.</p><p>While combinatorial partitioning aggregates results from multiple classifiers, it is conceptually different from ensemble models whose base classifiers predict labels in the same output space. In combinatorial partitioning, while each coarsegrained partition is scored by a corresponding classifier, fine-grained partitions are given different scores by the combinations of multiple unique geoclass sets. Also, combinatorial partitioning is closely related to product quantization <ref type="bibr" target="#b4">[5]</ref> for approximate nearest neighbor search in the sense that they both generate a large number of quantized regions by either a Cartesian product of quantized subspaces or a combination of coarse space quantizations. Note that combinatorial partitioning is a general framework and applicable to other tasks, especially where labels have to be defined on the same embedded space as in geographical maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Benefits of Combinatorial Partitioning</head><p>The proposed classification model with combinatorial partitioning has the following three major benefits.</p><p>Fine-grained classes with fewer parameters Combinatorial partitioning generates fine-grained geoclasses using a smaller number of parameters because a single geoclass in a class set can be divided into many subregions by intersections with geoclasses from multiple geoclass sets. For instance in <ref type="figure">Figure 1</ref>, two sets with 5 geoclasses form 14 distinct classes by the combinatorial partitioning. If we design a single flat classifier with respect to the fine-grained classes, it requires more parameters, i.e., 14 Ã— F &gt; 2 Ã— (5 Ã— F ), where F is the number of input dimensions to the classification layers.</p><p>More training data per class Training with fine-grained geoclass sets is more desirable for higher resolution of output space, but is not straightforward due to training data deficiency; the more we divide the maps, the less training images remain per geoclass. Our combinatorial partitioning technique enables us to learn models with coarsely divided geoclasses and maintain more training data in each class than a naÃ¯ve classifier with the same number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More reasonable class sets</head><p>There is no standard method to define geoclasses for image geolocalization, so that images associated with the same classes have common characteristics. An arbitrary choice of partitioning may incur undesirable artifacts due to heterogeneous images located near class territories; the features trained on loosely defined class sets tend to be insufficiently discriminative  <ref type="table" target="#tab_7">Table 1</ref>. Each distinct region is marked by a different color. The first two sets, (a) and (b), are generated by manually designed parameters while parameters for the others are randomly sampled.</p><p>and less representative. On the other hand, our framework constructs diverse partitions based on various criteria observed in the images. We can define more tightly-coupled classes through combinatorial partitioning by distilling noisy information from multiple sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating Multiple Geoclass Sets</head><p>The geoclass set organization is an inherently ill-posed problem as there is no consensus about ideal region boundaries for image geolocalization. Consequently, it is hard to define the optimal class configuration, which motivates the use of multiple random boundaries in our combinatorial partitioning. We therefore introduce a mutable method of generating geoclass sets, which considers both visual and geographic distances between images. The generation method starts with an initial graph for a map, where a node represents a region in the map and an edge connects two nodes of adjacent regions. We construct the initial graph based on S2 cells 4 at a certain level. Empty S2 cells, which contain no training image, do not construct separate nodes and are randomly merged with one of their neighboring non-empty S2 cells. This initial graph covers the entire surface of the earth. Both nodes and edges are associated with numbers-scores for nodes and weights for edges. We give a score to each node by a linear combination of three different factors: the number of images in <ref type="table" target="#tab_7">Table 1</ref>: Parameters for geoclass set generation. Parameters for geoclass set 1 and 2 are manually given while the ones for rest geoclass sets are randomly sampled. the node and the number of empty and non-empty S2 cells. An edge weight is computed by the weighted sum of geolocational and visual distances between two nodes. The geolocational distance is given by the distance between the centers of two nodes while the visual distance is measured by cosine similarity based on the visual features of nodes, which are computed by averaging the associated image features extracted from the bottleneck layer of a pretrained CNN. Formally, a node score Ï‰(Â·) and an edge weight Î½(Â·, Â·) are defined respectively as</p><formula xml:id="formula_0">Ï‰(v i ) = Î± 1 Â· n img (v i ) + Î± 2 Â· n S2+ (v i ) + Î± 3 Â· n S2 (v i ) (1) Î½(v i , v j ) = Î² 1 Â· dist vis (v i , v j ) + Î² 2 Â· dist geo (v i , v j )<label>(2)</label></formula><p>where n img (v), n S2+ (v) and n S2 (v) are functions that return the number of images, non-empty S2 cells and all S2 cells in a node v, respectively, and dist vis (Â·, Â·) and dist geo (Â·, Â·) are the visual geolocational distances between two nodes. Note that the weights (Î± 1 , Î± 2 , Î± 3 ) and (Î² 1 , Î² 2 ) are free parameters in [0, 1].</p><p>After constructing the initial graph, we merge two nodes hierarchically in a greedy manner until the number of remaining nodes becomes the desired number of geoclasses. To make each geoclass roughly balanced, we select the node with the lowest score first and merge it with its nearest neighbor in terms of edge weight. A new node is created by the merge process and corresponds to the region given by the union of two merged regions. The score of the new node is set to the sum of the scores of the two merged nodes.</p><p>The generated geoclass sets are diversified by the following free parameters: 1) the desired number of final geoclasses, 2) the weights of the factors in the node scores, 3) the weights of the two distances in computing edge weights and 4) the image feature extractor. Each parameter setting constructs a unique geoclass set. Note that multiple geoclass set generation is motivated by the fact that geoclasses are often ill-defined and the perturbation of class boundaries is a natural way to address the ill-posed problem. <ref type="figure">Figure 2</ref> illustrates generated geoclass sets using different parameters described in <ref type="table" target="#tab_7">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning and Inference</head><p>This section describes more details about CPlaNet including network architecture, and training and testing procedure. We also discuss data structures and the detailed inference algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network Architecture</head><p>Following <ref type="bibr" target="#b3">[4]</ref>, we construct our network based on the Inception architecture <ref type="bibr" target="#b40">[41]</ref> with batch normalization <ref type="bibr" target="#b41">[42]</ref>. Inception v3 without the final classification layer (fc with softmax) is used as our feature extractor, and multiple branches of classification layers are attached on top of the feature extractor as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. We train the multiple classifiers independently while keeping the weights of the Inception module fixed. Note that, since all classifiers share the feature extractor, our model requires marginal increase of memory to maintain multiple classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inference with Multiple Classifiers</head><p>Once the predicted scores in each class set are assigned to the corresponding regions, the subregions overlapped by multiple class sets are given cumulated scores from multiple classifiers. A simple strategy to accumulate geoclass scores is to add the scores to individual S2 cells within the geoclass. Such a simple strategy is inappropriate since it gives favor to classifiers that have geoclasses corresponding to large regions covering more S2 cells. To make each classifier contribute equally to the final prediction regardless of its class configuration, we normalize the scores from individual classifiers with consideration of the number of S2 cells per class before adding them to the current S2 cell scores. Formally, given a geoclass score distributed to S2 cell g k within a class in a geoclass set C i , denoted by geoscore(g k ; C i ), an S2 cell is given a score s(Â·) by</p><formula xml:id="formula_1">s(g k ) = N i=1 geoscore(g k ; C i ) K t=1 geoscore(g t ; C i ) ,<label>(3)</label></formula><p>where K is the total number of S2 cells and N is the number of geoclass sets. Note that this process implicitly creates fine-grained partitions because the regions defined by different geoclass combinations are given different scores.</p><p>After this procedure, we select the S2 cells with the highest scores and compute their center for the final prediction of geolocation by averaging locations of images in the S2 cells. That is, the predicted geolocation l pred is given by</p><formula xml:id="formula_2">l pred = kâˆˆG eâˆˆg k geolocation (e) kâˆˆG |g k | ,<label>(4)</label></formula><p>where G = argmax k s(g k ) is an index set of the S2 cells with the highest scores and geolocation(Â·) is a function to return the ground-truth GPS coordinates of a training image e. Note that an S2 cell g k may contain a number of training examples.</p><p>In our implementation, all fine-grained partitions are precomputed offline by generating all existing combinations of the multiple geoclass sets, and an index mapping from each geoclass to its corresponding partitions is also constructed offline to accelerate inference. Moreover, we precompute the center of images in each partition. To compute the center of a partition, we convert the latitude and longitude values of GPS tags into 3D Cartesian coordinates. This is because a naÃ¯ve average of latitude and longitude representations introduces significant errors as the target locations become distant from the equator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We train our network using a private dataset collected from Flickr, which has 30.3M geotagged images for training. We have sanitized the dataset by removing noisy examples to weed out unsuitable photos. For example, we disregard unnatural images (e.g., clipart images, product photos, etc.) and accept photos with a minimum size of 0.1 megapixels.</p><p>For evaluation, we mainly employ two public benchmark datasets-Im2GPS3k and YFCC4k <ref type="bibr" target="#b2">[3]</ref>. The former contains 3,000 images from the Im2GPS dataset whereas the latter has 4,000 random images from the YFCC100m dataset. In addition, we also evaluate on Im2GPS test set <ref type="bibr" target="#b0">[1]</ref> to compare with previous work. Note that Im2GPS3k is a different test benchmark from the Im2GPS test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Parameters and Training Networks</head><p>We generate three geoclass sets using randomly generated parameters, which are summarized in <ref type="table" target="#tab_7">Table 1</ref>. The number of geoclasses for each set is approximately between 10K and 13K, and the generation parameters for edge weights and node scores are randomly sampled. Specifically, we select random axis-aligned subspaces out of the full 2,048 dimensions for image representations to diversify dissimilarity metrics between image representations. Note that the image representations are extracted by a reproduced PlaNet <ref type="bibr" target="#b3">[4]</ref> after removing the final classification layer. In addition to these geoclass sets, we generate two more sets with manually designed parameters; the edge weights in these two cases are given by either visual or geolocational distance exclusively, and their node scores are based on the number of images to mimic the setting of PlaNet. <ref type="figure">Figure 2</ref> visualizes five geoclass sets generated by the parameters presented in <ref type="table" target="#tab_7">Table 1</ref>.</p><p>We use S2 cells at level 14 to construct the initial graph, where a total of âˆ¼2.8M nodes are obtained after merging empty cells to their non-empty neighbors.</p><p>To train the proposed model, we employ the pretrained model of the reproduced PlaNet with its parameters fixed while the multiple classification branches are randomly initialized and fine-tuned using our training dataset. The network is trained by RMSprop with a learning rate of 0.005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Metrics</head><p>Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, we evaluate the models using geolocational accuracies at multiple scales by varying the allowed errors in terms of distances from ground-truth locations as follows: 1 km, 5 km, 10 km, 25 km, 50 km, 100 km, 200 km, 750 km and 2500 km. Our evaluation focuses more on high accuracy range compared to the previous papers as we believe that fine-grained geolocalization is more important in practice. A geolocational accuracy a r at a scale is given by the fraction of images in the test set localized within radius r from ground-truths, which is given by</p><formula xml:id="formula_3">a r â‰¡ 1 M M i=1 u geodist l i gt , l i pred &lt; r ,<label>(5)</label></formula><p>where M is the number of examples in the test set, u[Â·] is an indicator function and geodist(l i gt , l i pred ) is the geolocational distance between the true image location l i gt and the predicted location l i pred of the i-th example. <ref type="table" target="#tab_2">Table 2</ref> presents the geolocational accuracies of the proposed model on the Im2GPS3k dataset. The proposed models outperform the baselines and the existing methods at almost all scales on this dataset. ClassSet 1 through 5 in <ref type="table" target="#tab_2">Table 2</ref> are the models trained with the geoclass sets generated from the parameters presented in <ref type="table" target="#tab_7">Table 1</ref>. Using the learned models as the base classifiers, we construct two variants of the proposed method-CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref> using the first two base classifiers with manual parameter selection and CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> using all the base classifiers.   <ref type="table" target="#tab_2">Table 2</ref> presents that both options of our models outperform all the underlying classifiers at every scale. Compared to naÃ¯ve average of the underlying classifiers denoted by Average <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> and Average <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref>, CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> and CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref> have âˆ¼16 % and âˆ¼13 % of accuracy gains at street level, respectively, compared to their counterparts. We emphasize that CPlaNet achieves substantial improvements by a simple combination of the existing base classifiers and a generation of fine-grained partitions without extra training procedure. The larger performance improvement in CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> compared to CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref> makes sense as using more classifiers constructs more fine-grained geoclasses via combinatorial partitioning and increases prediction resolution. Note that the number of distinct partitions formed by CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref> is 46,294 while it is 107,593 in CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits of Combinatorial Partitioning</head><p>The combinatorial partitioning of the proposed model is not limited to geoclass sets from our generation methods, but is generally applicable to any geoclass sets. Therefore, we construct an additional instance of the proposed method, CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr">PlaNet]</ref>, which also incorporates PlaNet (reprod), reproduced version of PlaNet model <ref type="bibr" target="#b3">[4]</ref> with our training data, additionally. CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr">PlaNet]</ref> shows extra performance gains over CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> and achieves the state-of-the-art performance at all scales. These experiments show that our combinatorial partitioning is a useful framework for image geolocalization through ensemble classification, where multiple classifiers with heterogeneous geoclass sets complement each other.</p><p>We also present results on YFCC4k <ref type="bibr" target="#b2">[3]</ref> dataset in <ref type="table" target="#tab_3">Table 3</ref>. The overall tendency is similar to the one in Im2GPS3k. Our full model outperforms Deep-Ret <ref type="bibr" target="#b2">[3]</ref> consistently and significantly. The proposed algorithm also shows substantially better performance compared to PlaNet (reprod) in the low threshold range while two methods have almost identical accuracy at coarse-level evaluation.</p><p>On the Im2GPS dataset, our model outperforms other classification-based approaches-Deep-Cls and PlaNet, which are single-classifier models with a different geoclass schema-significantly at every scale, as shown in <ref type="table" target="#tab_4">Table 4</ref>. The performance of our models is also better than the retrieval-based models at most scales. Moreover, our model, like other classification-based approaches, requires much less space than the retrieval-based models for inference. Although Deep-Ret+ improves Deep-Ret by increasing the size of the database, it even worsens space and time complexity. In contrast, the classification-based approaches including ours do not require extra space when we have more training images. <ref type="figure">Figure 4</ref> presents qualitative results of CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> on Im2GPS. It shows how the combinatorial partitioning process improves the geolocalization quality. Given an input image, each map shows an intermediate prediction as we accumulate the scores on different geoclass sets one by one. The region with the highest score is progressively sharded into a smaller region with fewer S2 cells, and the center of the region gradually approaches to the ground-truth location as we integrate more classifiers for inference.</p><p>Computational Complexity Although CPlaNet achieves competitive performance through combinatorial partitioning, one may be concerned about potential increase of time complexity for its inference due to additional classification layers and overhead in combinatorial partitioning process. However, it turns out that the extra computational cost is negligible since adding few more classification layers on top of the shared feature extractor does not increase inference time substantially and the required information for combinatorial partitioning is precomputed as described in Section 4.2. Specifically, when we use 5 classification branches with combinatorial partitioning, theoretical computational costs for multi-head classification and combinatorial partitioning are only 2% and 0.004% of that of feature extraction process. In terms of space complexity, classification based methods definitely have great advantages over retrieval based ones, which need to maintain the entire image database. Compared to a single-head classifier,  <ref type="figure">Fig. 4</ref>: Qualitative results of CPlaNet <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> on Im2GPS. Each map illustrates the progressive results of combinatorial partitioning by adding classifiers one by one. S2 cells with the highest score and their centers are marked by green area and red pins respectively while the ground-truth location is denoted by the blue dots. We also present the number of S2 cells in the highlighted region and distance between the ground-truth location and the center of the region in each map. our model with five base classifiers requires just four additional classification layers, which incurs moderate increase of memory usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of Visual Features</head><p>For geoclass set generation, all the parameters of ClassSet 1 and 2 are set to the same values except for the relative importance of two factors for edge weight definition; edge weights for ClassSet 1 are determined by visual distances only whereas those for ClassSet 2 are based on geolocational distances between the cells without any visual information of images. ClassSet 1 presents better accuracies at almost all scales as in <ref type="table" target="#tab_2">Table 2</ref>. This result shows how important visual information of images is when defining geoclass sets. Moreover, we build another model (ImageNetFeat) learned with the same geoclass set with ClassSet 1 but using a different feature extractor pretrained on ImageNet <ref type="bibr" target="#b42">[43]</ref>. The large margin between ImageNetFeat and ClassSet 1 indicates importance of feature representation methods, and implies unique characteristics of visual cues required for image geolocalization compared to image classification.</p><p>Balancing Classifiers We normalize the scores assigned to individual S2 cells as discussed in Section 4.2, which is denoted by NormalizedSum, to address the artifact that sums of all S2 cell scores are substantially different across classifiers. To highlight the contribution of NormalizedSum, we conduct an additional experiment with classsets that have large variations in number of classes. <ref type="table" target="#tab_6">Table 5</ref> presents that NormalizedSum clearly outperforms the combinatorial partitioning without normalization (SimpleSum) while SimpleSum still illustrates competitive accuracy compared to the base classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a novel classification-based approach for image geolocalization, referred to as CPlaNet. Our model obtains the final geolocation of an image using a large number of fine-grained regions given by combinatorial partitioning of multiple classifiers. We also introduced an inference procedure appropriate for classification-based image geolocalization. The proposed technique improves image geolocalization accuracy with respect to other methods in multiple benchmark datasets especially at fine scales, and also outperforms the individual coarsegrained classifiers. 01778], and the Technology Innovation Program [10073166] funded by the Korea government MSIT and MOTIE, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Inference Complexity</head><p>We show the additional complexity for the combinatorial partitioning is theoretically negligible in Section 5.4.2 of the main document To support the analysis, we measure the inference time of our model with five base classifiers and compare it with the result from a single classifier model. <ref type="table" target="#tab_7">Table A</ref> presents that time for feature extraction is the most dominant factor in inference time while classification layers have minor overhead in prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Number of Geoclasses and Data Deficiency</head><p>In classification-based image geolocalization, the number of geoclasses in the classifier is closely related to precision of a prediction. In other words, since the geolocation of an image is often given by the center of its predicted geoclass, a coarse-grained partitioning inherently have large errors quantitatively due to its low resolution. So, it is preferable to have more geoclasses by a fine-grained partitioning. However, it is not always straightforward to increase the number of classes due to training data deficiency; as the partitions are more fine-grained, the number of training examples per geoclass decreases. <ref type="figure">Figure A</ref> presents image geolocalization accuracy at five different levels of distance thresholds while varying the number of geoclasses in a classifier. According to our experiment, the accuracy with a small distance threshold typically improves when trained with more geoclasses whereas the accuracy with a large distance threshold decreases. We believe that such inconsistent phenomenon results from skewed distribution of image geolocations over the map. Since images in geoclasses with dense image population often contain common landmarks and share visual features, dividing these geoclasses into more finegrained ones leads to reducing the prediction error. On the other hand, images are more heterogeneous in sparse geoclasses and subdividing these geoclasses leads to the data deficiency problem causing accuracy drops. Note that, since predictions on sparse geoclasses are unlikely to be very accurate in coarse-grained partitioning, further subdivisions do no harm to the the low-threshold results and accuracy drops mostly happen in high-threshold areas. The accuracy with 1 km increases as more geoclasses are employed, but the benefit of using more classes is gradually reduced and even becomes negative as the thresholds increase. Note that the geoclasses are generated by our generation algorithm using the parameters for ClassSet 1 in the main paper except for the number of classes.</p><p>Thus, it is not always desirable to simply increase the number of geoclasses for improving performance. In contrast, our method achieves the highest geolocalization accuracy at almost every threshold level with an increased number of distinct geoclasses given by combinatorial partitioning. Note that combinatorial partitioning enables the model to work around the data deficiency problem. We also emphasize that we can apply our method to any base classifiers even with different design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Evaluation</head><p>We conducted qualitative analysis comparing CP <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> and PlaNet (reprod). <ref type="figure">Figure B</ref>(left) presents a 2D matrix (A) made by counting the number of geolocalization prediction pairs corresponding to each element given by the two models on Im2GPS3k dataset while <ref type="figure">Figure B</ref>(middle) shows another matrix (A âˆ’ A T ) whose lower triangle shows how much CP <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> improves accuracy with respect to PlaNet. According to our observation, the gain is most significant in the pair of  <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b4">5]</ref> of PlaNet to bin (0,1] of CP <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>.</p><p>bin corresponding to <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b4">5]</ref> of PlaNet and bin corresponding to (0,1] of CP <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. The images that belong to the observation frequently contain landmark photos as shown in <ref type="figure">Figure B</ref>(right).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 Fig. 2 :</head><label>32</label><figDesc>Visualization of the geoclass sets on the maps of the United States generated by the parameters shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Network architecture of our model. A single Inception v3 architecture is used as our feature extractor after removing the final classification layer. An image feature is fed to multiple classification branches and classification scores are predicted over multiple geoclass sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Geolocalization accuracies of a single-head classifier at five different distance thresholds-1 km, 25 km, 200 km, 750 km, and 2500 km-by varying the number of geoclasses of the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Qualitative comparison of our algorithm and PlaNet. (left) 2D heat map of prediction quality on Im2GPS3k. Axis ticks denote geo-distance bins. (right) images from bin</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Geolocational accuracies [%] of models at different scales on Im2GPS3k.</figDesc><table><row><cell>Models</cell><cell cols="9">1 km 5 km 10 km 25 km 50 km 100 km 200 km 750 km 2500 km</cell></row><row><cell>ImageNetFeat</cell><cell>3.0</cell><cell>5.5</cell><cell>6.4</cell><cell>6.9</cell><cell>7.7</cell><cell>9.0</cell><cell>10.8</cell><cell>18.5</cell><cell>37.5</cell></row><row><cell>Deep-Ret [3]</cell><cell>3.7</cell><cell>-</cell><cell>-</cell><cell>19.4</cell><cell>-</cell><cell>-</cell><cell>26.9</cell><cell>38.9</cell><cell>55.9</cell></row><row><cell>PlaNet (reprod) [4]</cell><cell>8.5</cell><cell cols="4">18.1 21.4 24.8 27.7</cell><cell>30.0</cell><cell>34.3</cell><cell>48.4</cell><cell>64.6</cell></row><row><cell>ClassSet 1</cell><cell>8.4</cell><cell cols="4">18.3 21.7 24.7 27.4</cell><cell>29.8</cell><cell>34.1</cell><cell>47.9</cell><cell>64.5</cell></row><row><cell>ClassSet 2</cell><cell>8.0</cell><cell cols="4">17.6 20.6 23.8 26.2</cell><cell>29.2</cell><cell>32.7</cell><cell>46.6</cell><cell>63.9</cell></row><row><cell>ClassSet 3</cell><cell>8.8</cell><cell cols="4">18.9 22.4 25.7 27.9</cell><cell>29.8</cell><cell>33.5</cell><cell>47.8</cell><cell>64.1</cell></row><row><cell>ClassSet 4</cell><cell>8.7</cell><cell cols="4">18.5 21.4 24.6 26.8</cell><cell>29.6</cell><cell>33.0</cell><cell>47.6</cell><cell>64.4</cell></row><row><cell>ClassSet 5</cell><cell>8.8</cell><cell cols="4">18.7 21.7 24.7 27.3</cell><cell>29.3</cell><cell>32.9</cell><cell>47.1</cell><cell>64.5</cell></row><row><cell>Average[1-2]</cell><cell>8.2</cell><cell cols="4">18.0 21.1 24.2 26.8</cell><cell>29.5</cell><cell>33.4</cell><cell>47.3</cell><cell>64.2</cell></row><row><cell>Average[1-5]</cell><cell>8.5</cell><cell cols="4">18.4 21.5 24.7 27.1</cell><cell>29.5</cell><cell>33.2</cell><cell>47.4</cell><cell>64.3</cell></row><row><cell>CPlaNet[1-2]</cell><cell>9.3</cell><cell cols="4">19.3 22.7 25.7 27.7</cell><cell>30.1</cell><cell>34.4</cell><cell>47.8</cell><cell>64.5</cell></row><row><cell>CPlaNet[1-5]</cell><cell>9.9</cell><cell cols="4">20.2 23.3 26.3 28.5</cell><cell>30.4</cell><cell>34.5</cell><cell>48.8</cell><cell>64.6</cell></row><row><cell cols="7">CPlaNet[1-5,PlaNet] 10.2 20.8 23.7 26.5 28.6 30.6</cell><cell>34.6</cell><cell>48.6</cell><cell>64.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Geolocational accuracies [%] on YFCC4k.</figDesc><table><row><cell>Models</cell><cell cols="9">1 km 5 km 10 km 25 km 50 km 100 km 200 km 750 km 2500 km</cell></row><row><cell>Deep-Ret [3]</cell><cell>2.3</cell><cell>-</cell><cell>-</cell><cell>5.7</cell><cell>-</cell><cell>-</cell><cell>11.0</cell><cell>23.5</cell><cell>42.0</cell></row><row><cell>PlaNet (reprod) [4]</cell><cell>5.6</cell><cell cols="5">10.1 12.2 14.3 16.6 18.7</cell><cell>22.2</cell><cell>36.4</cell><cell>55.8</cell></row><row><cell>CPlaNet[1-5]</cell><cell>7.3</cell><cell cols="4">11.7 13.1 14.7 16.1</cell><cell>18.2</cell><cell>21.7</cell><cell>36.2</cell><cell>55.6</cell></row><row><cell cols="6">CPlaNet[1-5,PlaNet] 7.9 12.1 13.5 14.8 16.3</cell><cell>18.5</cell><cell>21.9</cell><cell>36.4</cell><cell>55.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Geolocational accuracies [%] on Im2GPS. 1 km 5 km 10 km 25 km 50 km 100 km 200 km 750 km 2500 km</figDesc><table><row><cell></cell><cell>Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Retrieval</cell><cell>Im2GPS [1] Im2GPS [2] Deep-Ret [3] Deep-Ret+ [3]</cell><cell cols="5">-2.5 12.2 16.9 21.9 25.3 --12.0 -12.2 --33.3 -14.4 --33.3 -</cell><cell>-28.7 --</cell><cell>15.0 32.1 44.3 47.7</cell><cell>23.0 35.4 57.4 61.6</cell><cell>47.0 51.9 71.3 73.4</cell></row><row><cell></cell><cell>Deep-Cls [3]</cell><cell>6.8</cell><cell>-</cell><cell>-</cell><cell>21.9</cell><cell>-</cell><cell>-</cell><cell>34.6</cell><cell>49.4</cell><cell>63.7</cell></row><row><cell>Classifier</cell><cell>PlaNet [4] PlaNet (reprod) [4] CPlaNet[1-2] CPlaNet[1-5]</cell><cell cols="5">8.4 19.0 21.5 24.5 27.8 11.0 23.6 26.6 31.2 35.4 14.8 28.7 31.6 35.4 37.6 16.0 29.1 33.3 36.7 39.7</cell><cell>30.4 30.5 40.9 42.2</cell><cell>37.6 37.6 43.9 46.4</cell><cell>53.6 64.6 60.8 62.4</cell><cell>71.3 81.9 80.2 78.5</cell></row><row><cell></cell><cell cols="7">CPlaNet[1-5,PlaNet] 16.5 29.1 33.8 37.1 40.5 42.6</cell><cell>46.4</cell><cell>62.0</cell><cell>78.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparisons between the models with and without normalization for combinatorial partitioning on Im2GPS3k. Each number in parentheses denotes the geoclass set size, which varies largely to highlight the effect of normalization for this experiment.</figDesc><table><row><cell>Models</cell><cell cols="7">1 km 5 km 10 km 25 km 50 km 100 km 200 km 750 km 2500 km</cell></row><row><cell>ClassSet 1 (9969)</cell><cell>8.4</cell><cell cols="2">18.3 21.7 24.7 27.4</cell><cell>29.8</cell><cell>34.1</cell><cell>47.9</cell><cell>64.5</cell></row><row><cell>ClassSet 2 (9969)</cell><cell>8.0</cell><cell cols="2">17.6 20.6 23.8 26.2</cell><cell>29.2</cell><cell>32.7</cell><cell>46.6</cell><cell>63.9</cell></row><row><cell>ClassSet 3 (3416)</cell><cell>4.2</cell><cell cols="2">15.9 19.1 22.8 24.9</cell><cell>28.0</cell><cell>31.4</cell><cell>46.1</cell><cell>63.5</cell></row><row><cell>ClassSet 4 (1444)</cell><cell>1.8</cell><cell>9.5</cell><cell>13.2 16.8 21.2</cell><cell>24.5</cell><cell>29.5</cell><cell>44.4</cell><cell>61.8</cell></row><row><cell cols="2">ClassSet 5 (10600) 8.2</cell><cell cols="2">19.1 22.3 25.2 27.3</cell><cell>29.9</cell><cell>33.6</cell><cell>47.3</cell><cell>65.5</cell></row><row><cell>SimpleSum</cell><cell>9.7</cell><cell cols="2">19.4 23.1 26.6 28.1</cell><cell>30.6</cell><cell>33.8</cell><cell>47.7</cell><cell>64.0</cell></row><row><cell>NormalizedSum</cell><cell cols="4">9.8 19.8 23.6 26.8 28.8 31.1</cell><cell>34.9</cell><cell>48.3</cell><cell>65.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A :</head><label>A</label><figDesc>Average inference time of a single classifier and five classifiers.</figDesc><table><row><cell></cell><cell>inference time (sec)</cell></row><row><cell>feature extraction (FE)</cell><cell>0.11</cell></row><row><cell>FE+1 classifier</cell><cell>0.12</cell></row><row><cell>FE+5 classifiers</cell><cell>0.13</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use Google's S2 library. S2 cells are given by a geographical partitioning of the earth into a hierarchy. The surface of the earth is projected onto six faces of a cube. Each face of the cube is hierarchically subdivided and forms S2 cells in a quad-tree. Refer to https://code.google.com/archive/p/s2-geometry-library/ formoredetails.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The part of this work was performed while the first and last authors were with Google, Venice, CA. This research is partly supported by the IITP grant [2017-0-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im2GPS: Estimating geographic information from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal Location Estimation of Videos and Images</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="41" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Revisiting IM2GPS in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Planet-photo geolocation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learned contextual feature reweighting for image geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Handling urban location recognition as a 2d homothetic problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koeser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph-based discriminative learning for location recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="254" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">City-scale landmark identification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>KÃ¶ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>PylvÃ¤nÃ¤inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roimela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Predicting good features for image geolocalization using per-bundle vlad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Avoiding confusing features in place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">City-scale location recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate image localization based on google maps street view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image geo-localization based on multiple nearest neighbor feature matching using generalized graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large-scale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">From structure-from-motion point clouds to fast location recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irschara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Location recognition using prioritized feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Worldwide pose estimation using 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient global 2d-3d matching for camera localization in a large-scale 3d map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast image-based localization using direct 2d-to-3d matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Image retrieval for image-based localization revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
		<editor>BMVC.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Are large-scale 3d models really necessary for accurate visual localization? In: CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Geometric loss functions for camera pose regression with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">PoseNet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Image-based localization using lstms for structured feature correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Walch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-TaixÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hilsenbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Retrieving landmark and non-landmark images from community photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Spyrou</surname></persName>
		</author>
		<editor>MM.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">I know what you did last summer: Object-level auto-annotation of holiday snaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gammeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">From images to scenes: Compressing an image cluster into a single scene model for place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">World-scale mining of objects and events from community photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<editor>CIVR.</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Tour the world: Building a web-scale landmark recognition engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Buddemeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual landmark recognition from internet photo collections: A large-scale evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Leveraging structure from motion to learn discriminative codebooks for scalable landmark classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Landmark classification in large-scale image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning per-location classifiers for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Wide-area image geolocalization with aerial reference imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-view image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cross-view image matching for geo-localization in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
