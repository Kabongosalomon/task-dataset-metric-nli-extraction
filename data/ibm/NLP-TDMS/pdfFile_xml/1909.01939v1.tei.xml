<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XX XXX 1 EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jianru</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XX XXX 1 EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Element-wise-Attention Gate (EleAttG)</term>
					<term>recur- rent neural networks</term>
					<term>EleAtt-RNN</term>
					<term>skeleton based action recog- nition</term>
					<term>gesture recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs) are capable of modeling temporal dependencies of complex sequential data. In general, current available structures of RNNs tend to concentrate on controlling the contributions of current and previous information. However, the exploration of different importance levels of different elements within an input vector is always ignored. We propose a simple yet effective Element-wise-Attention Gate (EleAttG), which can be easily added to an RNN block (e.g. all RNN neurons in an RNN layer), to empower the RNN neurons to have attentiveness capability. For an RNN block, an EleAttG is used for adaptively modulating the input by assigning different levels of importance, i.e., attention, to each element/dimension of the input. We refer to an RNN block equipped with an EleAttG as an EleAtt-RNN block. Instead of modulating the input as a whole, the EleAttG modulates the input at fine granularity, i.e., element-wise, and the modulation is content adaptive. The proposed EleAttG, as an additional fundamental unit, is general and can be applied to any RNN structures, e.g., standard RNN, Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU). We demonstrate the effectiveness of the proposed EleAtt-RNN by applying it to different tasks including the action recognition, from both skeleton-based data and RGB videos, gesture recognition, and sequential MNIST classification. Experiments show that adding attentiveness through EleAttGs to RNN blocks significantly improves the power of RNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>E XPLORATION of the temporal dynamics and spatial correlations of time series data plays an important role in sequence understanding. Recurrent neural networks, based on recursive connection, are powerful in modeling temporal dynamics and learning appropriate feature representations. Standard RNN (sRNN) and its variants, including Long Short-Term Memory (LSTM) <ref type="bibr" target="#b17">[18]</ref>, Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">[4]</ref>, etc., have proven effective for tasks using sequential information, such as action recognition <ref type="bibr" target="#b12">[13]</ref>, machine translation <ref type="bibr" target="#b3">[4]</ref>, pedestrian trajectory prediction <ref type="bibr" target="#b65">[66]</ref>, video summation <ref type="bibr" target="#b68">[69]</ref>, and image caption <ref type="bibr" target="#b48">[49]</ref>.</p><p>Recursive connection inside RNN structures is achieved by taking the output of the previous time step as the input of the current time step, which facilitates the processing of sequential data. At each time step, RNN neurons perform the same operation to embed the current input and the historical information to the representation of the hidden state of this time step. However, the sRNN suffers from gradient vanishing, which has difficulties in learning long-range dependencies. To address this problem, researchers propose some gate-based RNN structures, such as LSTM and GRU, which introduce gates and linear memory units inside RNN neurons to control the information flow. Gates provide a way to optionally let information through or stop softly, which balances the contributions of the information of the current time slot and historical information. For an RNN neuron, a gate produces a scalar value ranging from 0 to 1 which controls the amount of information flow. However, it is a scalar which imposes the same control on each element of a vector, rather than element-wise adaptation. They are not capable of exploring the potential different characteristics of different elements.</p><p>Attention mechanisms which selectively focus on different parts of the data have proven effective for many tasks <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Inspired by both the attention and gate mechanisms, we develop an Element-wise-Attention Gate (EleAttG) to empower the RNN neurons to have the attentiveness capability, which makes the RNN neurons focus on the important elements of inputs adaptively. For all neurons of an RNN block, we design a sharable EleAttG which outputs an attention vector with the same dimension as the input. Then the original input is modulated by the attention vector to strengthen the impact of important elements while suppressing the impact of unimportant elements. Note that similar to <ref type="bibr" target="#b36">[37]</ref>, we use an RNN block to represent an ensemble of a group of N RNN neurons, which could be all RNN neurons in an RNN layer. <ref type="figure">Fig. 1</ref> illustrates the structure of the proposed EleAttG within a general RNN block, as shown in (a), and a special case when the RNN block is constrained as GRU, as shown in (b). The response a t of the EleAttG is used to modulate the input x t to x t . Then x t takes the place of the role of x t for subsequent operations. We refer to an RNN block equipped with an EleAttG as an EleAtt-RNN block. According to the RNN structure adopted in the EleAtt-RNN block, e.g., standard RNN, LSTM, and GRU, we refer to the blocks as EleAtt-sRNN, EleAtt-LSTM, and EleAtt-GRU, respectively. An RNN layer with such EleAttG can replace the original RNN layer and multiple EleAtt-RNN layers can be stacked.</p><p>We demonstrate the effectiveness and good generalization of our proposed EleAtt-RNN by applying it to three different  <ref type="figure">Fig. 1</ref>: Illustration of Element-wise-Attention Gate (EleAttG) (marked in red) for (a) a generic RNN block, where the RNN structure could be the standard RNN, LSTM, or GRU and (b) a GRU block which consists of a group of (e.g., N ) GRU neurons. In the diagram, each line carries a vector. The brown circles denote element-wise operation, e.g., element-wise vector product or vector addition. The yellow boxes denote the units of the original GRU with the output dimension of N . The red box denotes the EleAttG with an output dimension of D, which is the same as the dimension of the input x t .</p><p>tasks including action recognition, gesture recognition, and sequential MNIST classification. For action recognition, we evaluate EleAttG on both 3D skeleton-based human action recognition and RGB-based action recognition tasks. For 3D skeleton-based human action recognition, we build the classification network by stacking several EleAtt-RNN layers and evaluate the effectiveness of EleAttG on three types of RNN structures, i.e., standard RNN, LSTM, and GRU, respectively. EleAtt-RNNs consistently outperform the original RNNs for all the three types of RNNs. Our scheme based on EleAtt-GRU achieves state-of-the-art performance on three challenging datasets, i.e., the NTU <ref type="bibr" target="#b37">[38]</ref>, N-UCLA <ref type="bibr" target="#b52">[53]</ref>, and SYSU <ref type="bibr" target="#b19">[20]</ref> datasets. For RGB-based action recognition, we design our system by applying an EleAtt-GRU network to the sequence of frame-level CNN features. Experiments on both the JHMDB <ref type="bibr" target="#b21">[22]</ref> and NTU <ref type="bibr" target="#b37">[38]</ref> datasets show that adding EleAttGs brings significant gain. Experiments on the gesture recognition dataset DHG <ref type="bibr" target="#b8">[9]</ref> prove EleAttG is also helpful for gesture recognition. We also evaluate the effectiveness of the EleAttG on sequential MNIST classification, which is widely used to evaluate the performance of different RNN structures <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b30">[31]</ref>. We summarize the merits of the proposed EleAttG as follows:</p><p>• EleAttG is capable of adaptively modulating the input at a fine granularity, paying different levels of attention to different elements of the input, resulting in higher performance. • The design is very simple. For an RNN layer, only one line of code needs to be added in the implementation. • The EleAttG is general and can be added to any RNN structure, e.g., standard RNN, LSTM, and GRU, and to any layer. EleAttG can be applied to multiple tasks and different input types.</p><p>It should be noted that this paper is an extension of our previous conference paper <ref type="bibr" target="#b66">[67]</ref>. In our conference paper, we only apply the proposed EleAttG to the action recognition based on skeleton data and CNN features of RGB videos. As an extension, we evaluate the generality of EleAttG by applying it to two additional tasks including gesture recognition and sequential MNIST classification. Sequention MNIST classification is widely used to evaluate the performance of RNNs. The superior performance on different tasks and datasets demonstrates the effectiveness and good generalization performance of the "EleAtt-RNN". We also add visualization analyses and training loss curve on the sequential MNIST dataset to help understand the effectiveness of EleAttG on different types of input data. We also discuss the influence of EleAttG on the input vector and the hidden state, respectively. In addition, we discuss the difference between the EleAttG and the input gate of LSTM to have a better understanding of our proposed EleAttG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Recurrent Neural Networks</head><p>Recurrent neural networks have many variants. To address the vanishing gradient problem that exists in the standard RNN, Hochreiter et al. propose LSTM, which adds a memory cell that allows "constant error carrousels" and introduces several gates to open and close access to the constant error flow <ref type="bibr" target="#b17">[18]</ref>. Gers et al. propose the "forget gate" for the previous LSTM to enable the LSTM cell to learn to reset itself (historical information) to prohibit the growth of the state indefinitely <ref type="bibr" target="#b14">[15]</ref>. Another variant of LSTM is the peephole LSTM, which lets the gates look at the cell state <ref type="bibr" target="#b15">[16]</ref>. GRU, is a much simpler variant of LSTM. A GRU has a reset gate and an update gate which control the memory and new input information. Comparing LSTM with GRU, there is no clear winner <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b22">[23]</ref> which differs for different application tasks. A differential gating scheme is introduced in LSTM in <ref type="bibr" target="#b46">[47]</ref> which leverages the derivative of the cell state to gate the information flow, and proves effective for action recognition. Residual connection is another effective way to solve the vanishing gradient problem. Campos et al. propose the Skip RNN with residual connection to skip some state updates during both training and test procedures, which shortens the effective size of the computation graph <ref type="bibr" target="#b2">[3]</ref>. Kusupati et al. add the residual connection to connect the current hidden state and previous hidden state <ref type="bibr" target="#b26">[27]</ref>. In addition, they share the same matrices for computing the hidden state and gate value to reduce model size.</p><p>In this work, we augment the capability of RNNs by adding attentiveness to the RNN structures. We propose a simple yet effective EleAttG which adaptively modulates the elements of inputs in fine-grained manner to explore their different blue levels of importance for an RNN block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanisms</head><p>Attention mechanisms which selectively focus on different parts of the data have proven effective for many tasks such as machine translation <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b45">[46]</ref>, image caption <ref type="bibr" target="#b61">[62]</ref>, object detection <ref type="bibr" target="#b29">[30]</ref>, and action recognition <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b55">[56]</ref>.</p><p>For machine translation, Luong et al. study some simple attention methods. The attention model infers the attention weights at each step, and utilizes the weights to average the embedding vectors of the source words <ref type="bibr" target="#b34">[35]</ref>. For image caption, Xu et al. split an image into several parts with each part represented by the CNN features. To enable the decoder focus more on the informative parts of the image, at each time step, the attention module outputs attention weights with respect to those parts and the weighted average is taken as the input to the decoder at that time step <ref type="bibr" target="#b61">[62]</ref>. A similar idea is adopted in RGB-based action recognition in <ref type="bibr" target="#b38">[39]</ref>. The above attention models mainly focus on how to average a set of feature vectors with suitable weights to generate a pooled vector of the same dimension as the input to RNN. However, they do not consider the fine-grained adjustment based on different levels of importance across the input dimensions. In addition, they address attention at the network level, but not RNN block level.</p><p>For skeleton-based action recognition, Weng et al. propose a ST-NBNN model to identify the key temporal stages and spatial joints with bilinear classifier <ref type="bibr" target="#b57">[58]</ref>. Liu et al. propose a global context-aware attention module to assign different joints with different attention weights <ref type="bibr" target="#b33">[34]</ref>. Since the global information of a sequence is required to learn the attention weights, the system suffers from time delay. Song et al. propose a spatio-temporal attention model without requiring global information <ref type="bibr" target="#b41">[42]</ref>. A spatial attention subnetwork is used to modulate the skeleton input to selectively focus on discriminative joints before being fed into the main classification network. However, their designs of <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b41">[42]</ref>are not general and have not been extended to higher RNN layers. In contrast, our proposed enhanced RNN, with EleAttG included as a fundamental unit of RNN block, is general, simple yet effective, which can be applied to any RNN block/layer. Note that SENet <ref type="bibr" target="#b18">[19]</ref> and CBAM <ref type="bibr" target="#b58">[59]</ref> share similar high level ideas with our proposed EleAttG. Hu et al. design a squeeze-and-excitation attention module to explore the interdependencies of channels by enhancing the influence of important channels and suppressing the influence of trivial channels. Woo et al. decouple the attention module by performing spatial-wise attention and channel-wise attention separately. SENet and CBAM are designed to selectively focus on important channels or/and spatial positions for CNN-based networks. The investigations on efficient attention designs for RNNs are still under-exploited. We propose a simple yet effective attention gate (i.e., EleAttG) for RNNs, e.g., standard RNN, LSTM, and GRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Action Recognition and Gesture Recognition</head><p>Many traditional approaches focus on how to design efficient features to solve the problems of small inter-class variation, large view variations, and the modeling of complicated spatial and temporal evolution <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>For skeleton-based action recognition and gesture recognition, Du et al. propose a hierarchical RNN model with the hierarchical body partitions as inputs to different RNN layers <ref type="bibr" target="#b12">[13]</ref>. In order to exploit the co-occurrence of discriminative joints, Zhu et al. propose a deep regularized LSTM networks with group sparse regularization <ref type="bibr" target="#b69">[70]</ref>. In addition, they add inner dropout scheme to avoid overfitting. Shahroudy et al. propose a part-aware LSTM network by separating the original LSTM cell into five sub-cells corresponding to five major groups of the human body <ref type="bibr" target="#b37">[38]</ref>. Liu et al. propose a spatiotemporal LSTM structure to explore the contextual dependency of joints in both spatial and temporal domains <ref type="bibr" target="#b32">[33]</ref>. Li et al. propose an RNN tree network with a hierarchical structure to classify easy action classes at the lower layers and hard action classes at the higher layers <ref type="bibr" target="#b31">[32]</ref>. To address the large view variation of the captured data, Zhang et al. propose a view adaptive subnetwork which automatically determines the best observation viewpoints within an end-to-end network for recognition <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>. Núñez et al. <ref type="bibr" target="#b35">[36]</ref> combine CNN and LSTM together for action recognition and gesture recognition with a two stage training strategy. To explore the influence of contextual joints of one joint, Weng et al. <ref type="bibr" target="#b56">[57]</ref> propose a Deformable Pose Traversal Convolution scheme to traverse all the joints of a skeleton for both action recognition and gesture recognition.</p><p>For RGB-based action recognition, convolutional neural networks are usually used to exploit the spatial dependencies <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Considering the temporal dependencies is also important for video, some approaches simply averaged/multiplied the scores or features of the frames for fusion <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Some other approaches leverage RNNs to model temporal correlations, with frame-wise CNN features as input at every time slot <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sequential MNIST Classification</head><p>MNIST classification is a digital recognition task, which has been widely used to evaluate the effectiveness of different algorithms <ref type="bibr" target="#b28">[29]</ref>. Recently, Le et al. use the MNIST to evaluate the capability of modeling the long term dependencies of RNN structures by flattening the 784 pixels sequentially <ref type="bibr" target="#b27">[28]</ref>. It has become a widely used standard protocol to evaluate RNNs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p><p>Here we present some works which evaluate their improved RNNs on the sequential MNIST dataset. Le et al. propose a simple IRNN composed of rectified linear units which is initialized with the identity matrix to avoid gradient vanishing and exploding problems <ref type="bibr" target="#b27">[28]</ref>. Cooijmans et al. apply batch normalization to hidden-to-hidden transition to reduce the covariate shift among time steps <ref type="bibr" target="#b7">[8]</ref>. Li et al. treat all neurons in the same layer independently and build relationships between neurons at the next layer to address the gradient vanishing and exploding problems. <ref type="bibr" target="#b30">[31]</ref>.</p><p>Our proposed EleAttG is a fundamental unit that aims to enhance the capability of an RNN block. We will demonstrate its effectiveness on different tasks, including 3D skeletonbased action recognition, RGB-based action recognition, gesture recognition, and sequential MNIST classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW OF STANDARD RNN, LSTM, AND GRU</head><p>Recurrent neural networks are capable of modeling temporal dynamics and spatial correlations of a time sequence. The key is the "memory" mechanism which stores and updates historical information accumulated from previous time steps as time goes. To better understand the proposed EleAttG and its generalization capability, we briefly review the popular RNN structures, i.e., standard RNN, and another two gate-based invariants, including Long Short Term Memory (LSTM) <ref type="bibr" target="#b17">[18]</ref>, and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">[4]</ref>:</p><p>Standard RNN. For a standard RNN layer, the output response h t at time step t is calculated from the input x t of this layer and the output h t−1 of the last time slot as follows:</p><formula xml:id="formula_0">h t = tanh (W xh x t + W hh h t−1 + b h ) ,<label>(1)</label></formula><p>where W αβ denotes the weight matrix related with α and β, where α ∈ {x, h} and β ∈ {h}. b γ is the bias vector, where γ ∈ {h}. LSTM. The standard RNN suffers from the gradient vanishing problem because of insufficient, decaying error back flow <ref type="bibr" target="#b17">[18]</ref>. LSTM alleviates this problem by enforcing constant error flow through "constant error carrousels" within the cell unit c t . The input gate i t , forget gate f t , and output gate o t open and close access to the constant error flow. For an LSTM layer, the recursive computations of activations of the units are as follows:</p><formula xml:id="formula_1">i t = σ (W xi x t + W hi h t−1 + b i ) , f t = σ (W xf x t + W hf h t−1 + b f ) , c t = f t c t−1 + i t tanh(W xc x t +W hc h t−1 +b c ) , o t = σ (W xo x t + W ho h t−1 + b o ) , h t = o t tanh (c t ) ,<label>(2)</label></formula><p>where denotes the element-wise product. Note that i t is a vector representing the responses of a set of input gates of all LSTM neurons in this layer. f t , c t , o t , and h t are the output vectors of the forget gates, cells, output gates, hidden outputs of LSTM, respectively.</p><p>GRU. GRU is an architecture that is similar to but much simpler than LSTM. A GRU has two gates: reset gate r t and update gate z t . When the response of the reset gate is close to 0, the hidden state h t is forced to ignore the previous hidden state and reset with the current input only. The update gate controls how much information from the previous hidden state will be carried over to the current hidden state h t . The hidden state acts in a similar role to the memory cell in LSTM. For a GRU layer, the recursive computations of activations of the units are as follows:</p><formula xml:id="formula_2">r t = σ (W xr x t + W hr h t−1 + b r ) , z t = σ (W xz x t + W hz h t−1 + b z ) , h t = tanh (W xh x t + W hh (r t h t−1 ) + b h ) , h t = z t h t−1 + (1 − z t ) h t .</formula><p>(3) r t and z t are the output vectors of the reset gates, update gates of GRU. h t denotes the output vector of the hidden state.</p><p>Note that in the weight matrices, e.g., W xr , W xz and W xh in Eq. (3), the weights of the different dimensions of an input vector x t differ. However, for different samples, each weight matrix is shared. The limitation of W xr , W xz and W xh is the lack of content adaptiveness. In contrast, our proposed EleAttG is able to control the contribution of different elements of the input depending on the input content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ELEMENT-WISE-ATTENTION GATE FOR AN RNN BLOCK</head><p>For an RNN block, we propose an Element-wise-Attention Gate (EleAttG) to empower the RNN neurons to have attentiveness capabilities. The response of an EleAttG is a vector a t with the same dimension as the input x t of the RNNs, which is calculated as follows:</p><formula xml:id="formula_3">a t = φ (W xa x t + W ha h t−1 + b a ) ,<label>(4)</label></formula><p>where φ denotes the activation function of Sigmoid, i.e., φ(s) = 1/(1+e −s ). The current input x t and the previous hidden states h t−1 are used to determine the levels of importance of each element of the input x t . The attention response modulates the input to the updated input x t which is represented as:</p><formula xml:id="formula_4">x t = a t x t .<label>(5)</label></formula><p>The recursive computations of activations of the other units in the RNN block are then based on the updated input x t , instead of the original input x t , as illustrated in <ref type="figure">Fig. 1</ref>. EleAtt-GRU. For a GRU block with EleAttG (denoted as EleAtt-GRU), together with (5), the computations for an EleAtt-GRU block are as follows.</p><formula xml:id="formula_5">r t = σ (W xr x t + W hr h t−1 + b r ) , z t = σ (W xz x t + W hz h t−1 + b z ) , h t = tanh (W xh x t + W hh (r t h t−1 ) + b h ) , h t = z t h t−1 + (1 − z t ) h t .<label>(6)</label></formula><p>Similarly, we can get the recursive computations for EleAtt-sRNN and EleAtt-LSTM by setting x t to be x t in (1) and <ref type="bibr" target="#b1">(2)</ref>.</p><p>Note that in our design, in an RNN block/layer, all neurons share the same EleAttG (see <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula" target="#formula_5">(6)</ref> for the GRU block). Theoretically, each RNN neuron (instead of block) can have its own attention gate but the cost of computation complexity and number of parameters will largely increase, especially when the dimension of the input is large. We focus on the shared design in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>We perform comprehensive studies to evaluate the effectiveness of our proposed EleAtt-RNN with EleAttG by applying it to action recognition from 3D skeleton data, and RGB video, gesture recognition, and sequential MNIST classification, respectively.</p><p>To demonstrate the generalization capability of EleAttG, we add EleAttG to the standard RNN, LSTM, and GRU structures, respectively. We also evaluate the effectiveness of EleAttG on different types of input signals including skeleton data, CNN features, and raw image pixels.</p><p>For 3D skeleton-based action recognition we use three challenging datasets, i.e., the NTU RGB+D dataset (NTU) <ref type="bibr" target="#b37">[38]</ref>, the Northwestern-UCLA dataset (N-UCLA) <ref type="bibr" target="#b52">[53]</ref>, and the SYSU Human-Object Interaction dataset (SYSU) <ref type="bibr" target="#b19">[20]</ref>. The NTU dataset is currently the largest dataset with diverse subjects, various viewpoints and small inter-class differences. Therefore, our in-depth analyses are performed on the NTU dataset. For RGB-based action recognition, we take the CNN features extracted from existing, pre-trained models without finetuning as the input to the RNN-based recognition networks and evaluate the effectiveness of EleAttG on the RGB videos of the NTU and the JHMDB datasets <ref type="bibr" target="#b21">[22]</ref>. For gesture recognition, we use the Dynamic Hand Gesture 14/28 (DHG) dataset <ref type="bibr" target="#b8">[9]</ref>. For sequential MNIST classification, we use the MNIST handwritten digits benchmark dataset <ref type="bibr" target="#b28">[29]</ref>. We conduct most of our experiments based on GRU, as it has a simpler structure than LSTM and better performance than the standard RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>NTU RGB+D Dataset (NTU) <ref type="bibr" target="#b37">[38]</ref>. NTU is currently the largest RGB+D+Skeleton dataset for action recognition, which includes 56880 videos with more than 4 million frames in total. There are 60 kinds of action classes performed by 40 subjects. Each subject has 25 body joints and each joint has 3D coordinates. Three cameras placed in different positions are used to capture the data at the same time and there are over 80 views. We follow the standard protocols proposed in <ref type="bibr" target="#b37">[38]</ref> including the Cross Subject (CS) and Cross View (CV) settings. For the CS setting, 40 subjects are equally split into training and testing groups. For the CV setting, the samples of cameras 2 and 3 are adopted for training while those of camera 1 are for testing.</p><p>Northwestern-UCLA dataset (N-UCLA) <ref type="bibr" target="#b52">[53]</ref>. N-UCLA is a small RGB+D+Skeleton dataset including 1494 sequences performed by 10 subjects. It records 10 different actions in total. 20 joints with 3D coordinates are provided for each human body in this dataset. Following <ref type="bibr" target="#b52">[53]</ref>, we use samples from the first two cameras as training data, and the samples from the third camera as testing data.</p><p>SYSU Human-Object Interaction dataset (SYSU) <ref type="bibr" target="#b19">[20]</ref>. SYSU is a small RGB+D+Skeleton dataset, including 480 sequences performed by 40 different subjects. It contains a total of 12 actions. A subject has 20 joints with 3D coordinates. We follow the standard protocols proposed in <ref type="bibr" target="#b19">[20]</ref> for evaluation. They include two settings. For the Cross Subject (CS) setting, half of the subjects are used for training and the others for testing. For the Same Subject (SS) setting, half of the sequences of each subject are used for training and others for testing. The average performance of 30-fold cross validation is reported.</p><p>JHMDB dataset (JHMDB) <ref type="bibr" target="#b21">[22]</ref>. JHMDB is an RGB-based dataset which has 928 RGB videos with each video containing about 15-40 frames. It contains 21 actions performed by different actors. This dataset is challenging where the videos are collected on the Internet which also includes outdoor activities.</p><p>Dynamic Hand Gesture 14/28 dataset (DHG) <ref type="bibr" target="#b8">[9]</ref>. DHG is an Intel Real Sense captured dataset for gesture recognition, including 2800 sequences performed by 20 subjects. Each gesture is represented by 22 joints with 3D coordinates. It contains 14 classes of gestures and each gesture is collected in two ways: using the whole hand or just one finger. We follow the standard leave-one-subject-out cross-validation strategy under two evaluation protocols: 14 classes or 28 classes which treats the same gesture performed by one finger or whole hand separately <ref type="bibr" target="#b8">[9]</ref>.</p><p>MNIST dataset (MNIST) <ref type="bibr" target="#b28">[29]</ref>. MNIST is a handwritten digit dataset which is widely used to evaluate different methods, such as CNN and RNN. In order to utilize MNIST to evaluate RNN structure, we flat the images with size of 28 × 28 to 784-dimension vectors following <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b30">[31]</ref>. RNN treats each dimension as the input of a time slot. We follow the standard data split protocol for training and testing, containing 60000 training samples and 10000 testing samples. In addition, we randomly select 5000 samples from training sets for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We perform our experiments on the deep learning platform of Keras <ref type="bibr" target="#b5">[6]</ref> with Theano <ref type="bibr" target="#b0">[1]</ref> as the backend. Dropout <ref type="bibr" target="#b43">[44]</ref> with the probability of 0.5 is used to alleviate overfitting. Gradient clipping similar to <ref type="bibr" target="#b44">[45]</ref> is used by constraining the maximum amplitude of the gradient to 1. Adam <ref type="bibr" target="#b24">[25]</ref> is used to train the networks from end-to-end. The initial learning rate is set to 0.005 for 3D skeleton-based action recognition, gesture recognition, and sequential MNIST classification and 0.001 for RGB-based action recognition. During training, the learning rate will be reduced by a factor of 10 when the training accuracy does not increase for some epochs. We use crossentropy as the loss function to train all the networks.</p><p>For 3D skeleton-based action recognition and gesture recognition, similar to the classification network design in <ref type="bibr" target="#b63">[64]</ref>, we build our recognition networks by stacking three RNN layers with EleAttGs and one fully connected (FC) layer for classification. We use 100 RNN neurons in each layer. Considering the large differences in the size of the datasets, we set the batch size for the NTU, N-UCLA, SYSU and DHG datasets to 256, 32, 32, and 32, respectively. We use the sequence-level pre-processing method in <ref type="bibr" target="#b63">[64]</ref> by setting the body center in the first frame as the coordinate origin to make the system invariant to the initial position of human body. To improve the robustness to view variations at the sequence level, we perform data augmentation by randomly rotating the skeleton around the X, Y and Z axes by various degrees ranging from -17 to 17 during training for 3D skeleton-based action recognition. For the N-UCLA and SYSU datasets, we use the RNN models pre-trained on a sub NTU dataset, where each subject has 20 joints and only the actions performed by one subject are used, to initialize the baseline schemes and the proposed schemes.</p><p>For RGB-based action recognition, we feed an RNN network with the CNN features to further explore temporal dynamics. Because our aim is to evaluate whether the proposed EleAttG can generally improve recognition accuracy in different types of input signals, we extract CNN features using some available pre-trained models without finetuning for the specific dataset or task. For the JHMDB dataset, we use the TSN model from <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b60">[61]</ref> which was trained on the HMDB dataset <ref type="bibr" target="#b25">[26]</ref> to extract a 1024 dimensional feature for each frame. For the NTU dataset which has more videos, we take the ResNet50 model <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b4">[5]</ref> which has been pre-trained on ImageNet as our feature extractor (2048 dimensional feature for each frame). The implementation details of the RNN networks are similar to those discussed above. For the NTU dataset, we stack three EleAtt-GRU layers, with each layer consisting of 512 GRU neurons. For the JHMDB dataset, we use only one GRU layer (512 GRU neurons) with EleAttG to avoid overfitting, considering that the number of video samples is much smaller than that of the NTU dataset. We set the batch size for the NTU, and JHMDB datasets to 256 and 32, respectively.</p><p>For sequential MNIST classification, we build the same classification network as the skeleton-based recognition network mentioned above, i.e. stacking three RNN layers with EleAttGs and one fully connected layer (FC). We set the batch size to 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effectiveness of Element-wise-Attention-Gates</head><p>In this section, we first demonstrate the effectiveness of the proposed EleAttG . Then we demonstrate good generalization performance on different RNN structures and on different types of signals. Finally, comparisons with the state-of-theart approaches are performed followed by the visualization of the learned attention.</p><p>Effectiveness on GRU network. We evaluate the performance on 3D skeleton-based action recognition on the NTU and N-UCLA datasets. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the effectiveness of EleAttG on the baseline network which consists of 3 GRU layers. We find that our final scheme with three EleAtt-GRU layers ("3-EleAtt-GRU") outperforms the baseline scheme ("3-GRU(Baseline)") by 4.6%, 5.6%, and 4.7%, for the NTU-CS, NTU-CV, and N-UCLA, respectively. However, there is something different between the NTU dataset and N-UCLA dataset. 1) For the N-UCLA dataset, the improvement of "1-EleAtt-GRU+2-GRU" in comparison with "3-GRU(Baseline)" is not as obvious as that of the NTU dataset.</p><p>2) The improvement when increasing one GRU layer with EleAttG to two GRU layers with EleAttGs is significant in the N-UCLA dataset, i.e., 3.1% in accuracy. The potential reason is caused by the difference of action class distributions of the two datasets. We find that the diversity of action classes of the N-UCLA dataset (10 classes) is much smaller than that of the NTU dataset (60 classes). The important joint tends to be related with hands for most actions which can be learned by the dataset level weights. Therefore, the gain by using EleAttG of the first layer is smaller for the N-UCLA dataset. For the input features of the second layer, our EleAttG can help to better adaptively capture the discriminative feature dimensions and improve performance. Similar phenomena are also observed on the SYSU dataset which has less action class diversity.</p><p>The overall trend is same for both datasets. The performance grows when more GRU layers with EleAttGs are used. This indicates that, besides the attention on skeleton joints, the suitable attention on features can also significantly improve performance.</p><p>Generalization to other input signals. The proposed RNN block with EleAttG is generic and can be applied to different types of source data. To demonstrate this, we use (1) CNN features extracted from RGB frames as the input of the RNNs for RGB based action recognition, and (2) raw image pixels as the inputs of the RNNs for sequential MNIST classification.</p><p>For RGB-based action recognition, <ref type="table" target="#tab_0">Table I</ref> shows the performance comparisons on the NTU and JHMDB datasets. The implementation details have been described in Section V-B.  The "EleAtt-GRU" outperforms the "Baseline-GRU" by about 2-4% on the NTU dataset, and 2% on the JHMDB dataset. Note that the performance is not optimized since we have not used the fine-tuned CNN model on this dataset for this task. For sequential MNIST classification, <ref type="table" target="#tab_0">Table VII</ref> shows the performance comparison on the sequential MNIST dataset, where the inputs are raw pixels. The implementation details have been described in Section V-B. The "EleAtt-GRU" is superior to the "Baseline-GRU" by 0.4%. Note that it is difficulty to further improve the performance significantly since the final accuracy is already very high.</p><p>Generalization on various RNN structures. The proposed EleAttG is generic and can be applied to various types of RNN structures. We evaluate the effects of EleAttGs on three classical RNN structures, i.e., the standard RNN (sRNN), LSTM, and GRU respectively and show the results in <ref type="table" target="#tab_0">Table II</ref>. Compared with LSTM and GRU, the standard RNN neurons do not have the gating designs which control the contributions of the current input to the network. The EleAttG can elementwisely control the contribution of the current input, which remedies the lack of gate designs to some extent. The gate designs in LSTM and GRU can only control the information flow input-wisely. In contrast, the proposed EleAttGs are capable of modulating the input element-wisely, which empowers the attentiveness capability to RNNs. We can see that the adding of EleAttGs enhances performance significantly. Note that for sRNN, we build both the "Baseline(1-sRNN)" and our scheme using only one sRNN layer rather than three as those for LSTM and GRU, in considering that the three-layer sRNN baseline converges to a poorer performance, i.e., 33.6% and 42.8% for the CS and CV settings, which may be caused by the gradient vanishing of sRNN.</p><p>Comparisons with state-of-the-arts on skeleton-based  76.0 HBRNN-L <ref type="bibr" target="#b11">[12]</ref> 78.5 DA-Net <ref type="bibr" target="#b49">[50]</ref> 86.5</p><p>Baseline-GRU 84.3 EleAtt-GRU 89.0 EleAtt-GRU(aug.) 90.7  <ref type="bibr" target="#b19">[20]</ref> 75.5 76.9 VA-LSTM <ref type="bibr" target="#b63">[64]</ref> 76.9 77.5 SR-TSL <ref type="bibr" target="#b39">[40]</ref> 80.7 81.9</p><p>Baseline-GRU 82.1 82.1 EleAtt-GRU 84.9 84.5 EleAtt-GRU(aug.) 85.7 85.7 action recognition, gesture recognition, and sequential MNIST classification. For 3D skeleton-based human action recognition, a great deal of approaches have been proposed for enhancing the recognition accuracy as discussed in Section II-C. To achieve good performance, some approaches require complicated designs <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> while some others are specially designed considering human body characteristics <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>. In contrast, our proposed EleAttGs for RNN blocks are not specially designed for human body signals and can be used for other tasks directly. For action recognition, <ref type="table" target="#tab_0">Tables III, IV</ref>, and V show the performance comparisons with state-of-the-art approaches on the NTU, N-UCLA and SYSU datasets, respectively. "Baseline-GRU" denotes our baseline scheme which is built by stacking three GRU layers while "EleAtt-GRU" denotes our proposed scheme which replaces the GRU layers by the proposed GRU layers with EleAttGs. Implementation details can be found in Section V-B. "EleAtt-GRU(aug.)" denotes that data argu- <ref type="figure">Fig. 3</ref>: Visualization based on the attention responses of the first GRU layer for the actions of kicking, touching neck and making a phone call. For each joint, the size of the yellow circle indicates the learned level of importance. Here, the levels of importance for the X, Y , Z coordinates of a joint are summed for visualization. <ref type="figure">Fig. 4</ref>: Illustration of the statistical energy of each joint. The energy of one joint is the summation of the statistical energies of its three elements, i.e., X, Y , and Z coordinates. The size of the circle on each joint is proportional to the energy of that joint. The larger of the circle size, the larger of the energy. We only show the main joints of human body for clarify. mentation by rotating skeleton sequences is performed during training. We achieve the best performance in comparison with other state-of-the-art approaches on all three datasets. Our scheme "EleAtt-GRU" achieves significant gains over the baseline scheme "Baseline-GRU", of 4.6-5.6%, 4.7%, and 2.4-2.8% on the NTU, N-UCLA, and SYSU datasets, respectively.</p><p>For gesture recognition, even without any special designs for gesture like <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b56">[57]</ref>, we achieve the best results as shown in <ref type="table" target="#tab_0">Table VI</ref>. Our proposed "EleAtt-GRU" outperforms the current best method by 5.2% and 6.3% for the "C=14" and "C=28" settings, respectively.  Method Accuracy TANH-RNN <ref type="bibr" target="#b27">[28]</ref> 35.0 iRNN <ref type="bibr" target="#b27">[28]</ref> 97.0 uRNN <ref type="bibr" target="#b1">[2]</ref> 95.1 sTANH-RNN <ref type="bibr" target="#b67">[68]</ref> 98.1 LSTM <ref type="bibr" target="#b7">[8]</ref> 98.9 BN-LSTM <ref type="bibr" target="#b7">[8]</ref> 99.0 Skip GRU <ref type="bibr" target="#b2">[3]</ref> 97.6 Skip LSTM <ref type="bibr" target="#b2">[3]</ref> 97.3 IndRNN (6 layers) <ref type="bibr" target="#b30">[31]</ref> 99.0</p><p>Bseline-GRU 98.8 EleAtt-GRU 99.2</p><p>For sequential MNIST classification, we achieve the best performance in comparison with the state-of-the-arts as shown in <ref type="table" target="#tab_0">Table VII</ref>. Additionally, our scheme "EleAtt-GRU" is superior to the baseline scheme "Baseline-GRU" by 0.4%.</p><p>Visualization of the responses of EleAttG. To better <ref type="figure">Fig. 5</ref>: Visualization based on the attention responses of the first GRU layer for different handwritten digits. The first row denotes the original digit images and the second row denotes the corresponding attention responses of images pixels. Note that we use larger value to denote larger attention response value.</p><p>understand the learned element-wise attention, we observe the responses of the EleAttG in the first GRU layer for the skeleton-based action recognition and sequential MNIST classification.</p><p>For skeleton-based action recognition, in the first layer, the input (with dimension of 3×J) at a time step has J joints and each joint is represented by the X, Y , and Z coordinate values. Apparently, the physical meaning of the attention responses in the first layer is clear. However, in a higher layer, the EleAttG modulates the input features on each element which is more difficult to interpret and the attention value is hard to visualize. Therefore, we perform visualization based on the attention responses of the first GRU layer in <ref type="figure">Fig. 3</ref> for the actions consisting of kicking, touching the neck and making a phone call.</p><p>Actually, the absolute response values cannot represent the relative importance of different elements of input very well. The statistical energies of different elements of the original input are different. The statistical energy of one element denotes the mean of energy of all skeleton sequences, where the energy is defined as the square of the value of one element. Considering each joint has three elements (3D coordinates), we denote the sum of the statistical energies of its three elements as the statistical energy of one joint. We illustrate the statistical energy of each joint in <ref type="figure">Fig 4.</ref> For example, the foot joint, which is in general far away from the body center, has a higher energy than that of the body center joint. We can imagine that there is a static modulation a i on the i th element of the input, which can be calculated by the energy before and after the modulation. For the i th element of an sample j with attention value a i,j , we use the relative response value a i,j = a i,j /a i for visualization to better reflect importance among joints. Note that the sum of the relative responses for the X, Y , and Z of a joint is utilized for visualization. For the actions of touching neck and making a phone call which are highly concerned with the joints on the arms and heads, thus, the relative attention on those joints are larger. For kicking, the relative attention on the legs is large. These are consistent with a human's perception. For sequential MNIST classification, similar to the skeleton- based action recognition, we only perform visualization based on the attention responses of the first GRU layer and show some results in <ref type="figure">Fig. 5</ref> for the same reason. Different from skeleton-based action recognition, we directly use the attention response value a i,j for visualization. Note that the input to the first layer at each time slot corresponds to the pixel values of a position and the pixels are scanned pixel by pixel. The response value of the EleAttG reflects the importance level of the corresponding image pixel directly very well. From <ref type="figure">Fig.  5</ref>, we find that the pixels with respect to the handwritten digit are more important than other pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussions</head><p>Training curves on the NTU and MNIST datasets. <ref type="figure" target="#fig_2">Fig.  6 (a)</ref> shows the training loss curves for the training and validation sets on the NTU dataset with CS setting during the training precedure for the proposed "EleAtt-GRU" and the baseline "Baseline-GRU", respectively. By adding the EleAttGs, "EleAtt-GRU" takes fewer epochs than "Baseline-GRU" to achieve the same training and validation losses, especially at the beginning of training. In addition, "EleAtt-GRU" is consistently better than the "Baseline-GRU". Similar phenomenon can be found on the sequential MNIST dataset in <ref type="figure" target="#fig_2">Fig. 6 (b)</ref>. The modulation of input can control the information flow of each input element adaptively and make the subsequent learning within the neurons much easier.</p><p>Applying EleAttG to x t or h t−1 ? We experimentally show the effectiveness of EleAttG when it operates on the hidden state h t−1 and the input vector x t respectively in <ref type="table" target="#tab_0">Table VIII</ref>.  Note that for a time slot t, besides the input vector x t , the hidden state of the last time slot h t−1 is another input vector. "Baseline-GRU" denotes the baseline model that is built by three GRU layers without EleAttG.</p><p>"EleAtt-GRU(h t−1 )" denotes the model that is built by three GRU layers with EleAttG, where the EleAttG is used to modify the hidden state h t−1 , which is also the input vector of the current time slot.</p><p>"EleAtt-GRU(x t )" denotes the model that is built by three GRU layers with EleAttG, where the EleAttG is used to modify the input vector x t .</p><p>"EleAtt-GRU(h t−1 &amp; x t )" denotes the model that is built by three GRU layers with two EleAttGs, which are used to modify the hidden state h t−1 and the input vector x t , respectively.</p><p>From <ref type="table" target="#tab_0">Table VIII</ref>, we observe that accuracy improves when the EleAttG is used to the hidden state h t−1 or the input vector x t , respectively. "EleAtt-GRU(x t )" outperforms "EleAtt-GRU(h t−1 )" by 0.5% and 1.0% for the CS and CV settings, which demonstrates that applying the EleAttG to the input vector x t is more effective. However, when applying the EleAttGs to both the hidden state h t−1 and the input vector x t simultaneously, performance does not improve further and even becomes poorer in comparison with the cases when applying the EleAttG to the hidden state h t−1 or the input vector x t , respectively. One potential reason is that the number of parameters increases when applying two EleAttGs simultaneously in comparison with the model using only one EleAttG, which makes it harder to optimize the model. In addition, the physical meaning of applying modulation on x t is more clear than that on h t−1 . On the other hand, whenever suitable attention is applied to the input x t , the output h t may already be the attended feature for the next time slot t + 1.</p><p>Relaxing the sum-to-1 constraint on EleAttG responses. Unlike other works <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b33">[34]</ref>, we do not use Softmax, which enforces the sum of attention responses to be 1, as the activation function of EleAttG. On the contrary, we use the Sigmoid activation function to avoid introducing mutual influence of elements. If the sum-to-1 constraint is not relaxed, the attention response of the k th element will be affected by the changes of other elements' response values even when the levels of importance of this element are the same over consecutive time slots. Especially for a sequence, the constraint could break the continuity of the features/inputs after applying attention.</p><p>We show the experimental comparisons between the cases with the sum-to-1 constraint (w/constraint) by using Softmax, and our case without such constraint (wo/constraint) by using Sigmoid in <ref type="table" target="#tab_0">Table IX</ref>. "EleAttG-n th " denotes that the n th GRU layer uses the GRU with EleAttG while the other layers still use the original GRU. "Baseline-GRU" denotes the baseline scheme with three GRU layers. We can see that wo/constraint always performs better than that with constraint w/constraint. Specially, adding EleAttG with constraint on the second or the third layer even decreases the accuracy by about 2.4-3.2% in comparison with the baselines.</p><p>Different from the input gate of LSTM model. An input gate is designed to control the contribution of the current input to memory versus the contribution of the historical information controlled by a forget gate. It uses a scalar (rather than a vector) to control the contribution of the current input TABLE X: Comparisons about "EleAtt-GRU" and "EleAtt 1 -GRU" on the NTU dataset in terms of accuracy (%). "EleAtt 1 -GRU" denotes all elements of the input vector share the same importance level.  vector. The proposed element-wise attention gate (EleAttG) applies element-wise adaptive modulation to the input vector to achieve element-wise attention before further processing in the RNN neurons. EleAttG is designed to control the contribution of each element of the current input to the RNN by suppressing the amplitudes of the unimportant elements while preserving the amplitudes of the important elements. We can modify our proposed EleAttG to produce a scalar to represent the importance of the current input vector, where all elements of the input vector share the same importance value. We denote the modified EleAttG as EleAttG 1 . The scheme with EleAttG is denoted as "EleAtt-GRU" and the scheme with EleAttG 1 is denoted as "EleAtt 1 -GRU". Baseline-GRU denotes the schemes without EleAttGs. We compare the "Baseline-GRU", "EleAtt 1 -GRU", and "EleAtt-GRU" in <ref type="table">Table  X</ref>. We observe that both "EleAtt-GRU" and "EleAtt 1 -GRU" are superior to the "Basleine-GRU". Our proposed "EleAtt-GRU" outperforms "EleAtt 1 -GRU" by 3.0% and 3.2% for the CS and CV settings, respectively.</p><p>Number of parameters versus performance. For an RNN block, the adding of an EleAttG increases the number of parameters. One may wonder whether the performance is increased by EleAttG or just the additional parameters. We analyze the influence of parameters as follows.</p><p>Taking a GRU block consisting of N neurons with the input dimension of D as an example, the numbers of parameters for the original GRU block and the proposed EleAttG-GRU block are 3N (D + N + 1), and 3N (D + N + 1) + D(D + N + 1), respectively. We calculate the computational complexity by counting the number of floating-point operations (FLOPs) including all multiplication and addition operations. At a time slot, adding attention to the layer as in 4 and 5 takes D(D + N + 1) multiplication operations and D(D + N ) addition operations. Then the complexity increases from N (6D+6N +5) to N (6D+6N +5)+D(2D+2N +1), which is approximately proportional to the number of parameters. <ref type="table" target="#tab_0">Table XI</ref> shows the effect of the number of parameters under different experimental settings on the NTU dataset. Note that "m-GRU(n)" denotes the baseline scheme which is built by m GRU blocks (layers) with each layer composed of n neurons. "m-EleAtt-GRU(100)" denotes our scheme which includes m EleAtt-GRU layers with each layer composed of 100 neurons. We can see that the performance increases only a little when more neurons ("2-GRU(128)") or more layers ("3-GRU(100)") are used in comparison with the baseline "2-GRU(100)". In contrast, our scheme "2-EleAtt-GRU(100)", achieves significant gains of 3.1-4.1% in comparison with "2-GRU(100)". Similar observations are made in three-layer cases. With similar numbers of parameters, adding EleAttG is much more effective than increasing the number of neurons or the number of layers. It demonstrates that EleAttG significantly boosts performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we propose a simple yet effective EleAttG to empower the neurons in recurrent neural networks to have the attentiveness capability. It can explore the varying importance of different element of the input. Experiments show that our proposed EleAttG can be used in any RNN structures (e.g standard RNN, LSTM and GRU), any layers of the multilayer RNN networks, and different types of input signals (e.g skeleton data, CNN features, and raw image pixels). Abundant experiments show that the proposed EleAttG boosts the performance significantly. We expect that, as a fundamental unit, the proposed EleAttG will be effective for improving many RNN-based learning tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Effectiveness of our proposed EleAttG on the three-layer GRU network for 3D skeleton-based human action recognition for the CS and CV settings of the NTU dataset, and the N-UCLA dataset. "m-EleAtt-GRU+n-GRU" denotes that the first m layers are EleAtt-GRU layers and the remaining n layers are the original GRU layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Loss curves during training on (a) the CS setting of the NTU dataset, and (b) the sequential MNIST dataset with respect to the proposed scheme "EleAtt-GRU" and the baseline scheme "Baseline-GRU".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Effectiveness of EleAttGs in the GRU network for RGB-based action recognition on the NTU and JHMDB datasets. Here, CNN features for each RGB frame is taken as the input to the GRU network. The performance is evaluated in terms of recognition accuracy(%).</figDesc><table><row><cell>Dataset</cell><cell cols="2">NTU</cell><cell></cell><cell cols="2">JHMDB</cell><cell></cell></row><row><cell></cell><cell>CS</cell><cell>CV</cell><cell>Split1</cell><cell>Split2</cell><cell>Split3</cell><cell>Average</cell></row><row><cell>Baseline-GRU</cell><cell>61.3</cell><cell>66.8</cell><cell>60.6</cell><cell>59.2</cell><cell>62.9</cell><cell>60.9</cell></row><row><cell>EleAtt-GRU</cell><cell>63.3</cell><cell>70.6</cell><cell>64.5</cell><cell>59.2</cell><cell>65.0</cell><cell>62.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Effectiveness of EleAttG on three types of RNN structures. We evaluate the recognition performance of RNN networks with EleAttGs for the CS and CV settings of the NTU dataset. "EleAtt-X" denotes the scheme with EleAttGs based on the RNN structure of X.</figDesc><table><row><cell>RNN structure</cell><cell>Scheme</cell><cell>CS</cell><cell>CV</cell></row><row><cell>Standard RNN</cell><cell>Baseline(1-sRNN) EleAtt-sRNN</cell><cell>51.6 61.6</cell><cell>57.6 67.2</cell></row><row><cell>LSTM</cell><cell>Baseline(3-LSTM) EleAtt-LSTM</cell><cell>77.2 78.4</cell><cell>83.0 85.0</cell></row><row><cell>GRU</cell><cell>Baseline(3-GRU) EleAtt-GRU</cell><cell>75.2 79.8</cell><cell>81.5 87.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Performance comparisons on the NTU dataset in terms of accuracy (%).</figDesc><table><row><cell>Method</cell><cell>CS</cell><cell>CV</cell></row><row><cell>Skeleton Quads [14]</cell><cell>38.6</cell><cell>41.4</cell></row><row><cell>Lie Group [48]</cell><cell>50.1</cell><cell>52.8</cell></row><row><cell>Dynamic Skeletons [20]</cell><cell>60.2</cell><cell>65.2</cell></row><row><cell>HBRNN-L [13]</cell><cell>59.1</cell><cell>64.0</cell></row><row><cell>Part-aware LSTM [38]</cell><cell>62.9</cell><cell>70.3</cell></row><row><cell>ST-LSTM + Trust Gate [33]</cell><cell>69.2</cell><cell>77.7</cell></row><row><cell>STA-LSTM [43]</cell><cell>73.4</cell><cell>81.2</cell></row><row><cell>GCA-LSTM [34]</cell><cell>74.4</cell><cell>82.8</cell></row><row><cell>URNN-2L-T [32]</cell><cell>74.6</cell><cell>83.2</cell></row><row><cell>Clips+CNN+MTLN [24]</cell><cell>79.6</cell><cell>84.8</cell></row><row><cell>VA-LSTM [64]</cell><cell>79.4</cell><cell>87.2</cell></row><row><cell>Baseline-GRU</cell><cell>75.2</cell><cell>81.5</cell></row><row><cell>EleAtt-GRU</cell><cell>79.8</cell><cell>87.1</cell></row><row><cell>EleAtt-GRU(aug.)</cell><cell>80.7</cell><cell>88.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Performance comparisons on the N-UCLA dataset in terms of accuracy (%).</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>HOJ3D [60]</cell><cell>54.5</cell></row><row><cell>AE [52]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Performance comparisons on the SYSU dataset in terms of accuracy (%).</figDesc><table><row><cell>Method</cell><cell>SS</cell><cell>CS</cell></row><row><cell>LAFF [21]</cell><cell>-</cell><cell>54.2</cell></row><row><cell>DS</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>Performance comparisons on the gesture recognition dataset DHG in terms of accuracy (%).</figDesc><table><row><cell>Method</cell><cell>C=14</cell><cell>C=28</cell></row><row><cell>Skeleton Quads [14]</cell><cell>84.5</cell><cell>79.4</cell></row><row><cell>SoCJ+HoHD+HoWR [9]</cell><cell>83.1</cell><cell>80.0</cell></row><row><cell>CNN+LSTM [36]</cell><cell>85.6</cell><cell>81.1</cell></row><row><cell>D-Pose Traversal Conv [57]</cell><cell>85.8</cell><cell>80.2</cell></row><row><cell>Baseline-GRU</cell><cell>90.0</cell><cell>85.9</cell></row><row><cell>EleAtt-GRU</cell><cell>91.0</cell><cell>86.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>Performance comparisons on the sequential MNIST dataset in terms of accuracy (%).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII :</head><label>VIII</label><figDesc>Effectiveness of EleAttG on the hidden state h t−1 and the input vector x t in terms of accuracy (%) on the NTU dataset.</figDesc><table><row><cell>Method</cell><cell>CS</cell><cell>CV</cell></row><row><cell>Baseline-GRU</cell><cell>75.2</cell><cell>81.5</cell></row><row><cell>EleAtt-GRU(h t−1 )</cell><cell>79.3</cell><cell>86.1</cell></row><row><cell>EleAtt-GRU(xt)</cell><cell>79.8</cell><cell>87.1</cell></row><row><cell>EleAtt-GRU(h t−1 &amp; xt)</cell><cell>79.0</cell><cell>85.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX :</head><label>IX</label><figDesc>Performance comparisons about relaxing the constraint to EleAttG on the NTU dataset in terms of accuracy (%).</figDesc><table><row><cell>Protocols</cell><cell>Method</cell><cell cols="4">Baseline-GRU EleAttG-1 st EleAttG-2 nd EleAttG-3 rd</cell></row><row><cell>CS</cell><cell>w/ constraint wo/ constrain</cell><cell>75.2 75.2</cell><cell>75.0 78.7</cell><cell>72.7 77.3</cell><cell>72.0 76.4</cell></row><row><cell>CV</cell><cell>w/ constraint wo/ constrain</cell><cell>81.5 81.5</cell><cell>83.7 84.9</cell><cell>79.1 83.5</cell><cell>78.8 82.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XI :</head><label>XI</label><figDesc>Effect of the number of parameters on the NTU dataset.</figDesc><table><row><cell>Scheme</cell><cell># Parameters</cell><cell>CS</cell><cell>CV</cell></row><row><cell>2-GRU(100)</cell><cell>0.14M</cell><cell cols="2">75.5 81.4</cell></row><row><cell>2-GRU(128)</cell><cell>0.21M</cell><cell cols="2">75.8 81.7</cell></row><row><cell>3-GRU(100)</cell><cell>0.20M</cell><cell cols="2">75.2 81.5</cell></row><row><cell>3-GRU(128)</cell><cell>0.31M</cell><cell cols="2">76.5 81.3</cell></row><row><cell>2-EleAtt-GRU(100)</cell><cell>0.20M</cell><cell cols="2">78.6 85.5</cell></row><row><cell>3-EleAtt-GRU(100)</cell><cell>0.28M</cell><cell cols="2">79.8 87.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belopolsky</surname></persName>
		</author>
		<title level="m">A python framework for fast computation of mathematical expressions. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skip rnn: learning to skip state updates in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Campos Camunez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Torres</forename><surname>Viñals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50weightstfdimorderingtfkernels.h5" />
		<title level="m">Resnet50 model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NIPSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Recurrent batch normalization. arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vandeborre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep temporal linear encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Representation learning of temporal dynamics for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3010" to="3022" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time rgb-d activity prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attentive contexts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="944" to="954" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive rnn tree for large-scale human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Convolutional neural networks and long short-term memory for skeleton-based human activity and hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Vélez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PR</publisher>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="80" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lstm</surname></persName>
		</author>
		<ptr target="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spatio-temporal attentionbased lstm networks for 3d action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3459" to="3471" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dividing and aggregating network for multi-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="914" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graph based skeleton motion representation and similarity measurement for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Hierarchical attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deformable pose traversal convolution for 3d action and gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spatio-temporal naive-bayes nearestneighbor (st-nbnn) for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://github.com/yjxiong/temporal-segment-networks" />
		<title level="m">TSN model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sr-lstm: State refinement for lstm towards pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Adding attentiveness to the neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Architectural complexity measures of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Hsa-rnn: Hierarchical structure-adaptive rnn for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Cooccurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
