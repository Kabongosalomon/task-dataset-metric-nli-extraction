<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
							<email>theo.trouillon@xrce.xerox.com</email>
							<affiliation key="aff2">
								<orgName type="department">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Université Grenoble Alpes</orgName>
								<address>
									<addrLine>621 avenue Centrale</addrLine>
									<postCode>38400</postCode>
									<settlement>Saint Martin d&apos;Hères</settlement>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>Gower St</addrLine>
									<postCode>WC1E 6BT</postCode>
									<settlement>London</settlement>
									<country key="GB">UNITED KINGDOM</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>Gower St</addrLine>
									<postCode>WC1E 6BT</postCode>
									<settlement>London</settlement>
									<country key="GB">UNITED KINGDOM</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Université Grenoble Alpes</orgName>
								<address>
									<addrLine>621 avenue Centrale</addrLine>
									<postCode>38400</postCode>
									<settlement>Saint Martin d&apos;Hères</settlement>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>Gower St</addrLine>
									<postCode>WC1E 6BT</postCode>
									<settlement>London</settlement>
									<country key="GB">UNITED KINGDOM</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Bouchard@cs Ac</forename><surname>Ucl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uk</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">S.RIEDEL@CS.UCL.AC.UḰ</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">ERIC.GAUSSIER@IMAG.FR</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Web-scale knowledge bases (KBs) provide a structured representation of world knowledge, with projects such as DBPedia <ref type="bibr" target="#b1">(Auer et al., 2007)</ref>, Freebase <ref type="bibr" target="#b5">(Bollacker et al., 2008)</ref> or the Google Knowledge Vault <ref type="bibr" target="#b9">(Dong et al., 2014)</ref>. They enable a wide range of applications such as recommender systems, question answering or automated personal agents. The incompleteness of these KBs has stimulated research into predicting missing entries, a task known as link prediction that is one of the main problems in Statistical Relational Learning (SRL, <ref type="bibr" target="#b11">Getoor &amp; Taskar, 2007)</ref>.</p><p>KBs express data as a directed graph with labeled edges (relations) between nodes (entities). Natural redundancies among the recorded relations often make it possible to fill in the missing entries of a KB. As an example, the relation CountryOfBirth is not recorded for all entities, but it can easily be inferred if the relation CityOfBirth is known. The goal of link prediction is the automatic discovery of such regularities. However, many relations are non-deterministic: the combination of the two facts IsBornIn(John,Athens) and IsLocatedIn(Athens,Greece) does not always imply the fact HasNationality(John,Greece). Hence, it is required to handle other facts involving these relations or entities in a probabilistic fashion.</p><p>To do so, an increasingly popular method is to state the link prediction task as a 3D binary tensor completion problem, where each slice is the adjacency matrix of one relation type in the knowledge graph. Completion based on low-rank factorization or embeddings has been popularized with the Netflix challenge <ref type="bibr" target="#b14">(Koren et al., 2009)</ref>. A partially observed matrix or tensor is decomposed into a product of embedding matrices with much smaller rank, resulting in fixed-dimensional vector representations for each entity and relation in the database. For a given fact r(s,o) in which subject s is linked to object o through relation r, the score can then be recovered as a multi-linear product between the embedding vectors of s, r and o <ref type="bibr" target="#b18">(Nickel et al., 2016a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary relations in KBs exhibit various types of patterns: hierarchies and compositions like FatherOf, OlderThan</head><p>or IsPartOf-with partial/total, strict/non-strict orders-and equivalence relations like IsSimilarTo. As described in <ref type="bibr" target="#b6">Bordes et al. (2013a)</ref>, a relational model should (a) be able to learn arXiv:1606.06357v1 [cs.AI] 20 Jun 2016 all combinations of these properties, namely reflexivity/irreflexivity, symmetry/antisymmetry and transitivity, and (b) be linear in both time and memory in order to scale to the size of present day KBs, and keep up with their growth.</p><p>Dot products of embeddings scale well and can naturally handle both symmetry and (ir-)reflexivity of relations; using an appropriate loss function even enables transitivity <ref type="bibr" target="#b8">(Bouchard et al., 2015)</ref>. However, dealing with antisymmetric relations has so far almost always implied an explosion of the number of parameters <ref type="bibr" target="#b16">(Nickel et al., 2011;</ref><ref type="bibr" target="#b21">Socher et al., 2013</ref>) (see <ref type="table" target="#tab_0">Table 1</ref>), making models prone to overfitting. Finding the best ratio between expressiveness and parameter space size is the keystone of embedding models.</p><p>In this work we argue that the standard dot product between embeddings can be a very effective composition function, provided that one uses the right representation. Instead of using embeddings containing real numbers we discuss and demonstrate the capabilities of complex embeddings. When using complex vectors, i.e. vectors with entries in C, the dot product is often called the Hermitian (or sesquilinear) dot product, as it involves the conjugate-transpose of one of the two vectors. As a consequence, the dot product is not symmetric any more, and facts about antisymmetric relations can receive different scores depending on the ordering of the entities involved. Thus complex vectors can effectively capture antisymmetric relations while retaining the efficiency benefits of the dot product, that is linearity in both space and time complexity.</p><p>The remainder of the paper is organized as follows. We first justify the intuition of using complex embeddings in the square matrix case in which there is only a single relation between entities. The formulation is then extended to a stacked set of square matrices in a third-order tensor to represent multiple relations. We then describe experiments on large scale public benchmark KBs in which we empirically show that this representation leads not only to simpler and faster algorithms, but also gives a systematic accuracy improvement over current state-of-the-art alternatives.</p><p>To give a clear comparison with respect to existing approaches using only real numbers, we also present an equivalent reformulation of our model that involves only real embeddings. This should help practitioners when implementing our method, without requiring the use of complex numbers in their software implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Relations as Real Part of Low-Rank Normal Matrices</head><p>In this section we discuss the use of complex embeddings for low-rank matrix factorization and illustrate this by considering a simplified link prediction task with merely a single relation type.</p><p>Understanding the factorization in complex space leads to a better theoretical understanding of the class of matrices that can actually be approximated by dot products of embeddings. These are the so-called normal matrices for which the left and right embeddings share the same unitary basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Modelling Relations</head><p>Let E be a set of entities with |E| = n. A relation between two entities is represented as a binary value Y so ∈ {−1, 1}, where s ∈ E is the subject of the relation and o ∈ E its object. Its probability is given by the logistic inverse link function:</p><formula xml:id="formula_0">P (Y so = 1) = σ(X so )<label>(1)</label></formula><p>where X ∈ R n×n is a latent matrix of scores, and Y the partially observed sign matrix.</p><p>Our goal is to find a generic structure for X that leads to a flexible approximation of common relations in real world KBs. Standard matrix factorization approximates X by a matrix product U V T , where U and V are two functionally independent n × K matrices, K being the rank of the matrix. Within this formulation it is assumed that entities appearing as subjects are different from entities appearing as objects. This means that the same entity will have two different embedding vectors, depending on whether it appears as the subject or the object of a relation. This extensively studied type of model is closely related to the singular value decomposition (SVD) and fits well to the case where the matrix X is rectangular. However, in many link prediction problems, the same entity can appear as both subject and object. It then seems natural to learn joint embeddings of the entities, which entails sharing the embeddings of the left and right factors, as proposed by several authors to solve the link prediction problem <ref type="bibr" target="#b16">(Nickel et al., 2011;</ref><ref type="bibr" target="#b7">Bordes et al., 2013b;</ref><ref type="bibr" target="#b25">Yang et al., 2015)</ref>.</p><p>In order to use the same embedding for subjects and objects, researchers have generalised the notion of dot products to scoring functions, also known as composition functions, that combine embeddings in specific ways. We briefly recall several examples of scoring functions in Table 1, as well as the extension proposed in this paper.</p><p>Using the same embeddings for right and left factors boils down to Eigenvalue decomposition:</p><formula xml:id="formula_1">X = EW E −1 .<label>(2)</label></formula><p>It is often used to approximate real symmetric matrices such as covariance matrices, kernel functions and distance or similarity matrices. In these cases all eigenvalues and eigenvectors live in the real space and E is orthogonal: <ref type="bibr" target="#b7">(Bordes et al., 2013b)</ref> ||</p><formula xml:id="formula_2">Model Scoring Function Relation parameters O time O space RESCAL (Nickel et al., 2011) e T s W r e o W r ∈ R K 2 O(K 2 ) O(K 2 ) TransE</formula><formula xml:id="formula_3">(e s + w r ) − e o || p w r ∈ R K O(K) O(K)</formula><p>NTN <ref type="bibr" target="#b21">(Socher et al., 2013)</ref> u</p><formula xml:id="formula_4">T r f (e s W [1..D] r e o + V r e s e o + b r ) W r ∈ R K 2 D , b r ∈ R K V r ∈ R 2KD , u r ∈ R K O(K 2 D) O(K 2 D)</formula><p>DistMult <ref type="bibr" target="#b25">(Yang et al., 2015)</ref> &lt; w r , e s , e o &gt; w r ∈ R K O(K) O(K) HolE <ref type="bibr" target="#b19">(Nickel et al., 2016b)</ref> w </p><formula xml:id="formula_5">T r (F −1 [F[e s ] F[e o ]])) w r ∈ R K O(K log K) O(K) ComplEx Re(&lt; w r , e s ,ē o &gt;) w r ∈ C K O(K) O(K)</formula><formula xml:id="formula_6">E T = E −1 .</formula><p>We are in this work however explicitly interested in problems where matrices -and thus the relations they represent -can also be antisymmetric. In that case eigenvalue decomposition is not possible in the real space; there only exists a decomposition in the complex space where embeddings x ∈ C K are composed of a real vector component Re(x) and an imaginary vector component Im(x). With complex numbers, the dot product, also called the Hermitian product, or sesquilinear form, is defined as:</p><formula xml:id="formula_7">u, v :=ū T v<label>(3)</label></formula><p>where u and v are complex-valued vectors, i.e. u = Re(u) + iIm(u) with Re(u) ∈ R K and Im(u) ∈ R K corresponding to the real and imaginary parts of the vector u ∈ C K , and i denoting the square root of −1. We see here that one crucial operation is to take the conjugate of the first vector:ū = Re(u) − iIm(u). A simple way to justify the Hermitian product for composing complex vectors is that it provides a valid topological norm in the induced vectorial space. For example,x T x = 0 implies x = 0 while this is not the case for the bilinear form x T x as there are many complex vectors for which x T x = 0.</p><p>Even with complex eigenvectors E ∈ C n×n , the inversion of E in the eigendecomposition of Equation (2) leads to computational issues. Fortunately, mathematicians defined an appropriate class of matrices that prevents us from inverting the eigenvector matrix: we consider the space of normal matrices, i.e. the complex n × n matrices X, such that XX T =X T X. The spectral theorem for normal matrices states that a matrix X is normal if and only if it is unitarily diagonalizable:</p><formula xml:id="formula_8">X = EWĒ T<label>(4)</label></formula><p>where W ∈ C n×n is the diagonal matrix of eigenvalues (with decreasing modulus) and E ∈ C n×n is a unitary matrix of eigenvectors, withĒ representing its complex conjugate.</p><p>The set of purely real normal matrices includes all symmetric and antisymmetric sign matrices (useful to model hierarchical relations such as IsOlder), as well as all orthogonal matrices (including permutation matrices), and many other matrices that are useful to represent binary relations, such as assignment matrices which represent bipartite graphs. However, far from all matrices expressed as EWĒ T are purely real, and equation 1 requires the scores X to be purely real. So we simply keep only the real part of the decomposition:</p><formula xml:id="formula_9">X = Re(EWĒ T ) .<label>(5)</label></formula><p>In fact, performing this projection on the real subspace allows the exact decomposition of any real square matrix X and not only normal ones, as shown by <ref type="bibr" target="#b23">Trouillon et al. (2016)</ref>.</p><p>Compared to the singular value decomposition, the eigenvalue decomposition has two key differences:</p><p>• The eigenvalues are not necessarily positive or real;</p><p>• The factorization (5) is useful as the rows of E can be used as vectorial representations of the entities corresponding to rows and columns of the relation matrix X. Indeed, for a given entity, its subject embedding vector is the complex conjugate of its object embedding vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Low-Rank Decomposition</head><p>In a link prediction problem, the relation matrix is unknown and the goal is to recover it entirely from noisy observations. To enable the model to be learnable, i.e. to generalize to unobserved links, some regularity assumptions are needed. Since we deal with binary relations, we assume that they have low sign-rank. The sign-rank of a sign matrix is the smallest rank of a real matrix that has the same sign-pattern as Y :</p><formula xml:id="formula_10">rank ± (Y ) = min A∈R m×n {rank(A)|sign(A) = Y } .<label>(6)</label></formula><p>This is theoretically justified by the fact that the signrank is a natural complexity measure of sign matrices <ref type="bibr" target="#b15">(Linial et al., 2007)</ref> and is linked to learnability <ref type="bibr" target="#b0">(Alon et al., 2015)</ref>, and empirically confirmed by the wide success of factorization models <ref type="bibr" target="#b18">(Nickel et al., 2016a)</ref>.</p><p>If the observation matrix Y is low-sign-rank, then our model can decompose it with a rank at most the double of the sign-rank of Y . That is, for any Y ∈ {−1, 1} n×n , there always exists a matrix X = Re(EWĒ T ) with the same sign pattern sign(X) = Y , where the rank of EWĒ T is at most twice the sign-rank of Y <ref type="bibr" target="#b23">(Trouillon et al., 2016)</ref>.</p><p>Although twice sounds bad, this is actually a good upper bound. Indeed, the sign-rank is often much lower than the rank of Y . For example, the rank of the n × n identity matrix I is n, but rank ± (I) = 3 <ref type="bibr" target="#b0">(Alon et al., 2015)</ref>. By permutation of the columns 2j and 2j + 1, the I matrix corresponds to the relation marriedTo, a relation known to be hard to factorize <ref type="bibr" target="#b17">(Nickel et al., 2014</ref>). Yet our model can express it in rank 6, for any n.</p><p>By imposing a low-rank K n on EWĒ T , only the first K values of diag(W ) are non-zero. So we can directly have E ∈ C n×K and W ∈ C K×K . Individual relation scores X so between entities s and o can be predicted through the following product of their embeddings e s , e o ∈ C K :</p><formula xml:id="formula_11">X so = Re(e T s Wē o ) .<label>(7)</label></formula><p>We summarize the above discussion in three points:</p><p>1. Our factorization encompasses all possible binary relations.</p><p>2. By construction, it accurately describes both symmetric and antisymmetric relations.</p><p>3. Learnable relations can be efficiently approximated by a simple low-rank factorization, using complex numbers to represent the latent factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Application to Binary Multi-Relational Data</head><p>The previous section focused on modeling a single type of relation; we now extend this model to multiple types of relations. We do so by allocating an embedding w r to each relation r, and by sharing the entity embeddings across all relations.</p><p>Let R and E be the set of relations and entities present in the KB. We want to recover the matrices of scores X r for all the relations r ∈ R. Given two entities s and o ∈ E, the log-odd of the probability that the fact r(s,o) is true is:</p><formula xml:id="formula_12">P (Y rso = 1) = σ(φ(r, s, o; Θ))<label>(8)</label></formula><p>where φ is a scoring function that is typically based on a factorization of the observed relations and Θ denotes the parameters of the corresponding model. While X as a whole is unknown, we assume that we observe a set of true and false facts {Y rso } r(s,o)∈Ω ∈ {−1, 1} |Ω| , corresponding to the partially observed adjacency matrices of different relations, where Ω ⊂ R ⊗ E ⊗ E is the set of observed triples. The goal is to find the probabilities of entries Y r s o being true or false for a set of targeted unobserved triples r (s , o ) / ∈ Ω.</p><p>Depending on the scoring function φ(s, r, o; Θ) used to predict the entries of the tensor X, we obtain different models. Examples of scoring functions are given in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Our model scoring function is:</p><formula xml:id="formula_13">φ(r, s, o; Θ) = Re(&lt; w r , e s ,ē o &gt;) (9) = Re( K k=1 w rk e skēok ) (10) = Re(w r ), Re(e s ), Re(e o ) + Re(w r ), Im(e s ), Im(e o ) + Im(w r ), Re(e s ), Im(e o ) − Im(w r ), Im(e s ), Re(e o )<label>(11)</label></formula><p>where w r ∈ C K is a complex vector . These equations provide two interesting views of the model:</p><p>• Changing the representation: Equation <ref type="formula" target="#formula_0">(10)</ref> would correspond to DistMult with real embeddings, but handles asymmetry thanks to the complex conjugate of one of the embeddings 2 .</p><p>• Changing the scoring function: Equation <ref type="formula" target="#formula_0">(11)</ref> only involves real vectors corresponding to the real and imaginary parts of the embeddings and relations.</p><p>One can easily check that this function is antisymmetric when w r is purely imaginary (i.e. its real part is zero), and symmetric when w r is real. Interestingly, by separating the real and imaginary part of the relation embedding w r , we obtain a decomposition of the relation matrix X r as the sum of a symmetric matrix Re(E diag(Re(w r ))Ē T ) and a antisymmetric matrix Im(E diag(−Im(w r ))Ē T ). Geometrically, each relation embedding w r is an anisotropic scaling of the basis defined by the entity embeddings E, followed by a projection onto the real subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In order to evaluate our proposal, we conducted experiments on both synthetic and real datasets. The synthetic dataset is based on relations that are either symmetric or antisymmetric, whereas the real datasets comprise different types of relations found in different, standard KBs. We refer to our model as ComplEx, for Complex Embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic Task</head><p>To assess the ability of our proposal to accurately model symmetry and antisymmetry, we randomly generated a KB of two relations and 30 entities. One relation is entirely symmetric, while the other is completely antisymmetric. This dataset corresponds to a 2 × 30 × 30 tensor. <ref type="figure" target="#fig_0">Figure  2</ref> shows a part of this randomly generated tensor, with a symmetric slice and an antisymmetric slice, decomposed into training, validation and test sets. The diagonal is unobserved as it is not relevant in this experiment.</p><p>The train set contains 1392 observed triples, whereas the validation and test sets contain 174 triples each. <ref type="figure" target="#fig_1">Figure  1</ref> shows the best cross-validated Average Precision (area under Precision-Recall curve) for different factorization models of ranks ranging up to 50. Models were trained using Stochastic Gradient Descent with mini-batches and AdaGrad for tuning the learning rate <ref type="bibr" target="#b10">(Duchi et al., 2011)</ref>, by minimizing the negative log-likelihood of the logistic model with L 2 regularization on the parameters Θ of the considered model:</p><formula xml:id="formula_14">min Θ r(s,o)∈Ω log(1 + exp(−Y rso φ(s, r, o; Θ))) + λ||Θ|| 2 2 .</formula><p>(12) In our model, Θ corresponds to the embeddings e s , w r , e o ∈ C K . We describe the full algorithm in Appendix A.</p><p>λ is validated in {0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.00001, 0.0}. As expected, DistMult <ref type="bibr" target="#b25">(Yang et al., 2015)</ref> is not able to model antisymmetry and only predicts the symmetric relations correctly. Although TransE <ref type="bibr" target="#b7">(Bordes et al., 2013b)</ref> is not a symmetric model, it performs poorly in practice, particularly on the antisymmetric relation. RESCAL <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref>, with its large number of parameters, quickly overfits as the rank grows. Canonical Polyadic (CP) decomposition <ref type="bibr" target="#b12">(Hitchcock, 1927)</ref> fails on both relations as it has to push symmetric and antisymmetric patterns through the entity embeddings. Surprisingly, only our model succeeds on such simple data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets: FB15K and WN18</head><p>Dataset |E| |R| #triples in Train/Valid/Test WN18 40,943 18 141,442 / 5,000 / 5,000 FB15K 14,951 1,345 483,142 / 50,000 / 59,071 <ref type="table">Table 3</ref>. Number of entities, relations, and observed triples in each split for the FB15K and WN18 datasets.</p><p>We next evaluate the performance of our model on the FB15K and WN18 datasets. FB15K is a subset of Freebase, a curated KB of general facts, whereas WN18 is a subset of Wordnet, a database featuring lexical relations between words. We use original training, validation and test set splits as provided by <ref type="bibr" target="#b7">Bordes et al. (2013b)</ref>. <ref type="table">Table 3</ref> summarizes the metadata of the two datasets.</p><p>Both datasets contain only positive triples. As in Bordes et al. (2013b), we generated negatives using the local closed world assumption. That is, for a triple, we randomly change either the subject or the object at random, to form a negative example. This negative sampling is performed at runtime for each batch of training positive examples.</p><p>For evaluation, we measure the quality of the ranking of each test triple among all possible subject and object substitutions : r(s , o) and r(s, o ), ∀s , ∀o ∈ E. Mean Reciprocal Rank (MRR) and Hits at m are the standard evaluation measures for these datasets and come in two flavours: raw and filtered <ref type="bibr" target="#b7">(Bordes et al., 2013b)</ref>. The filtered metrics are computed after removing all the other positive observed triples that appear in either training, validation or test set from the ranking, whereas the raw metrics do not remove these.</p><p>Since ranking measures are used, previous studies generally preferred a pairwise ranking loss for the task <ref type="bibr" target="#b7">(Bordes et al., 2013b;</ref><ref type="bibr" target="#b19">Nickel et al., 2016b)</ref>. We chose to use the negative log-likelihood of the logistic model, as it is a continuous surrogate of the sign-rank, and has been shown to learn compact representations for several important relations, especially for transitive relations <ref type="bibr" target="#b8">(Bouchard et al., 2015)</ref>. In preliminary work, we tried both losses, and indeed the loglikelihood yielded better results than the ranking loss (except with TransE), especially on FB15K.</p><p>We report both filtered and raw MRR, and filtered Hits at 1, 3 and 10 in <ref type="table" target="#tab_2">Table 2</ref> for the evaluated models. Furthermore, we chose TransE, DistMult and HolE as baselines since they are the best performing models on those datasets to the best of our knowledge <ref type="bibr" target="#b19">(Nickel et al., 2016b;</ref><ref type="bibr" target="#b25">Yang et al., 2015)</ref>. We also compare with the CP model to emphasize empirically the importance of learning unique embeddings for entities. For experimental fairness, we reimplemented these methods within the same framework as the ComplEx model, using theano <ref type="bibr" target="#b4">(Bergstra et al., 2010)</ref>. However, due to time constraints and the complexity of an efficient implementation of HolE, we record the original results for HolE as reported in <ref type="bibr" target="#b19">Nickel et al. (2016b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>WN18 describes lexical and semantic hierarchies between concepts and contains many antisymmetric relations such as hypernymy, hyponymy, or being "part of". Indeed, the DistMult and TransE models are outperformed here by ComplEx and HolE, which are on par with respective filtered MRR scores of 0.941 and 0.938. <ref type="table">Table 4</ref> shows the filtered test set MRR for the models considered and each relation of WN18, confirming the advantage of our model on antisymmetric relations while losing nothing on the others. 2D projections of the relation embeddings provided in Appendix B visually corroborate the results.</p><p>On FB15K, the gap is much more pronounced and the ComplEx model largely outperforms HolE, with a filtered MRR of 0.692 and 59.9% of Hits at 1, compared to 0.524 and 40.2% for HolE. We attribute this to the simplicity of our model and the different loss function. This is supported by the relatively small gap in MRR compared to DistMult (0.654); our model can in fact be interpreted as a complex number version of DistMult. On both datasets, TransE and CP are largely left behind. This illustrates the power of the simple dot product in the first case, and the importance of learning unique entity embeddings in the second. CP performs poorly on WN18 due to the small number of relations, which magnifies this subject/object difference.</p><p>Reported results are given for the best set of hyper-parameters evaluated on the validation set for each model, after grid search on the following values: K ∈ {10, 20, 50, 100, 150, 200}, λ ∈ {0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0}, α 0 ∈ {1.0, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01}, η ∈ {1, 2, 5, 10} with λ the L 2 regularization parameter, α 0 the initial learning rate (then tuned at runtime with AdaGrad), and η the number of negatives generated per positive training triple. We also tried varying the batch size but this had no impact and we settled with 100 batches per epoch. Best ranks were generally 150 or 200, in both cases scores were always very close for all models. The number of negative samples per positive sample also had a large influence on the filtered MRR on FB15K (up to +0.08 improvement from 1 to 10 negatives), but not much on WN18. On both datasets regularization was important (up to +0.05 on filtered MRR between λ = 0 and optimal one). We found the initial learning rate to be very important on FB15K, while not so much on WN18. We think this may also explain the large gap of improvement our model provides on this dataset compared to previously published results -as DistMult results are also better than those previously reported <ref type="bibr" target="#b25">(Yang et al., 2015)</ref> -along with the use of the log-likelihood objective. It seems that in general AdaGrad is relatively insensitive to the initial learning rate, perhaps causing some overconfidence in its ability to tune the step size online and consequently leading to less efforts when selecting the initial step size.</p><p>Training was stopped using early stopping on the validation set filtered MRR, computed every 50 epochs with a maximum of 1000 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Influence of Negative Samples</head><p>We further investigated the influence of the number of negatives generated per positive training sample. In the previous experiment, due to computational limitations, the number of negatives per training sample, η, was validated among the possible numbers {1, 2, 5, 10}. We want to explore here whether increasing these numbers could lead to better results. To do so, we focused on FB15K, with the best validated λ, K, α 0 , obtained from the previous experiment. We then let η vary in <ref type="bibr">{1, 2, 5, 10, 20, 50, 100, 200}</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> shows the influence of the number of generated negatives per positive training triple on the performance of our model on FB15K. Generating more negatives clearly improves the results, with a filtered MRR of 0.737 with 100 negative triples (and 64.8% of Hits@1), before decreasing again with 200 negatives. The model also converges with fewer epochs, which compensates partially for the additional training time per epoch, up to 50 negatives. It then grows linearly as the number of negatives increases, making 50 a good trade-off between accuracy and training time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>In the early age of spectral theory in linear algebra, complex numbers were not used for matrix factorization and mathematicians mostly focused on bi-linear forms <ref type="bibr" target="#b3">(Beltrami, 1873)</ref>. The eigen-decomposition in the complex domain as taught today in linear algebra courses came 40 years later <ref type="bibr" target="#b2">(Autonne, 1915)</ref>. Similarly, most of the existing approaches for tensor factorization were based on decompositions in the real domain, such as the Canonical Polyadic (CP) decomposition <ref type="bibr" target="#b12">(Hitchcock, 1927)</ref>. These methods are very effective in many applications that use different modes of the tensor for different types of entities. But in the link prediction problem, antisymmetry of relations was quickly seen as a problem and asymmetric extensions of tensors were studied, mostly by either considering independent embeddings <ref type="bibr" target="#b22">(Sutskever, 2009)</ref> or considering relations as matrices instead of vectors in the RESCAL model <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref>. Direct extensions were based on uni-,bi-and trigram latent factors for triple data, as well as a low-rank relation matrix <ref type="bibr" target="#b13">(Jenatton et al., 2012)</ref>.</p><p>Pairwise interaction models were also considered to improve prediction performances. For example, the Universal Schema approach <ref type="bibr" target="#b20">(Riedel et al., 2013)</ref> factorizes a 2D unfolding of the tensor (a matrix of entity pairs vs. relations) while <ref type="bibr" target="#b24">Welbl et al. (2016)</ref> extend this also to other pairs.</p><p>In the Neural Tensor Network (NTN) model, <ref type="bibr" target="#b21">Socher et al. (2013)</ref> combine linear transformations and multiple bilinear forms of subject and object embeddings to jointly feed them into a nonlinear neural layer. Its non-linearity and multiple ways of including interactions between embeddings gives it an advantage in expressiveness over models with simpler scoring function like DistMult or RESCAL. As a downside, its very large number of parameters can make the NTN model harder to train and overfit more easily.</p><p>The original multi-linear DistMult model is symmetric in subject and object for every relation <ref type="bibr" target="#b25">(Yang et al., 2015)</ref> and achieves good performance, presumably due to its simplicity. The TransE model from <ref type="bibr" target="#b7">Bordes et al. (2013b)</ref> also embeds entities and relations in the same space and imposes a geometrical structural bias into the model: the subject entity vector should be close to the object entity vector once translated by the relation vector.</p><p>A recent novel way to handle antisymmetry is via the Holographic Embeddings (HolE) model by <ref type="bibr" target="#b19">(Nickel et al., 2016b)</ref>. In HolE the circular correlation is used for combining entity embeddings, measuring the covariance between embeddings at different dimension shifts. This generally suggests that other composition functions than the classical tensor product can be helpful as they allow for a richer interaction of embeddings. However, the asymmetry in the composition function in HolE stems from the asymmetry of circular correlation, an O(nlog(n)) operation, whereas ours is inherited from the complex inner product, in O(n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We described a simple approach to matrix and tensor factorization for link prediction data that uses vectors with complex values and retains the mathematical definition of the dot product. The class of normal matrices is a natural fit for binary relations, and using the real part allows for efficient approximation of any learnable relation. Results on standard benchmarks show that no more modifications are needed to improve over the state-of-the-art.</p><p>There are several directions in which this work can be extended. An obvious one is to merge our approach with known extensions to tensor factorization in order to further improve predictive performance. For example, the use of pairwise embeddings together with complex numbers might lead to improved results in many situations that involve non-compositionality. Another direction would be to develop a more intelligent negative sampling procedure, to generate more informative negatives with respect to the positive sample from which they have been sampled. It would reduce the number of negatives required to reach good performance, thus accelerating training time.</p><p>Also, if we were to use complex embeddings every time a model includes a dot product, e.g. in deep neural networks, would it lead to a similar systematic improvement?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SGD algorithm</head><p>We describe the algorithm to learn the ComplEx model with Stochastic Gradient Descent using only real-valued vectors.</p><p>Let us rewrite equation 11, by denoting the real part of embeddings with primes and the imaginary part with double primes: e i = Re(e i ), e i = Im(e i ), w r = Re(w r ), w r = Im(w r ). The set of parameters is Θ = {e i , e i , w r , w r ; ∀i ∈ E, ∀r ∈ R}, and the scoring function involves only real vectors: where is the element-wise (Hadamard) product.</p><p>As stated in equation 8 we use the sigmoid link function, and minimize the L 2 -regularized negative log-likelihood: γ(Ω; Θ) = r(s,o)∈Ω log(1 + exp(−Y rso φ(s, r, o; Θ))) +λ||Θ|| 2 2 .</p><p>To handle regularization, note that the squared L 2 -norm of a complex vector v = v + iv is the sum of the squared modulus of each entry:</p><formula xml:id="formula_15">||v|| 2 2 = j v 2 j + v 2 j 2 = j v 2 j + j v 2 j = ||v || 2 2 + ||v || 2 2</formula><p>which is actually the sum of the L 2 -norms of the vectors of the real and imaginary parts.</p><p>Algorithm 1 SGD for the ComplEx model input Training set Ω, Validation set Ω v , learning rate α, embedding dim. k, regularization factor λ, negative ratio η, batch size b, max iter m, early stopping s. e i ← randn(k), e i ← randn(k) for each i ∈ E w i ← randn(k), w i ← randn(k) for each i ∈ R for i = 1, · · · , m do for j = 1..|Ω| where σ(x) = 1 1+e −x is the sigmoid function. Algorithm 1 describes SGD for this formulation of the scoring function. When Ω contains only positive triples, we generate η negatives per positive train triple, by corrupting either the subject or the object of the positive triple, as described in <ref type="bibr" target="#b7">Bordes et al. (2013b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. WN18 embeddings visualization</head><p>We used principal component analysis (PCA) to visualize embeddings of the relations of the wordnet dataset (WN18). We plotted the four first components of the best DistMult and ComplEx model's embeddings in <ref type="figure" target="#fig_5">Figure 4</ref>. For the ComplEx model, we simply concatenated the real and imaginary parts of each embedding.</p><p>Most of WN18 relations describe hierarchies, and are thus antisymmetric.</p><p>Each of these hierarchic relations has its inverse relation in the dataset. For example: hypernym / hyponym, part of / has part, synset domain topic of / member of domain topic. Since DistMult is unable to model antisymmetry, it will correctly represent the nature of each pair of opposite relations, but not the direction of the relations. Loosely speaking, in the hypernym / hyponym pair the nature is sharing semantics, and the direction is that one entity generalizes the semantics of the other. This makes DistMult reprensenting the opposite relations with very close embeddings, as <ref type="figure" target="#fig_5">Figure 4</ref> shows. It is especially striking for the third and fourth principal component (bottom-left). Conversely, ComplEx manages to oppose spatially the opposite relations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Parts of the training, validation and test sets of the generated experiment with one symmetric and one antisymmetric relation. Red pixels are positive triples, blue are negatives, and green missing ones. Top: Plots of the symmetric slice (relation) for the 10 first entities. Bottom: Plots of the antisymmetric slice for the 10 first entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Average Precision (AP) for each factorization rank ranging from 1 to 50 for different state of the art models on the combined symmetry and antisymmetry experiment. Top-left: AP for the symmetric relation only. Top-right: AP for the antisymmetric relation only. Bottom: Overall AP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Influence of the number of negative triples generated per positive training example on the filtered test MRR and on training time to convergence on FB15K for the ComplEx model with K = 200, λ = 0.01 and α0 = 0.5. Times are given relative to the training time with one negative triple generated per positive training sample (= 1 on time scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>φ(r, s, o; Θ) = w r , e s , e o + w r , e s , e o + w r , e s , e o − w r , e s , e o where each entity and each relation has two real embeddings. Gradients are now easy to write: ∇ e s φ(r, s, o; Θ) = (w r e o ) + (w r e o ) ∇ e s φ(r, s, o; Θ) = (w r e o ) − (w r e o ) ∇ e o φ(r, s, o; Θ) = (w r e s ) − (w r e s ) ∇ e o φ(r, s, o; Θ) = (w r e s ) + (w r e s ) ∇ w r φ(r, s, o; Θ) = (e s e o ) + (e s e o ) ∇ w r φ(r, s, o; Θ) = (e s e o ) − (e s e o )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>/b do Ω b ← sample(Ω, b, η) Update embeddings w.r.t.: r(s,o)∈Ω b ∇γ({r(s, o)}; Θ) Update learning rate α using Adagrad end for if i mod s = 0 then break if filteredMRR or AP on Ω v decreased end if end forWe can finally write the gradient of γ with respect to a real embedding v for one triple r(s, o):∇ v γ({r(s, o)}; Θ) = −Y rso φ(s, r, o; Θ)σ(∇ v φ(r, s, o; Θ)) +2λv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Plots of the first and second (Top), third and fourth (Bottom) components of the WN18 relations embeddings using PCA. Left: DistMult embeddings. Right: ComplEx embeddings. Opposite relations are clustered together by DistMult while correctly separated by ComplEx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Scoring functions of state-of-the-art latent factor models for a given fact r(s, o), along with their relation parameters, time and space (memory) complexity. The embeddings es and eo of subject s and object o are in R K for each model, except for our model (ComplEx) where es, eo ∈ C K . D is an additional latent dimension of the NTN model. F and F −1 denote respectively the Fourier transform and its inverse, and is the element-wise product between two vectors.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Relation embeddings naturally act as weights on each latent dimension: Re(w r ) over the symmetric, real part of e o , e s , and Im(w) over the antisymmetric, imaginary part of e o , e s . Indeed, one has e o , e s = e s , e o , meaning that Re( e o , e s ) is symmetric, while Im( e o , e s ) is antisymmetric. This enables us to accurately describe both symmetric and antisymmetric relations between pairs of entities, while still using joint representations of entities, whether they appear as subject or object of relations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Filtered and Raw Mean Reciprocal Rank (MRR) for the models tested on the FB15K and WN18 datasets. Hits@m metrics are filtered. *Results reported from<ref type="bibr" target="#b19">(Nickel et al., 2016b)</ref> for HolE model.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FB15K</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">MRR</cell><cell></cell><cell>Hits at</cell><cell></cell><cell cols="2">MRR</cell><cell></cell><cell>Hits at</cell><cell></cell></row><row><cell>Model</cell><cell>Filter</cell><cell>Raw</cell><cell>1</cell><cell>3</cell><cell>10</cell><cell>Filter</cell><cell>Raw</cell><cell>1</cell><cell>3</cell><cell>10</cell></row><row><cell>CP</cell><cell>0.075</cell><cell>0.058</cell><cell>0.049</cell><cell>0.080</cell><cell>0.125</cell><cell>0.326</cell><cell>0.152</cell><cell>0.219</cell><cell>0.376</cell><cell>0.532</cell></row><row><cell>TransE</cell><cell>0.454</cell><cell>0.335</cell><cell>0.089</cell><cell>0.823</cell><cell>0.934</cell><cell>0.380</cell><cell>0.221</cell><cell>0.231</cell><cell>0.472</cell><cell>0.641</cell></row><row><cell>DistMult</cell><cell>0.822</cell><cell>0.532</cell><cell>0.728</cell><cell>0.914</cell><cell>0.936</cell><cell>0.654</cell><cell>0.242</cell><cell>0.546</cell><cell>0.733</cell><cell>0.824</cell></row><row><cell>HolE*</cell><cell>0.938</cell><cell>0.616</cell><cell>0.93</cell><cell>0.945</cell><cell>0.949</cell><cell>0.524</cell><cell>0.232</cell><cell>0.402</cell><cell>0.613</cell><cell>0.739</cell></row><row><cell>ComplEx</cell><cell>0.941</cell><cell>0.587</cell><cell>0.936</cell><cell>0.945</cell><cell>0.947</cell><cell>0.692</cell><cell>0.242</cell><cell>0.599</cell><cell>0.759</cell><cell>0.840</cell></row><row><cell>Relation name</cell><cell></cell><cell cols="3">ComplEx DistMult TransE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hypernym</cell><cell></cell><cell>0.953</cell><cell>0.791</cell><cell>0.446</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hyponym</cell><cell></cell><cell>0.946</cell><cell>0.710</cell><cell>0.361</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">member meronym</cell><cell>0.921</cell><cell>0.704</cell><cell>0.418</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">member holonym</cell><cell>0.946</cell><cell>0.740</cell><cell>0.465</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">instance hypernym</cell><cell>0.965</cell><cell>0.943</cell><cell>0.961</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">instance hyponym</cell><cell>0.945</cell><cell>0.940</cell><cell>0.745</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>has part</cell><cell></cell><cell>0.933</cell><cell>0.753</cell><cell>0.426</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>part of</cell><cell></cell><cell>0.940</cell><cell>0.867</cell><cell>0.455</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">member of domain topic</cell><cell>0.924</cell><cell>0.914</cell><cell>0.861</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">synset domain topic of</cell><cell>0.930</cell><cell>0.919</cell><cell>0.917</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">member of domain usage</cell><cell>0.917</cell><cell>0.917</cell><cell>0.875</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">synset domain usage of</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">member of domain region 0.865</cell><cell>0.635</cell><cell>0.865</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">synset domain region of</cell><cell>0.919</cell><cell>0.888</cell><cell>0.986</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">derivationally related form 0.946</cell><cell>0.940</cell><cell>0.384</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>similar to</cell><cell></cell><cell>1.000</cell><cell>1.000</cell><cell>0.244</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>verb group</cell><cell></cell><cell>0.936</cell><cell>0.897</cell><cell>0.323</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>also see</cell><cell></cell><cell>0.603</cell><cell>0.607</cell><cell>0.279</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Table 4. Filtered Mean Reciprocal Rank (MRR) for the models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">tested on each relation of the Wordnet dataset (WN18).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at: https://github.com/ ttrouill/complex</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that in Equation(10)we used the standard componentwise multi-linear dot product &lt; a, b, c &gt;:= k a k b k c k . This is not the Hermitian extension as it is not properly defined in the linear algebra literature.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the Paul Allen Foundation through an Allen Distinguished Investigator grant and in part by a Google Focused Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Yehudayoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.07648</idno>
		<title level="m">Sign rank versus vc dimension</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th Intl Semantic Web Conference</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sur les matrices hypohermitiennes et sur les matrices unitaires</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Autonne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Univ. Lyons, Nouvelle Srie I</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="77" />
			<date type="published" when="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Giornale di Matematiche ad Uso degli Studenti Delle Universita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Beltrami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1873" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
	<note>Sulle funzioni bilineari</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frédéric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
	<note>Oral Presentation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Praveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD 08 Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Irreflexive and Hierarchical Relations as Translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On approximate reasoning capabilities of low-rank vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evgeniy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geremy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>ISBN 0262072882</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Phys</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Latent Factor Model for Highly Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Complexity measures of sign matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Schechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Shraibman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="463" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Three-Way Model for Collective Learning on Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hans-Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reducing the rank in relational factorization models by including observable patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Limin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlin</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modelling Relational Data using Bayesian Clustered Tensor Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Decomposing real square matrices via unitary diagonalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07103</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sebastian. A factorization machine framework for testing bigram embeddings in knowledgebase completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riedel</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1604.05878</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
