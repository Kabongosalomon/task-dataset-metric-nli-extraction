<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Tamura</surname></persName>
							<email>masato.tamura.sf@hitachi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Lumada Data Science Lab</orgName>
								<address>
									<settlement>Hitachi</settlement>
									<region>Ltd</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ohashi</surname></persName>
							<email>hiroki.ohashi.uo@hitachi.com</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Technology Innovation -Artificial Intelligence</orgName>
								<address>
									<settlement>Hitachi</settlement>
									<region>Ltd</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoaki</forename><surname>Yoshinaga</surname></persName>
							<email>tomoaki.yoshinaga.xc@hitachi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Lumada Data Science Lab</orgName>
								<address>
									<settlement>Hitachi</settlement>
									<region>Ltd</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a simple, intuitive yet powerful method for human-object interaction (HOI) detection. HOIs are so diverse in spatial distribution in an image that existing CNN-based methods face the following three major drawbacks; they cannot leverage image-wide features due to CNN's locality, they rely on a manually defined locationof-interest for the feature aggregation, which sometimes does not cover contextually important regions, and they cannot help but mix up the features for multiple HOI instances if they are located closely. To overcome these drawbacks, we propose a transformer-based feature extractor, in which an attention mechanism and query-based detection play key roles. The attention mechanism is effective in aggregating contextually important information imagewide, while the queries, which we design in such a way that each query captures at most one human-object pair, can avoid mixing up the features from multiple instances. This transformer-based feature extractor produces so effective embeddings that the subsequent detection heads may be fairly simple and intuitive. The extensive analysis reveals that the proposed method successfully extracts contextually important features, and thus outperforms existing methods by large margins (5.37 mAP on HICO-DET, and 5.7 mAP on V-COCO). The source codes are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-object interaction (HOI) detection has attracted enormous interest in recent years for its potential in deeper scene understanding <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. Given an image, the task of HOI detection is to localize a human and object, and identify the interactions between them, typically represented as human bounding box, object bounding box, object class, action class .</p><p>Conventional HOI detection methods can be roughly divided into two types: two-stage methods <ref type="bibr">[3-6, 8, 11, 13, 15, 16, 19, 20, 24, 27, 29-31, 33-37]</ref> and single-stage meth- ods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref>. In the two-stage methods, humans and objects are first individually localized by off-the-shelf object detectors, and then the region features from the localized area are used to predict action classes. To incorporate contextual information, auxiliary features such as the features from the union region of a human and object bounding box, and locations of the bounding boxes in an image are often utilized. The single-stage methods predict interactions using the features of a heuristically-defined position such as a midpoint between a human and object center <ref type="bibr" target="#b16">[17]</ref>.</p><p>While both two-and single-stage methods have shown significant improvement, they often suffer from errors attributed to the nature of convolutional neural networks (CNNs) and the heuristic way of using CNN features. <ref type="figure" target="#fig_0">Figure 1</ref> shows typical failure cases of conventional methods. In <ref type="figure" target="#fig_0">Fig. 1a</ref>, we can easily recognize from an entire image that a boy is washing a car. It is difficult, however, for twostage methods to predict the action class "wash" since they typically use only the cropped bounding-box regions. The regions sometimes miss contextually important cues located outside the human and object bounding box such as the hose in <ref type="figure" target="#fig_0">Fig. 1a</ref>. Even though the features of union regions may contain such cues, these regions are frequently dominated by disturbing contents such as background and irrelevant humans and objects. <ref type="figure" target="#fig_0">Figure 1b</ref> shows an example where multiple HOI instances are overlapped. In such a case, CNN-based feature extractors are forced to capture features of both instances in the overlapped region, ending up in obtaining contaminated features. The detection based on the contaminated features easily results in failures. The singlestage methods attempt to capture the contextual information by pairing a target human and object from an early stage in feature extraction and extracting integrated features rather than individually treating the targets. To determine the regions from which integrated features are extracted, they rely on heuristically-designed location-of-interest such as a midpoint between a human and object center <ref type="bibr" target="#b16">[17]</ref>. However, such reliance sometimes causes a problem. <ref type="figure" target="#fig_0">Fig. 1c</ref> shows an example where a target human and object are located distantly. In this example, the midpoint is located close to the man in the middle, who is not relevant to the target HOI instance. Therefore, it is difficult to detect the target on the basis of the features around the midpoint. <ref type="figure" target="#fig_0">Fig. 1d</ref> is an example where the midpoints of multiple HOI instances are close to each other. In this case, CNN-based methods tend to make mis-detection due to the same reason as the one for the failure in <ref type="figure" target="#fig_0">Fig. 1b</ref>, i.e. contaminated features.</p><p>To overcome these drawbacks, we propose QPIC, a query-based HOI detector that detects a human and object in a pairwise manner with image-wide contextual information. QPIC has a transformer <ref type="bibr" target="#b27">[28]</ref> as a key component. The attention mechanism used in QPIC scans through the entire area of an image and is expected to selectively aggregate contextually important information according to the contents of an image. Moreover, we design QPIC's queries so that each query captures at most one human-object pair. This enables to separately extract features of multiple HOI instances without contaminating them even when the instances are located closely. These key designs of the attention mechanism and query-based pairwise detection make QPIC robust even under the difficult conditions such as the case where contextually important information appears outside the human and object bounding box ( <ref type="figure" target="#fig_0">Fig. 1a</ref>), the target human and object are located distantly <ref type="figure" target="#fig_0">(Fig. 1c)</ref>, and multiple instances are close to each other ( <ref type="figure" target="#fig_0">Fig. 1b and 1d</ref>). The key designs produce so effective embeddings that the subsequent detection heads may be fairly simple and intuitive.</p><p>To summarize, our contributions are three-fold: (1) We propose a simple yet effective query-based HOI detector, QPIC, which incorporates contextually important information aggregated image-wide. To the best of our knowledge, this is the first work to introduce an attention-and querybased method to HOI detection. (2) We achieve signifi-cantly better performance than state-of-the-art methods on two challenging HOI detection benchmarks. (3) We conduct detailed analysis on the behavior of QPIC in relation to that of conventional methods, and reveal some of the important characteristics of HOI detection tasks that conventional methods could not capture but QPIC does relatively well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Two-stage HOI detection methods <ref type="bibr">[3-6, 8, 11, 13, 15, 16, 19, 20, 24, 27, 29-31, 33-37]</ref> utilize Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> or Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> to localize targets. Then, they crop features of backbone networks inside the localized regions. The cropped features are typically processed with multistream networks. Each stream processes features of target humans, those of objects, and some auxiliary features such as spatial configurations of the targets, and human poses either alone or in combination. Some of the two-stage methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> utilize graph neural networks to refine the features. These methods mainly focus on the second stage architecture, which uses cropped features to predict action classes. However, the cropped features sometimes lack contextual information outside the cropped regions or are contaminated by features of irrelevant targets, which results in the degradation of the performance.</p><p>Recently, single-stage methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref> that utilize integrated features from a pair of a human and object have been proposed to solve the problem in the individually cropped features. Liao et al. <ref type="bibr" target="#b16">[17]</ref> and Wang et al. <ref type="bibr" target="#b31">[32]</ref> proposed a point-based interaction detection method that utilizes CenterNet <ref type="bibr" target="#b37">[38]</ref> as a base detector. This method predicts action classes using integrated features collected at a midpoint between a human and object center. In particular, Liao et al.'s PPDM <ref type="bibr" target="#b16">[17]</ref> achieves simultaneous object and interaction detection training, which is the most similar to our training approach. Kim et al. <ref type="bibr" target="#b11">[12]</ref> proposed UnionDet, which predicts the union bounding box of a human-object pair to extract integrated features. Although these methods attempt to capture contextual information by integrated features, they are still insufficient and sometimes contaminated due to the CNN's locality and heuristically-designed location-of-interests.</p><p>Our method differs from conventional methods in that we leverage a transformer to aggregate image-wide contextual features in a pairwise manner. We use DETR <ref type="bibr" target="#b1">[2]</ref> as a base detector and extend it for HOI detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>To effectively extract important features for each HOI instance taking image-wide contexts into account, we propose to leverage transformer-based architecture as a base feature extractor. We first explain the overall architecture in Sec. 3.1 and show that the detection heads following the  base feature extractor can be simplified due to the rich features obtained in the base feature extractor. In Sec. 3.2, we show the concrete formulation of the loss function involved in the training. Finally we explain how to use our method to detect HOI instances given a new image in Sec. 3.3. The transformer encoder takes this feature map with the reduced dimension z c ∈ R Dc×H ×W to produce another feature map with richer contextual information on the basis of the self-attention mechanism. A fixed positional encoding p ∈ R Dc×H ×W <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22]</ref> is additionally input to the encoder to supplement the positional information, which the self-attention mechanism alone cannot inherently incorporate. The encoded feature map z e ∈ R Dc×H ×W is then obtained as z e = f enc (z c , p), where f enc (·, ·) is a set of stacked transformer encoder layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>The transformer decoder transforms a set of learnable</p><formula xml:id="formula_0">query vectors Q = {q i |q i ∈ R Dc } Nq i=1 into a set of em- beddings D = {d i |d i ∈ R Dc } Nq i=1</formula><p>that contain imagewide contextual information for HOI detection, referring to the encoded feature map z e using the attention mechanism. N q is the number of query vectors. The queries are designed in such a way that one query captures at most one human-object pair and an interaction(s) between them. N q is therefore set to be large enough so that it is always larger than the number of actual human-object pairs in an image. The decoded embeddings are then obtained as D = f dec (z e , p, Q), where f dec (·, ·, ·) is a set of stacked transformer decoder layers. We use a positional encoding p again to incorporate the spatial information.</p><p>The subsequent interaction detection heads further processes the decoded embeddings to produce N q prediction results. Here, we note that one or more HOIs corresponding to a human-object pair are mathematically defined by the following four vectors: a human-bounding-box vector normalized by the corresponding image size b (h) ∈ [0, 1] 4 , a normalized object-bounding-box vector b (o) ∈ [0, 1] 4 , an object-class one-hot vector c ∈ {0, 1} N obj , where N obj is the number of object classes, and an action-class vector a ∈ {0, 1} Nact , where N act is the number of action classes. Note that a is not necessarily a one-hot vector because there may be multiple actions that correspond to a human-object pair. Our interaction detection heads are composed of four small feed-forward networks (FFNs): human-bounding-box FFN f h , object-bounding-box FFN f o , object-class FFN f c , and action-class FFN f a , each of which is dedicated to predict one of the aforementioned 4 vectors, respectively. This design of the interaction detection heads is fairly intuitive and simple compared with a number of state-of-the-art methods such as the pointdetection and point-matching branch in PPDM <ref type="bibr" target="#b16">[17]</ref> and the human, object, and spatial-semantic stream in DRG <ref type="bibr" target="#b3">[4]</ref>. Thanks to the powerful embeddings that contain imagewide contextual information, QPIC does not have to rely on a rather complicated and heuristic design to produce the prediction. One thing to note is that unlike many existing methods <ref type="bibr">[3-6, 8, 11, 13, 15, 16, 19, 20, 24, 27, 29-31, 33-37]</ref>, which first attempt to detect humans and objects individually and later pair them to find interactions, it is crucial to design queries in such a way that one query directly captures a human and object as a pair to more effectively extract features for interactions. We will experimentally verify this claim in Sec. 4.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The prediction of normalized human bounding boxes {b</head><formula xml:id="formula_1">(h) i |b (h) i ∈ [0, 1] 4 } Nq i=1 , that of object bounding boxes {b (o) i |b (o) i ∈ [0, 1] 4 } Nq i=1 , the probability of object classes {ĉ i |ĉ i ∈ [0, 1] N obj +1 , N obj +1 j=1ĉ i (j) = 1} Nq i=1 , where v(j)</formula><p>denotes the j-th element of v, and the probability of action</p><formula xml:id="formula_2">classes {â i |â i ∈ [0, 1] Nact } Nq i=1 , are calculated asb (h) i = σ (f h (d i )) ,b (o) i = σ (f o (d i )) ,ĉ i = ς (f c (d i )) ,â i = σ (f a (d i ))</formula><p>, respectively. σ, ς are the sigmoid and softmax functions, respectively. Note thatĉ i has the (N obj + 1)-th element to indicate that the i-th query has no corresponding human-object pair, while an additional element ofâ i to indicate "no action" is not necessary because we use the sigmoid function rather than the softmax function to calculate the action-class probabilities for co-occuring actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss Calculation</head><p>The loss calculation is composed of two stages: the bipartite matching stage between predictions and ground truths, and the loss calculation stage for the matched pairs.</p><p>For the bipartite matching, we follow the training procedure of DETR <ref type="bibr" target="#b1">[2]</ref> and use the Hungarian algorithm <ref type="bibr" target="#b13">[14]</ref>. Note that this design obviates the process of suppressing over-detection as described in <ref type="bibr" target="#b1">[2]</ref>. We first pad the groundtruth set of human-object pairs with φ (no pairs) so that the ground-truth set size becomes N q . We then leverage the Hungarian algorithm to determine the optimal assignment ω among the set of all possible permutations of N q elements Ω Nq , i.e.ω = arg min ω∈Ω Nq Nq i=1 H i,ω(i) , where H i,j is the matching cost for the pair of i-th ground truth and j-th prediction. The matching cost H i,j consists of four types of costs: the box-regression cost H</p><formula xml:id="formula_3">(b) i,j , intersection-over-union (IoU) cost H (u) i,j , object-class cost H (c) i,j , and action-class cost H (a) i,j . Denoting i-th ground truth for the normalized human bounding box by b (h) i ∈ [0, 1] 4 , normalized object bounding box by b (o) i ∈ [0, 1] 4</formula><p>, object-class one-hot vector by c i ∈ {0, 1} N obj , and action class by a i ∈ {0, 1} Nact , the aforementioned costs are formulated as follows.</p><formula xml:id="formula_4">H i,j = 1 {i ∈Φ} η b H (b) i,j + η u H (u) i,j + η c H (c) i,j + η a H (a) i,j ,<label>(1)</label></formula><formula xml:id="formula_5">H (b) i,j = max b (h) i −b (h) j 1 , b (o) i −b (o) j 1 ,<label>(2)</label></formula><formula xml:id="formula_6">H (u) i,j = max −GIoU b (h) i ,b (h) j , −GIoU b (o) i ,b (o) j ,<label>(3)</label></formula><formula xml:id="formula_7">H (c) i,j = −ĉ j (k) s.t. c i (k) = 1,<label>(4)</label></formula><formula xml:id="formula_8">H (a) i,j = − 1 2 a iâ j a i 1 + + (1 − a i ) (1 −â j ) 1 − a i 1 + ,<label>(5)</label></formula><p>where Φ is a set of ground-truth indices that correspond to φ, GIoU (·, ·) is the generalized IoU <ref type="bibr" target="#b25">[26]</ref>, is a small positive value introduced to avoid zero divide, and η b , η u , η c , and η a are the hyper-parameters. We use two types of bounding-box cost H</p><formula xml:id="formula_9">(b) i,j and H (u) i,j following [2]. In calcu- lating H (b) i,j and H (u)</formula><p>i,j , instead of minimizing the average of a human and object-bounding-box cost, we minimize the larger of the two to prevent the matching from being undesirably biased to either if one cost is significantly lower than the other. We design H (a) i,j so that the costs of both positive and negative action classes are taken into account. In addition, we formulate it using the weighted average of the two with the inverse number of nonzero elements as the weights rather than using the vanilla average. This is necessary to balance the effect from the two costs because the number of positive action classes is typically much smaller than that of negative action classes.</p><p>The loss to be minimized in the training phase is calculated on the basis of the matched pairs as follows.</p><formula xml:id="formula_10">L = λ b L b + λ u L u + λ c L c + λ a L a ,<label>(6)</label></formula><formula xml:id="formula_11">L b = 1 |Φ| Nq i=1 1 {i ∈Φ} b (h) i −b (h) ω(i) 1 + b (o) i −b (o) ω(i) 1 ,<label>(7)</label></formula><formula xml:id="formula_12">L u = 1 |Φ| Nq i=1 1 {i ∈Φ} 2 − GIoU b (h) i ,b (h) ω(i) −GIoU b (o) i ,b (o) ω(i) ,<label>(8)</label></formula><formula xml:id="formula_13">L c = 1 N q Nq i=1 1 {i ∈Φ} − logĉω (i) (k) +1 {i∈Φ} − logĉω (i) (N obj + 1) s.t. c i (k) = 1,<label>(9)</label></formula><formula xml:id="formula_14">L a = 1 Nq i=1 1 {i ∈Φ} a i 1 Nq i=1 1 {i ∈Φ} l f a i ,âω (i) +1 {i∈Φ} l f 0,âω (i) ,<label>(10)</label></formula><p>where λ b , λ u , λ c and λ a are the hyper-parameters for adjusting the weights of each loss, and l f (·, ·) is the elementwise focal loss function <ref type="bibr" target="#b17">[18]</ref>. For the hyper-parameters of the focal loss, we use the default settings described in <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference for Interaction Detection</head><p>As previously mentioned, the detection result of an HOI is represented by the following four components, human bounding box, object bounding box, object class, action class . Our interaction detection heads are designed so intuitively that all we need to do is to pick up the corresponding information from each head. Formally, we set the prediction results corresponding to the i-th query and j-th ac-</p><formula xml:id="formula_15">tion as b (h) i ,b (o) i , arg max kĉi (k), j .</formula><p>We define a score of the HOI instance as {max kĉi (k)}â i (j), and regard this instance to be present if the score is higher than a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We conducted extensive experiments on two HOI detection datasets: HICO-DET <ref type="bibr" target="#b2">[3]</ref> and V-COCO <ref type="bibr" target="#b6">[7]</ref>. We followed the standard evaluation scheme. HICO-DET contains 38,118 and 9,658 images for training and testing, respectively. The images are annotated with 80 object and 117 action classes. V-COCO, which originates from the COCO dataset, contains 2,533, 2,867, and 4,946 images for training, validation, and testing, respectively. The images are annotated with 80 object and 29 action classes.</p><p>For the evaluation metrics, we use the mean average precision (mAP). A detection result is judged as a true positive if the predicted human and object bounding box have IoUs larger than 0.5 with the corresponding ground-truth bounding boxes, and the predicted action class is correct. In the HICO-DET evaluation, the object class is also taken into account for the judgment. The AP is calculated per object and action class pair in the HICO-DET evaluation, while that is calculated per action class in the V-COCO evaluation.</p><p>For HICO-DET, we evaluate the performance in two different settings following <ref type="bibr" target="#b2">[3]</ref>: default setting and knownobject setting. In the former setting, APs are calculated on the basis of all the test images, while in the latter setting, each AP is calculated only on the basis of images that contain the object class corresponding to each AP. In each setting, we report the mAP over three set types: a set of 600 HOI classes (full), a set of 138 HOI classes that have less than 10 training instances (rare), and a set of 462 HOI classes that have 10 or more training instances (non-rare). Unless otherwise stated, we use the default full setting in the analysis. In V-COCO, a number of HOIs are defined with no object labels. To deal with this situation, we evaluate the performance in two different scenarios following V-COCO's official evaluation scheme. In scenario 1, detectors are required to report cases in which there is no object, while in scenario 2, we just ignore the prediction of an object bounding box in these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use ResNet-50 and ResNet-101 <ref type="bibr" target="#b9">[10]</ref> as a backbone feature extractor. Both transformer encoder and decoder consist of 6 transformer layers with a multi-head attention of 8 heads. The reduced dimension size D c is set to 256, and the number of query vectors N q is set to 100. The human-and object-bounding-box FFNs have 3 linear layers with ReLU activations, while the object-and action-class FFNs have 1 linear layer.</p><p>For training QPIC, we initialize the network with the parameters of DETR <ref type="bibr" target="#b1">[2]</ref> trained with the COCO dataset. Note that for the V-COCO training, we exclude the COCO's training images that are contained in the V-COCO test set when pre-training DETR 1 . QPIC is trained for 150 epochs using the AdamW <ref type="bibr" target="#b20">[21]</ref> optimizer with the batch size 16, initial learning rate of the backbone network 10 −5 , that of the others 10 −4 , and the weight decay 10 −4 . Both learning rates are decayed after 100 epochs. The hyper-parameters for the Hungarian costs η b , η u , η c , and η a , and those for the loss weights λ b , λ u , λ c , and λ a are set to 2.5, 1, 1, 1,  <ref type="bibr" target="#b3">[4]</ref> 51.0 -ACP <ref type="bibr" target="#b12">[13]</ref> 53.0 -FCMNet <ref type="bibr" target="#b19">[20]</ref> 53. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to State-of-the-Art</head><p>We first show the comparison of our QPIC with the latest HOI detection methods including both two-and singlestage methods in <ref type="table" target="#tab_6">Table 5</ref>. As seen from the table, QPIC outperforms both state-of-the-art two-and single-stage methods in all the settings. QPIC with the ResNet-101 backbone yields an especially significant gain of 5.37 mAP (relatively 21.9%) compared with DRG [4] and 8.17 mAP (37.6%) compared with PPDM <ref type="bibr" target="#b16">[17]</ref> in the default full setting. <ref type="table">Table 6</ref> shows the comparison results on V-COCO. QPIC achieves state-of-the-art performance among all the baseline methods. QPIC with the ResNet-50 backbone achieves a 5.7 mAP (10.7%) gain over FCMNet <ref type="bibr" target="#b19">[20]</ref>, which is the strongest baseline. Unlike in the HICO-DET result, the ResNet-50 backbone shows better performance than the ResNet-101 backbone probably because the number of training samples in V-COCO is insufficient to train the large network. Overall, these comparison results demonstrate the dataset-invariant effectiveness of QPIC.</p><p>We then investigate in which cases QPIC especially  achieves superior performance compared with the strong baselines. To do so, we compare the performance of QPIC in detail with DRG <ref type="bibr" target="#b3">[4]</ref> and PPDM <ref type="bibr" target="#b16">[17]</ref>, which are the strongest baselines of the two-and single-stage methods, respectively. We use the ResNet-50 backbone for QPIC in this comparison. Note that hereinafter the distance and area are calculated in normalized image coordinates. <ref type="figure" target="#fig_4">Figure 3a</ref> shows how the performances change as the distance between the center points of a paired human and object bounding box grows. We split HOI instances into bins of size 0.1 according to the distances, and calculate the APs of each bin that has at least 1,000 HOI instances. As shown in <ref type="figure" target="#fig_4">Fig. 3a</ref>, the relative gaps of the performance between QPIC and the other two methods become more evident as the distance grows. The graph suggests three things; HOI detection tends to become more difficult as the distance grows, the distant case is especially difficult for CNN-based methods, and QPIC relatively better deals with this difficulty. The possible explanation for these results is that the features of the CNN-based methods, which rely on limited receptive fields for the feature aggregation, cannot include contextually important information or are dominated by irrelevant information in the distant cases, while the features of QPIC are more effective thanks to the ability of selectively extracting image-wide contextual information. <ref type="figure" target="#fig_4">Figure 3b</ref> presents how the performances change as the areas of target human and object bounding boxes grow. We pick up the larger area of a target human and object bounding box involved in each HOI instance. We then split HOI instances into bins of size 0.1 according to the area, and calculate the APs of each bin that has at least 1,000 HOI instances. As illustrated in <ref type="figure" target="#fig_4">Fig. 3b</ref>, the gaps of the APs between the conventional methods and QPIC tend to grow as the area increases. This is probably because of the combination of the following two reasons; if the area becomes bigger, the area tends to more often include harmful regions such as another HOI instance, and the conventional methods mix up the irrelevant features in such situation, whereas the attention mechanism and the query-based framework enable to selectively aggregate effective features in a separated manner for each HOI instance. These results reveal that the QPIC's significant improvement shown in <ref type="table" target="#tab_6">Table 5</ref> and <ref type="table">Table 6</ref> is likely to  <ref type="figure" target="#fig_6">(Fig. 4a)</ref> 29.04 Two-stage like <ref type="figure" target="#fig_6">(Fig. 4b</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To understand the key ingredients of QPIC's superiority shown in Sec. 4.3, we analyze the key building blocks one by one in detail. We first analyze the interaction detection heads in Sec. 4.4.1 and subsequently analyze the transformer-based feature extractor in Sec. 4.4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Analysis on Detection Heads</head><p>Feasibility of simple heads. As previously mentioned, the inference process of QPIC is simplified thanks to the enriched features from the transformer-based feature extractor. To confirm that this simple prediction is sufficient for QPIC, we investigated if the detection accuracy increases by leveraging a typical point-matching-based detection heads presented in <ref type="bibr" target="#b16">[17]</ref>, which is one of the best performing heuristically-designed heads. <ref type="figure" target="#fig_6">Figure 4a</ref> represents the implemented heads. A notable difference from the original simple heads lies in that the interaction detection heads output center points of target humans and objects instead of bounding boxes. Consequently, the outputs from the interaction detection heads need to be fused with the outputs from the object detection heads with point matching. Note that in this implementation, duplicate detection results that share an identical human-object pair needs to be suppressed by some means such as non-maximum suppression. <ref type="table" target="#tab_3">Table 3</ref> shows the evaluation results. As seen from <ref type="table" target="#tab_3">Table 3</ref>, the point-matching-based heads exhibit no perfor-  mance improvement over the simple heads, which indicates that the simple detection heads are enough and we do not have to manually design complicated detection heads.</p><p>Importance of pairwise detection. Although the detection heads can be as simple as we present, we claim that there is a crucial aspect that must be covered in the design of the heads. It is to treat a target human and object as a pair from early stages rather than to first detect them individually and later integrate the features from the cropped regions corresponding to the detection, as typically done in two-stage approaches. We assume the features from the cropped regions do not contain enough contextual information because sometimes the regions in an image other than a human and object bounding boxes play a crucial role in HOI detection (see <ref type="figure" target="#fig_0">Fig. 1a</ref> for example). We verify this claim by looking into the performance of the two-stage like detection-heads on top of our transformer-based feature extractor, which is exactly the same as original QPIC. <ref type="figure" target="#fig_6">Figure 4b</ref> illustrates the implemented detection heads. This model first derives object detection results from the object detection heads. Then, the results are used to create all the possible human-object pairs. The features of each pair is constructed by concatenating the features from the human and object bounding boxes. The interaction detection head predicts action classes of all the pairs on the basis of the concatenated features. As seen from <ref type="table" target="#tab_3">Table 3</ref>, two-stage like method yields worse performance than the original. This observation indicates that the two-stage methods, which rely on individual feature extraction, do not perform well even with our strong feature extractor, and suggests the importance of the pairwise feature extraction in heads for HOI detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Analysis on Feature Extractor</head><p>Importance of a transformer. To confirm that a transformer-based feature extractor is key to make the simple heads sufficiently work for HOI detection as discussed in Sec. 4.4.1, we replace QPIC's transformer-based feature extractor by a CNN-based counterpart and examine how the performance changes. We utilize the Hourglass-104 backbone used in PPDM <ref type="bibr" target="#b16">[17]</ref> in this experiment. <ref type="table" target="#tab_3">Table 3</ref> shows the performance of the original point-matching- based PPDM as well as its simple-heads variant. The simple-heads variant directly predicts all the information corresponding to a human-object pair on the basis of the features extracted in the feature-extraction stage, just as QPIC's simple heads do. More concretely, not only a human point, an object point, and action classes, but also a human-bounding-box size, an object-bounding-box size, and an object class are directly predicted on the basis of the features at the midpoint between the human and object centers. As <ref type="table" target="#tab_3">Table 3</ref> shows, the simple-heads variant exhibits far worse performance than QPIC. This implicates that the CNN-based feature extractor is not as powerful as our transformer-based feature extractor, so the simple heads cannot be leveraged with it. In addition, we find that the point-matching-based heads, which is the original version of PPDM, achieve higher performance than the simple ones, implying that there is a room for increasing accuracy by heuristically designing the heads if the feature extractor is not so powerful, which is not the case with our powerful transformer-based feature extractor.</p><p>Importance of a decoder. To further dig into the transformer to find out the essential component for HOI detection, we compare four variants listed in <ref type="table" target="#tab_5">Table 4</ref>. The model without the decoder leverages the point-matchingbased method like PPDM <ref type="bibr" target="#b16">[17]</ref> on top of the encoder's output (with encoder) or on top of the base features (without encoder). The model with the decoder utilizes the pointmatching-based heads <ref type="figure" target="#fig_6">(Fig. 4a)</ref> for fair comparison. We use the ResNet-101 backbone for all the variants. As seen from <ref type="table" target="#tab_5">Table 4</ref>, the transformer encoder yields merely slight improvement on HICO-DET (2.52 and 1.18 mAP with and without the decoder, respectively), while the decoder remarkably boosts the performance (9.20 and 7.86 mAP with and without the encoder, respectively). These results indicate that the decoder plays a vital role in HOI detection. Additionally, we evaluate the performance on COCO to compare the degrees of improvement in object detection and HOI detection. As seen in the table, the relative performance improvement brought by the decoder for object detection (on COCO) is 23.9% and 11.8% with and without the encoder, respectively, while that for HOI detection (on HICO-DET) is 45.8% and 41.6% with and without the encoder, respectively. This means that the decoder is more effective in an HOI detection task than in an object detection task. This is probably because the regions of interest (ROI) are mostly consolidated in a single area in object detection tasks, while in HOI detection tasks, the ROI can be diversely distributed image-wide. CNNs, which rely on localized receptive fields, can deal with the former case relatively easily, whereas the image-wide feature aggregation of the decoder is crutial for the latter case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis</head><p>To qualitatively reveal the characteristics of QPIC and the main reasons behind its superior performance over existing methods, we analyze the failure cases of existing methods and QPIC's behavior in the cases. The top row in <ref type="figure" target="#fig_7">Fig. 5</ref> shows the failure cases shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, and the bottom row illustrates the attentions of QPIC on the images. <ref type="figure" target="#fig_7">Figure 5a</ref> and 5b show the cases where DRG fails to detect the action classes, but QPIC does not. As previously discussed, the regions in an image other than a human and object bounding box sometimes contain useful information. <ref type="figure" target="#fig_7">Figure 5a</ref> is a typical example, where the hose held by the boy is likely to be the important contextual information. Two-stage methods that utilize only the region features, namely the human and object bounding box (and sometimes the union region of the two), cannot fully leverage the contextual information, whereas QPIC successfully places the distinguishing focus on such information and leverages it as shown in the attention map. Furthermore, the region features are sometimes contaminated by other region features when target bounding boxes are overlapped. <ref type="figure" target="#fig_7">Figure 5b</ref> shows such an example, where the hand of the blocking man is contained in the bounding box of the catching man. The typical two-stage methods, which rely on region features, cannot exclude this disturbing information, resulting in incorrect detection. QPIC, however, can selectively aggregate only the helpful information for each HOI as shown in the attention map, resulting in the correct detection. <ref type="figure" target="#fig_7">Figure 5c</ref> and 5d illustrate the failure cases of PPDM, whose detection points are drawn in yellow circles. As discussed in Sec. 4.3, features of heuristic detection points are sometimes dominated by irrelevant information such as the non-target human in <ref type="figure" target="#fig_7">Fig. 5c</ref> and another HOI features in <ref type="figure" target="#fig_7">Fig. 5d</ref>. Consequently, the detection based on those confusing features tends to result in failures. QPIC alleviates this problem by incorporating the attention mechanism that selectively captures image-wide features as shown in the attention maps, and thus correctly detects these HOIs.</p><p>Overall, these qualitative analysis demonstrates the QPIC's capability of acquiring image-wide contextual features, which lead to its superior performance over the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed QPIC, a novel detector that can selectively aggregate image-wide contextual information for HOI detection. QPIC leverages an attention mechanism to effectively aggregate features for detecting a wide variety of HOIs. This aggregation enriches HOI features, and as a result, simple and intuitive detection heads are realized. The evaluation on two benchmark datasets showed QPIC's significant superiority over existing methods. The extensive analysis showed that the attention mechanism and querybased detection play a crucial role for HOI detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary V-COCO Settings</head><p>As mentioned in the main manuscript, the images of the V-COCO dataset are split into three sets: a training set, validation set, and testing set. Following previous works, the training and validation sets are combined to train QPIC.</p><p>For calculating the mAP, 5 action classes out of the 29 classes are excluded from the evaluation following <ref type="bibr" target="#b5">[6]</ref>. This is because four of the excluded action classes ("run", "smile", "stand", and "walk") are the action without an object, and one of them ("point") has an insufficient number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Supplementary Implementation Note</head><p>As usual training, we use data augmentation to alleviate over-fitting. We use random horizontal flipping augmentation, scale augmentation, random crop augmentation, which are used in DETR's training <ref type="bibr" target="#b1">[2]</ref>, and color augmentation, which is used in PPDM's training <ref type="bibr" target="#b16">[17]</ref>.</p><p>Since each layer of a transformer decoder output its own set of embeddings D = {d i |d i ∈ R Dc } Nq i=1 , the loss calculation described in Sec. 3.2 of the main manuscript can be conducted for each layer. Following the DETR's training <ref type="bibr" target="#b1">[2]</ref>, these auxiliary losses are calculated to optimize QPIC. To calculate the losses, FFNs are added on top of each decoder layer's output. Note that the parameters of the FNNs are shared among all the decoder layers, In the evaluation time, the second highest scoring class and confidence of the object-class predictionĉ i are used to generate the detection result ifĉ i has the highest score in "no pair" class. This is the technique used in <ref type="bibr" target="#b1">[2]</ref> to optimize the mAPs. <ref type="table" target="#tab_6">Table 5</ref> and <ref type="table">Table 6</ref> show the additional list of the comparison against state-of-the-art on HICO-DET <ref type="bibr" target="#b2">[3]</ref> and V-COCO <ref type="bibr" target="#b6">[7]</ref>, respectively. Six methods (PMFNet <ref type="bibr" target="#b28">[29]</ref>, Wang et al. <ref type="bibr" target="#b29">[30]</ref>, In-GraphNet <ref type="bibr" target="#b33">[34]</ref>, VSGNet <ref type="bibr" target="#b26">[27]</ref>, PD-Net <ref type="bibr" target="#b34">[35]</ref>, and DJ-RM <ref type="bibr" target="#b14">[15]</ref>) are additionally compared in these tables. As stated in Sec. 4.3 of the main manuscript, our QPIC significantly outperforms conventional two-and single-stage methods on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional List of Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational Efficiency Comparison</head><p>To analyze the model efficiency of our QPIC, we compare the inference times of QPIC and PPDM <ref type="bibr" target="#b16">[17]</ref>, which is one of the highest speed models. We used the publicly available source code of PPDM 2 , and tested each model on a single Tesla V100 GPU with CUDA ver. 10.1 and PyTorch ver. 1.5 <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_7">Table 7</ref> shows the comparison result. As the 2 https://github.com/YueLiao/PPDM  <ref type="table">Table 6</ref>. Comparison against state-of-the-art methods on V-COCO. The split of the blocks are the same as <ref type="table" target="#tab_6">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Scenario 1 Scenario 2 VCL <ref type="bibr" target="#b10">[11]</ref> 48.3 -In-GraphNet <ref type="bibr" target="#b33">[34]</ref> 48.9 -DRG <ref type="bibr" target="#b3">[4]</ref> 51.0 -VSGNet <ref type="bibr" target="#b26">[27]</ref> 51.8 57.0 PMFNet <ref type="bibr" target="#b28">[29]</ref> 52.0 -PD-Net <ref type="bibr" target="#b34">[35]</ref> 52.6 -Wang et al. <ref type="bibr" target="#b29">[30]</ref> 52.7 -ACP <ref type="bibr" target="#b12">[13]</ref> 53.0 -FCMNet <ref type="bibr" target="#b19">[20]</ref> 53. 29.90 63 table shows, the inference time of QPIC with the ResNet-50 backbone is smaller by 18 ms than that of PPDM. In particular, PPDM takes 17 ms to organize outputs from the network, while QPIC takes only 5.4 ms to do that. These results indicate that QPIC is more efficient than conventional methods mainly because the simple detection heads of QPIC realize the simple inference procedures. E. Additional Qualitative Analysis <ref type="figure" target="#fig_8">Figure 6</ref> shows the additional failure cases of conventional methods. <ref type="figure" target="#fig_8">Figure 6a</ref> and 6b show the failure cases of DRG <ref type="bibr" target="#b3">[4]</ref>, and <ref type="figure" target="#fig_8">Fig. 6c and 6d</ref> show those of PPDM <ref type="bibr" target="#b16">[17]</ref>, where QPIC successfully detects the human-object interactions (HOIs). As discussed in the main manuscript, the regions in an image other than a human and object bounding box sometimes contain useful information. <ref type="figure" target="#fig_8">Fig. 6a</ref> is a typical example case, where the basketball goal is likely to be the important contextual information. The attention of QPIC shows that it aggregates features from the region of the basketball goal, resulting in the correct detection. <ref type="figure" target="#fig_8">Figure 6b</ref> shows an example case where multiple HOI instances are overlapped. As shown in the figure, the bounding box of the track includes that of the driving human, which may induce contaminated features. The performance is degraded by this contamination. Unlike DRG, QPIC selectively aggregates features for each HOI using the attention mechanism as shown in the attention map, and successfully detects the HOIs. In <ref type="figure" target="#fig_8">Fig. 6c and 6d</ref>, the features of the detection points, which are the locations to predict HOIs in PPDM and drawn in the yellow circles in the figures, are likely to be dominated by irrelevant information because the points are on the background or irrelevant human. As is the case with DRG, PPDM cannot predict HOIs with these contaminated features, while QPIC can do it with the selectively aggregated features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Typical failure cases of conventional methods. The ground-truth human bounding boxes, object bounding boxes, object classes, and action classes are drawn with red boxes, blue boxes, blue characters, and yellow characters, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of the proposed QPIC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>illustrates the overall architecture of QPIC. Given an input image x ∈ R 3×H×W , a feature map z b ∈ R D b ×H ×W is calculated by an arbitrary off-theshelf backbone network, where H and W are the height and width of the input image, H and W are those of the output feature map, and D b is the number of channels. Typically H &lt; H and W &lt; W . z b is then input to a projection convolution layer with a kernel size of 1 × 1 to reduce the dimension from D b to D c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>AP depending on the distance between a human and object center. AP depending on the larger area of a human and object bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Performance analysis on different spatial distribution of HOIs evaluated on HICO-DET.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Interaction detection with two-stage like approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Implemented variants for analyzing detection heads. These heads are on top of our transformer-based feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Failure cases of conventional detectors (top row, same as Fig. 1) and attentions of QPIC (bottom row). In (b) and (d), the attentions corresponding to different HOI instances are drawn with blue and orange, and the areas where two attentions overlap are drawn with white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Typical failure cases of conventional detectors (top row) and attentions of QPIC (bottom row). The ground-truth human bounding boxes, object bounding boxes, object classes, and action classes are drawn with red boxes, blue boxes, blue characters, and yellow characters, respectively. In (b) and (d), the attentions corresponding to different HOI instances are drawn with blue and orange, and the areas where two attentions overlap are drawn with white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison against state-of-the-art methods on HICO-DET. The top, middle, and bottom blocks show the mAPs of the two-stage, single-stage, and our methods, respectively. 17.21 25.55 25.98 19.12 28.03 DRG [4] 24.53 19.47 26.04 27.98 23.11 29.43 UnionDet [12] 17.58 11.72 19.33 19.76 14.68 21.27 Wang et al. [32] 19.56 12.79 21.58 22.05 15.77 23.92 PPDM [17] 21.73 13.78 24.10 24.58 16.65 26.84 Ours (ResNet-50) 29.07 21.85 31.23 31.68 24.14 33.93 Ours (ResNet-101) 29.90 23.92 31.69 32.38 26.06 34.27 Comparison against state-of-the-art methods on V-COCO.The split of the blocks are the same asTable 5.</figDesc><table><row><cell></cell><cell></cell><cell>Default</cell><cell cols="3">Known object</cell></row><row><cell>Method</cell><cell cols="5">full rare non-rare full rare non-rare</cell></row><row><cell>FCMNet [20]</cell><cell cols="5">20.41 17.34 21.56 22.04 18.97 23.13</cell></row><row><cell>ACP [13]</cell><cell cols="2">20.59 15.92 21.98</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">VCL [11] 23.63 Method</cell><cell cols="3">Scenario 1 Scenario 2</cell></row><row><cell>VCL [11]</cell><cell></cell><cell>48.3</cell><cell>-</cell><cell></cell></row><row><cell>DRG</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results of the various detection heads.</figDesc><table><row><cell>Base method</cell><cell>Detection heads</cell><cell>HICO-DET (mAP)</cell></row><row><cell>Ours (ResNet-50)</cell><cell>Simple (original) Point matching</cell><cell>29.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Effect of the transformer encoder and decoder.</figDesc><table><row><cell>Transformer</cell><cell>Transformer</cell><cell>HICO-DET</cell><cell>COCO</cell></row><row><cell>encoder</cell><cell>decoder</cell><cell>(mAP)</cell><cell>(mAP)</cell></row><row><cell></cell><cell></cell><cell>18.89</cell><cell>34.6</cell></row><row><cell></cell><cell></cell><cell>20.07</cell><cell>35.1</cell></row><row><cell></cell><cell></cell><cell>26.75</cell><cell>38.7</cell></row><row><cell></cell><cell></cell><cell>29.27</cell><cell>43.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison against state-of-the-art methods on HICO-DET. The top, middle, and bottom blocks show the mAPs of the two-stage, single-stage, and our methods, respectively. 15.90 22.28 24.78 18.88 26.54 DJ-RM [15] 21.34 18.53 22.18 23.69 20.64 24.60 VCL [11] 23.63 17.21 25.55 25.98 19.12 28.03 DRG [4] 24.53 19.47 26.04 27.98 23.11 29.43 UnionDet [12] 17.58 11.72 19.33 19.76 14.68 21.27 Wang et al. [32] 19.56 12.79 21.58 22.05 15.77 23.92 PPDM [17] 21.73 13.78 24.10 24.58 16.65 26.84 Ours (ResNet-50) 29.07 21.85 31.23 31.68 24.14 33.93 Ours (ResNet-101) 29.90 23.92 31.69 32.38 26.06 34.27</figDesc><table><row><cell></cell><cell>Default</cell><cell></cell><cell cols="2">Known object</cell></row><row><cell>Method</cell><cell cols="4">full rare non-rare full rare non-rare</cell></row><row><cell>PMFNet [29]</cell><cell cols="4">17.46 15.65 18.00 20.34 17.47 21.20</cell></row><row><cell>Wang et al. [30]</cell><cell cols="4">17.57 16.85 17.78 21.00 20.74 21.08</cell></row><row><cell cols="2">In-GraphNet [34] 17.72 12.93 19.31</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VSGNet [27]</cell><cell>19.80 16.05 20.91</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FCMNet [20]</cell><cell cols="4">20.41 17.34 21.56 22.04 18.97 23.13</cell></row><row><cell>ACP [13]</cell><cell>20.59 15.92 21.98</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PD-Net [35]</cell><cell>20.81</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Comparison of the efficiency.</figDesc><table><row><cell></cell><cell></cell><cell>1</cell><cell>-</cell></row><row><cell cols="2">UnionDet [12]</cell><cell>47.5</cell><cell>56.2</cell></row><row><cell cols="2">Wang et al. [32]</cell><cell>51.0</cell><cell>-</cell></row><row><cell cols="2">Ours (ResNet-50)</cell><cell>58.8</cell><cell>61.0</cell></row><row><cell cols="2">Ours (ResNet-101)</cell><cell>58.3</cell><cell>60.7</cell></row><row><cell>Method</cell><cell cols="3">HICO-DET (mAP) Inference time (ms)</cell></row><row><cell>PPDM [17]</cell><cell></cell><cell>21.73</cell><cell>64</cell></row><row><cell>Ours (ResNet-50)</cell><cell></cell><cell>29.07</cell><cell>46</cell></row><row><cell>Ours (ResNet-101)</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A few previous works inappropriately use COCO train2017 set for pre-training, whose images are contained in the V-COCO test set.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DRG: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">iCAN: Instance-centric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Visual semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">UnionDet: Union-level detector towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions with action co-occurrence priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryn</forename><surname>Yaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Quart</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PPDM: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action-guided attention mining and relation reasoning network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xixia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Amplifying key cues for human-object-interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">VS-GNet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contextual heterogeneous graph network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yingbiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep contextual attention for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interact as you intend: Intention-driven human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A graph-based interactive reasoning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for robust human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cascaded human-object interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
