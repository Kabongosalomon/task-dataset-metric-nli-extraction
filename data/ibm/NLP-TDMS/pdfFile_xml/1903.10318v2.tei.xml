<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-tune BERT for Extractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<email>yang.liu2@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-tune BERT for Extractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>, a pre-trained Transformer <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref> model, has achieved ground-breaking performance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple variant of BERT, for extractive summarization. Our system is the state of the art on the CNN/Dailymail dataset, outperforming the previous best-performed system by 1.65 on ROUGE-L. The codes to reproduce our results are available at https://github. com/nlpyang/BertSum</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information. The task has received much attention in the natural language processing community due to its potential for various information access applications. Examples include tools which digest textual content (e.g., news, social media, reviews), answer questions, or provide recommendations.</p><p>The task is often divided into two paradigms, abstractive summarization and extractive summarization. In abstractive summarization, target summaries contains words or phrases that were not in the original text and usually require various text rewriting operations to generate, while extractive approaches form summaries by copying and concatenating the most important spans (usually sentences) in a document. In this paper, we focus on extractive summarization.</p><p>Although many neural models have been proposed for extractive summarization recently <ref type="bibr" target="#b4">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b10">Nallapati et al., 2017;</ref><ref type="bibr" target="#b11">Narayan et al., 2018;</ref><ref type="bibr" target="#b6">Dong et al., 2018;</ref><ref type="bibr" target="#b16">Zhang et al., 2018;</ref>, the improvement on automatic metrics like ROUGE † Please see https://arxiv.org/abs/1908.08345 for the full and most current version of this paper has reached a bottleneck due to the complexity of the task. In this paper, we argue that, BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>, with its pre-training on a huge dataset and the powerful architecture for learning complex features, can further boost the performance of extractive summarization .</p><p>In this paper, we focus on designing different variants of using BERT on the extractive summarization task and showing their results on CNN/Dailymail and NYT datasets. We found that a flat architecture with inter-sentence Transformer layers performs the best, achieving the state-ofthe-art results on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Let d denote a document containing several sentences [sent 1 , sent 2 , · · · , sent m ], where sent i is the i-th sentence in the document. Extractive summarization can be defined as the task of assigning a label y i ∈ {0, 1} to each sent i , indicating whether the sentence should be included in the summary. It is assumed that summary sentences represent the most important content of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extractive Summarization with BERT</head><p>To use BERT for extractive summarization, we require it to output the representation for each sentence. However, since BERT is trained as a masked-language model, the output vectors are grounded to tokens instead of sentences. Meanwhile, although BERT has segmentation embeddings for indicating different sentences, it only has two labels (sentence A or sentence B), instead of multiple sentences as in extractive summarization. Therefore, we modify the input sequence and embeddings of BERT to make it possible for extracting summaries.</p><p>Encoding Multiple Sentences As illustrated in <ref type="figure">Figure 1</ref>, we insert a [CLS] token before each sen- Interval Segment Embeddings We use interval segment embeddings to distinguish multiple sentences within a document. For sent i we will assign a segment embedding E A or E B conditioned on i is odd or even. For example, for [sent 1 , sent 2 , sent 3 , sent 4 , sent 5 ] we will assign</p><formula xml:id="formula_0">[E A , E B , E A , E B , E A ].</formula><p>The vector T i which is the vector of the i-th [CLS] symbol from the top BERT layer will be used as the representation for sent i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-tuning with Summarization Layers</head><p>After obtaining the sentence vectors from BERT, we build several summarization-specific layers stacked on top of the BERT outputs, to capture document-level features for extracting summaries. For each sentence sent i , we will calculate the final predicted scoreŶ i . The loss of the whole model is the Binary Classification Entropy ofŶ i against gold label Y i . These summarization layers are jointly fine-tuned with BERT.</p><p>Simple Classifier Like in the original BERT paper, the Simple Classifier only adds a linear layer on the BERT outputs and use a sigmoid function to get the predicted score:</p><formula xml:id="formula_1">Y i = σ(W o T i + b o )<label>(1)</label></formula><p>where σ is the Sigmoid function.</p><p>Inter-sentence Transformer Instead of a simple sigmoid classifier, Inter-sentence Transformer applies more Transformer layers only on sentence representations, extracting document-level features focusing on summarization tasks from the BERT outputs:</p><formula xml:id="formula_2">h l = LN(h l−1 + MHAtt(h l−1 )) (2) h l = LN(h l + FFN(h l ))<label>(3)</label></formula><p>where h 0 = PosEmb(T ) and T are the sentence vectors output by BERT, PosEmb is the function of adding positional embeddings (indicating the position of each sentence) to T ; LN is the layer normalization operation <ref type="bibr">(Ba et al., 2016)</ref>; MHAtt is the multi-head attention operation <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref>; the superscript l indicates the depth of the stacked layer. The final output layer is still a sigmoid classifier:</p><formula xml:id="formula_3">Ŷ i = σ(W o h L i + b o )<label>(4)</label></formula><p>where h L is the vector for sent i from the top layer (the L-th layer ) of the Transformer. In experiments, we implemented Transformers with L = 1, 2, 3 and found Transformer with 2 layers performs the best.  <ref type="bibr">, 2018)</ref>. Therefore, we apply an LSTM layer over the BERT outputs to learn summarization-specific features.</p><p>To stabilize the training, pergate layer normalization <ref type="bibr">(Ba et al., 2016)</ref> is applied within each LSTM cell. At time step i, the input to the LSTM layer is the BERT output T i , and the output is calculated as:</p><formula xml:id="formula_4">    F i I i O i G i     = LN h (W h h i−1 ) + LN x (W x T i ) (5) C i = σ(F i ) C i−1 + σ(I i ) tanh(G i−1 ) (6) h i =σ(O t ) tanh(LN c (C t ))<label>(7)</label></formula><p>where F i , I i , O i are forget gates, input gates, output gates; G i is the hidden vector and C i is the memory vector; h i is the output vector; LN h , LN x , LN c are there difference layer normalization operations; Bias terms are not shown. The final output layer is also a sigmoid classifier:Ŷ</p><formula xml:id="formula_5">i = σ(W o h i + b o )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section we present our implementation, describe the summarization datasets and our evaluation protocol, and analyze our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>We use PyTorch, OpenNMT <ref type="bibr" target="#b9">(Klein et al., 2017)</ref> and the 'bert-base-uncased' * version of BERT to implement the model. BERT and summarization layers are jointly fine-tuned. Adam with β 1 = 0.9, β 2 = 0.999 is used for fine-tuning. Learning rate schedule is following <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref> with warming-up on first 10,000 steps: lr = 2e −3 · min(step −0.5 , step · warmup −1.5 )</p><p>All models are trained for 50,000 steps on 3 GPUs (GTX 1080 Ti) with gradient accumulation * https://github.com/huggingface/pytorch-pretrained-BERT per two steps, which makes the batch size approximately equal to 36. Model checkpoints are saved and evaluated on the validation set every 1,000 steps. We select the top-3 checkpoints based on their evaluation losses on the validations set, and report the averaged results on the test set.</p><p>When predicting summaries for a new document, we first use the models to obtain the score for each sentence. We then rank these sentences by the scores from higher to lower, and select the top-3 sentences as the summary.</p><p>Trigram Blocking During the predicting process, Trigram Blocking is used to reduce redundancy. Given selected summary S and a candidate sentence c, we will skip c is there exists a trigram overlapping between c and S. This is similar to the Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) but much simpler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Summarization Datasets</head><p>We evaluated on two benchmark datasets, namely the CNN/DailyMail news highlights dataset <ref type="bibr" target="#b8">(Hermann et al., 2015)</ref> and the New York Times Annotated Corpus (NYT; <ref type="bibr" target="#b13">Sandhaus 2008)</ref>. The CNN/DailyMail dataset contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. We used the standard splits of <ref type="bibr" target="#b8">Hermann et al. (2015)</ref> for <ref type="bibr">training,</ref><ref type="bibr">validation,</ref><ref type="bibr">and testing (90,</ref><ref type="bibr">266/1,</ref><ref type="bibr">220/1,</ref><ref type="bibr">093 CNN documents and 196,</ref><ref type="bibr">961/12,</ref><ref type="bibr">148/10,</ref>. We did not anonymize entities. We first split sentences by CoreNLP and preprocess the dataset following methods in <ref type="bibr" target="#b14">See et al. (2017)</ref>.</p><p>The NYT dataset contains 110,540 articles with abstractive summaries. Following <ref type="bibr" target="#b7">Durrett et al. (2016)</ref>, we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We took 4,000 examples from the training set as the validation set. We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The filtered test set (NYT50) includes 3,452 test examples. We first split sentences by CoreNLP and pre-process the dataset following methods in <ref type="bibr" target="#b7">Durrett et al. (2016)</ref>.</p><p>Both datasets contain abstractive gold summaries, which are not readily suited to training extractive summarization models. A greedy algorithm was used to generate an oracle summary for each document. The algorithm greedily select sentences which can maximize the ROUGE scores as the oracle sentences. We assigned label 1 to sentences selected in the oracle summary and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>The experimental results on CNN/Dailymail datasets are shown in <ref type="table">Table 1</ref>. For comparison, we implement a non-pretrained Transformer baseline which uses the same architecture as BERT, but with smaller parameters. It is randomly initialized and only trained on the summarization task.</p><p>The Transformer baseline has 6 layers, the hidden size is 512 and the feed-forward filter size is 2048.</p><p>The model is trained with same settings following <ref type="bibr" target="#b15">Vaswani et al. (2017)</ref>. We also compare our model with several previously proposed systems.</p><p>• LEAD is an extractive baseline which uses the first-3 sentences of the document as a summary.</p><p>• REFRESH <ref type="bibr" target="#b11">(Narayan et al., 2018)</ref> is an extractive summarization system trained by globally optimizing the ROUGE metric with reinforcement learning.</p><p>• NEUSUM  is the state-ofthe-art extractive system that jontly score and select sentences.</p><p>• PGN <ref type="bibr" target="#b14">(See et al., 2017)</ref>, is the Pointer Generator Network, an abstractive summarization system based on an encoder-decoder architecture.</p><p>• DCA <ref type="bibr" target="#b2">(Celikyilmaz et al., 2018)</ref> is the Deep Communicating Agents, a state-of-the-art abstractive summarization system with multiple agents to represent the document as well as hierarchical attention mechanism over the agents for decoding.</p><p>As illustrated in the table, all BERT-based models outperformed previous state-of-the-art models by a large margin. BERTSUM with Transformer achieved the best performance on all three metrics. The BERTSUM with LSTM model does not have an obvious influence on the summarization performance compared to the Classifier model.</p><p>Ablation studies are conducted to show the contribution of different components of BERTSUM. The results are shown in in <ref type="table" target="#tab_4">Table 2</ref>. Interval segments increase the performance of base model. Trigram blocking is able to greatly improve the summarization results. This is consistent to previous conclusions that a sequential extractive decoder is helpful to generate more informative summaries. However, here we use the trigram blocking as a simple but robust alternative.  The experimental results on NYT datasets are shown in <ref type="table">Table 3</ref>. Different from CNN/Dailymail, we use the limited-length recall evaluation, fol-lowing <ref type="bibr" target="#b7">Durrett et al. (2016)</ref>. We truncate the predicted summaries to the lengths of the gold summaries and evaluate summarization quality with ROUGE Recall. Compared baselines are (1) Firstk words, which is a simple baseline by extracting first k words of the input article; (2) Full is the best-performed extractive model in <ref type="bibr" target="#b7">Durrett et al. (2016)</ref>; (3) Deep Reinforced <ref type="bibr" target="#b12">(Paulus et al., 2018</ref>) is an abstractive model, using reinforce learning and encoder-decoder structure. The BERTSUM+Classifier can achieve the state-of-theart results on this dataset.  <ref type="table">Table 3</ref>: Test set results on the NYT50 dataset using ROUGE Recall. The predicted summary are truncated to the length of the gold-standard summary. Results with * mark are taken from the corresponding papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we explored how to use BERT for extractive summarization. We proposed the BERT-SUM model and tried several summarization layers can be applied with BERT. We did experiments on two large-scale datasets and found the BERT-SUM with inter-sentence Transformer layers can achieve the best performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>...... ...... ......</figDesc><table><row><cell>Input Docum ent</cell><cell></cell><cell></cell></row><row><cell>Token Em beddings</cell><cell></cell><cell></cell></row><row><cell>Inter val Segm ent</cell><cell></cell><cell></cell></row><row><cell>Em beddings</cell><cell></cell><cell></cell></row><row><cell>Position Em beddings</cell><cell></cell><cell></cell></row><row><cell cols="2">Sum m ar ization Layer s</cell><cell></cell></row><row><cell>Y 1</cell><cell>Y 2</cell><cell>Y 3</cell></row><row><cell cols="3">Figure 1: The overview architecture of the BERTSUM model.</cell></row><row><cell>tence and a [SEP] token after each sentence. In</cell><cell></cell><cell></cell></row><row><cell>vanilla BERT, The [CLS] is used as a symbol to</cell><cell></cell><cell></cell></row><row><cell>aggregate features from one sentence or a pair of</cell><cell></cell><cell></cell></row><row><cell>sentences. We modify the model by using mul-</cell><cell></cell><cell></cell></row><row><cell>tiple [CLS] symbols to get features for sentences</cell><cell></cell><cell></cell></row><row><cell>ascending the symbol.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Recurrent Neural Network Although the Transformer model achieved great results on several tasks, there are evidence that Recurrent Neural Networks still have their advantages, especially when combining with techniques in Transformer (Chen et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Results of ablation studies of BERTSUM on</cell></row><row><cell>CNN/Dailymail test set using ROUGE F 1 (R-1 and R-</cell></row><row><cell>2 are shorthands for unigram and bigram overlap, R-L</cell></row><row><cell>is the longest common subsequence).</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The use of mmr and diversity-based reranking for reodering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep communicating agents for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Conference</title>
		<meeting>the NAACL Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Conference</title>
		<meeting>the ACL Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Conference</title>
		<meeting>the ACL Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Banditsum: Extractive summarization as a contextual bandit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Crawford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP Conference</title>
		<meeting>the EMNLP Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Herke van Hoof, and Jackie Chi Kit Cheung</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning-based single-document summarization with compression and anaphoricity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Conference</title>
		<meeting>the ACL Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1701.02810</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference</title>
		<meeting>the AAAI Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ranking sentences for extractive summarization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Conference</title>
		<meeting>the NAACL Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICLR Conference</title>
		<meeting>the ICLR Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The New York Times Annotated Corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Conference</title>
		<meeting>the ACL Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural latent extractive document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP Conference</title>
		<meeting>the EMNLP Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural document summarization by jointly learning to score and select sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Conference</title>
		<meeting>the ACL Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
