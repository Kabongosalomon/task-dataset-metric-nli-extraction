<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D2-Net: Weakly-Supervised Action Localization via Discriminative Embeddings and Denoised Activations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fahad</roleName><forename type="first">Shahbaz</forename><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">D2-Net: Weakly-Supervised Action Localization via Discriminative Embeddings and Denoised Activations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes a weakly-supervised temporal action localization framework, called D2-Net, which strives to temporally localize actions using video-level supervision. Our main contribution is the introduction of a novel loss formulation, which jointly enhances the discriminability of latent embeddings and robustness of the output temporal class activations with respect to foreground-background noise caused by weak supervision. The proposed formulation comprises a discriminative and a denoising loss term for enhancing temporal action localization. The discriminative term incorporates a classification loss and utilizes a topdown attention mechanism to enhance the separability of latent foreground-background embeddings. The denoising loss term explicitly addresses the foreground-background noise in class activations by simultaneously maximizing intra-video and inter-video mutual information using a bottom-up attention mechanism. As a result, activations in the foreground regions are emphasized whereas those in the background regions are suppressed, thereby leading to more robust predictions. Comprehensive experiments are performed on two benchmarks: THUMOS14 and ActivityNet1.2. Our D2-Net performs favorably in comparison to the existing methods on both datasets, achieving gains as high as 3.6% in terms of mean average precision on THUMOS14.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action localization is a challenging problem, which aims to jointly classify and localize the temporal boundaries of actions in videos. Most existing approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b27">28]</ref> are based on strong supervision, requiring manually annotated temporal boundaries of actions during training. In contrast to these strong framelevel supervision based methods, weakly-supervised action localization learns to localize actions in videos, leveraging only video-level supervision. Weakly-supervised action lo-calization is therefore of greater importance since the manual annotation of temporal boundaries in videos is laborious, expensive and prone to large variations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Existing methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref> for weaklysupervised action localization typically use video-level annotations in the form of action classes and learn a sequence of class-specific scores, called temporal class activation maps (TCAMs). In general, a classification loss is used to obtain the discriminative foreground regions in TCAMs. Some approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref> learn TCAMs using action labels and obtain temporal boundaries via a post-processing step, while others <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b12">13]</ref> use a TCAM-generating video classification branch along with an explicit localization branch to directly regress action boundaries. Nevertheless, the localization performance is heavily dependent on the quality of the TCAMs. The quality of TCAMs is likely to improve in fully-supervised settings where frame-level annotations are available. Such frame-level information (true foreground and background regions) are unavailable in the weakly-supervised paradigm. In such a paradigm, the predicted foreground regions often overlap with the groundtruth background regions, while predicted background regions are likely to overlap with the ground-truth foreground regions. This leads to noisy activations, i.e., false positives and false negatives, in the learned TCAMs. Most existing weakly-supervised action localization methods that learn TCAMs typically rely on separating foreground and background regions (foreground-background separation) and do not explicitly handle its noisy outputs.</p><p>In this work, we address the problem of foregroundbackground separation along with explicit tackling of noise in TCAMs for weakly-supervised action localization. We propose a unified loss formulation that is jointly optimized to classify and temporally localize action snippets (group of frames) in videos. Our loss formulation comprises a discriminative and a denoising loss term. The discriminative loss seeks to maximally separate backgrounds from actions (foregrounds) via interlinked classification and localization learning objectives (Sec. 3.1). The denoising loss (Sec. <ref type="bibr" target="#b2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2)</head><p>Ground-truth TCAM Discriminative loss term Baseline: Without Discriminative and Denoising loss terms D2-Net: Discriminative + Denoising loss terms <ref type="figure">Figure 1</ref>. Impact of our proposed loss formulation on the quality of the output TCAMs. Compared to the baseline (without our discriminative and denoising loss terms), the introduction of the discriminative loss term improves the separation between foreground and background activations (e.g., third and fourth ground-truth action instance from the left). Furthermore, our final D2-Net comprising both the discriminative and the denoising loss terms reduces the noise in the TCAMs, leading to more robust TCAMs. complements the discriminative term by explicitly addressing the foreground-background noise in activations, thereby producing robust TCAMs (see <ref type="figure">Fig. 1</ref>).</p><p>In our loss formulation, we learn distinct latent embeddings such that their foreground-background separation is maximized based upon the corresponding top-down attention generated from the output TCAMs. Furthermore, the embeddings are employed to generate pseudo-labels based on their foreground scores (bottom-up attention). These pseudo-labels are utilized to explicitly handle the noise by emphasizing the corresponding output activations in pseudoforeground regions, while suppressing the activations in pseudo-background regions. This pseudo-background suppression and pseudo-foreground enhancement is achieved by maximizing the mutual information (MI) between activations and generated pseudo-labels within an action video (intra-video). Maximizing MI between predicted activations and labels decreases the uncertainty of predictions, leading to more robust predictions. In addition to capturing intra-video MI, our formulation also strives to maximize MI between the action class predictions and video-level ground-truth labels, across videos in a mini-batch (inter-video). Contributions: We introduce a weakly-supervised action localization framework, D2-Net, which incorporates a novel loss formulation that jointly enhances the foregroundbackground separability and explicitly tackles the noise to robustify the output TCAMs. Our main contributions are:</p><p>• We introduce a discriminative loss term, which simultaneously aims at video categorization and enhanced foreground-background separation. • We introduce a denoising loss term to improve the robustness of TCAMs. Our denoising loss explicitly addresses noise in TCAMs by maximizing the MI be-tween activations and labels within a video (intra-video) and across videos (inter-video). To the best of our knowledge, we are the first to introduce a loss term that simultaneously captures MI across multiple snippets within a video and across all videos in a batch for weakly-supervised action localization. • Comprehensive experiments are performed on the THU-MOS14 <ref type="bibr" target="#b5">[6]</ref> and ActivityNet1.2 <ref type="bibr" target="#b2">[3]</ref> benchmarks. Our D2-Net performs favorably against existing weaklysupervised methods on both datasets, achieving gains as high as 3.6% in terms of mAP on THUMOS14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several weak supervision strategies have been explored in the context of action localization, including category labels <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>, sparse temporal points <ref type="bibr" target="#b14">[15]</ref>, order of actions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref> and instance count <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref>. Most existing weakly-supervised action localization methods employ category labels as weak supervision and typically utilize features extracted from backbone networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4]</ref> trained on the action recognition task. The work of <ref type="bibr" target="#b29">[30]</ref> proposes a selection module for detecting the relevant temporal segments and employs a classification loss for training. The Autoloc method <ref type="bibr" target="#b26">[27]</ref> extends <ref type="bibr" target="#b29">[30]</ref> by adding an explicit localization branch and utilizes an outer-inner contrastive loss for its training. While in <ref type="bibr" target="#b17">[18]</ref>, additional supervision of action instance count is used to delineate adjacent action instances, the training of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref> comprises several refinement passes, where the model is trained iteratively in each pass using the results from the previous pass as pseudo-supervision. In contrast, the approach of <ref type="bibr" target="#b20">[21]</ref> utilizes classification and similarity-based losses to match similar segments of actions in paired videos, while <ref type="bibr" target="#b6">[7]</ref> employs a deep metric learning approach towards the same end. Both these methods constrain the mini-batch to contain multiple videos of the same actions. Furthermore, existing approaches, including <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref>, do not explicitly address the issue of a large number of easy negatives overwhelming a smaller number of hard positives.</p><p>Different from aforementioned works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref>, our approach explicitly addresses the issue of large number of easy negatives overwhelming a smaller number of hard positives through sample re-weighting. Furthermore, our approach places no batch constraint and performs foregroundbackground separation by inter-linking classification and localization objectives.</p><p>The work of <ref type="bibr" target="#b19">[20]</ref> employs a background-aware loss along with a self-guided loss for modeling the background. The approach of <ref type="bibr" target="#b16">[17]</ref> additionally utilizes an iterative multi-pass erasing step for discovering different action segments in TCAMs. Unlike these works, the training in <ref type="bibr" target="#b13">[14]</ref> alternates between updating a key-instance assignment branch and a classification branch via Expectation Maximization. However, all these approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14]</ref> aggregate per-snippet  losses for training and do not explicitly capture the mutual information (MI) between the activations and labels, which is likely to be more beneficial due to the absence of snippetlevel labels in a weakly-supervised setting.</p><p>Different from existing methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>, our approach addresses the problem of foregroundbackground noise by exploiting both inter-video and intravideo MI between the class activations and their corresponding labels, resulting in robust TCAMs. To the best of our knowledge, we are the first to propose a weakly-supervised action localization approach that simultaneously captures MI across multiple snippets within a video and across all videos in a mini-batch (see also <ref type="figure">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Our D2-Net strives to improve the separation of foreground-background feature representations in videos, while jointly enhancing the robustness of output TCAMs w.r.t. foreground-background noise. This leads to better differentiation between foreground actions and surrounding background regions, resulting in enhanced action localization in the challenging weakly-supervised setting. Here, we first present our overall architecture, followed by a detailed description of our proposed losses for training D2-Net. Overall architecture of D2-Net is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Given a video v, we divide it into non-overlapping snippets of L = 16 frames each. Features are then extracted to encode appearance (RGB) and motion (optical flow) information. Similar to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18]</ref>, we use the Inflated 3D (I3D) <ref type="bibr" target="#b3">[4]</ref> to obtain d = 2048 dimensional features for each 16-frame snippet. Let F ∈ R s×d denote features for a video, where s is the number of snippets. The extracted features become the inputs to our D2-Net, which comprises two parallel streams for RGB and optical flow. Each stream consists of three temporal convolutional (TC) layers. The first two layers learn latent discriminative embeddings x(t) ∈ R d/2 (with time t ∈ [1, s]), from the input features F. The output of the final TC layer is passed through a sigmoid activation. Subsequently, the outputs from both streams are averaged to obtain TCAMs T ∈ R s×C representing a sequence of class-specific scores over time for C action classes. The main contribution of our work is the introduction of a novel loss formulation to train the proposed D2-Net. Our training objective combines a discriminative (L Dis ) and a denoising term (L D ), with a balancing weight α,</p><formula xml:id="formula_0">L = L Dis + αL D .<label>(1)</label></formula><p>These two loss terms utilize foreground-background attention sequences computed in opposite directions: (i) the discriminative loss L Dis utilizes a top-down attention, which is computed from the output TCAMs (the top-most layer) and (ii) the denoising loss L D utilizes a bottom-up attention, which is derived from the foreground scores of the latent embeddings (intermediate layer features). We describe these losses in detail in Sec. 3.1 and 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.Foreground-Background Discriminability:L Dis</head><p>In this work, we introduce a discriminative loss (L Dis ) to learn separable class-agnostic foreground and action-free background feature representations, in terms of latent embeddings, using a top-down attention from the TCAMs. The embedding of a video with s snippets is defined by a weighted temporal pooling based on the class activations T ∈ R s×C . Let the top-down foreground attention λ(t) = max c T[t, c] denote the maximum foreground activation across all action classes c ∈ {1, . . . , C}, where t ∈ [1, s] and C is the number of classes. Then, the class-agnostic foreground and background embeddings are:</p><formula xml:id="formula_1">x f g = λ(t)&gt;τ λ(t)x(t), x bg = λ b (t)&gt;τ λ b (t)x(t), (2) where τ =0.5 and λ b (t)=1−λ(t)</formula><p>is the background attention. Maximizing the distance between foreground and background embeddings enhances the separability of the corresponding output activations, leading to improved localization. Furthermore, enhancing the separation between a foreground embedding of a video and background embeddings of different videos improves the generalizability. In addition, minimizing the intra-class (foreground/background classes) distance at a coarse-level amongst embeddings of different videos ensures that embeddings of the same class are clustered together. This intra-class compactness is achieved by using a weight γ with low magnitude. Three weight terms, w f b , w f g and w bg , are introduced in our L Dis , targeting foreground-background separation, foreground grouping and background grouping, respectively. They are defined as:</p><formula xml:id="formula_2">w f b = max(0, cos(x f g ,x bg )), w f g = γ(1 − cos(x f g ,x f g )), w bg = γ(1 − cos(x bg ,x bg )),<label>(3)</label></formula><p>where x andx denote embeddings from different videos in a mini-batch. Alongside robust localization, our other objective is the multi-label classification of action categories. A major challenge is introduced by the class-imbalance problem, where easy background snippets overwhelmingly outnumber the hard foregrounds. To address this, inspired by the focal loss for object detection <ref type="bibr" target="#b10">[11]</ref>, we propose to include penalty terms based on the weights (Eq. 3), in our L Dis . To this end, a video-level prediction p ∈ R C is obtained by performing a temporal top-k pooling on T. Our L Dis term, which jointly addresses the class-imbalance and enhances foreground-background separation, is defined by</p><formula xml:id="formula_3">L Dis = − c:y[c]=1 (1 − p[c] + w f g + w f b ) β log(p[c]) − c:y[c]=0 (p[c] + w bg + w f b ) β log(1 − p[c]),<label>(4)</label></formula><p>where y ∈ {0, 1} C denotes the video-level label and β is the focusing parameter. The first term in Eq. 4 denotes the loss for a positive action class, while the second term incorporates the loss for a negative class. The weight term w f b (see Eq. 3) is added for both positive action classes and background classes since it represents the foregroundbackground separation. The terms w f g and w bg enhance intra-class compactness for the positive and background classes, respectively. The first term in Eq. 4 indicates that the loss due to a positive action class c is low only when (i) its predicted probability p[c] is high, and (ii) the foreground grouping w f g and foreground-background separation w f b for the corresponding video are both simultaneously low. A similar observation holds in the second term for the negative class. Thus, L Dis enhances the discriminability of embeddings x(t) by encouraging foreground-background separation while simultaneously achieving classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Robust Temporal Class Activation Maps: L D</head><p>Our discriminative loss L Dis , introduced in Sec. 3.1, aims to enhance the distinctiveness of latent embeddings, leading to improved action localization. However, under weak supervision, the temporal locations of foreground and background regions are unknown. This results in a noisy top-down attention as it relies on noisy output temporal class activations learned from video-level labels. Consequently, noise is likely to be introduced in the foreground and background embeddings (x f g and x bg ), which are learned from the top-down attention λ(t). Our goal is to explicitly reduce the noise caused by the absence of snippet-level labels under weak supervision, thereby improving the robustness of the output class activations with respect to the foreground-background noise. To this end, we introduce a denoising loss L D . Our L D comprises a novel pseudo-Determinant based Mutual Information (pDMI) loss, which exploits both intra-video and inter-video mutual information (MI) between the class activations and corresponding labels.</p><p>Our pseudo-Determinant based Mutual Information (pDMI) loss is inspired by the Determinant based Mutual Information (DMI) <ref type="bibr" target="#b33">[34]</ref>. The original DMI, proposed for multi-class classification, is computed as the determinant of a joint distribution matrix, i.e., DMI(P, Y)=| det(U)|.</p><p>Here, U = 1 /nPY is the joint distribution over the predicted posterior probabilities P and the ground-truth (noisy) labels Y. The matrices P and Y are of sizes C × n and n × C, where n denotes the mini-batch size and C the number of classes. The DMI loss L dmi is defined as</p><formula xml:id="formula_4">L dmi = −E[log(| det(U)|)],<label>(5)</label></formula><p>where E denotes Expectation. Note that L dmi depends on the determinant of U. To ensure a non-zero det(U), the label matrix Y must be full-rank, i.e., a mini-batch must contain instances from all classes. This is prohibitive for a large number of classes. Furthermore, such mini-batch sampling for action localization leads to memory issues in GPUs due </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-snippet / per-video loss Mutual Information (MI) based loss</head><p>At video-level, C = #Action classes.</p><p>n: #snippets in a video (snippet-level) OR #videos in a batch (video-level) p 1 p 2 p n <ref type="figure">Figure 3</ref>. A conceptual illustration of loss computation with (on the right) and without (on the left) capturing mutual information (MI). Typically, existing methods compute the loss without MI (e.g., cross-entropy loss) by aggregating individual losses (Li) between prediction pi and labels yi either at a per-video or per-snippet level. Instead, we compute a collective loss across (i) all snippets within a video (snippet-level) and (ii) all videos in a batch (video-level), by capturing the MI between predictions (P) and labels (Y).</p><p>to the long duration of untrimmed videos in the dataset, especially when capturing inter-video mutual information.</p><p>Our pDMI loss overcomes these limitations and ensures a non-degenerate value of DMI, since it avoids an explicit computation of the determinant. To this end, we first observe that, when the DMI loss tends to zero, the determinant of the joint distribution tends to one. Formally,</p><formula xml:id="formula_5">L dmi − → 0 =⇒ det(U) − → 1 =⇒ U − → I.<label>(6)</label></formula><p>As a result, DMI is maximum when | det(U)| = 1 with the identity matrix I as an optimal solution for the joint distribution matrix U of size C × C (since elements of U ∈ [0, 1]). Furthermore, the condition number η for the optimal solution I is minimum, i.e. η = 1. Hence, instead of maximizing the determinant of the distribution U, we can alternatively minimize its condition number. In effect, U becomes better-conditioned and this improves the robustness of the activations towards label noise. The proposed pseudo-DMI loss L pdmi is then given by</p><formula xml:id="formula_6">L pdmi = E[log(pDMI(P, Y))] = E[log(η U )],<label>(7)</label></formula><p>where η U denotes the condition number of U. Since the rank of U is r ≤ C, η U is computed as σ 1 /σ r , where {σ 1 , . . . , σ r } are non-zero singular values of U. Thus, our pDMI loss avoids an explicit computation of the determinant and overcomes the limitations of the standard DMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Snippet-level and Video-level Noise Removal</head><p>To enhance the robustness of TCAMs, we employ the pDMI loss L pdmi at two levels: (i) a snippet-level to exploit intravideo MI, and (ii) a video-level to exploit inter-video MI. The snippet-level denoising step incorporates a bottom-up attention to emphasize the foreground activations, while suppressing the background ones by capturing the MI between the temporal activations and their corresponding foreground labels within a video. On the other hand, the video-level denoising step exploits MI between the video representations and corresponding labels, across videos, to achieve the same objective. <ref type="figure">Fig. 3</ref> shows a conceptual illustration of loss computation with and without capturing MI. Snippet-level joint distribution: The snippet-level joint distribution captures the MI between the foregroundbackground activations and the snippet-level pseudo-labels within a video. For this, we utilize a bottom-up attention mechanism, which encodes the foreground scores of latent embeddings for the corresponding snippets. These foreground scores are computed w.r.t. a reference background feature embedding x ref . The foreground score λ (t) of a latent embedding x(t) at time t is then given by </p><formula xml:id="formula_7">λ (t) = 0.5(1 − cos(x(t), x ref )), t ∈ [1, s],<label>(8)</label></formula><formula xml:id="formula_8">P 1 = λ f λ b 1 − λ f 1 − λ b , Y 1 = 1 /z 1 n f 0 n f 0 n b 1 n b ,<label>(9)</label></formula><p>where z = n f + n b , P 1 ∈ R 2×z , Y 1 ∈ R z×2 , 1 k and 0 k are k dimensional column vectors of ones and zeros. The snippet-level foreground-background joint distribution is defined as</p><formula xml:id="formula_9">U 1 = P 1 Y 1 .</formula><p>Video-level joint distribution: Unlike the snippet-level joint distribution where the noise arises due to the absence of ground-truth (snippet-level) labels, the noise in the videolevel joint distribution stems from the video-level prediction p ∈ R C . This noise in p is predominantly caused by the temporal top-k pooling operation. Under the weakly-supervised setting, there is no guarantee that all the top-k activations for an action class will belong to that class. Furthermore, actions in untrimmed videos may not necessarily span k = s/8 snippets. Hence, denoising the video-level prediction p eventually results in improved robustness of the class activations output at the snippet-level. In order to obtain a video-level joint distribution U 2 that captures the MI between class activations and action classes across videos, we compute the prediction P 2 and label Y 2 as P 2 = p 1 , . . . , p n , Y 2 = 1 /n y 1 , . . . , y n , <ref type="bibr" target="#b9">(10)</ref> where p i ∈ R C and y i ∈ {0, 1} C denote the video-level representation and associated label of the i-th video in the mini-batch. The joint distribution is defined as</p><formula xml:id="formula_10">U 2 = P 2 Y 2 .</formula><p>Since the joint distribution is over the action classes C, U 2 is a C × C matrix. We finally define our denoising loss as</p><formula xml:id="formula_11">L D = L DS + L DV (11) = E[log(pDMI(P 1 , Y 1 ))] + E[log(pDMI(P 2 , Y 2 ))],</formula><p>where the pDMI loss is given by Eq. 7. Here, L DS and L DV denote the snippet-level and video-level losses. Thus, our denoising loss improves the TCAMs, at the snippet-level and video-level, by making them robust to the foregroundbackground noise under the weakly-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference: Action Localization from TCAMs</head><p>At inference, given a video, D2-Net outputs a bottom-up attention sequence λ (Eq. 8) of length s and a class activation map T of size s × C. We perform top-k pooling to obtain the predicted class probabilities p ∈ R C , which are then used to find the relevant action classes above a threshold p th = 0.5 max(p). For every relevant class c, its corresponding class activations T c ∈ R s are multiplied element-wise with λ ∈ R s to obtain a refined sequence r c = λ T c . The snippets with activations above a threshold are retained and a 1-D connected component is used to obtain segment proposals. Multiple thresholds are used to obtain a larger pool of proposals. Each proposal is then scored using the contrast between the mean activation of the proposal itself and its surrounding areas <ref type="bibr" target="#b26">[27]</ref>, S = S i − S o , where S i and S o respectively denote the mean activation of the proposal and its neighboring background. The neighboring background is obtained by inflating the proposal on either side by 25% of its width, as in <ref type="bibr" target="#b26">[27]</ref>. Proposals with high overlap are removed using class-wise non-maximal suppression. Finally, a threshold S th on the proposal score is applied to retain the high-scoring detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets: We evaluate D2-Net on two challenging temporal action localization benchmark datasets, containing untrimmed videos with varying degrees of activity duration. The THUMOS14 <ref type="bibr" target="#b5">[6]</ref> dataset contains temporal annotations for 200 validation and 212 test videos from 20 action categories. The dataset is challenging since each video contains 15 action instances on average. Similar to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1]</ref>, the validation and test set are used for training and evaluating our D2-Net, respectively. The ActivityNet1.2 <ref type="bibr" target="#b2">[3]</ref> dataset has temporal annotations of 100 action categories with 4819 <ref type="table">Table 1</ref>. State-of-the-art comparison on the THUMOS14 dataset. Methods with superscript '+' require strong frame-level supervision for training. Our D2-Net performs favorably in comparison to existing weakly-supervised methods and achieves consistent improvements, in terms of mean average precision (mAP). training and 2383 validation videos, with each video having 1.5 activity instances on average. As in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">21]</ref>, we use the training and validation sets to respectively train and evaluate. Implementation details: The optical flow frames of a video are generated using the TV-L1 flow <ref type="bibr" target="#b35">[36]</ref>. For each 16-frame snippet, 2048-d features are extracted by concatenating the activations of Mixed_5c after average pooling from the RGB and Flow I3D models pre-trained on Kinetics <ref type="bibr" target="#b3">[4]</ref>. The kernel size and dilation rate of the temporal convolutional layers are: (3, 1) for THUMOS14 and (5, 2) for ActivityNet1.2. The first two convolutions in each stream are followed by a leaky ReLU with 0.2 negative slope. Our D2-Net is trained with a mini-batch size of 10 for 20K iterations, using the Adam <ref type="bibr" target="#b7">[8]</ref> optimizer with a 10 −4 learning rate and 0.005 weight decay. The k for top-k is set to s/8 , as in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18]</ref>. All the hyperparameters are chosen via cross-validation. The balancing parameter α is set to 0.2 and 10 −3 for THUMOS14 and ActivityNet1.2, respectively. The intra-class compactness weight γ and focusing parameter β are set to 0.01 and 2, respectively for both datasets. Multiple thresholds from 0.025 to 0.5 with increments of 0.025 are used for proposal generation. The overlap threshold for NMS is set to 0.5 and the score threshold S th for retaining detections in a video is set to 10% of the maximum proposal score in that video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">State-of-the-art Comparison</head><p>Tab. 1 and 2 compare D2-Net with state-of-the-art methods on THUMOS14 and ActivityNet1.2, respectively. Methods with '+' require strong supervision for training. THUMOS14: Similar to ours, all weakly-supervised methods in Tab. 1 use an I3D backbone, except Autoloc <ref type="bibr" target="#b26">[27]</ref>, which uses TSN <ref type="bibr" target="#b30">[31]</ref>. While BM <ref type="bibr" target="#b19">[20]</ref> and BaS-Net <ref type="bibr" target="#b9">[10]</ref> consider an additional background class and train using a <ref type="table">Table 2</ref>. State-of-the-art comparison on the ActivityNet1.2 dataset. Our D2-Net performs favorably compared to existing weakly-supervised approaches. Furthermore, our D2-Net performs comparably to SSN <ref type="bibr" target="#b37">[38]</ref>, which is trained with strong supervision (denoted with superscript '+'). AVG denotes the mean of the mAP values for IoU in [0.5, 0.95] with steps of 0.05. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>As discussed earlier, our D2-Net comprises a discriminative L Dis and a denoising loss L D . Here, we perform comparisons by replacing the two proposed loss terms (L Dis and L D ) in our framework with either the standard crossentropy loss L CE or the focal loss L F . In addition, we also show the performance of our D2-Net with only L Dis . Tab. 3 presents these performance comparisons, in terms of mAP and F1, on THUMOS14. Employing a standard cross-entropy loss (L CE in Tab. 3) in our framework results in an mAP score of 23.0 at IoU=0.5. We observe that training with the standard focal loss (obtained by zeroing the weights w in Eq. 4) helps alleviate the issue of a large number of easy samples overwhelming hard samples. This setting, L F in Tab. 3, gains 3.7% mAP at IoU=0.5 over L CE , thereby highlighting the need to tackle imbalance between easy backgrounds and hard foregrounds. To the best of our <ref type="table">Table 3</ref>. Performance comparison by replacing our two loss terms (LDis and LD) in the proposed D2-Net with either the standard cross-entropy loss (LCE) or the focal loss (LF ). In addition, we also show the performance of our D2-Net with only LDis. Results are shown in terms of mAP and F1 score at IoU=0.5, on THUMOS14. Replacing the proposed loss terms in our framework with LCE and LF results in mAP scores at IoU=0.5 of 23.0 and 26.7, respectively. Our D2-Net with the discriminative loss term LDis achieves consistent improvement in performance over LF with an absolute gain of 5.5% in terms of mAP at IoU=0.5. Furthermore, our final D2-Net comprising both loss terms (LDis and LD) achieves the best performance with absolute gains of 12.9% and 9.2% in terms of mAP at IoU=0.5 over LCE and LF , respectively. knowledge, we are the first to evaluate the standard focal loss, L F , in weakly-supervised action localization setting. Our D2-Net with the discriminative loss term L Dis , which jointly addresses class-imbalance and enhances backgroundforeground separation, provides consistent improvements over L F and achieves 32.2% mAP at IoU=0.5. An absolute gain of 5.5% in terms of mAP at IoU=0.5 is obtained by the introduction of our proposed L Dis in place of L F . Furthermore, our D2-Net comprising both L Dis and L D obtains the best results with an mAP score of 35.9% at IoU=0.5. Our D2-Net achieves absolute gains of 12.9% and 9.2% in terms of mAP at IoU=0.5, over L CE and L F , respectively. It is noteworthy that our final D2-Net, containing both L Dis and L D , obtains a significant gain of 5.9% in terms of F1 score over L Dis alone. This improvement over L Dis alone is obtained due to explicitly addressing the noise in TCAMs by our L D , leading to a substantial reduction (28%) in the number of false positives without affecting the recall. Impact of MI-based denoising: We also perform an experiment by replacing the proposed pDMI loss in our L D with the standard L1 and BCE losses for denoising the snippetlevel activations. On THUMOS14, the L1 and BCE losses, which do not explicitly capture MI, achieve mAP scores of 32.9% and 33.5% at IoU=0.5, respectively. Our proposed D2-Net, which employs MI-based pDMI loss in L D , achieves improved results with an mAP score at IoU=0.5 of 35.9%. These results suggest that our MI-based denoising is able to robustify the TCAMs in a weakly-supervised setting. Qualitative results: <ref type="figure">Fig. 4</ref> shows a qualitative comparison between the baseline (red) and D2-Net (blue), along with the ground-truth (GT) action segments (green). Here, the baseline employs L F . Example test videos with Diving and Throw Discus actions from THUMOS14 are shown in the first two rows. The baseline incorrectly merges multiple  <ref type="figure">Figure 4</ref>. Qualitative temporal action localization results of our proposed D2-Net on example test videos, with Diving, Throw Discus actions from THUMOS14, and Mowing Lawn activity from ActivityNet1.2. For each video, example frames (top row), ground-truth GT segments (green), baseline detections (red) and D2-Net detections (blue) are shown. The height of a detection is indicative of its score. The Baseline incorrectly merges multiple GT instances, has false positives in background regions and falsely detects the presence of the activity over the entire video length. Our D2-Net correctly detects multiple instances (e.g., 1 to 5 GT in Diving, 3 to 5 in Throw Discus) and suppresses most false positives in the background regions, achieving promising localization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>Proposed (D2-Net)</p><p>Background embedding Foreground embedding <ref type="figure">Figure 5</ref>. Illustration of foreground-background separability obtained in the latent embedding space of (a) the baseline using the standard focal loss and (b) our D2-Net via t-SNE scatter plots on the THUMOS14 test set. In both cases, foreground and background embeddings per video are obtained as the mean of latent embeddings at their respective ground-truth locations. Our D2-Net better separates the foreground and background, compared to the baseline.</p><p>GT instances (e.g., 1 to 5 GT in Diving) and produces false positives in background regions (e.g., towards the beginning of Diving video). Our D2-Net correctly detects these multiple action instances and suppresses most false positives in the background regions. The third row shows an example test video with Mowing Lawn activity from ActivityNet1.2.</p><p>The baseline incorrectly detects the presence of the activity over the entire video length. In contrast, our D2-Net improves the detection of multiple activity instances, leading to promising localization performance. <ref type="figure">Fig. 5</ref> shows the foreground-background separability comparison, in terms of t-SNE scatter plots, between the baseline and our D2-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a weakly-supervised action localization approach, called D2-Net, that comprises a discriminative and a denoising loss. The discriminative loss term strives for improved foreground-background separability through interlinked classification and localization objectives. The denoising loss term complements the discriminative term by tackling the foreground-background noise in the activations. This is achieved by maximizing the mutual information between activations and labels within a video (intra-video) and across videos (inter-video). We evaluate our D2-Net on two benchmarks. Our results show that D2-Net performs favorably against existing methods on both datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of our D2-Net. The focus of our design is the introduction of a novel loss formulation that jointly enhances the discriminability of latent embeddings and explicitly addresses the foreground-background noise in the output class activations. The network comprises two identical parallel streams (RGB and flow) consisting of three temporal convolutional TC layers. The second TC layer activations from both streams are averaged to obtain latent embeddings x. The final outputs of both streams are then averaged to obtain the temporal class activation maps (TCAMs) T of untrimmed input videos. A discriminative loss LDis (Sec. 3.1) is introduced to enhance the foreground-background separability ( ) of embeddings x by utlizing a top-down attention mechanism, in addition to achieving video classification. Furthermore, a denoising loss LD (Sec. 3.2) is introduced to explicitly address the foreground-background noise ( ) in the class activations of T, by utilizing a bottom-up attention. The network is trained jointly using both loss terms LDis and LD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>:</head><label></label><figDesc>Prediction vector of size C y i : Label vector of size C At snippet-level, C = 2 (fg vs bg).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where x ref [m] = 0.9x ref [m − 1] + 0.1x µ bg [m] is progressively computed as a running mean of the embeddings x bg over m iterations. Here, x µ bg [m] denotes the mean of the background embeddings in a mini-batch at iteration m. Let t f = {t : λ (t) &gt; 0.5} and t b = {t : λ (t) &lt; 0.5} represent the time instants for selecting the foreground and background activations with respect to λ (t). Using the set of pseudo-foreground temporal locations t f , a row matrix λ f of width n f = |t f | is constructed using top-down attention values, λ(t), t ∈ t f . Similarly, λ b of width n b = |t b | is constructed for the pseudo-background snippets. Then, a prediction matrix P 1 representing the top-down attention values at the corresponding foreground and background pseudo-locations obtained from the bottom-up attention and a pseudo-label matrix Y 1 are given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Dis + L D 65.6 60.0 52.1 43.3 35.9 36.6</figDesc><table><row><cell>term</cell><cell>0.1</cell><cell>mAP @ IoU 0.2 0.3 0.4</cell><cell>0.5</cell><cell>F1</cell></row><row><cell>L CE</cell><cell cols="4">55.0 47.6 38.7 30.7 23.0 23.5</cell></row><row><cell>L F</cell><cell cols="4">58.8 52.4 44.3 35.7 26.7 27.2</cell></row><row><cell>L Dis</cell><cell cols="4">65.4 59.7 50.1 40.4 32.2 30.7</cell></row><row><cell>D2-Net: L</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Refineloc: Iterative refinement for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00227</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The thumos challenge on action recognition for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization using deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization through contrast based evaluation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Weaklysupervised action localization with expectationmaximization multi-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00163</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial backgroundaware loss for weakly-supervised temporal activity localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06643</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action completeness modeling with background aware networks for weaklysupervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Moniruzzaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruwen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">C</forename><surname>Leu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3c-net: Category count and center loss for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Phuc Xuan Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">W-talc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fineto-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling the temporal extent of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Satkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action snippets: How many frames does human action recognition require</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autoloc: weaklysupervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multistage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hide-andseek: Forcing a network to be meticulous for weaklysupervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">L_dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Segregated temporal assembly recurrent networks for weakly supervised multiple action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JPRS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Two-stream consensus network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11594</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
