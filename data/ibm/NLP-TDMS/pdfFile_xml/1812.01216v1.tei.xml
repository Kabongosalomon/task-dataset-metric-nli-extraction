<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parameter Re-Initialization through Cyclical Batch Size Schedules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
							<email>mahoneymw@berkeley.edu</email>
						</author>
						<title level="a" type="main">Parameter Re-Initialization through Cyclical Batch Size Schedules</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optimal parameter initialization remains a crucial problem for neural network training. A poor weight initialization may take longer to train and/or converge to sub-optimal solutions. Here, we propose a method of weight re-initialization by repeated annealing and injection of noise in the training process. We implement this through a cyclical batch size schedule motivated by a Bayesian perspective of neural network training. We evaluate our methods through extensive experiments on tasks in language modeling, natural language inference, and image classification. We demonstrate the ability of our method to improve language modeling performance by up to 7.91 perplexity and reduce training iterations by up to 61%, in addition to its flexibility in enabling snapshot ensembling and use with adversarial training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite many promising empirical results at using stochastic optimization methods to train highly non-convex modern deep neural networks, we still lack theoretically robust practical methods which are able to escape saddle points and/or sub-optimal local minima and converge to parameters that retain high testing performance. This lack of understanding leads to practical training challenges.</p><p>Stochastic Gradient Descent (SGD) is currently the de-facto optimization method for training deep neural networks (DNNs). Through extensive hyper-parameter tuning, SGD can avoid poor local optima and achieve good generalization ability. One important hyper-parameter that can significantly affect SGD performance is the weight initialization. For instance, initializing the weights to all zeros or all ones leads to extremely poor performance <ref type="bibr" target="#b23">[24]</ref>. Different approaches have been proposed for weight initialization such as Xavier, MSRA, Ortho, LSUV <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>. These are mostly agnostic to the model architecture and the specific learning task.</p><p>Our work explores the idea of adapting the weight initialization to the optimization dynamics of the specific learning task at hand. From the Bayesian perspective, improved weight initialization can be viewed as starting with a better prior, which leads to a more accurate posterior and thus better generalization ability. This problem has been explored extensively in Bayesian optimization. For example, in the seminal works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>, an adaptive prior is implemented via Markov Chain Monte Carlo (MCMC) methods. Motivated by these ideas, we incorporate an "adaptive initialization" for neural network training (see section 2 for details), where we use cyclical batch size schedules to control the noise (or temperature) of SGD. As argued in <ref type="bibr" target="#b20">[21]</ref>, both learning rate and batch size can be used to control the noise of SGD but the latter has an advantage in that it allows more parallelization opportunity <ref type="bibr" target="#b3">[4]</ref>. The idea of using batch size to control the noise in a simple cyclical schedule was recently proposed in <ref type="bibr" target="#b10">[11]</ref>. Here, we build upon this work by studying different cyclical annealing strategies for a wide range of problems. Additionally, we discuss how this can be combined with a new adversarial regularization scheme recently proposed in <ref type="bibr" target="#b24">[25]</ref>, as well as prior work <ref type="bibr" target="#b9">[10]</ref> in order to obtain ensembles of models at no additional cost. In summary, our contributions are as follows:</p><p>• We explore different cyclical batch size (CBS) schedules for training neural networks inspired by Bayesian statistics, particularly adaptive MCMC methods. The CBS schedule leads to multiple perplexity improvement (up to 7.91) in language modeling and minor improvements in natural language inference and image classification. Furthermore, we show that CBS schedule can alleviate problems with overfitting and sub-optimal parameter initialization. • Additionally, CBS schedules require up to 3× fewer SGD iterations due to larger batch sizes, which allows for more parallelization opportunity. This reflects the benefit of cycling the batch size instead of the learning rate as in prior work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref> • We showcase the flexibility of CBS schedules for use with additional techniques. We propose a simple but effective ensembling method that combines models saved during different cycles at no additional training cost. In addition, we show that CBS schedule can be combined with other approaches such as the recently proposed adversarial regularization <ref type="bibr" target="#b24">[25]</ref> to yield further classification accuracy improvement of 0.26%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>[5] introduced Xavier initialization, which keeps the variance of input and output of all layers within a similar range in order to prevent vanishing or exploding values in both the forward and backward passes. Building off this idea, <ref type="bibr" target="#b6">[7]</ref> explored a new strategy known as MSRA to keep the variance constant for all convolutional layers. <ref type="bibr" target="#b18">[19]</ref> proposed an orthogonal initialization (Ortho) to achieve faster convergence, and more recently, <ref type="bibr" target="#b16">[17]</ref> combined ideas from previous work and showed that a unit variance orthogonal initialization is beneficial for deep models. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref> show that the noise of SGD is controlled by the ratio of learning rate to batch size. The authors argued that the SGD algorithm can be derived through Euler-Maruyama discretization of a Stochastic Differential Equation (SDE). The SDE dynamics are governed by a "noise scale" g ≈ N/B for the learning rate, N the training dataset size, and B the batch size. They conclude that a higher noise scale prevents SGD from settling into sharper minima. This result supports a prior empirical observation <ref type="bibr" target="#b11">[12]</ref> that under certain mild assumptions such as N B, the effect of dividing the learning rate by a constant factor is equivalent to that of multiplying the batch size by the same constant factor. In related work, <ref type="bibr" target="#b21">[22]</ref> applied this understanding and used batch size as a knob to control the noise, and empirically showed that the baseline performance could be matched. <ref type="bibr" target="#b24">[25]</ref> further explored how to use second-order information and adversarial training to control the noise for training large batch size. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> showed using a statistical mechanics argument that many other hyper-parameters in neural network training, e.g. data quality, can also act as temperature knobs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The goal of neural network optimization is to solve an empirical risk minimization, with a loss function of the form:</p><formula xml:id="formula_0">L(θ) = 1 N N i=1 l(x i , θ),<label>(1)</label></formula><p>where θ is the model parameters, X is the training dataset and l(x, θ) is the loss function. Here N = |X| is the cardinality of the training set. In SGD, a mini-batch, B ⊂ {1, 2, ..., N } is used to compute an (unbiased) gradient, i.e., g t = 1</p><p>|B| x∈B ∇ θ l(x, θ t ), and this is typically used to optimize (1) in the form:</p><formula xml:id="formula_1">θ t+1 = θ t − η t g t ,<label>(2)</label></formula><p>where η t is the learning rate (step size) at iteration t, and commonly annealed during training.</p><p>By Bayes' Theorem, given the input data, X, a prior distribution on the model parameters, P (θ), and a likelihood function, P (X|θ), the posterior distribution, P (θ|X), is:</p><formula xml:id="formula_2">P (θ|X) ∝ P (θ)P (X|θ).<label>(3)</label></formula><p>From this Bayesian perspective, the goal of the neural network training is to find the Maximum-A-Posteriori (MAP) point for a given prior distribution. Note that in this context weight initialization and prior distribution are similar, that is a better prior distribution would lead to more informative posterior. In general, it may be difficult to design a better prior given only data and a model architecture. Additionally, the high dimensionality of the NN's parameter space renders various approaches such as adaptive priors intractable (e.g. adaptive MCMC algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>). Hence, we look into an adaptive weight "re-initialization" strategy. We start with an input prior (weight initialization) and compute an approximate MAP point by annealing the noise in SGD. Once we compute the MAP point, we use it as a new initialization of the neural network weights, and restart the noise annealing schedule. We then iteratively repeat this process through the training process.</p><p>One approach to controlling the level of noise in SGD is via the learning rate, which is the approach used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>. However, as discussed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>, the batch size can also be used to control SGD noise. The motivation for this is that larger batch sizes allow for parallel execution which can accelerate training. We implement weight re-initialization through cyclical batch size schedules. The SGD training process is divided into one or more cycles, and in single cycle we gradually increase the batch size to decrease noise. As the noise level of SGD is annealed, θ will approaches a local minima i.e., an approximate MAP point of P (θ|X). Then at the beginning of the subsequent cycle we drop the batch size back down to the initial value, which increases the noise in SGD and "re-initializes" the neural network parameters using the previous estimate. Several CBS schedules are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We perform a variety of experiments across different tasks and neural network architectures in natural language processing as well as image classification. We report our experimental findings on language tasks in section 3.1, and image classification in section 3.2. We illustrate that CBS schedules can alleviate sub-optimal initialization in section 3.3. We follow the baseline training method for each task (for details please see Appendix A). Alongside testing/validation performance, we also report the number of training iterations (lower values are preferred).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Results</head><p>Language modeling is a challenging problem due to the complex and long-range interactions between distant words <ref type="bibr" target="#b15">[16]</ref>. One hope is that large/deep models might be able to capture these complex interactions, but large models easily overfit on these tasks and exhibit large gaps between training set and testing set performance. CBS schedules effectively help us avoid overfitting, and in addition snapshot ensembling enables even greater performance.</p><p>We evaluate a large variety of CBS schedules to positive results as shown in <ref type="table" target="#tab_0">Table 1</ref>. Results are measured in perplexity, a standard figure of merit for evaluating the quality of language models by measuring its prediction of the empirical distribution of words (lower perplexity value is better). As we can see, the best performing CBS schedules result in significant improvements in perplexity (up to 7.91) over the baseline schedules and also offer reductions in the number of SGD training iterations (up to 33%). For example, CBS schedules achieve improvement of 7.91 perplexity improvement on WikiText 2 via CBS-1-T and reduce the SGD iterations from 164k to 111k via the CBS-1-A schedule. Notice that almost all CBS schedules outperform the baseline schedule.  <ref type="table" target="#tab_0">Table 1</ref>). Notice the cyclical spikes in  training and testing perplexity. The peaks occur during decreases in batch size, i.e., increases in noise scale, which could help to escape sub-optimal local minima, and the troughs occur during increases in batch size, i.e., decreases with noise scale.</p><p>In order to support our claim that CBS schedules are especially useful for counteracting overfitting, we conducted additional language modeling experiments on models L1', L2' with PTB and WT2 which use significantly lower dropout (0.2 and 0.3) than the original L1, L2 models (0.5 and 0.65). Because these models heavily overfit the training data, we report both the final testing perplexity as well as the best testing perplexity achieve during training. As seen in As mentioned above the goal of every cycle is to get an approximate MAP point. A very interesting idea proposed in <ref type="bibr" target="#b9">[10]</ref> is to ensemble these MAP points by saving snapshots of the model at the end of every cycle. We follow that strategy with the only difference that we use a batch size cycle instead of cyclical learning rate proposed in <ref type="bibr" target="#b9">[10]</ref>  To further explore the properties of cyclical batch size schedules, we also evaluate these schedules on natural language inference tasks, as shown in <ref type="table" target="#tab_3">Table 2</ref>. In our experiments, CBS schedules do not yield large performance improvements on models like E1 which exhibit smaller disparities between training and testing performance. This is in line with our limitation in that CBS is more effective for models which tend to overfit. On the other hand, we see a large reduction in training iterations by up to 62% which is due to higher effective batch size used in CBS than baseline. We also test our CBS schedules on Cifar-10 and ImageNet. <ref type="table">Table.</ref> 3 reports the testing accuracy and the number of training iterations for different models on Cifar-10. We see that the CBS schedules match baseline performance, but the number of training iterations used in CBS schedules is up to 2× fewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image Classification Results</head><p>As seen in <ref type="figure" target="#fig_4">Fig. 3</ref>, the training curves of CBS schedules also exhibit the aforementioned cyclical spikes both in training loss and testing accuracy. Similarly in the previously discussed language experiments, these spikes correspond to cycles in the CBS schedules and can be thought of as re-initializations of the neural network weights. We observe that CBS achieves similar performance to the baseline. We offer further support for the hypothesis that CBS schedules are more effective for overfitting neural networks with experiments on model C4, which achieves 94.35% training accuracy and 55.55% testing accuracy on Cifar-10. With CBS-15, we see 90.71% training accuracy and 56.44% testing accuracy, which is a larger improvement than that offered by CBS on convolutional models on Cifar-10.</p><p>We also explore combining CBS with the recent adversarial regularization proposed by <ref type="bibr" target="#b24">[25]</ref>. Combining CBS-15 on C2 with this strategy improves accuracy to 94.82%. This outperforms other schedules shown in <ref type="table" target="#tab_4">Table 3</ref>. Applying snapshot ensembling on C3 trained with CBS-15-2 leads to improved accuracy of 93.56% as compared to 92.58%. After ensembling ResNet50 on Imagenet with snapshots from the last two cycles, the performance increases to 76.401% from 75.336%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sub-optimal Initialization</head><p>Various effective initialization methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>  We test a Gaussian initialization with mean 0 and standard deviation 0.1 on an AlexNet-like model (C1). The baseline (BL) training follows the same setting as described in Appendix A and achieves  final accuracy 84.27%. For CBS, we use cycle width of 10 with 3 steps. In particular, CBS 1 denotes a constant learning rate, and achieves final accuracy 85.41%. CBS 2 decays the learning rate by a factor of 5 at epoch 75 and achieves final accuracy 84.95%. We keep learning rate high during training because a high noise level helps θ escape sub-optimal local minima. Notice that all CBS methods achieve better generalization performance than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work we explored different cyclical batch size (CBS) schedules for training neural networks. We framed the motivation behind CBS schedules through the lens of Bayesian statistical methods, in particular adaptive MCMC algorithms, which seek out better estimates of the posterior starting with a (poor) prior distribution. In the context of neural network training, this translates to re-initialization of the weights via cycling between large and small batch sizes which control the noise in SGD. We show empirical results which find this cyclical batch size schedule can significantly outperform fixed batch size baselines, especially in networks prone to overfitting or initialized poorly, on the tasks of language modeling, natural language inference, and image classification with LSTMs, CNNs, and ResNets. In our language modeling experiments, we see that a wide variety of CBS schedules outperform the baseline by up to 7.91 perplexity and up to 33% fewer training iterations. For natural language inference and image classification tasks, we observe a reduction in the number of training iterations of up to 61%, which translates directly into reduced runtime. Finally, we demonstrate the flexibility of CBS as a building block for ensembling and adversarial training methods. Ensembling on language modeling yields improvements of up to 11.22 perplexity over the baseline and on image classification, an improvement of up to 1.07% accuracy. Adversarial training in conjunction with CBS gives a bump in image classification accuracy of 0.26%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>We believe that it is very important for every work to state its limitations (in general, but in particular in this area). We performed an extensive variety of experiments on different tasks in order to comprehensively test the algorithm. The primary limitation of our work is that cyclical batch size schedules introduce another hyper-parameter that requires manual tuning. We note that this is also true for cyclical learning rate schedules, and hope to address this using second order methods <ref type="bibr" target="#b24">[25]</ref> as part of future work. Furthermore, for well initialized models which are not prone to overfitting, single snapshot CBS achieves similar performance to the baseline, although the cyclical ensembling provides a modicum of improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head><p>Here we catalogue details regarding all tasks, datasets, models, batch schedules, and other hyperparameters used in our experiments. In all experiments, we try to copy as many hyper-parameters from the original papers as possible.</p><p>Tasks: We train networks to perform the following supervised learning tasks:</p><p>• Image classification. The network is trained to classify the content of images within a fixed set of object classes. • Language modeling. The network is trained to predict the last token in a sequence of English words. • Natural Language Inference. The network is trained to classify the relationship between pairs of English sentences such as that of entailment, contradiction, or neutral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We train networks on the following datasets.</p><p>• Cifar (image classification). The two Cifar (i.e., Cifar-10/Cifar-100) datasets <ref type="bibr" target="#b12">[13]</ref> contain 50k training images and 10k testing images, and 10/100 label classes. • ImageNet (image classification). The ILSVRC 2012 classification dataset consists of 1000 label classes, with a total of 1.2 million training images and 50,000 validation images. During training, we crop the image to 224 × 224. • PTB (language modeling). The Penn Tree Bank dataset consists of preprocessed and tokenized sentences from the Wall Street Journal. The training set is 929k words, the validation set 73k words, and test set 82k words. The total vocabulary size is 10k, and all words outside the vocabulary are replaced by a placeholder token. • WikiText 2 (language modeling). The Wikitext 2 dataset is modeled after the Penn Tree Bank dataset and consists of preprocessed and tokenized sentences from Wikipedia. The training set is 2089k words, the validation set 218k words, and the test set 246k words. The total vocabulary size is 33k, and all words outside the vocabulary are replaced by a placeholder token. • SNLI (natural language inference). The SNLI dataset <ref type="bibr" target="#b0">[1]</ref> consists of pairs of sentences annotated with one of three labels regarding textual entailment information: contradiction, neutral, or entailment. The training set contains 550k pairs, and the validation set contains 10k pairs. • MultiNLI (natural language inference. The MultiNLI dataset <ref type="bibr" target="#b22">[23]</ref> is modeled after the SNLI dataset and contains a training set of 393k pairs and a validation set of 20k pairs.</p><p>Model Architecture. We implement the following neural network architectures.</p><p>• C1. AlexNet-like on Cifar-10 dataset as in <ref type="bibr" target="#b25">[26]</ref>[C1], trained on the task of image classification. We train for 200 epochs with an initial learning rate 0.02 which we decay by a factor of 5 at epoch 30, 60. In particular, we use initial learning rate 0.05 for cyclic scheduling. • C2. WResNet 16-4 on Cifar-10 dataset <ref type="bibr" target="#b26">[27]</ref>, trained on the task of image classification. We train for 200 epochs with an initial learning rate 0.1 which we decay by a factor of 5 at epoch 60, 120, and 180. • C3. ResNet20 on Cifar-10 dataset <ref type="bibr" target="#b7">[8]</ref>. We train it for 160 epochs with initial learning rate 0.1, and decay a factor of 5 at epoch 80, 120. In particular, we use initial learning rate 0.05 for cyclic scheduling. • C4. MLP3 network from <ref type="bibr" target="#b14">[15]</ref>. The network consists of 3 fully connected layers with 512 units each and ReLU activations. As a baseline, we train this network with vanilla SGD for 240 epochs with a batch size of 100 and an initial learning rate of 0.1, which is decayed by a factor of 10 at 150 and 225 epochs. • I1. ResNet50 on ImageNet dataset <ref type="bibr" target="#b7">[8]</ref>, trained on the task of image classification for 90 epochs with initial learning rate 0.1 which we decay by a factor of 10 at epoch 30, 60 and 80. • L1. Medium Regularized LSTM <ref type="bibr" target="#b27">[28]</ref>, trained on the task of language modeling. We use 50% dropout on non-recurrent connections and train for 39 epochs with initial learning rate of 20, decaying by a factor of 1.2 every epoch after epoch 6. We set a backpropagation-through-time limit of 35 steps and clip the max gradient norm at 0.25. • L2. Large Regularized LSTM <ref type="bibr" target="#b27">[28]</ref>, trained on the task of language modeling. We use 65% dropout on non-recurrent connections and train for 55 epochs with initial learning rate of 20, decaying by a factor of 1.15 every epoch after epoch <ref type="bibr">14.</ref> We set a backpropagation-through-time limit of 35 steps and clip the max gradient norm at 0.5.</p><p>• L1', L2' Identical to L1, L2 except for lower dropout: 0.2, 0.3 respectively. Leads to significant overfitting, evidenced by test perplexity curve in <ref type="figure" target="#fig_6">Fig. 5</ref>. • E1. ESIM <ref type="bibr" target="#b1">[2]</ref>. We train the base ESIM model without the tree-LSTM, as in <ref type="bibr" target="#b22">[23]</ref>, on the task of natural language inference with ADAM for 10 epochs on MultiNLI and also SNLI.</p><p>Training Schedules: We use the following batch size schedules</p><p>• BL. Use a fixed small batch size as specified in the original paper introducing the model or as is standard. • CBS-k(-n). Use a Cyclical Batch Size schedule, where k is the width of each step measured in epochs and n is the integer number of steps per cycle. When n is not specified it refers to the default value of 4. At the beginning of each cycle the batch size is initialized to the base batch size, and after each step it is then doubled. • CBS-k(-n)-A. Use an aggressive Cyclical Batch Size schedule, which is equivalent to the original CBS schedule except after every step the batch size is quadrupled. • CBS-k(-n)-T. Use a triangular Cyclical Batch Size schedule, which is modeled after the triangular schedule. Each cycle consists of n steps doubling the batch size after each step, then n − 2 symmetrical steps halving the batch size after each step.</p><p>In all language modeling CBS experiments, we use an initial batch size of 10, that is, half the baseline batch size as reported in the respective papers of each baseline model tested. The intuition behind starting with a smaller batch size is to introduce additional noise to help models escape sub-optimal local minima.</p><p>For adversarial training used in image classification, we use FGSM method <ref type="bibr" target="#b5">[6]</ref> to generate adversarial examples. Adversarial training is implemented for the first half training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Results</head><p>This section shows additional experiment results.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of 6 different CBS schedules, with initial batch size of 10; see Appendix A for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>shows the training and testing perplexity of the L2 model on PTB and WikiTest 2 as trained via the baseline schedule along with our best CBS schedule (from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Training (left) and testing (right) perplexity as a function of iterations for the L2 model on PTB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>shows the results of ResNet50 on ImageNet. The baseline trains in 450k iterations and reaches 76.134% validation accuracy. With CBS, the final validation accuracy is 76.336%, trained in 262k parameter updates. CBS outperforms the baseline on both training loss and validation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>C2 model (WResNet) on Cifar-10. Training set loss (left), and testing set accuracy (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>I1 model (ResNet50) on ImageNet. Training set loss (left), and testing set accuracy (right), evaluated as a function of iterations (above) and epochs (below).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Training (left) and testing (right) perplexity as a function of epoch for overfitting L2' model on Penn Tree Bank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Training (left) and testing (right) perplexity as a function of epoch for L2 model on WikiText 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Testing perplexity and number of parameter updates of L1 and L2 models on Penn Tree Bank (PTB) and WikiText 2 (WT2) datasets. The best perplexity and lowest number of updates are bolded.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">L1 on PTB</cell><cell cols="3">L1 on WT2</cell><cell cols="2">L2 on PTB</cell><cell>L2 on WT2</cell></row><row><cell cols="3">Schedule</cell><cell>Per.</cell><cell># Iters</cell><cell>Per.</cell><cell cols="2"># Iters</cell><cell>Per.</cell><cell># Iters</cell><cell>Per.</cell><cell># Iters</cell></row><row><cell>BL 2</cell><cell></cell><cell></cell><cell>83.13</cell><cell>52k</cell><cell cols="3">96.41 116k</cell><cell>79.34</cell><cell>73k</cell><cell>99.69 164k</cell></row><row><cell cols="3">CBS-10</cell><cell>80.49</cell><cell>49k</cell><cell cols="3">94.93 111k</cell><cell>79.37</cell><cell>83k</cell><cell>95.43 187k</cell></row><row><cell cols="2">CBS-5</cell><cell></cell><cell>80.78</cell><cell>49k</cell><cell cols="3">94.31 111k</cell><cell>78.61</cell><cell>73k</cell><cell>94.32 164k</cell></row><row><cell cols="2">CBS-1</cell><cell></cell><cell>81.56</cell><cell>49k</cell><cell cols="3">94.52 111k</cell><cell>77.56</cell><cell>69k</cell><cell>91.78 156k</cell></row><row><cell cols="4">CBS-10-A 80.28</cell><cell>35k</cell><cell>95.91</cell><cell cols="2">79k</cell><cell>81.47</cell><cell>65k</cell><cell>95.28 146k</cell></row><row><cell cols="3">CBS-5-A</cell><cell>82.03</cell><cell>35k</cell><cell>95.23</cell><cell cols="2">79k</cell><cell>79.48</cell><cell>53k</cell><cell>93.63 118k</cell></row><row><cell cols="3">CBS-1-A</cell><cell>84.41</cell><cell>35k</cell><cell>95.66</cell><cell cols="2">79k</cell><cell>81.32</cell><cell>49k</cell><cell>93.19 111k</cell></row><row><cell cols="4">CBS-10-T 80.49</cell><cell>49k</cell><cell cols="3">94.93 111k</cell><cell>79.42</cell><cell>83k</cell><cell>94.39 187k</cell></row><row><cell cols="3">CBS-5-T</cell><cell>80.94</cell><cell>53k</cell><cell>94.9</cell><cell cols="2">120k</cell><cell>78.95</cell><cell>63k</cell><cell>94.68 142k</cell></row><row><cell cols="3">CBS-1-T</cell><cell>81.82</cell><cell>46k</cell><cell cols="3">95.38 104k</cell><cell>77.39</cell><cell>65k</cell><cell>93.78 147k</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">L2 Model on Penn Tree Bank</cell><cell></cell><cell></cell><cell cols="2">L2 Model on Penn Tree Bank</cell></row><row><cell>Training perplexity</cell><cell>2 5 2 6 2 7 2 8</cell><cell></cell><cell></cell><cell></cell><cell>BL CBS-1-T</cell><cell>Testing perplexity</cell><cell>2 7 2 8</cell><cell></cell><cell>BL CBS-1-T</cell></row><row><cell></cell><cell>2 4</cell><cell cols="4">0k 10k 20k 30k 40k 50k 60k 70k Iteration</cell><cell></cell><cell></cell><cell cols="2">0k 10k 20k 30k 40k 50k 60k 70k Iteration</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 5</head><label>5</label><figDesc>(in Appendix B), with L2' CBS yields improvements of a staggering 60.3 on final testing perplexity and 36.2 on best testing perplexity. CBS yields smaller improvements on L1' of 26.0 and 25.3, which are still much larger than the improvement achieved by CBS on L1 and L2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>due to higher parallelization opportunities for the former. We perform experiments on snapshot ensembling with the L2 model with the respective best performing CBS schedules on PTB and WikiText 2 (CBS-1-T and CBS-1), as well as the fixed batch size baseline. The CBS ensembles on PTB and WikiText 2 result in test set perplexity of 76.14 and 88.47, outperforming baseline ensembles on both datasets (76.52, 89.99 respectively) and CBS single models (77.39, 91.78 respectively).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Validation accuracy and number of parameter updates of E1 on MultiNLI and SNLI datasets. The best accuracy and lowest number of updates are bolded.</figDesc><table><row><cell></cell><cell cols="2">MultiNLI</cell><cell cols="2">SNLI</cell></row><row><cell>Strategy</cell><cell cols="4">Acc. # Iters Acc. # Iters</cell></row><row><cell>BL</cell><cell cols="2">72.87 123k</cell><cell cols="2">86.86 172k</cell></row><row><cell>CBS-1</cell><cell>73.17</cell><cell>64k</cell><cell>86.73</cell><cell>90k</cell></row><row><cell>CBS-2</cell><cell>73.07</cell><cell>71k</cell><cell>86.56</cell><cell>99k</cell></row><row><cell cols="2">CBS-1-A 72.23</cell><cell>48k</cell><cell>86.26</cell><cell>67k</cell></row><row><cell cols="2">CBS-2-A 72.04</cell><cell>57k</cell><cell>85.83</cell><cell>80k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy and number of parameter updates of different models on Cifar-10. The best accuracy and lowest number of iterations are bolded.</figDesc><table><row><cell cols="2">AlexNet-like (C1)</cell><cell></cell><cell cols="2">WResNet (C2)</cell><cell></cell><cell cols="2">ResNet18 (C3)</cell><cell></cell></row><row><cell>Strategy</cell><cell cols="3">Acc. # Iters Strategy</cell><cell cols="3">Acc. # Iters Strategy</cell><cell cols="2">Acc. # Iters</cell></row><row><cell>Baseline</cell><cell>86.94</cell><cell>35k</cell><cell>Baseline</cell><cell>94.53</cell><cell>78k</cell><cell>Baseline</cell><cell>92.71</cell><cell>63k</cell></row><row><cell>CBS-10-3</cell><cell>86.83</cell><cell>20k</cell><cell>CBS-15</cell><cell>94.46</cell><cell>40k</cell><cell>CBS-10</cell><cell>92.47</cell><cell>32k</cell></row><row><cell>CBS-15-2</cell><cell>86.87</cell><cell>26k</cell><cell>CBS-10-3</cell><cell>94.56</cell><cell>45k</cell><cell>CBS-5-3</cell><cell>92.45</cell><cell>37k</cell></row><row><cell>CBS-5-3</cell><cell>87.03</cell><cell>20k</cell><cell>CBS-5-3</cell><cell>94.44</cell><cell>45k</cell><cell>CBS-15-2</cell><cell>92.58</cell><cell>48k</cell></row><row><cell cols="2">CBS-5-3-A 86.75</cell><cell>15k</cell><cell cols="2">CBS-5-3-A 94.34</cell><cell>33k</cell><cell cols="2">CBS-15-2-A 92.27</cell><cell>39k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>have been proposed previously; however, when presented with new architectures and new tasks, initialization still needs to be explored empirically and often the final performance varies greatly with different initializations. In this section, we test if CBS schedules can alleviate the problem of sub-optimal initialization.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Test perplexity of L1, L2 snapshot ensembled models on Penn Tree Bank (PTB) and WikiText-2 (WT2) datasets. Each CBS ensemble model is trained on its best-performing CBS schedule and each baseline ensemble model is snapshotted at the same epochs as its corresponding CBS ensemble model.</figDesc><table><row><cell></cell><cell cols="4">L1 PTB L1 WT2 L2 PTB L2 WT2</cell></row><row><cell>BL Single</cell><cell>83.13</cell><cell>96.41</cell><cell>79.34</cell><cell>99.69</cell></row><row><cell>BL Ens.</cell><cell>82.22</cell><cell>97.18</cell><cell>76.52</cell><cell>89.99</cell></row><row><cell>CBS Ens</cell><cell>81.51</cell><cell>94.27</cell><cell>76.14</cell><cell>88.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Final testing perplexity and best testing perplexity of low-dropout L1' and L2' models on Penn Tree Bank (PTB) and WikiText 2 (WT2) datasets. The best perplexity values are bolded. 108.4 132.2 129.8 231.7 106.9 177.3 136.6 CBS-10-A 119.4 105.4 119.0 115.7 177.2 97.8 145.1 116.7 CBS-5-A 115.1 95.6 116.6 111.4 257.0 88.3 178.5 106.9 CBS-1-A 106.4 93.6 113.5 104.5 171.4 88.7 147.1 100.4</figDesc><table><row><cell></cell><cell cols="2">L1' on PTB</cell><cell cols="2">L1' on WT2</cell><cell cols="2">L2' on PTB</cell><cell cols="2">L2' on WT2</cell></row><row><cell>Schedule</cell><cell>Final</cell><cell>Best</cell><cell>Final</cell><cell>Best</cell><cell>Final</cell><cell>Best</cell><cell>Final</cell><cell>Best</cell></row><row><cell>BL</cell><cell>132.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"><ref type="bibr" target="#b27">[28]</ref> reports testing perplexity of 82.7 and 78.4 for L1 and L2 respectively on PTB, which we could not reproduce. The best perplexity and lowest number of updates are bolded.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Devarakonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02029</idno>
		<title level="m">Adabatch: Adaptive batch sizes for training deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Integrated model, batch and domain parallelism in training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariful</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aydin</forename><surname>Buluc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Parallelism in Algorithms and Architectures(SPAA&apos;18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On an adaptive preconditioned crank-nicolson mcmc algorithm for infinite dimensional bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">332</biblScope>
			<biblScope unit="page" from="492" to="503" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00109</idno>
		<title level="m">Snapshot ensembles: Train 1, get m for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04623</idno>
		<title level="m">Three factors influencing minima in sgd</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<title level="m">One weird trick for parallelizing convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01075</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06422</idno>
		<title level="m">All you need is a good init</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Examples of adaptive mcmc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="349" to="367" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A Bayesian perspective on generalization and Stochastic Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06451</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Don&apos;t decay the learning rate, increase the batch size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00489</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N18-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Second-order optimization for non-convex machine learning: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farbod</forename><surname>Roosta-Khorasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07827</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Large batch size training of neural networks with adversarial training and second-order information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01021</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Hessian-based analysis of large batch training and robustness to adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08241</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
