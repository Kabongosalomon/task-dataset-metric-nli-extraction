<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
						</author>
						<title level="a" type="main">PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Predictive learning</term>
					<term>recurrent neural networks</term>
					<term>spatiotemporal modeling !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical context, where the visual dynamics are believed to have modular structures that can be learned with compositional subsystems. This paper models these structures by presenting PredRNN, a new recurrent network, in which a pair of memory cells are explicitly decoupled, operate in nearly independent transition manners, and finally form unified representations of the complex environment. Concretely, besides the original memory cell of LSTM, this network is featured by a zigzag memory flow that propagates in both bottom-up and top-down directions across all layers, enabling the learned visual dynamics at different levels of RNNs to communicate. It also leverages a memory decoupling loss to keep the memory cells from learning redundant features. We further improve PredRNN with a new curriculum learning strategy, which can be generalized to most sequence-to-sequence RNNs in predictive learning scenarios. We provide detailed ablation studies, gradient analyses, and visualizations to verify the effectiveness of each component. We show that our approach obtains highly competitive results on three standard datasets: the synthetic Moving MNIST dataset, the KTH human action dataset, and a radar echo dataset for precipitation forecasting.</p><p>Index Terms-Predictive learning, recurrent neural networks, spatiotemporal modeling ! arXiv:2103.09504v2 [cs.LG]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As a key application of predictive learning, generating future frames from historical consecutive frames has received growing interest in machine learning and computer vision communities. It benefits many practical applications and downstream tasks, such as the precipitation forecasting <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, traffic flow prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, physical scene understanding <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, early activity recognition <ref type="bibr" target="#b8">[9]</ref>, deep reinforcement learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, and the vision-based model predictive control <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Many of these existing approaches suggested leveraging Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> with stacked Long Short-Term Memory (LSTM) units <ref type="bibr" target="#b15">[16]</ref> to capture the temporal dependencies of spatiotemporal data. Such an architecture is mainly inspired by similar ideas from other wellexplored tasks of sequential data, such as neural machine translation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, speech recognition <ref type="bibr" target="#b18">[19]</ref>, video action recognition <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, and video captioning <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Why spatiotemporal memory?</head><p>For stacked LSTMs, a network structure named memory cell plays an important role in alleviating the vanishing gradient problem of RNNs. Strong theoretical and empirical evidence has shown that it can latch the gradients of hidden states inside each LSTM unit in the training process and thereby preserve valuable information of underlying temporal dynamics <ref type="bibr" target="#b15">[16]</ref>. However, the state transition pathway of LSTM memory cells may not be optimal for the spatiotemporal predictive learning, as this task requires different focuses on the learned representations in many aspects from other tasks of sequential data. First, most predictive networks for language or speech modeling <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> focus on capturing the long-term, non-Markovian properties of sequential data, rather than spatial deformations of visual appearance. But for future frames prediction, both data structures in space-time are crucial and need to be carefully considered. Second, in other supervised tasks of video data, such as action recognition, high-level semantical features can be informative enough, and the low-level features are less important to final outputs. Due to the absence of complex structures of supervision signals, the stacked LSTMs don't need to preserve finegrained representations from the bottom up. Although the existing recurrent architecture based on inner-layer memory transitions can be sufficient to capture temporal variations at each level of the network, it may not be the best choice for predictive learning, where low-level details and high-level semantics of spatiotemporal data are both significant to generating future frames.</p><p>To jointly model the spatial correlations and temporal dynamics at different levels of RNNs, we propose a new memoryprediction framework named Predictive Recurrent Neural Network (PredRNN), which extends the inner-layer transition function of memory states in LSTMs to spatiotemporal memory flow. The spatiotemporal memory flow traverses all nodes of PredRNN in a zigzag path of bi-directional hierarchies: At each timestamp, the low-level information is delivered vertically from the input to the output via a newly-designed memory cell, while at the top layer, the spatiotemporal memory flow brings the high-level memory state to the bottom layer at the next timestamp. By this means, the top-down expectations interact with the bottom-up signals to both analyse those inputs and generate predictions of subsequent expected inputs, which is different from stacked LSTMs, where the memory state is latched inside each individual recurrent unit. Accordingly, we define the central building block of PredRNN as the Spatiotemporal LSTM (ST-LSTM), in which the proposed spatiotemporal memory flow interacts with the original, unidirectional memory state of LSTMs. The intuition is that if we expect a vivid imagination of multiple future images, we need a unified memory mechanism to cope with both short-term deformations of spatial details and long-term dynamics: On one hand, the new design of the spatiotemporal memory cell enables the network to learn complex transition functions within short neighborhoods of consecutive frames by increasing the depth of non-linear neurons between time-adjacent RNN states. It thus significantly improves the modeling capability of ST-LSTM for short-term dynamics. On the other hand, ST-LSTM still retains the temporal memory cell of LSTMs and closely combines it with the newly proposed spatiotemporal memory cell, in pursuit of both long-term coherence of hidden states and their quick response to short-term dynamics.</p><p>This journal paper extends our previous work <ref type="bibr" target="#b1">[2]</ref> in a number of aspects: First, we demonstrate that decoupling the interlayer spatiotemporal memory and inner-layer temporal memory in latent space can greatly improve the predictive ability of PredRNN, which is largely inspired by the theoretical argument that distributed representations can bring a potentially exponential advantage <ref type="bibr" target="#b21">[22]</ref>, if this matches properties of the underlying data distribution. In other words, we explicitly train the two memory cells to focus on different aspects of spatiotemporal variations. Second, we introduce a new curriculum learning strategy inspired by scheduled sampling <ref type="bibr" target="#b22">[23]</ref> to improve the consistency of sequence-to-sequence modeling. The motivation is that, for sequence prediction, the current approach to training RNNs consists of maximizing the likelihood of each frame in the sequence given the previous ground truth and the previous generation. According to scheduled sampling, the forecasting part of RNNs gently changes the training process from a fully guided scheme using the true previous frame, towards a less guided scheme which mostly uses the generated one instead. However, it only considers the discrepancy between training and inference for the forecasting part of RNNs, while the encoding part always takes true frames in the input sequence as the prediction context. Such a training approach hampers the encoder to learn more complex non-Markovian dependencies in the long term. To solve this problem, we propose a "reverse" curriculum learning strategy for sequence prediction. As opposed to scheduled sampling, it gradually changes the training process of the PredRNN encoder from using the previously generated frame to using the previous ground truth.</p><p>Our approach achieves state-of-the-art performance on three datasets for spatiotemporal prediction: a synthetic dataset with moving digits, the KTH dataset with human motions, and a precipitation forecasting dataset with natural radar echo sequences. In addition, we perform more ablation studies to understand the effectiveness of each component of PredRNN. We release the code 1 of our approach to facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>For spatiotemporal predictive learning, different inductive biases are encoded into deep networks by using different network architectures. In general, the mainstream models can be divided into three groups: convolutional models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, recurrent models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, and others including the Transformer-like, autoregressive models and flow-based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>The use of convolutional layers has introduced the inductive bias of group invariance over space to spatiotemporal predictive 1. Code available at https://github.com/thuml/predrnn-pytorch learning. Oh et al. <ref type="bibr" target="#b23">[24]</ref> defined an action-conditioned autoencoder with convolutions for next-frame prediction in Atari games. Later on, in particular video prediction scenarios, the Generative Adversarial Networks (GANs) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> are jointly used with the convolutional architectures <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, which managed to reduce the uncertainty of the learning process and thus improve the sharpness of the generated frames. By stacking convolutional layers, these models learn complicated state transition functions as compositions of simpler ones, thus being able to capture very short-term variations between adjacent frame pairs.</p><p>Recent advances in RNNs provide some useful insights on how to predict future visual sequences based on historical observations. Ranzato et al. <ref type="bibr" target="#b35">[36]</ref> defined an RNN architecture inspired by language modeling, predicting future frames in a discrete space of patch clusters. Srivastava et al. <ref type="bibr" target="#b26">[27]</ref> adopted a sequence-tosequence LSTM model from neural machine translation <ref type="bibr" target="#b16">[17]</ref> as a solution to video prediction. Denton et al. <ref type="bibr" target="#b36">[37]</ref> introduced a new adversarial loss in the recurrent predictive learning framework to learn representations that can factorize each frame into a stationary part and a temporally varying component. Babaeizadeh et al. <ref type="bibr" target="#b27">[28]</ref> combined RNNs and Variational AutoEncoders (VAEs) to cope with the uncertainty of the spatiotemporal sequences, i.e., the multimodal mappings from the historical observations to future frames. Many other approaches based on RNNs have been proposed to learn disentangled representations from videos in an unsupervised predictive manner <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, or to provide probabilistic solutions to modeling the past-future uncertainties <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. The main contribution of this paper is to improve the basic architecture and the training approach of RNN-based predictive models.</p><p>To combine the advantages of convolutions and recurrent architectures, Shi et al. <ref type="bibr" target="#b0">[1]</ref> replaced matrix multiplication by convolutions in both input-to-state and state-to-state transitions of the original LSTM unit <ref type="bibr" target="#b15">[16]</ref>. The Convolutional LSTM (Con-vLSTM) layer unifies the learning processes of visual appearances and temporal dynamics, and provides a foundation for the followup research work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>. Finn et al. <ref type="bibr" target="#b46">[47]</ref> designed a network based on ConvLSTMs for visual planning and control. Lotter et al. <ref type="bibr" target="#b47">[48]</ref> presented a deep network based on ConvLSTMs for next-frame prediction (instead of sequence prediction). In addition to the generated image, it also learns to produce an error term that is then fed back into the network for future prediction. Kalchbrenner et al. <ref type="bibr" target="#b48">[49]</ref> proposed the Video Pixel Network (VPN) that uses ConvLSTMs and PixelCNNs <ref type="bibr" target="#b55">[56]</ref> to unfold an image into a sequence of pixels and capture the correlation between the pixel values. This method achieves good performance on synthetic datasets such as Moving MNIST, but it also suffers from high computational complexity. Shi et al. <ref type="bibr" target="#b49">[50]</ref> improved their previous work with TrajGRU, which allows non-local receptive fields in the convolutional state-to-state transitions. This approach was shown particularly effective for precipitation nowcasting. More recently, Su et al. <ref type="bibr" target="#b54">[55]</ref> further extended ConvLSTM to long-term prediction with an efficient mechanism in terms of computation and memory requirements to combine convolutional features across time.</p><p>To sum up, due to different inductive biases in the architecture design, the above deep networks have different preferences for specific properties of spatiotemporal variations. Deep convolutional networks implicitly assume complex changes in spatial appearance <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, as well as the Markov property over time (also characterized as "memorylessness" <ref type="bibr" target="#b56">[57]</ref>), and may therefore fall short in learning long-term dependencies in dynamic systems.</p><p>The recurrent networks learn to model the spatiotemporal non-Markovian properties with LSTMs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>, ConvLSTMs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>, or temporally skip connections <ref type="bibr" target="#b12">[13]</ref>. However, the quality of the generated images is largely limited by a suboptimal combination of convolutions and recurrent state transitions. In this paper, we improve upon the prior work and mainly discuss how to leverage a new set of recurrent memory states (i.e., spatiotemporal memory flow) to unify the inductive biases of convolutions, deep networks, and recurrent networks in short-and long-term dynamics modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatiotemporal Predictive Learning</head><p>Suppose we are monitoring a dynamical system (e.g., a video clip) of P measurements over time, where each measurement (e.g., an RGB channel) is recorded at all locations in a spatial region represented by an M × N grid (e.g., video frames). From the spatial view, the observation of these P measurements at any time can be represented by a tensor of X ∈ R P ×M ×N . To get a better picture of the 3D tensors, we may imagine them as vectors standing on a spatial grid. From the temporal view, the observations over T timestamps form a sequence of {X 1 , . . . , X T }, which can be denoted as X in . The spatiotemporal predictive learning problem is to predict the most probable length-K sequence in the future, X out = { X T +1 , . . . , X T +K }, given the observation sequence X in . In this paper, we train neural networks parametrized by θ to solve such tasks. Concretely, we use stochastic gradient descent to find a set of parameters θ that maximizes the log likelihood of producing the true target sequence X out given the input data X in for all training pairs {(X n in , X n out )} n :</p><formula xml:id="formula_0">θ = arg max θ (X n in ,X n out )</formula><p>log P (X n out |X n in ; θ).</p><p>In this paper, we take video prediction as a typical predictive learning domain, where the observed data at each timestamp is an RGB video frame, in the sense that the number of measured channels is 3. Another experimental domain is precipitation nowcasting, where the observed data at each timestamp is a grayscale radar echo map in a certain area 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional LSTM</head><p>Compared with the standard LSTM that consists of fully-connected layers, the Convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b0">[1]</ref> can model the spatial and temporal data structures simultaneously by explicitly encoding the spatial information into tensors, and applying convolution operators to both the state-to-state and input-to-state recurrent transitions. It overcomes the limitation of vector-variate representations in standard LSTM, where the spatial correlation is not explicitly modeled. In a ConvLSTM layer, all of the input state X t , memory state C t , hidden state H t , and gated signals i t , f t , g t , o t at each timestamp are 3D tensors in R P ×M ×N . The first dimension is either the number of measurements (for input states) or the number of feature maps (otherwise), and the last two dimensions are the numbers of spatial rows M and columns N . ConvLSTM determines the future state of a certain cell in the 2. We usually use a color transform method based on radar echo intensity to visualize the observed/forecasted data as a color image.</p><p>M × N grid based on the input frame and past states of its local neighbors:</p><formula xml:id="formula_2">g t = tanh(W xg * X t + W hg * H t−1 + b g ) i t = σ(W xi * X t + W hi * H t−1 + W ci C t−1 + b i ) f t = σ(W xf * X t + W hf * H t−1 + W cf C t−1 + b f ) C t = f t C t−1 + i t g t o t = σ(W xo * X t + W ho * H t−1 + W co C t + b o ) H t = o t tanh(C t ),<label>(2)</label></formula><p>where σ is the Sigmoid activation function, * and denote the convolution operator and the Hadamard product respectively. Like standard LSTM, the input gate i t , forget gate f t , output gate o t , and input-modulation gate g t control the information flow across the memory state C t . In this way, the gradient will be kept from quickly vanishing by being trapped in the memory cell. There are three points in the design of the ConvLSTM network that could be further improved.</p><p>Challenge 1: For a stacked ConvLSTM network, the input sequence X in is fed into the bottom layer, and the output sequence X out is generated at the top one. With hidden states H t being delivered from the bottom up, spatial representations are encoded layer by layer. However, the memory states C t are merely updated along the arrow of time within each ConvLSTM layer, being less dependent on the hierarchical visual features at other layers. Thus, the first layer at the current timestamp may largely ignore what had been memorized by the top layer at the previous timestamp. In this paper, we solve this problem by introducing an interlayer memory cell and show that it can significantly benefit the predictive learning of spatiotemporal data.</p><p>Challenge 2: In each ConvLSTM layer, the output hidden state H t is dependent on the memory state C t and the output gate o t , which means that the memory cell is forced to cope with long-term and short-term dynamics simultaneously. Therefore, the modeling capability of the memory state transition function to the complex spatiotemporal variations may greatly limit the overall performance of the predictive model.</p><p>Challenge 3: The ConvLSTM adopts the sequence-to-sequence RNN architecture <ref type="bibr" target="#b59">[60]</ref>, which might be harmed by the training discrepancy between the encoder and the forecaster (w.r.t. input/output sequences) in the predictive learning context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>To solve the above problems, we propose the Predictive Recurrent Neural Network (PredRNN), which consists of a new model and a new training scheme for generic sequence-to-sequence architectures. In this section, we first tackle Challenge 1 by presenting the spatiotemporal memory flow, which improves the state-to-state transition functions of the original ConvLSTM network and facilitates the modeling of short-term dynamics. Next, we give detailed descriptions about the Spatiotemporal LSTM (ST-LSTM), which serves as the building block of PredRNN. It provides a solution to Challenge 2 with a pair of memory cells that are jointly learned and explicitly decoupled to cover different aspects (e.g., long-and short-term dynamics) of spatiotemporal variations. Last but not least, we present the improved training method of PredRNN, which partly solves Challenge 3 by reducing the training discrepancy of the sequence-to-sequence model and greatly encourages the predictive network to learn non-Markovian dynamics from longer input contexts.  </p><formula xml:id="formula_3">b X t+2 b X t+1 b X t X t X t 1 X t+1 C 4 t C 2 t C 3 t C 1 t M 1 t M 2 t M 3 t b X t+2 b X t+1 X t X t+1 H 2 t 1 H 4 t 1 M 4 t H 4 t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spatiotemporal Memory Flow</head><p>PredRNN employs a stack of convolutional recurrent units to learn unified representations of the spatial correlations and temporal dynamics from the input sequence, and then transforms these features back to the data space to predict future spatiotemporal frames. We initially adopt the original ConvLSTM layer as the basic building block of PredRNN and apply a novel memory state transition method between the recurrent nodes. In the original ConvLSTM network <ref type="bibr" target="#b0">[1]</ref> shown in <ref type="figure" target="#fig_1">Fig. 1 (right)</ref>, the memory states C l t are constrained inside individual recurrent layers and updated along the arrow of time. Only the hidden states H l t can be delivered from the observation to the final prediction. This temporal memory flow has been widely used in supervised learning tasks such as video classification, where the hidden representations are more and more abstract and class-specific, starting at the bottom. However, in predictive learning, the input frames and the expected outputs share the same data space, i.e., they may have very close data distributions in the spatial domain and very related underlying dynamics in the temporal domain. Therefore, it becomes important to make the predictions more effectively dependent on the memory representations learned at different levels of recurrent layers. If we want to frame the future vividly, we need both high-level understandings of global motions and more detailed memories of the subtle variations in the input sequence.</p><p>Considering that the memory cell of ConvLSTM can latch the gradients <ref type="bibr" target="#b2">3</ref> and thereby store valuable information across recurrent nodes, we improve the above horizontal memory flow by updating the memory state in the zigzag direction, so that it can better deliver knowledge from input to output. We name this new memory state transition method the spatiotemporal memory flow, and show its transition direction by the orange arrows in <ref type="figure" target="#fig_1">Fig. 1 (left)</ref>. In this way, the memory states in different ConvLSTM layers are on longer independent, and all nodes in the entire recurrent network jointly maintain a memory bank denoted by M. We show here the key equations of the spatiotemporal memory flow using ConvLSTM as <ref type="bibr" target="#b2">3</ref>. Like the standard LSTM, the memory cell of ConvLSTM was originally designed to alleviate the gradient vanishing problem during training. the building block:</p><formula xml:id="formula_4">g t = tanh(W xg * X t 1 {l=1} + W hg * H l−1 t ) i t = σ(W xi * X t 1 {l=1} + W hi * H l−1 t + W ci * M l−1 t ) f t = σ(W xf * X t 1 {l=1} + W hf * H l−1 t + W cf * M l−1 t ) M l t = f t M l−1 t + i t g t o t = σ(W xo * X t 1 {l=1} + W ho * H l−1 t + W co * M l t ) H l t = o t tanh(M l t ).<label>(3)</label></formula><p>The input gate, input modulation gate, forget gate, and output gate are no longer dependent on the hidden state and the temporal memory state from the previous timestamp at the same layer. Instead, they rely on the hidden state H l−1 t and the spatiotemporal memory state M l−1 t (l ∈ {1, ..., L}) supplied by the previous layer at current timestamp (see <ref type="figure" target="#fig_1">Fig. 1 (left)</ref>). In particular, the bottom recurrent unit (l = 1) receives state values from the top layer at the previous timestamp:</p><formula xml:id="formula_5">H l−1 t = H L t−1 , M l−1 t = M L t−1 .</formula><p>The four layers in this figure have different sets of convolutional parameters regarding the input-to-state and state-to-state transitions. They thereby read and update the values of the memory state based on their individual understandings of the spatiotemporal dynamics as the information flows through the current node. Note that we replace the notation for memory state from C to M to emphasize that it flows in the zigzag direction in PredRNN, instead of the horizontal direction in standard recurrent networks. Different from ConvLSTM that uses Hadamard product for the computation of the gates, we adopt convolution operators * for finer-grained memory transitions. An additional benefit of this change is that the learned PredRNN can be deployed directly on the input sequence of different spatial resolutions.</p><p>The spatiotemporal memory flow provides a recurrent highway for the hierarchical visual representations that can reduce the loss of information from the bottom layers to the top of the network. Besides, by introducing more nonlinear gated neurons within temporal neighborhoods, it expands the deep transition path of hidden states and enhances the modeling capability of the network for short-term dynamics. In contrast, the original ConvLSTM network requires larger convolution kernels for input-to-state and</p><formula xml:id="formula_6">ST-LSTM 1 ST-LSTM 1 ST-LSTM 1 ST-LSTM 2 ST-LSTM 2 ST-LSTM 2 ST-LSTM 3 ST-LSTM 3 ST-LSTM 3 ST-LSTM 4 ST-LSTM 4 ST-LSTM 4 ⊗ ⊗ ⊗ ⊗ Input Gate Output Gate Forget Gate Modulation Gate Temporal Memory g t i t f t o t ′ f t Spatiotemporal Memory ⊗ ′ i t ′ g t b X t+2 b X t+1 b X t X t M l</formula><p>state-to-state transitions in order to capture faster motion, resulting in unnecessary increase in model parameters.</p><p>Alternatively, we can understand the spatiotemporal memory flow from the perspective of memory networks <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>. Here, the proposed spatiotemporal memory state M can be viewed as a simple version of the so-called external memory with continuous memory representations, and the stacked recurrent layers can be viewed as multiple computational steps. The layer-wise forget gates, input gates, and input modulation gates respectively determine the read and write functions, as well as the content to be written. One advantage of the classic memory networks is to capture long-term structure (even with multiple temporal hops) within sequences. Our spatiotemporal memory flow is analogous to their mechanism, as it enables our model to consider different levels of video representations before making a prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spatiotemporal LSTM with Memory Decoupling</head><p>As described previously, the spatiotemporal memory state is updated first upwards across layers then forwards to the next timestamp. It stretches the state transition path across time and adds extra neurons between horizontally adjacent nodes at the same level of the network. It thus enables the network to learn complex non-linear transition functions of short-term motions. However, this deep-in-time architecture may also bring in the gradient vanishing problem. The roundabout memory transition path may make it difficult to capture long-term temporal dependencies. In pursuit of both short-term recurrence depth and long-term coherence, we introduce a double-flow memory transition mechanism that combines the original memory flow of C and the new spatiotemporal memory flow of M. It derives a novel recurrent unit named Spatiotemporal LSTM (ST-LSTM).</p><p>In <ref type="figure">Fig. 2</ref>, we present the final PredRNN model by taking ST-LSTM as the building block in place of ConvLSTM. There are two memory states: C l t is the temporal memory that transits within each ST-LSTM layer from the previous node at t − 1 to the current timestamp. M l t is the spatiotemporal memory, which transits vertically to the current node from the lower l−1 ST-LSTM layer at the same timestamp. In particular, we assign</p><formula xml:id="formula_7">M L t−1 to M 0 t for the bottom ST-LSTM where l = 1.</formula><p>We adopt the original gated structures for C l t from the standard LSTM, and construct another set of input gate i t , forget gate f t , and input modulation gate g t for M l t , because the memory transition functions in distinct directions are supposed to be controlled by different signals. Specifically, in each ST-LSTM, we compute:</p><formula xml:id="formula_8">g t = tanh(W xg * X t + W hg * H l t−1 ) i t = σ(W xi * X t + W hi * H l t−1 ) f t = σ(W xf * X t + W hf * H l t−1 ) C l t = f t C l t−1 + i t g t g t = tanh(W xg * X t + W mg * M l−1 t ) i t = σ(W xi * X t + W mi * M l−1 t ) f t = σ(W xf * X t + W mf * M l−1 t ) M l t = f t M l−1 t + i t g t o t = σ(W xo * X t + W ho * H l t−1 + W co * C l t + W mo * M l t ) H l t = o t tanh(W 1×1 * [C l t , M l t ]).<label>(4)</label></formula><p>The final hidden states H l t of each node are dependent on a combination of the horizontal and zigzag memory states: we concatenate the C l t and M l t , and then apply the 1 × 1 convolutional layer for dimensionality reduction, which makes the hidden state H l t have the same dimensions as the memory states. In addition to simple concatenation, pairs of memory states are finally twisted and unified by output gates with bidirectional control signals (horizontal and vertical), resulting in a comprehensive modeling of long-term and short-term dynamics. This dual-memory mechanism benefits from the compositional advantage with distributed representations <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>. Intuitively, due to different mechanisms of recurrent state transitions, the pair of memory cells in ST-LSTM are expected to deal with different aspects of spatiotemporal variations:</p><p>• M introduces a deeper transition path that zigzags across ST-LSTM layers. With the forget gate f t and the inputrelated modules i t g t , it improves the ability to model complex short-term dynamics from one timestamp to the next, and allows H to transit adaptively at different rates.</p><p>• C operates on a slower timescale. It provides as shorter gradient path between distant hidden states, thus facilitating the learning process of long-term dependencies.</p><p>However, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we visualized the increments of memory states at each timestamp (i.e., ∆C l t and ∆M l t ) using t-SNE <ref type="bibr" target="#b65">[66]</ref>, and found that they were not automatically separated as expected. In fact, the two memory states are often so intertwined that they are difficult to decouple spontaneously through their respective network architectures. To some extent, it results in the inefficient utilization of network parameters.</p><p>Building upon the first version of ST-LSTM <ref type="bibr" target="#b1">[2]</ref>, we present a new decoupling loss, as shown in <ref type="figure">Fig. 2 (right)</ref>, that keeps C and M from learning redundant features. In each ST-LSTM unit, we first add the convolutional layer upon the increments of C l t and M l t at every timestamp, and leverage a new decoupling loss to explicitly extend the distance between them in latent space. By this means, different memory states are trained to focus on different aspects of spatiotemporal variations. The overall memory decoupling method can be formulated as follows:</p><formula xml:id="formula_9">∆C l t = W decouple * (i t g t ) ∆M l t = W decouple * (i t g t ) L decouple = t l c | ∆C l t , ∆M l t c | ∆C l t c · ∆M l t c ,<label>(5)</label></formula><p>where W decouple denotes 1 × 1 convolutions shared by all ST-LSTM layers; ·, · c and · c are respectively dot product and 2 norm of flattened feature maps, which are calculated channel by channel. At training time, the increments of memory states, i t g t and i t g t , are derived from Eq. (4). Notably, the new parameters are only used at training time and are removed from the entire model at inference. That is, there is no increase in model size compared to the previous version of ST-LSTM <ref type="bibr" target="#b1">[2]</ref>. By defining the decoupling loss with the cosine similarity, our approach encourages the increments of the two memory states to be orthogonal at any timestamp. It unleashes the respective strengths of C and M for long-term and short-term dynamic modeling. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, at test time, ∆C l t and ∆M l t can be easily separated by t-SNE techniques. The decoupling loss is largely inspired by the theoretical evidence that using reasonable inductive bias to construct distributed representations can bring great performance improvement if it matches properties of the underlying data distribution <ref type="bibr" target="#b21">[22]</ref>. There is a similar idea in ensemble learning that generally, to get a good ensemble, the base learners should be as more accurate as possible, and as more diverse as possible <ref type="bibr" target="#b66">[67]</ref>. The diversity of base learners can be enhanced in different ways, such as subsampling the training examples, manipulating the input attributes, and employing randomness into training procedures <ref type="bibr" target="#b67">[68]</ref>. Similarly, in adversarial learning, the experimental results showed that diversifying features of multiple discriminators can effectively improve the discriminability of the ensemble model, making it easy to discover the discrepancy between generated data and real data from a diverse perspective <ref type="bibr" target="#b68">[69]</ref>. It is worth noting that the proposed memory decoupling method has not been used by existing ensemble learning algorithms, though it is inspired by the general idea of enhancing the diversity of base learners. We use it to diversify the distance of pairs of memory states of recurrent networks, intuitively in pursuit of a more disentangled representation of long-term and short-term dynamics in predictive learning.</p><p>The final PredRNN model is trained end-to-end in an unsupervised manner. The overall training objective is:</p><formula xml:id="formula_10">L final = T +K t=2 X t − X t 2 2 + λL decouple ,<label>(6)</label></formula><p>where the first term is the frame reconstruction loss that works on the network output at each timestamp, the second term is the memory decoupling regularization from Eq. (5), and λ is a hyperparameter between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training with Reverse Scheduled Sampling</head><p>We propose an improved scheduled sampling approach to facilitate the training process of PredRNN, which consists of two components, respectively used in the sequence encoding part and the forecasting part of PredRNN. For the encoding phase, we propose a new curriculum learning strategy named Reverse Scheduled Sampling, which has the following benefits to the training procedure:</p><p>• It enforces the encoder to learn temporal dynamics from longer periods in X in ;</p><p>• It reduces the discrepancy between the training processes of the encoder and the forecaster.</p><p>As for the forecasting phase, we follow the common practice of scheduled sampling <ref type="bibr" target="#b22">[23]</ref> to bridge the gap between training and inference of the forecaster.</p><p>Encoder-forecaster training discrepancy. As shown in <ref type="figure">Fig. 4</ref>, we follow the common practice <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47]</ref> to train a unified recurrent network, which can be denoted by f θ (·), for both sequence encoding and forecasting. The intuition is that in pursuit of a perfect predictive model, the input frames X in and the output frames X out are supposed to be modeled in the same space. During inference, the model generates the full sequence by generating one frame at a time, and advancing time by one step. The temporal dynamics is estimated with a combination of previous states, which can be denoted by S t−1 = {H t−1 , C t−1 , M t−1 }. Here, we leave out the layer index for simplicity, and these tensors are initialized with zero values. At each timestamp t ∈ {1, . . . , T + K}, X t+1 is computed by f θ (·) as follows:</p><formula xml:id="formula_11">X t+1 = f θ (X t , S t−1 ) if t ≤ T, f θ ( X t , S t−1 ) otherwise,<label>(7)</label></formula><p>ST-LSTMs ✓ ST-LSTMs ✓ Sequence Forecaster: Use the true previous frame with a decreasing probability over training iterations  <ref type="figure">Fig. 4</ref>: An illustration of reverse scheduled sampling, which is used to enhance the training process of the encoding part in spatiotemporal predictive learning models (PredRNN as an example). It is different from the original scheduled sampling approach that is used to bridge the training-inference gap for the forecasting part of a sequence-to-sequence model. We jointly use these two approaches in a PredRNN parameterized by θ. Notably, θ is shared by the encoder and the forecaster, which is a special case of the sequence-to-sequence model and is particularly suitable for the predictive learning scenario where the input and output data can be modeled in the same space.</p><formula xml:id="formula_12">X T X T +K 1 b X T +K 1 b X T b X T +K b X T +2 ST-LSTMs ✓ Reverse scheduled sampling X T +1 b X T +1 b X t ST-LSTMs ✓ Reverse scheduled sampling b X t+1 X t … … … b X T +1</formula><p>where the main difference between encoder (t ≤ T ) and forecaster (t &gt; T ) is whether to use the previous true frame X t or an estimate X t coming from the model itself. We argue that, although widely employed by previous literature, using the same set of parameters θ may result in an encoder-forecaster training discrepancy. In the encoding phase, the model is mainly learned to make the onestep prediction, because X t tends to be a more informative signal than S t−1 . The availability of the true current frame may largely spare the efforts for PredRNN in digging deeper into the complex, non-Markovian properties of previous observations. But in the forecasting phase, the model has to depend more on S t−1 , because new observations are now inaccessible and only the memory states can store trustworthy long-term dynamics of spatiotemporal variations. Obviously, the model is trained under a more challenging task for the forecasting part than the encoding part. If the model is well pre-trained, in the sense that the distribution of generated data is close to that of the true data, it may be fine to train θ by implementing forward propagation as Eq. <ref type="formula" target="#formula_11">(7)</ref>. But if it is not, especially in the early stage of training, the encoder-forecaster gap would lead to an ineffective optimization of θ, hampering the model from learning long-term dynamics from X in .</p><p>Scheduled sampling. The original scheduled sampling approach <ref type="bibr" target="#b22">[23]</ref> can partly address the dependency between the training of encoder and forecaster and improve the recurrent models. A typical training procedure with scheduled sampling in the spatiotemporal prediction context <ref type="bibr" target="#b46">[47]</ref> computes:</p><formula xml:id="formula_13">X t+1 = f θ (X t , S t−1 ) if t ≤ T, f θ (X t −→ X t , S t−1 ) otherwise,<label>(8)</label></formula><p>where −→ denotes a gradual change over the training time from taking the true frame X t as the input of the forecaster to taking the previous estimation X t instead. The use of X t or X t is randomly decided with a sampling mechanism. We flip a coin to use X t with probability η k , or X t with probability 1 − η k , where k is the index of training iterations. η k is a function of k and decreases linearly or exponentially in a similar manner used to gradually reduce the learning rate in modern stochastic gradient descent algorithms.</p><p>When η k = 1, at the very beginning of training procedure, the forecaster is trained exactly as the encoder, while when it is down to 0, the entire model is trained in the same setting as the inference stage. We may conclude that this approach has two benefits. First, compared with the above-mentioned training method, it enables the model to perceive the distribution of X out during training. Second, it can partly bridge the encoder-forecaster gap from the forecaster side: intuitively, at the beginning of training, sampling from true frames enables the parameters θ to be optimized consistently over the two parts of the sequence. But still, an alternative way to bridge the encoder-forecaster gap is to modify the training approach regarding the encoding side of the model. Moreover, the original scheduled sampling method does not improve the ineffective training of the encoder in learning long-term dynamics.</p><p>Reverse scheduled sampling. To enforce the recurrent model to learn more temporal dynamics from consecutive input frames, we propose an alternative scheduled sampling method, which mainly improves the training strategy in the encoding phase:</p><formula xml:id="formula_14">X t+1 = f θ ( X t −→ X t , S t−1 ) if t ≤ T, f θ ( X t , S t−1 ) otherwise,<label>(9)</label></formula><p>where −→ indicates a curriculum learning schedule that goes from one to the other, and S t−1 is the combination of previous</p><formula xml:id="formula_15">states {H t−1 , C t−1 , M t−1 }.</formula><p>In the encoding stage, there is a probability ( k ∈ [0, 1]) of sampling the true frame X t ∈ X in or a corresponding probability (1 − k ) of sampling the previous estimation X t . For the entire encoder, a sequence of sampling outcomes can be seen as a Bernoulli process with T independent trials. Different from above, k is an increasing function starting from s and increasing to e of the training iteration i, which has the following forms: parameters jointly decide the increasing curve of k . Examples of such schemes are shown in <ref type="figure">Fig. 5</ref> in the red curves. We name this approach the Reverse Scheduled Sampling, as the encoder is trained with a progressively simplified curriculum. It gradually changes from generating multi-step future frames, which is challenging due to the absence of some historical observations, to making one-step predictions, just as the encoder does at test time. Intuitively, this method encourages the model to extract longterm, non-Markovian dynamics from the input sequence. Besides, it provides an alternative solution to the encoder-forecaster training discrepancy at the encoder end. At the early stage of training, both parts of the sequence-to-sequence model are optimized consistently under similar settings (i.e., with a higher probability of using the previous estimation). Also shown in <ref type="figure">Fig. 5</ref> are two feasible strategies to integrate the reverse scheduled sampling with the original scheduled sampling. For simplicity, we make η k (the sampling probability of the forecasting part) decay linearly, although other scheduled sampling schemes could be employed (such as an exponential decay). The biggest difference between the two strategies lies in whether the variation range of the sampling probabilities of encoder and forecaster are close at the early stage of training. Empirically, the second strategy performs slightly better (Table (4) in Section 5.3). It fits well with our expectation: using similar sampling probabilities for the encoding and forecasting phases at the beginning of training epochs does indeed reduce the encoder-forecaster training discrepancy as shown in Eq. <ref type="bibr" target="#b6">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate the ability of PredRNN for multi-step future prediction on three spatiotemporal datasets, including a synthetic dataset of flying digits, a human motion dataset, and a radar echo dataset for precipitation forecasting. The code is available at https://github.com/thuml/predrnn-pytorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Compared Models</head><p>We use the ConvLSTM network <ref type="bibr" target="#b0">[1]</ref> as the primary baseline model of PredRNN, and include more advanced video prediction models for further comparison, including Conv-TT-LSTM <ref type="bibr" target="#b54">[55]</ref>, VPN <ref type="bibr" target="#b48">[49]</ref>, MCnet <ref type="bibr" target="#b37">[38]</ref>, TrajGRU <ref type="bibr" target="#b49">[50]</ref>, and MIM <ref type="bibr" target="#b3">[4]</ref>. For simplicity, we refer to different versions of PredRNN as follows:</p><p>• PredRNN-M-Only. This model improves the ConvLSTM network with the spatiotemporal memory flow (M), whose architecture is shown in <ref type="figure" target="#fig_1">Fig. 1 (left)</ref>.</p><formula xml:id="formula_16">• PredRNN [2]</formula><p>. This model uses ST-LSTMs as building blocks, but compared with PredRNN-V2, it is not trained with memory decoupling or reverse scheduled sampling.</p><p>• PredRNN-V2. This is the final proposed model that improves the training process of the original PredRNN with memory decoupling and reverse scheduled sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We use the ADAM optimizer <ref type="bibr" target="#b69">[70]</ref> to train the models and use a mini-batch of 8 sequences at each training iteration. Unless otherwise specified, we set the learning rate to 10 −4 and stop the training process after 80,000 iterations. We observe that the number of channels of hidden states has a strong impact on the final performance. We typically use four ST-LSTM layers in PredRNN with 128 channels for each hidden state and memory state to strike a balance between the prediction quality and training efficiency.</p><p>In each ST-LSTM layer, we set the size of convolutional kernels to 5 × 5. Notably, we use a similar number of model parameters for the compared ConvLSTM network. The entire training time of PredRNN is around 18 hours for Moving MNIST with a TITANX GPU. For other datasets, the learning processes are similar to that of Moving MNIST, except that the maximum numbers of iterations are adjusted according to the size of the training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Synthetic Moving MNIST Dataset</head><p>Dataset. We generate the Moving MNIST sequences in the same way as the previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>. Each sequence consists of 20 consecutive frames, including 10 historical observations and 10 future predictions. At the beginning of each sequence, we sample 2 handwritten digits randomly from the static MNIST training set and place initially at random locations of a 64 × 64 grid of an image. We randomly initialize the speed of the digits and make them move at a constant speed throughout the sequence. The digits bounce off the edges of the image at a fixed, predictable angle. We construct a training set with 10,000 sequences and a test set with 5,000 sequences, in which the digits are respectively sampled from the training/test set of the static MNIST dataset, such that the spatial information of the test data would not be overly exposed during the training process. In this task, although future trajectories are certain and only dependent on historical observations, it is still challenging to infer the underlying dynamics and the initial velocities. Moreover, the frequent occlusions of moving digits lead to complex and short-term changes of local spatial information, which brings more difficulties to spatiotemporal prediction.</p><p>Main results. We adopt evaluation metrics that were widely used by previous methods: the Mean Squared Error (MSE), the Structural Similarity Index Measure (SSIM) <ref type="bibr" target="#b70">[71]</ref>, and the Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b71">[72]</ref>. The difference    between these metrics is that MSE estimates the absolute pixelwise errors, SSIM measures the similarity of structural information within the spatial neighborhoods, while LPIPS is based on deep features and aligns better to human perceptions. <ref type="table">Table (</ref> Qualitative comparisons. <ref type="figure" target="#fig_5">Fig. 7</ref> shows two examples randomly sampled from the test set, where most of the frames produced by the compared models are severely blurred, especially for long-term predictions. In contrast, PredRNN produces clearer images, which means it can be more certain of future variations due to its stronger long-term modeling capabilities. When we look closely, we can see that the most challenging issues on this dataset are to make accurate predictions of the future trajectories of moving digits and to maintain the correct shape of each digit after occlusions (in the second example, the compared VPN model incorrectly predicts the digit 8 as 3 after the occlusion of 8 and 0). In both cases, the original PredRNN and the newly proposed PredRNN-V2 progressively improve the quality of the prediction results.</p><p>Ablation studies on the spatiotemporal memory flow. For a detailed analysis of the spatiotemporal memory flow, we compare the performance of (a) removing the bottom-up transition path of PredRNN from M 1 t to M L t with (b) removing the top-down transition path from M L t to M 1 t+1 . The results in <ref type="table">Table (</ref>2) show that the top-down transition path contributes more to the final performance. In <ref type="figure" target="#fig_7">Fig. 8 (left)</ref>, we compare the normalized gradient of the hidden states H 1 t at the first ST-LSTM layer with respect to the loss function at the last timestamp: ∇ H 1 t L T +K , t ∈ [1, T + K). It shows that the full spatiotemporal memory flow helps to alleviate the vanishing gradient problem, being especially good at learning the long-term information from the very beginning of the sequence (Note that the first frame usually contains clearer appearance information, while there are often occlusions of digits in the middle of the input sequence).</p><p>Contributions of memory decoupling. To show that memory decoupling facilitates both long-term and short-term dependencies, </p><formula xml:id="formula_17">T −1 T τ =2 ∇ H 1 τ L t , t ≥ T + 1,</formula><p>which shows the importance of the reverse scheduled sampling (the 2nd training strategy in <ref type="figure">Fig. 5</ref> with an exponentially increasing k ). <ref type="bibr" target="#b9">10</ref> 20 <ref type="formula" target="#formula_4">30</ref>   as shown in <ref type="figure" target="#fig_8">Fig. 9</ref>, we make the pixel intensity of the images change regularly or irregularly over time. Thanks to the decoupled memory cells of ST-LSTMs, our approach can respond to sudden changes more rapidly and adapt to video dynamics at different timescales. In <ref type="table">Table (</ref>3), we use the MIM model <ref type="bibr" target="#b3">[4]</ref>, which is also based on the ST-LSTM unit, to show the generality of the memory decoupling loss to different network backbones.</p><p>Contributions of the reverse scheduled sampling. We compare different combinations of the original and the reverse scheduled sampling techniques. As shown in <ref type="table">Table (</ref> <ref type="bibr" target="#b3">4)</ref>, the second strategy in <ref type="figure">Fig. 5</ref> with an exponentially increased k performs best, in the sense that the encoder-forecaster discrepancy can be effectively reduced by keeping their probabilities of sampling the true context frames close to each other in the early stage of training. To further demonstrate that the reverse scheduled sampling contributes to learning long-term dynamics, we perform another gradient analysis in <ref type="figure" target="#fig_7">Fig. 8 (right)</ref>. We evaluate the gradients of the encoder's hidden states with respect to the loss functions at different output timestamps, and average the results over the entire input sequence:</p><formula xml:id="formula_18">1 T −1 T τ =2 ∇ H 1 τ L t , t ∈ [T + 1, T + K].</formula><p>The normalized gradient curves show that the context information can be encoded more effectively by using the reverse scheduled sampling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCnet + Residual</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PredRNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PredRNN-V2 PredRNN-V2</head><p>Conv-TT-LSTM Conv-TT-LSTM <ref type="figure" target="#fig_1">Fig. 11</ref>: Prediction examples on the KTH test set, where we predict 20 frames into the future based on the past 10 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Real-World KTH Action Dataset</head><p>Dataset. The KTH action dataset <ref type="bibr" target="#b72">[73]</ref>   <ref type="bibr" target="#b37">[38]</ref> and resize the video frames into a resolution of 128×128. All compared models are trained across the 6 action categories by predicting 10 future frames from 10 context frames. At test time, we expand the prediction horizon to 20 timestamps into the future. We manually select reasonable video sequences to ensure that there is always someone in the first frame. We obtain a training set of 108,717 sequences, and a test set of 4,086 sequences.</p><p>Results. We adopt the Peak Signal to Noise Ratio (PSNR) from the previous literature as the third evaluation metric, in addition to SSIM and LPIPS. Like MSE, PSNR also estimates the pixel-level similarity of two images (higher is better). The evaluation results of different methods are shown in <ref type="table">Table (</ref> <ref type="bibr" target="#b4">5)</ref>, and the corresponding frame-wise comparisons are shown in <ref type="figure" target="#fig_1">Fig. 10</ref>, from which we have two observations: First, our models show significant improvements in both short-term and long-term predictions over the ConvLSTM network. Second, with the newly proposed memory decoupling and reverse scheduled sampling, PredRNN-V2 improves the conference version by a large margin in LPIPS (from 0.204 to 0.139). As mentioned above, LPIPS is more sensitive to perceptual human judgments, indicating that PredRNN-V2 has a stronger ability to generate high-fidelity images. In accordance with these results, we can see from the visual examples in <ref type="figure" target="#fig_1">Fig. 11</ref> that our approaches (especially PredRNN-V2) obtain more accurate predictions of future movement and body details. The increase in image sharpness is an evidence that PredRNN-V2 is more certain about the future. By decoupling memory states, it learns to capture the complex spatiotemporal variations from different timescales.  <ref type="figure" target="#fig_1">Fig. 12</ref>: Prediction examples on the radar echo test set, in which 10 future frames are generated from the past 10 observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Precipitation Forecasting from Radar Echoes</head><p>The accurate prediction of the movement of radar echoes in the next 0-2 hours is the foundation of precipitation forecasting. It is a challenging task because the echoes tend to have non-rigid shapes and may move, accumulate or dissipate rapidly due to complex atmospheric physics, which makes it important to learn the dynamics in a unified spatiotemporal feature space.</p><p>Dataset. We collect the radar echo dataset by following the data handling method described in the work from Shi et al. <ref type="bibr" target="#b0">[1]</ref>. Our dataset consists of 10,000 consecutive radar observations recorded every 6 minutes at Guangzhou, China. For data preprocessing, we first map the radar observations to pixel values and represent them as 128 × 128 gray-scale images (slightly different from the conference version), and then slice the sequential radar maps with a sliding window and obtain a total number of 9,600 sequences. Each sequence contains 10 input frames and 10 output frames, covering the historical data for the past hour and that for the future hour. We use 7,800 sequences for training and leave the rest of them for model evaluation.</p><p>Results. In Table <ref type="formula" target="#formula_10">(6)</ref>, we compare PredRNN with three existing methods that have been shown effective for precipitation forecasting. In addition to MSE, following a common practice, we also evaluate the predicted radar maps with the Critical Success Index (CSI). Concretely, we first transform the pixel values back to echo intensities in dBZ, and then take 30, 40 and 50 dBZ as thresholds to compute: CSI = hits hits+misses+false alarms , where "hits" indicates the true positive, "misses" indicates the false positive, and "false alarms" is the false negative. From Table <ref type="formula" target="#formula_10">(6)</ref>, PredRNN consistently achieves the best performance over all CSI thresholds. Further, we visualize the predicted radar frames by mapping them into RGB space. The results are shown in <ref type="figure" target="#fig_1">Fig. 12</ref>. Note that areas with echo intensities over 40 dBZ tend to have severe weather phenomena and should be considered carefully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a recurrent network named PredRNN for spatiotemporal predictive learning. With a new Spatiotemporal LSTM unit, PredRNN models the short-term deformations in spatial appearance and the long-term dynamics over multiple frames simultaneously. The core of the Spatiotemporal LSTM is a zigzag memory flow that propagates across stacked recurrent layers vertically and through all temporal states horizontally, which enables the interactions of the hierarchical memory representations at different levels of PredRNN. Building upon the conference version of this paper, we introduce a new method to decouple the twisted memory states along the horizontal and the zigzag pathway of recurrent state transitions. It enables the model to benefit from learning distributed representations that could cover different aspects of spatiotemporal variations. Furthermore, we also propose a new curriculum learning strategy named reverse scheduled sampling, which enforces the encoding part of PredRNN to learn temporal dynamics from longer periods of the input sequence. Another benefit of reverse scheduled sampling is to reduce the training discrepancy between the encoding part and the forecasting part. Our approach achieves state-of-the-art performance on multiple datasets, including both synthetic and natural spatiotemporal sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Y. Wang is with MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China. • H. Wu, Z. Gao, J. Wang, P. S. Yu, and M. Long are with the School of Software, BNRist, Tsinghua University, China. • J. Zhang is with the Microsoft Corporation, China. • Y. Wang and H. Wu contributed equally to this work. • Corresponding author: Mingsheng Long, mingsheng@tsinghua.edu.cn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Left: the spatiotemporal memory flow architecture that uses ConvLSTM as the building block. The orange arrows show the deep-in-time path of memory state transitions. Right: the original ConvLSTM network proposed by Shi et al. [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of ∆C l t (red points) and ∆M l t (black points) using t-SNE<ref type="bibr" target="#b65">[66]</ref> on the KTH action dataset. Models are respectively trained without or with memory decoupling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>• 4 Fig. 5 :</head><label>45</label><figDesc>Linear increase: k = min{ s + α l × k, e };• Exponential increase: k = e − ( e − s ) × exp(− k αe ); • Sigmoid increase: k = s + ( e − s ) × 1 1+exp( βs−k αs ) ,where α l , α e , α s &gt; 0 denote the increasing factors and β s &gt; 0 denotes the starting point of the sigmoid function. These hyper-Two feasible strategies to combine the original scheduled sampling and the reverse scheduled sampling (RSS), both including three schedules (in red curves) for the evolution of k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Frame-wise MSE (↓), SSIM (↑), and LPIPS (↓) on the Moving MNIST test set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Prediction examples on the Moving MNIST test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>1) shows the results of all compared models averaged per frame. Fig. 6 provides the corresponding frame-wise comparisons. The final PredRNN model significantly outperforms all previous approaches. With the proposed spatiotemporal memory flow, the PredRNN-M-only model reduces the per-frame MSE of the ConvLSTM baseline from 103.3 down to 74.0. By using the ST-LSTM in place of the ConvLSTM unit, our model further reduces the MSE down to 56.8. Finally, the employment of the memory decoupling and the reverse scheduled sampling techniques brings another 14.8% improvement in MSE (from 56.8 to 48.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Gradient analyses on the Moving MNIST dataset (T = 10, K = 10). Left: ∇ H 1 t L T +K , t ≥ 1, which shows the importance of the spatiotemporal memory flow of ST-LSTMs; Right: 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Model performance under different types of temporal dynamics, where the Y axis represents the per-pixel intensity of each predicted frame. By leveraging ST-LSTMs with memory decoupling, the proposed model can respond more rapidly to unexpected, sudden variations (left), and simultaneously capture temporal dynamics at different timescales (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Sequence Encoder :</head><label>Encoder</label><figDesc>Use the true previous frame with an increasing probability εk over training iterations</figDesc><table><row><cell>…</cell><cell>✓</cell><cell>…</cell></row><row><cell></cell><cell>Scheduled</cell><cell>Scheduled</cell></row><row><cell></cell><cell>sampling</cell><cell>sampling</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Results on the Moving MNIST test set, averaged over a prediction time horizon of 10 timestamps.</figDesc><table><row><cell>Model</cell><cell cols="3">MSE (↓) SSIM (↑) LPIPS (↓)</cell></row><row><cell>FC-LSTM [27]</cell><cell>118.3</cell><cell>0.690</cell><cell>-</cell></row><row><cell>ConvLSTM [1]</cell><cell>103.3</cell><cell>0.707</cell><cell>0.156</cell></row><row><cell>CDNA [47]</cell><cell>97.4</cell><cell>0.721</cell><cell>-</cell></row><row><cell>Conv-TT-LSTM [55]</cell><cell>64.3</cell><cell>0.846</cell><cell>0.133</cell></row><row><cell>VPN Baseline [49]</cell><cell>64.1</cell><cell>0.870</cell><cell>-</cell></row><row><cell>MIM [4]</cell><cell>52.0</cell><cell>0.874</cell><cell>0.079</cell></row><row><cell>PredRNN-M-Only</cell><cell>74.0</cell><cell>0.851</cell><cell>0.109</cell></row><row><cell>PredRNN</cell><cell>56.8</cell><cell>0.867</cell><cell>0.107</cell></row><row><cell>PredRNN-V2</cell><cell>48.4</cell><cell>0.891</cell><cell>0.071</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>Comparisons of different variants of the spatiotemporal memory flow. A lower MSE indicates the smaller prediction errors.</figDesc><table><row><cell>Model</cell><cell>MSE (↓)</cell></row><row><cell>PredRNN</cell><cell>56.8</cell></row><row><cell>w/o the bottom-up memory flow M 1 t → . . . → M L t w/o the top-down memory flow M L t → M 1 t+1</cell><cell>57.6 59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>MSE (↓) of applying memory decoupling to different networks based on ST-LSTMs. Note that memory decoupling is effective for both models, while MIM performs better due to a larger number of parameters under the same feature size.</figDesc><table><row><cell cols="3">Base model W/o memory decoupling W/ memory decoupling</cell></row><row><cell>MIM [4]</cell><cell>52.0</cell><cell>47.9</cell></row><row><cell>PredRNN</cell><cell>56.8</cell><cell>51.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>Ablation studies on the scheduled sampling strategies. For reverse scheduled sampling, k changes from s to e . All models are trained without the memory decoupling loss.Fig. 10: Frame-wise PSNR (↑), SSIM (↑), and LPIPS (↓) on the KTH test set.</figDesc><table><row><cell>Method</cell><cell>s</cell><cell>e</cell><cell>Mode</cell><cell>MSE (↓)</cell></row><row><cell>PredRNN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.3</cell></row><row><cell>+ Scheduled sampling [23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Linear</cell><cell>51.8</cell></row><row><cell>+ 1st strategy in Fig. 5</cell><cell cols="2">0.0 1.0</cell><cell>Sigmoid</cell><cell>53.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Exponential</cell><cell>51.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Linear</cell><cell>51.9</cell></row><row><cell>+ 2nd strategy in Fig. 5</cell><cell cols="2">0.5 1.0</cell><cell>Sigmoid</cell><cell>50.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Exponential</cell><cell>50.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>contains 6 types of human actions, i.e., walking, jogging, running, boxing, hand-waving, and hand-clapping, performed by 25 persons in 4 different scenarios. The videos last 4 seconds on average and were taken against fairly uniform backgrounds with a static camera in a frame rate of 25 FPS. To make the results comparable, we adopt the training/testing protocol (persons 1-16 for training, and persons 17-25 for testing) from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 5 :</head><label>5</label><figDesc>Results on the KTH test set, averaged over the prediction time horizon of 20 timestamps.</figDesc><table><row><cell>Model</cell><cell cols="3">PSNR (↑) SSIM (↑) LPIPS (↓)</cell></row><row><cell>ConvLSTM [1]</cell><cell>23.58</cell><cell>0.712</cell><cell>0.231</cell></row><row><cell>MCnet + Residual [38]</cell><cell>26.29</cell><cell>0.806</cell><cell>-</cell></row><row><cell>TrajGRU [50]</cell><cell>26.97</cell><cell>0.790</cell><cell>-</cell></row><row><cell>DFN [74]</cell><cell>27.26</cell><cell>0.794</cell><cell>-</cell></row><row><cell>Conv-TT-LSTM [55]</cell><cell>27.62</cell><cell>0.815</cell><cell>0.196</cell></row><row><cell>PredRNN</cell><cell>27.55</cell><cell>0.839</cell><cell>0.204</cell></row><row><cell>PredRNN-V2</cell><cell>28.37</cell><cell>0.838</cell><cell>0.139</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 6 :</head><label>6</label><figDesc>Results on the Guangzhou radar echo dataset, averaged over the prediction time horizon of 10 timestamps.</figDesc><table><row><cell>Model</cell><cell cols="4">MSE (↓) CSI-30 (↑) CSI-40 (↑) CSI-50 (↑)</cell></row><row><cell>TrajGRU [50]</cell><cell>68.3</cell><cell>0.309</cell><cell>0.266</cell><cell>0.211</cell></row><row><cell>ConvLSTM [1]</cell><cell>63.7</cell><cell>0.381</cell><cell>0.340</cell><cell>0.286</cell></row><row><cell>MIM [4]</cell><cell>39.3</cell><cell>0.451</cell><cell>0.418</cell><cell>0.372</cell></row><row><cell>PredRNN</cell><cell>39.1</cell><cell>0.455</cell><cell>0.417</cell><cell>0.358</cell></row><row><cell>PredRNN-V2</cell><cell>36.4</cell><cell>0.462</cell><cell>0.425</cell><cell>0.378</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t H 3 t H 1 t H 4 t H 1 t H 2 t H 3 t H 1 t H 2 t H 3 t M 4</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t M l 1 t C l t C l t 1 H l t 1 H l t H 1 t H 2 t H 3 t C 4 t C 2 t C 3 t C 1 t M 1 t M 2 t M 3 t M 4 t X t X t 1 X t+1 L l,t decouple = cos( C l t , M l t ) H 1 t H 2 t H 3 t H 4 t M 4 t 1Fig. 2: Left: the main architecture of PredRNN, in which the orange arrows denote the state transition paths of M l t , namely the spatiotemporal memory flow. Right: the Spatiotemporal LSTM unit with twisted memory states that serves as the building block of the proposed PredRNN, where the orange circles denote the unique structures compared with ConvLSTM.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Key R&amp;D Program of China (2020AAA0109201), NSFC grants 62022050, 61772299, 62021002, 71690231, Beijing Nova Program (Z201100006820041), and CAAI-Huawei MindSpore Open Fund. The work was in part done when Y. Wang was a student at Tsinghua University. Y. Wang and H. Wu contributed equally to this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="802" to="810" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">PredRNN: Recurrent neural networks for predictive learning using spatiotemporal lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PredCNN: Predictive learning with cascade convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2940" to="2947" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Memory in memory: A predictive neural network for learning higher-order non-stationarity from spatiotemporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="9154" to="9162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to see physics via visual de-animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="153" to="164" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relational neural expectation maximization: Unsupervised discovery of objects and their interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="2688" to="2697" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised discovery of parts, structure, and dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Eidetic 3D LSTM: A model for video prediction and beyond</title>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent world models facilitate policy evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2450" to="2462" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2555" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visual foresight for planning robot motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2786" to="2793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Self-supervised visual planning with temporal skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the number of response regions of deep feed forward networks with piecewise linear activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6098</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1171" to="1179" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Actionconditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2863" to="2871" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Videoflow: A flow-based generative model for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep generative image models using a Laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1486" to="1494" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal coherency based criteria for predicting video frames using deep multi-stage generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4271" to="4280" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dual motion GAN for future-flow embedded video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1744" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Future video synthesis with object motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5539" to="5548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4417" to="4426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="517" to="526" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Disentangling physical dynamics from unknown factors for unsupervised video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Guen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="1174" to="1183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">High fidelity video prediction with large stochastic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="81" to="91" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved conditional vrnns for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7608" to="7617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stochastic latent residual video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delasalles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3233" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="64" to="72" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep learning for precipitation nowcasting: A benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5617" to="5627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pre-dRNN++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="5123" to="5132" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Contextvp: Fully context-aware video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koumoutsakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="753" to="769" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Folded recurrent neural networks for future video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="716" to="731" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Structure preserving video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1460" to="1469" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Convolutional tensor-train lstm for spatio-temporal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Conditional image generation with Pixel-CNN decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4790" to="4798" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Markov chains: from theory to implementation and experimentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Gagniuc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3560" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="6038" to="6046" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3104" to="3112" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2440" to="2448" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Taking on the curse of dimensionality in joint distributions using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="557" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Neural network ensembles, cross validation, and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vedelsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="231" to="238" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of biometrics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A multi-player minimax game for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="586" to="595" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="667" to="675" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
