<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Supervision Framework for Relation Extraction with Distant Supervision and Human Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woohwan</forename><surname>Jung</surname></persName>
							<email>whjung@kdd.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University Seoul</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyuseok</forename><surname>Shim</surname></persName>
							<email>kshim@snu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Supervision Framework for Relation Extraction with Distant Supervision and Human Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relation extraction (RE) has been extensively studied due to its importance in real-world applications such as knowledge base construction and question answering. Most of the existing works train the models on either distantly supervised data or human-annotated data. To take advantage of the high accuracy of human annotation and the cheap cost of distant supervision, we propose the dual supervision framework which effectively utilizes both types of data. However, simply combining the two types of data to train a RE model may decrease the prediction accuracy since distant supervision has labeling bias. We employ two separate prediction networks HA-Net and DS-Net to predict the labels by human annotation and distant supervision, respectively, to prevent the degradation of accuracy by the incorrect labeling of distant supervision. Furthermore, we propose an additional loss term called disagreement penalty to enable HA-Net to learn from distantly supervised labels. In addition, we exploit additional networks to adaptively assess the labeling bias by considering contextual information. Our performance study on sentence-level and document-level REs confirms the effectiveness of the dual supervision framework. This work is licensed under a Creative Commons Attribution 4.0 International License. License details:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) has been widely used in real-world applications such as knowledge base construction <ref type="bibr" target="#b4">(Dong et al., 2014a;</ref><ref type="bibr" target="#b5">Dong et al., 2014b;</ref><ref type="bibr" target="#b11">Jung et al., 2019)</ref>, question answering <ref type="bibr" target="#b22">(Xu et al., 2016)</ref> and biomedical data mining <ref type="bibr" target="#b0">(Ahmed et al., 2019)</ref>. Given a pair of entities in a text (e.g., sentence or document), the goal of RE is to discover the relationships between the entities expressed in the text. More specifically, we aim to extract triples from the text in the form of e h , r, e t where e h is a head entity, e t is a tail entity and r is a relationship between the entities.</p><p>To train a model for RE, we need a large volume of fully labeled training data in the form of text-triple pairs. Although human annotation provides high-quality labels to train the relation extraction models, it is difficult to produce a large-scale training data since manual labeling is expensive and time-consuming. Thus, <ref type="bibr" target="#b17">Mintz et al. (2009)</ref> proposed distant supervision to automatically produce a large labeled data by using an external knowledge base (KB). For a text with a head entity e h and a tail entity e t , when a triple e h , r, e t exists in the KB for any relation type r, distant supervision produces a label e h , r, e t even though the relationship is not expressed in the text. Thus, it suffers from the wrong labeling problem. For instance, if a triple U K, capital, London is in the KB, distant supervision labels the triple even for the sentence 'London is the largest city of the UK'.</p><p>Although each of the two labeling methods has a certain weakness, most of the existing works for RE utilize either human-annotated (HA) data or distantly supervised (DS) data. To take advantage of the high accuracy of human annotation and the cheap cost of distant supervision, we propose to effectively utilize a large DS data as well as a small amount of HA data. Since DS data is likely to have labeling bias, simply combining the two types of data to train a RE model may decrease the prediction accuracy. To take a close look at the labeling bias, let the inflation of a relation type be the ratio of the average frequencies of the relation type per text in DS data and HA data, respectively. We say that a relation type is unbiased if the average frequency of the relation type in DS data is the same as that in HA data (i.e., the inflation of the relation is 1). By examining a document-level RE dataset (DocRED) <ref type="bibr" target="#b23">(Yao et al., 2019)</ref> with 96 relation types, we found that the inflations of the relation types are from 0.48 to 85.9. It indicates that distant supervision tends to generate a large number of false labels for some relation types.</p><p>Recently,  introduced a domain adaptation approach to tackle the labeling bias problem for RE. It trains a RE model on DS data and adjusts the bias term of the output layer by using HA data. Although the bias adjustment achieves a meaningful accuracy improvement, it has a limitation. An underlying assumption of the method is that the labeling bias is static for every text since it adjusts the bias term only once after training and uses the same bias during the test time. However, the labeling bias varies depending on contextual information. For example, in DocRED dataset, most of the capital relation labeled by distant supervision are false positive. However, if the phrase 'is the capital city of' appears in the text, the label is likely to be a true label. Thus, we need to take account of contextual information to extract relations more accurately by considering the labeling bias.</p><p>To effectively utilize DS data and HA data for training RE models, we propose the dual supervision framework that can be applied to most existing RE models to achieve additional accuracy gain. Since the label distributions in HA data and DS data are quite different, we cast the task of training RE models with both data as a multi-task learning problem. Thus, we employ the two separate output modules HA-Net and DS-Net to predict the labels by human annotation and distant supervision, respectively, while previous works utilize a single output module. This allows the different predictions of the labels for human annotation and distant supervision, and thus it prevents the degradation of accuracy by incorrect labels in DS data. If we simply separate the prediction networks to apply the multi-task learning, HA-Net cannot learn from distantly supervised labels. To enable HA-Net to learn from DS data, we propose an additional loss term called disagreement penalty. It models the ratio of the output probabilities from the prediction networks HA-Net and DS-Net by using maximum likelihood estimation with log-normal distributions to generate the calibrated gradient to update HA-Net to effectively reflect distantly supervised labels. Furthermore, our framework exploits two additional networks µ-Net and σ-Net to adaptively estimate the log-normal distribution by considering contextual information. Moreover, we theoretically show that the disagreement penalty enables HA-Net to effectively utilize the labels generated by distant supervision. Finally, we validate the effectiveness of the dual supervision framework on two types of tasks: sentencelevel and document-level REs. The experimental results confirm that our dual supervision framework significantly improves the prediction accuracy of existing RE models. In addition, the dual supervision framework substantially outperforms the state-of-the-art method  in both sentence-level and document-level REs with the relative F1 score improvement of up to 32%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We present the problems of sentence-level and document-level relation extractions and next introduce existing works for relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Statement</head><p>Following the works <ref type="bibr" target="#b23">(Yao et al., 2019;</ref><ref type="bibr" target="#b21">Wang et al., 2019)</ref>, we assume that each text is annotated with entity mentions. For a pair of entities, since a sentence usually describes a single relationship between them, the sentence-level relation extraction is generally regarded as a multi-class classification problem.</p><p>Definition 2.1 (Sentence-level relation extraction) For a pair of the head and tail entities e h and e t , a relation type set R and a sentence s annotated with entity mentions, we determine the relation r ∈ R between e h and e t in the sentence. Note that R includes a special relation type NA which indicates that there does not exist any relation between e h and e t .</p><p>Since multiple relationships between a pair of entities can be expressed in a document, document-level relation extraction is usually defined as a multi-label classification problem. In this paper, we mainly discuss sentence-level RE and extend our framework to document-level RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Existing Works of Relation Extraction</head><p>A typical RE model consists of a feature encoder and a prediction network, as shown in <ref type="figure">Figure 1</ref>(a). The feature encoder converts a text into the hidden representations of the head and tail entities. <ref type="bibr" target="#b2">Cai et al. (2016)</ref> and <ref type="bibr" target="#b21">Wang et al. (2019)</ref> exploit Bi-LSTM and BERT, respectively, to encode the text. On the other hand, <ref type="bibr" target="#b26">Zeng et al. (2014)</ref> and <ref type="bibr" target="#b27">Zeng et al. (2015)</ref> use CNN for the encoder. In addition, <ref type="bibr" target="#b26">Zeng et al. (2014)</ref> propose the position embedding to consider the relative distance from each word to head and tail entities.</p><p>The prediction network outputs the probability distribution of the relations between the entities. Since sentence-level RE is a multi-class classification task, sentence-level RE models <ref type="bibr" target="#b2">(Cai et al., 2016;</ref><ref type="bibr" target="#b26">Zeng et al., 2014;</ref><ref type="bibr" target="#b27">Zeng et al., 2015)</ref> utilize a softmax classifier as the prediction network and use categorical cross entropy as the loss function. On the other hand, document-level RE models <ref type="bibr" target="#b23">(Yao et al., 2019;</ref><ref type="bibr" target="#b21">Wang et al., 2019</ref>) use a sigmoid classifier and binary cross entropy as the prediction network and the loss function, respectively. Since the labels obtained from distant supervision are noisy and biased, with a single prediction network, it is hard to make accurate predictions for DS data and HA data together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dual Supervision Framework</head><p>We first present an overview of the dual supervision framework which effectively utilizes both humanannotated (HA) data and distantly supervised (DS) data for training RE models. We next introduce the detailed structure of the output layer in our framework and propose our novel loss function with disagreement penalty that considers the labeling bias of distant supervision. Then, we describe how to train the proposed model with both types of data as well as how to extract relations from the test data. Finally, we discuss how the disagreement penalty makes each prediction network learn from the labels for the other prediction network although we use separate prediction networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">An Overview of the Dual Supervision Framework</head><p>As shown in <ref type="figure">Figure 1</ref>(b), our framework consists of a feature encoder and an output layer with 4 subnetworks. It is general enough to accommodate a variety of existing RE models to improve their accuracy. We can apply our framework to an existing RE model by using the feature encoder of the model and building the four sub-networks which exploit the structure of the original prediction network. Since our framework uses the feature encoder of the existing models, we briefly describe only the output layer here.</p><p>Unlike the previous works, to allow the difference in the predictions for human annotated labels and distantly supervised labels, we exploit multi-task learning by employing two separate prediction networks HA-Net and DS-Net to predict the labels in HA data and DS data, respectively. We also use HA-Net to extract relations from the test data. The separation of the prediction networks prevents the accuracy degradation caused by incorrect labels from distant supervision. If we simply utilize two prediction networks to apply the multi-task learning, HA-Net cannot learn from distantly supervised labels although the prediction networks share the feature encoder. To enable HA-Net to learn from distantly supervised labels, we introduce an additional loss term called disagreement penalty. It models the disagreement between the outputs of HA-Net and DS-Net by using maximum likelihood estimation with log-normal distributions. Furthermore, to adaptively estimate the parameters of the log-normal distribution by considering contextual information, we exploit two parameter networks µ-Net and σ-Net.</p><p>For a label e h , r, e t , let I HA be an indicator variable that is 1 if the label is obtained by human annotation and 0 otherwise. The proposed framework uses the following loss function for a label e h , r, e t</p><formula xml:id="formula_0">L h,t = I HA · L HA h,t + (1 − I HA ) · L DS h,t + λ · L DS-HA h,t<label>(1)</label></formula><p>where L HA h,t and L DS h,t denote the prediction loss of HA-Net and DS-Net, respectively, and L DS-HA h,t is the disagreement penalty to capture the distance between the predictions by HA-Net and DS-Net. The hyper parameter λ controls the relative importance of the disagreement penalty to the prediction errors. By using a separate prediction network for each type of data and introducing the disagreement penalty, HA-Net learns from distantly supervised labels while reducing overfitting to noisy DS data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Separate Prediction Networks</head><p>To alleviate the accuracy degradation from the noisy labels in DS data, we utilize two prediction networks. The network HA-Net is used to predict the human-annotated labels from the train data and to predict relations from the test data. The other prediction network DS-Net predicts the labels obtained by distant supervision. We use the prediction network of an existing model for both prediction networks of our framework without sharing the model parameters. The prediction networks HA-Net and DS-Net output the |R|-dimensional vectors p HA = [p(r 1 |e h , e t , HA), ... , p(r |R| |e h , e t , HA)] and p DS = [p(r 1 |e h , e t , DS), ... , p(r |R| |e h , e t , DS)], respectively, where p(r|e h , e t , HA) and p(r|e h , e t , DS) are the probabilities that there exists a label e h , r, e t , in HA data and DS data, respectively. We simply denote p(r|e h , e t , HA) and p(r|e h , e t , DS) by p HA r and p DS r , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Disagreement Penalty</head><p>Distant supervision labels are biased and the size of the bias varies depending on the type of relation. Moreover, the bias can vary depending on many other features such as the types of head and tail entities as well as the contents of a text. Thus, we propose to use an effective disagreement penalty to model the labeling bias depending on the context where the head and tail entities are located.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution p-value</head><p>Log-normal 0.008 Weibull 0.001 Chi-square 4.6 × 10 −10 Exponential 3.6 × 10 −13 Normal 1.2 × 10 −15 is widely used to determine whether an observed data is drawn from a given probability distribution, we used it to find the best-fit distribution of the inflations. Since the range of the inflation is [0, ∞), we evaluated p-values of the four probability distributions supported on [0, ∞): Log-normal, Weibull, chi-square and exponential distributions. In addition, we include the normal distribution as a baseline. <ref type="table" target="#tab_0">Table 1</ref> shows the result of K-S test for DocRED data. Note that a probability distribution has a high p-value if the probability distribution fits the data well. Since the log-normal distribution has the highest p-value, it is the best-fit distribution among the five probability distributions. Based on the observation, we model the disagreement penalty between the outputs of the two prediction networks.</p><p>Modeling the disagreement penalty. We develop the disagreement penalty based on the maximum likelihood estimation. Let X r be the random variable which denotes the ratio of p DS r to p HA r . Since the inflation is the ratio of the number of labels in DS data and HA data, the ratio p DS r /p HA r represents the conditional inflation of the relation type r conditioned on the text with head and tail entities. Thus, we assume that X r follows a log-normal distribution L(µ r , σ 2 r ) whose probability density function is</p><formula xml:id="formula_1">f (x) = 1 xσ r √ 2π exp − (log x − µ r ) 2 2σ 2 r .</formula><p>(2)</p><p>The disagreement penalty L DS-HA h,t is defined as the negative log likelihood of the conditional inflation p DS r /p HA r , which is obtained by substituting p DS r /p HA r into Equation <ref type="formula">(2)</ref> as follows:</p><formula xml:id="formula_2">− log f p DS r /p HA r = 1 2 log p DS r − log p HA r − µ r σ r 2 + log p DS r − log p HA r + log σ r + log 2π 2 .<label>(3)</label></formula><p>Since log 2π 2 is constant, we utilize the disagreement penalty in Equation <ref type="formula" target="#formula_2">(3)</ref> without the constant term. If we set µ r and σ r to fixed values, we cannot effectively assess the conditional inflation since it can vary depending on the context. For example, although the inflation of the relation type capital is high, the conditional inflation should be lower if a particular phrase such as 'is the capital city of' appears in the text. To take account of the contextual information, we employ two additional networks µ-Net and σ-Net to estimate the µ r and σ r that are the parameters of log-normal distribution L(µ r , σ 2 r ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter Networks</head><p>The parameter networks µ-Net and σ-Net output the vectors µ = [µ 1 , ..., µ |R| ] and σ = [σ 1 , ..., σ |R| ], respectively, which are the parameters of the log-normal distributions to represent the conditional inflation for r ∈ R. Both µ-Net and σ-Net have the same structure as those of the prediction networks except their output activation functions. For a log-normal distribution L(µ, σ), the parameter µ can be positive or negative, and σ is always positive. Thus, we use a hyperbolic tangent function and a softplus function <ref type="bibr" target="#b6">(Dugas et al., 2001)</ref> as the output activation functions of µ-Net and σ-Net, respectively. For example, if the prediction network of the original RE model consists of a bilinear layer and an output activation function, the parameter vectors µ ∈ R |R| and σ ∈ R |R| are computed from the head entity vector h ∈ R d and tail entity vector t ∈ R d as</p><formula xml:id="formula_3">µ = tanh(h W µ t + b µ ), σ = sof tplus(h W σ t + b σ ) + ε</formula><p>where sof tplus(x) = log (1 + e x ) and ε is a sanity bound preventing extremely small values of σ r from dominating the loss function, and W µ ∈ R d×|R|×d , W σ ∈ R d×|R|×d , b µ ∈ R |R| and b σ ∈ R |R| are learnable parameters. We set the sanity bound ε to 0.0001 in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>For sentence-level relation extraction, we use the categorical cross entropy loss as the prediction losses L HA h,t and L DS h,t . For a label e h , r, e t , we obtain the following loss function from Equations <ref type="formula" target="#formula_0">(1)</ref> and (3)</p><formula xml:id="formula_4">L h,t =I HA · L HA h,t + (1 − I HA ) · L DS h,t + λ · L DS-HA h,t = − I HA · log p HA r − (1 − I HA ) log p DS r + λ 1 2 r − µ r σ r 2 + r + log σ r<label>(4)</label></formula><p>where r = log p DS r − log p HA r , and I HA is 1 if the label is from HA data and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Analysis of the Disagreement Penalty</head><p>Let w HA be a learnable parameter of HA-Net which predicts relations in the test time. We investigate the effect of the disagreement penalty by comparing the gradients of loss functions with respect to w HA for a human annotated label and a distantly supervised label. For a label e h , r, e t , let φ r = (log (p DS r /p HA r ) − µ r )/σ 2 r . If the label is human annotated, we obtain the following gradient of the loss L h,t with respect to w HA from Equation <ref type="formula" target="#formula_4">(4)</ref> ∇L h,t = ∇L HA h,t + 0 + λ∇L DS-HA h,t = − (1 + λ(1+φ r )) 1 p HA r ∇p HA r .</p><p>On the other hand, if the label is annotated by distant supervision, the gradient becomes</p><formula xml:id="formula_6">∇L h,t = 0 + 0 + λ∇L DS-HA h,t = −λ (1 + φ r ) 1 p HA r ∇p HA r .<label>(6)</label></formula><p>The two gradients in Equations <ref type="formula" target="#formula_5">(5)</ref> and <ref type="formula" target="#formula_6">(6)</ref> have the same direction of −∇p HA r . It implies that a human annotated label and a distantly supervised label have similar effects on training HA-Net except that the magnitudes of gradients are calibrated by 1+λ(1+φ r ) and λ(1+φ r ), respectively. Thus, HA-Net can learn from not only human annotated labels but also distantly supervised labels by introducing the disagreement penalty. Recall that the log-normal distribution L(µ r , σ r ) describes the conditional inflation for a given sentence with a head entity and a tail entity. If the median e µr of L(µ r , σ r ) has a high value, the distantly supervised label is likely to be a false label. Thus, we decrease the size of φ r to reduce the effect of a distantly supervised label. On the other hand, as the median e µr becomes lower, the size of φ r increases to aggressively utilize the distantly supervised label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Extension to Document-level Relation Extraction</head><p>For the document-level RE, we use the binary cross entropy as the prediction losses L HA h,t and L DS h,t . For a pair of entities e h and e t , let R h,t be the set of relation types between the entities. In the train time, we use the following loss function for document relation extraction</p><formula xml:id="formula_7">L h,t =−I HA   r∈R h,t log p HA r + r∈R\R h,t log (1−p HA r )   −(1−I HA )   r∈R h,t log p DS r + r∈R\R h,t log (1−p DS r )   +λ r∈R h,t 1 2 r −µ r σ r 2 + r +log σ r .</formula><p>where r = log p DS r /p HA r , and I HA is 1 if the labels are from HA data and 0 otherwise. We obtain the same property shown in Section 3.6 for the above loss function. In the test time, we regard that the model outputs the triple e h , r, e t if p HA r is greater than a threshold which is tuned on the development dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted a performance study for sentence-level and document-level REs by following the experimental settings of  and <ref type="bibr" target="#b23">(Yao et al., 2019;</ref><ref type="bibr" target="#b21">Wang et al., 2019)</ref>, respectively. All models are implemented in PyTorch and trained on a V100 GPU. We initialized HA-Net and DS-Net to have the same initial parameters. More experimental details including implementations can be found in Appendix A.    Compared methods. We compare our dual supervision framework, denoted by DUAL, with the stateof-the-art methods BASet and BAFix in . For sentence-level RE, we compare DUAL with two additional baselines MaxThres  and EntThres <ref type="bibr" target="#b15">(Liu et al., 2017)</ref> which are only applicable to multi-class classification and cannot be used in document-level RE. MaxThres outputs NA if the maximum output probability is less than a threshold. Similarly, EntThres outputs NA if the entropy of the output probability distribution is greater than a threshold. Used relation extraction models. For sentence-level RE, we used the six models: BiGRU S <ref type="bibr" target="#b28">(Zhang et al., 2017)</ref>, PaLSTM S <ref type="bibr" target="#b28">(Zhang et al., 2017)</ref>, BiLSTM S <ref type="bibr" target="#b28">(Zhang et al., 2017)</ref>, CNN S <ref type="bibr" target="#b26">(Zeng et al., 2014)</ref>, PCNN S <ref type="bibr" target="#b27">(Zeng et al., 2015)</ref> and BERT S <ref type="bibr" target="#b21">(Wang et al., 2019)</ref>. On the other hand, for document-level RE, we used the five models: BERT D <ref type="bibr" target="#b21">(Wang et al., 2019)</ref>, CNN D <ref type="bibr" target="#b26">(Zeng et al., 2014)</ref>, LSTM D <ref type="bibr" target="#b23">(Yao et al., 2019)</ref>, BiLSTM D <ref type="bibr" target="#b2">(Cai et al., 2016)</ref> and CA D <ref type="bibr" target="#b20">(Sorokin and Gurevych, 2017)</ref>. Note that CNN D , BiLSTM D , and CA D are originally proposed for sentence-level RE and we used the adaptation of them to documentlevel RE by <ref type="bibr" target="#b23">Yao et al. (2019)</ref>. In addition, we adapt BERT D to the sentence-level RE by changing the output activation function from sigmoid to softmax and denote it by BERT S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Existing Methods</head><p>We compare the dual supervision framework with the existing methods. Sentence-level RE.  <ref type="table" target="#tab_4">Table 4</ref>. DUAL outperforms BASet and BAFix with all RE models. Especially, the F1 score of dual framework with BERT D shows more than 22% of improvement over BASet and BAFix. Since DocRED has a large human-annotated train data, HA-Only shows better performance than DS-Only. For BERT D and CNN D , the existing methods show lower F1 scores compared to HA-Only. It shows that the accuracy can be degraded although we use additional DA data in addition to HA data due to the labeling bias. Meanwhile, we achieve a consistent and significant improvement by applying DUAL. In the rest of this paper, we will provide a detailed evaluation of performance on DocRED data which is the largest dataset in this experiment. For the test  data of DocRED, the ground truth is not publicly available and only a F1 score can be obtained from the DocRED competition. Thus, we provide detailed evaluations of performance on the dev data only. Inflation vs. accuracy. To investigate the effect of the inflation to the accuracy of relation extraction, we split the relation types into 4 groups based on the inflation of the relation types. In <ref type="figure" target="#fig_1">Figure 2</ref>, we present the characteristics of each group and plot the F1 scores by groups for BERT D model and BiLSTM D model. All methods have the highest F1 scores when the inflation is close to 1 (at the 2nd group). Furthermore, the improvement of F1 score by DUAL compared to the second best performer increases as the inflation moves away from 1. Thus, it confirms that our dual supervision framework effectively utilizes both human annotation and distant supervision by modeling the bias of the distant supervision. Since the other models CA D , LSTM D and CNN D show similar results with BiLSTM D , we omit the result. We conducted an ablation study with the existing model BERT D on DocRED to validate the effectiveness of individual components of our framework. We compared DUAL (separate prediction networks + disagreement penalty) and two variations of our framework Multitask (separate prediction networks only) and Single. Multitask denotes a variation of DUAL which does not utilize the disagreement penalty, while BERT D without applying the dual supervision framework is referred to as Single. Note that Single is also trained on both HA data and DS data together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To show the effectiveness of the components depending on the size of HA data, we plotted the F1 scores with varying the number of human-annotated document from 152 to 3,053 (i.e., from 5% to 100% of the documents with HA). As we expected, DUAL outperforms both variations all the time. Furthermore, separation of the prediction networks significantly improves the accuracy when we have enough number of humanannotated labels. However, when we use less than 10% of the human annotated documents, Multitask suffers from the sparsity problem. By utilizing the disagreement penalty additionally, DUAL outperforms Single even when we use only 5% of the human-annotated documents for training the model. It implies that the disagreement penalty enables HA-Net to effectively learn from DS data as well as HA data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quality Comparison</head><p>To give an idea of what false relations are found by existing methods, we provide two example documents in the dev data of DocRED and the relations extracted by DUAL, BAFix and DS-Only with BERT D in <ref type="table" target="#tab_8">Table 5</ref>. The relation Sweden, capital, Stockholm is expressed in the document titled 'Kungliga Hovkapellet' and all methods find the relation correctly. In the document titled 'Loopline Bride', the relation Ireland, capital, Dublin does not exist. However, BAFix and DS-Only output the incorrect    <ref type="formula">(2020)</ref> proposed a topic-aware relation extraction (T-REX) model which is robust to the omitted mentions of topic entities in documents. We apply our dual supervision framework to the T-REX on DocRED dataset and report the result in <ref type="table" target="#tab_9">Table 6</ref>. The result shows that our dual supervision framework is also effective in the topic-aware RE task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>We briefly survey the existing works for RE. <ref type="bibr" target="#b17">Mintz et al. (2009)</ref> propose distant supervision to overcome the limitation of the quantity of human-annotated labels. They utilize lexical, syntactic and named entity tag features obtained by existing NLP tools to extract relations. Other early works in <ref type="bibr" target="#b19">(Riedel et al., 2010;</ref><ref type="bibr" target="#b9">Hoffmann et al., 2011)</ref> also utilized hand-crafted features to find the relations in text. However, since such RE models take the input features from NLP tools, the errors generated by the NLP tools are propagated to the RE models. In order to deal with the error propagation, as we discussed in Section 2.2, the works <ref type="bibr" target="#b13">(Lin et al., 2016;</ref><ref type="bibr" target="#b26">Zeng et al., 2014;</ref><ref type="bibr" target="#b27">Zeng et al., 2015;</ref><ref type="bibr" target="#b20">Sorokin and Gurevych, 2017;</ref><ref type="bibr" target="#b21">Wang et al., 2019)</ref> use deep neural networks such as CNN, LSTM and BERT instead of handcrafted features to encode the text for finding the relations. Since many relational facts are expressed across multiple sentences, the recent works <ref type="bibr" target="#b23">(Yao et al., 2019;</ref><ref type="bibr" target="#b21">Wang et al., 2019</ref>) studied document-level RE. <ref type="bibr" target="#b23">Yao et al. (2019)</ref> provide a document-level RE dataset (DocRED) as well as compare the models adapted from the sentence-level RE models <ref type="bibr" target="#b26">(Zeng et al., 2014;</ref><ref type="bibr" target="#b8">Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b2">Cai et al., 2016;</ref><ref type="bibr" target="#b20">Sorokin and Gurevych, 2017)</ref>. Moreover, a fine tuned model <ref type="bibr" target="#b21">(Wang et al., 2019)</ref> of BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> for document-level RE achieved a higher F1 score than the baselines on DocRED.</p><p>The wrong labeling problem in distant supervision has been addressed in many previous works <ref type="bibr" target="#b27">(Zeng et al., 2015;</ref><ref type="bibr" target="#b13">Lin et al., 2016;</ref><ref type="bibr" target="#b24">Ye and Ling, 2019;</ref><ref type="bibr" target="#b1">Beltagy et al., 2018)</ref>. Among them, <ref type="bibr" target="#b27">Zeng et al. (2015)</ref>, <ref type="bibr" target="#b13">Lin et al. (2016)</ref> and <ref type="bibr" target="#b24">Ye and Ling (2019)</ref> build a bag-of-sentences for a pair of entities and extract relational facts from the bag-of-sentences with attention over the sentences. <ref type="bibr" target="#b1">Beltagy et al. (2018)</ref> propose a bag-of-sentences-level model which utilizes human annotation. However, they use the human annotated labels only to determine whether there exists a relationship or not since the labels are obtained from a different domain. The goal of these works is different from ours which is to find the relations appearing in a given text (e.g., a document). Thus, the bag-of-sentences-level models have a limitation to be used for some applications such as question answering.</p><p>The most relevant work to ours is . This paper proposes the bias adjustment methods to utilize a small amount of HA data to improve RE models trained on DS data by considering the different distribution of human annotated labels and distantly supervised labels. However, they do not use HA data to train the models and use the HA data only to obtain a statistic to be used the determine the size of the bias adjustment. Thus, the bias adjustment methods cannot consider contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed the dual supervision framework to utilize human annotation and distant supervision based on the analysis of labeling bias in distant supervision. We devised a new structure for the output layer of RE models that consists of 4 sub networks. The new structure is robust to the noisy labeling of distant supervision since the labels obtained by human annotation and distant supervision are predicted by separate prediction networks HA-Net and DS-Net, respectively. In addition, we introduced an additional loss term called disagreement penalty which enables HA-Net to learn from distantly supervised labels. The parameter networks µ-Net and σ-Net adaptively assess the labeling bias by considering contextual information. Moreover, we theoretically analyzed the effect of the disagreement penalty. Our experiments showed that the dual supervision framework significantly outperforms the existing methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head><p>Our implementation is available at https://github.com/woohwanjung/dual. Document-level RE. For BiLSTM D , LSTM D , CA D and CNN D , we utilized the code which is available at https://github.com/thunlp/DocRED and implemented by <ref type="bibr" target="#b23">Yao et al. (2019)</ref>. In addition, we used the implementation of BERT D that is available at https://github.com/hongwang600/ DocRed and provided by <ref type="bibr" target="#b21">Wang et al. (2019)</ref>. We used Adam optimizer (Kingma and Ba, 2014) to optimize the RE models. For the BERT D model, we set the batch size to 12 and learning rate to 10 −5 . For the other models, we followed the setting provided in <ref type="bibr" target="#b23">(Yao et al., 2019)</ref>: batch size is 40, learning rate is 10 −3 . We set the hyperparameters λ and d to 10 −5 and 128, respectively. Each training batch has half of the instances with human-annotated labels and the other half of instances with distantly supervised labels.</p><p>Sentence-level RE. We use the code which is made publicly available by  at https: //github.com/INK-USC/shifted-label-distribution. All models except BERT S are trained by stochastic gradient descent. Learning rate is initially set to 1.0, and decreased to 10% if there is no improvement on the dev data for 3 consecutive epochs. For the models, we set the hyperparameters λ and d to 10 −3 and 200, respectively. To train BERT S model, we used Adam optimizer with learning rate 10 −5 . Moreover, the hyperparameters λ and d are set to 10 −4 and 128, respectively. We alternately used an HA batch and a DS batch for dual supervision where an HA batch consists of training instances with human annotated labels and a DS batch consists of training instances with distantly supervised labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experiments</head><p>The precision-recall curves of the compared methods are shown in <ref type="figure">Figure 4</ref>. As expected, DUAL consistently outperforms all compared methods. BAFix and BASet have similar precision-recall curves with DS-Only. Although HA-Only shows comparable precisions with DUAL when recall is low, the precision of HA-Only drops faster than that of DUAL with increasing recall. It implies that human annotated labels are not enough for training a model to extract a large number of relations. Meanwhile, DUAL extracts more relations from the document compared to existing models at the same precision level.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Definition 2.2 (Document-level relation extraction) For a pair of the head and tail entities e h and e t , a relation type set R and a document d annotated with entity mentions, we find the set of all relations(a) Existing models (b) The dual supervision framework Figure 1: The overall architectures of existing models and our framework R * ⊂ R between e h and e t appearing in document d. Note that R does not include NA in this case since it can be represented by an empty set of R * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>F1 scores of different groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Varying the size of HA data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: Precision-recall curves</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The result of K-S test Distribution of inflations. We measure the labeling bias by using the inflations of relations. Recall that the inflation of a relation type is the ratio of the average frequencies of the relation type per text in DS data and HA data, respectively. To investigate the distribution of inflations, we computed the inflations of 96 relation types in Do-cRED data. Since Kolmogorov-Smirnov (K-S) test<ref type="bibr" target="#b16">(Massey Jr, 1951)</ref> </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets</figDesc><table><row><cell>Dataset. KBP (Ling and Weld, 2012; Ellis,</cell></row><row><cell>2012) and NYT (Riedel et al., 2010; Hoff-</cell></row><row><cell>mann et al., 2011) are datasets for sentence-</cell></row><row><cell>level RE, and DocRED (Yao et al., 2019) is</cell></row><row><cell>a dataset for document-level RE. The statis-</cell></row><row><cell>tics of the datasets are summarized in Ta-</cell></row><row><cell>ble 2. Since KBP and NYT do not have HA</cell></row><row><cell>train data, we use 20% of the HA test data as</cell></row></table><note>the HA train data. In addition, we randomly split 10% of train data on KBP and NYT for the development (dev) data. Note that the ground truth of the test data in DocRED is not publicly available. However, we can get the F1 score of the result extracted from the test data by submitting the result to the DocRED competition hosted by CodaLab (available at https://competitions.codalab.org/competitions/20717). We report both the F1 scores computed from the dev data and the test data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Sentence-level RE datasets (KBP and NYT)    </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Dev</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell cols="3">RE models BERTD BiLSTMD</cell><cell>CAD</cell><cell>LSTMD</cell><cell>CNND</cell><cell cols="2">BERTD BiLSTMD</cell><cell>CAD</cell><cell>LSTMD</cell><cell>CNND</cell></row><row><cell>HA-Only</cell><cell>0.5513</cell><cell>0.4992</cell><cell>0.4986</cell><cell>0.4817</cell><cell>0.4788</cell><cell>0.5478</cell><cell>0.4982</cell><cell>0.4992</cell><cell>0.4815</cell><cell>0.4681</cell></row><row><cell>DS-Only</cell><cell>0.4683</cell><cell>0.4951</cell><cell>0.4890</cell><cell>0.4877</cell><cell>0.4166</cell><cell>0.4587</cell><cell>0.4809</cell><cell>0.4772</cell><cell>0.4713</cell><cell>0.4160</cell></row><row><cell>BASet</cell><cell>0.4807</cell><cell>0.5123</cell><cell>0.5024</cell><cell>0.5012</cell><cell>0.4349</cell><cell>0.4716</cell><cell>0.4949</cell><cell>0.4905</cell><cell>0.4905</cell><cell>0.4320</cell></row><row><cell>BAFix</cell><cell>0.4802</cell><cell>0.5136</cell><cell>0.5070</cell><cell>0.5166</cell><cell>0.4365</cell><cell>0.4730</cell><cell>0.5061</cell><cell>0.4989</cell><cell>0.4977</cell><cell>0.4354</cell></row><row><cell>DUAL</cell><cell>0.5880</cell><cell>0.5510</cell><cell>0.5372</cell><cell>0.5392</cell><cell>0.4967</cell><cell>0.5774</cell><cell>0.5379</cell><cell>0.5306</cell><cell>0.5277</cell><cell>0.4909</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Document-level RE dataset (DocRED)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table 3shows F1 scores for relation extraction on KBP and NYT. Note that DS-Only and HA-Only represent the original RE models trained only on distantly supervised and human-annotated labels, respectively. DUAL shows the highest F1 scores with all RE models except BiLSTM S . Since KBP and NYT have a small number of human-annotated labels in train data, HA-Only shows worse F1 scores than DS-Only. Furthermore, DUAL achieves improvements of F1 score from 5% to 40% over DS-Only by additionally using the small amount of human annotated labels. On the other hand, the compared methods BAFix, BASet, MaxThres and EntThres often perform worse than DS-Only and HA-Only. Document-level RE. We present F1 scores on DocRED in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The Loopline Bridge (or the Liffey Viaduct) is a railway bridge spanning the River Liffey and several streets in [Dublin], [Ireland]. [2] It joins ...</figDesc><table><row><cell>Document Title: Kungliga Hovkapellet</cell><cell>Title: Loopline Bridge</cell></row><row><cell cols="2">[1] Kungliga Hovkapellet is a Swedish orchestra, orig-inally part of the Royal Court in [Sweden]'s capital [Stockholm]. [2] Its existence ... [1] Relations True label: Sweden, capital, Stockholm True label:NA</cell></row><row><cell>DUAL: Sweden, capital, Stockholm</cell><cell>DUAL:NA</cell></row><row><cell>BAFix: Sweden, capital, Stockholm</cell><cell>BAFix: Ireland, capital, Dublin</cell></row><row><cell>DS-Only: Sweden, capital, Stockholm</cell><cell>DS-Only: Ireland, capital, Dublin</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Examples of documents and extracted relations relation. Since DUAL adaptively assess the labeling bias with µ-Net and σ-Net, DUAL does not output the false relation. In addition, since the RE models trained with BAFix and DS-Only fail to learn the text pattern corresponding to the relation type due to the labeling bias, they output many false labels such as V ietnam, capital, T aipei in many documents. It shows that the dual supervision framework effectively deal with the labeling bias of distant supervision by considering contextual information.</figDesc><table><row><cell>4.5 Topic-aware RE</cell><cell></cell></row><row><cell></cell><cell>F1</cell><cell>AUC</cell></row><row><cell cols="3">HA-Only 0.6569 0.6456</cell></row><row><cell cols="3">DS-Only 0.6624 0.6978</cell></row><row><cell>DUAL</cell><cell cols="2">0.6930 0.7125</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Topic-aware RE</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science, ICT (No. NRF-2017M3C4A7063570) and was also supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation(IITP) grant funded by the Korea government(MSIT) (No. 2020-0-00857, Development of cloud robot intelligence augmentation, sharing and framework technology to integrate and enhance the intelligence of multiple robots). This research was results of a study on the "HPC Support" Project, supported by the 'Ministry of Science and ICT' and NIPA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identifying protein-protein interaction using tree lstm and structured attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Samee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 13th International Conference on Semantic Computing (ICSC)</title>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="224" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Combining distant and direct supervision for neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12956</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From data fusion to knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Xin Luna Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="881" to="892" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating secondorder functional knowledge for better option pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Bélisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="472" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Linguistic resources for 2013 knowledge base population evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">T-rex: A topic-aware relation extraction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woohwan</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyuseok</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information Knowledge Management, CIKM &apos;20</title>
		<meeting>the 29th ACM International Conference on Information Knowledge Management, CIKM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2073" to="2076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crowdsourced truth discovery in the presence of hierarchies for knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woohwan</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyuseok</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Database Technology -22nd International Conference on Extending Database Technology, EDBT 2019</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-03-26" />
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Heterogeneous supervision for relation extraction: A representation learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00166</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The kolmogorov-smirnov test for goodness of fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Massey</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">253</biblScope>
			<biblScope unit="page" from="68" to="78" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cotype: Joint extraction of typed entities and relations with knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-aware representations for knowledge base relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1784" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fine-tune bert for docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Question answering on freebase via relation extraction and textual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00957</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06127</idno>
		<title level="m">Docred: A large-scale document-level relation extraction dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distant supervision relation extraction with intra-bag and inter-bag attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Zhi-Xiu Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00143</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Looking beyond label noise: Shifted label distribution matters in distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinyuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3832" to="3841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
