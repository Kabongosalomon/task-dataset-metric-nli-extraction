<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Machine Translation with Adequacy-Oriented Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kong</surname></persName>
							<email>xiangk@andrew.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
							<email>zptu@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
							<email>shumingshi@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Machine Translation with Adequacy-Oriented Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although Neural Machine Translation (NMT) models have advanced state-of-the-art performance in machine translation, they face problems like the inadequate translation. We attribute this to that the standard Maximum Likelihood Estimation (MLE) cannot judge the real translation quality due to its several limitations. In this work, we propose an adequacyoriented learning mechanism for NMT by casting translation as a stochastic policy in Reinforcement Learning (RL), where the reward is estimated by explicitly measuring translation adequacy. Benefiting from the sequence-level training of RL strategy and a more accurate reward designed specifically for translation, our model outperforms multiple strong baselines, including (1) standard and coverage-augmented attention models with MLE-based training, and (2) advanced reinforcement and adversarial training strategies with rewards based on both word-level BLEU and character-level CHRF3. Quantitative and qualitative analyses on different language pairs and NMT architectures demonstrate the effectiveness and universality of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>During the past several years, rapid progress has been made in the field of Neural Machine Translation (NMT) <ref type="bibr" target="#b8">(Kalchbrenner and Blunsom 2013;</ref><ref type="bibr" target="#b19">Sutskever, Vinyals, and Le 2014;</ref><ref type="bibr" target="#b1">Bahdanau, Cho, and Bengio 2015;</ref><ref type="bibr" target="#b5">Gehring et al. 2017;</ref><ref type="bibr" target="#b25">Wu et al. 2016;</ref><ref type="bibr" target="#b22">Vaswani et al. 2017)</ref>.</p><p>Although NMT models have advanced the community, they still face inadequate translation problems: one or multiple parts of the input sentence are not translated <ref type="bibr" target="#b20">(Tu et al. 2016)</ref>. We attribute this problem to the lack of the mechanism to guarantee the generated translation being as sufficient as human translation. NMT models are generally trained in an end-to-end manner to maximize the likelihood of the output sentence. Maximum Likelihood Estimation (MLE), however, could not judge the real quality of generated translation due to its several limitations 1. Exposure bias <ref type="bibr" target="#b15">(Ranzato et al. 2016)</ref>: The models are trained on the groundtruth data distribution, but at test time are used to generate target words based on previous model predictions, which can be erroneous;</p><p>2. Word-level loss <ref type="bibr" target="#b17">(Shen et al. 2016)</ref>: Likelihood is defined at word-level, which might hardly correlate well with sequence-level evaluation metrics like BLEU.</p><p>3. Focusing more on fluency than adequacy ): Likelihood does not measure how well the complete source information is transformed to the target side, thus does not correlate well with translation adequacy. Adequacy metric is regularly employed to assess the translation quality in practice.</p><p>Some recent work partially alleviates one or two of the above problems with advanced training strategies. For example, the first two problems are tackled by sequence level training using the REINFORCE algorithm <ref type="bibr" target="#b15">(Ranzato et al. 2016;</ref><ref type="bibr" target="#b0">Bahdanau et al. 2017)</ref>, minimum risk training <ref type="bibr" target="#b17">(Shen et al. 2016)</ref>, beam search optimization <ref type="bibr" target="#b24">(Wiseman and Rush 2016)</ref> or adversarial learning <ref type="bibr" target="#b26">(Wu et al. 2017;</ref>. The last problem can be alleviated by introducing an auxiliary reconstruction-based training objective to measure translation adequacy .</p><p>In this work, we aim to fully solve all the three problems in a unified framework. Specifically, we model the translation as a stochastic policy in Reinforcement Learning (RL) and directly perform gradient policy update. The RL reward is estimated on a complete sequence produced by the NMT model, which is able to correlate well with a sequencelevel task-specific metric. To explicitly measure translation adequacy, we propose a novel metric called Coverage Difference Ratio (CDR) which is calculated by counting how many source words are under-translated via directly comparing generated translation with human translation. Benefiting from the sequence-level training of RL strategy and a more accurate reward designed specifically for translation, the proposed approach is able to alleviate all the aforementioned limitations of MLE-based training.</p><p>We conduct experiments on Chinese⇒English and German⇔English translation tasks, using both the RNNbased NMT model <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2015)</ref> and the recently proposed TRANSFORMER <ref type="bibr" target="#b22">(Vaswani et al. 2017)</ref>. The consistent improvements across language pairs and NMT architectures demonstrate the effectiveness and universality of the proposed approach. The proposed adequacy-oriented learning improves translation performance not only over a standard attention model, but also over a coverage-augmented attention model <ref type="bibr" target="#b20">(Tu et al. 2016</ref>) that alleviates the inadequate translation problem at the word-level. In addition, the proposed metric -CDR score, consistently outperforms the commonly-used word-level BLEU <ref type="bibr" target="#b13">(Papineni et al. 2002)</ref> and character-level CHRF3 <ref type="bibr" target="#b14">(Popović 2015)</ref> scores in both the reinforcement learning and adversarial learning frameworks, indicating the superiority and necessity of an adequacy-oriented metric in training effective NMT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Neural Machine Translation (NMT) is an end-to-end structure which could directly model the translation probability between a source sentence x = x 1 , x 2 , . . . , x J and a target sentence y = y 1 , y 2 , . . . , y I word by word:</p><formula xml:id="formula_0">P (y|x) = I i=1 P (y i |y &lt;i , x; θ)<label>(1)</label></formula><p>where y &lt;i is the partial translation before decoding step i and θ is parameters of the NMT. The probability of generating the i-th word P (y i |y &lt;i , x; θ) is calculated by</p><formula xml:id="formula_1">with e i,j = a(s i−1 , h j ) (3)</formula><p>where a(·) is an attention model that scores how well y i and h j (i.e., x j ) match. The encoder and decoder can be implemented as Recurrent Neural Network (RNN) (Bahdanau, Cho, and Bengio 2015), Convolutional Neural Network (CNN) <ref type="bibr" target="#b5">(Gehring et al. 2017</ref>), or Self-Attention Network (SAN) <ref type="bibr" target="#b22">(Vaswani et al. 2017)</ref>. The parameters of the NMT θ are trained to maximize the likelihood of training instances {[x n , y n ]} N n=1 :</p><formula xml:id="formula_2">L(θ) = arg max θ N n=1 log P (y n |x n ; θ)<label>(4)</label></formula><p>Although likelihood is a widely-used training objective for its simpleness and effectiveness, it has several aforementioned limitations including exposure bias <ref type="bibr" target="#b15">(Ranzato et al. 2016;</ref><ref type="bibr" target="#b24">Wiseman and Rush 2016)</ref>, word-level estimation <ref type="bibr" target="#b17">(Shen et al. 2016)</ref>, and focusing more on fluency than adequacy ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach Intuition</head><p>In this work, we try to solve the three problems mentioned above in a unified framework. Our objective is three-fold: (b) Examples of covered source words (i.e. shadow boxes). <ref type="figure">Figure 1</ref>: An example to illustrate coverage difference ratio.</p><p>1. We solve the exposure bias problem by modeling the translation as a stochastic policy in reinforcement learning (RL) and directly performing policy gradient update.</p><p>2. The RL reward is estimated on a complete sequence, which correlates well with either sequence-level BLEU or a more adequacy-oriented metric, as described below.</p><p>3. We design a sequence-level metric -Coverage Difference Ratio (CDR) -to explicitly measure translation adequacy which focuses on the commonly-cited weaknesses of NMT models: producing fluent yet inadequate translations. We expect that the model can benefit from linguistic insights that correlate well with human intuitions.</p><p>Coverage Difference Ratio (CDR) We measure translation adequacy by the number of under-translated words via comparing generated translation with human translation. We take an example to illustrate how to measure translation adequacy in terms of coverage difference ratio. <ref type="figure">Figure 1</ref>(a) shows one inadequate translation. Following <ref type="bibr" target="#b10">(Luong, Pham, and Manning 2015;</ref><ref type="bibr" target="#b20">Tu et al. 2016)</ref>, we extract only oneto-one alignments (hard alignments) by selecting the source word with the highest alignment for each target word from the word alignments produced by NMT models. 1 A source word is considered to be translated when it is covered by the hard alignments, as shown in <ref type="figure">Figure 1</ref>(b). Comparing source words covered by generated translation with those covered by human translation, we can find that the two sets are very different for inadequate translation. Specifically, the difference generally lies in the untranslated source words that cause inadequate translation problem, indicating that coverage difference ratio is a good way to measure the adequacy of generated translation. Formally, we calculate the CDR score of a given generated  <ref type="figure">Figure 2</ref>: Architecture of adequacy-oriented NMT. The newly added orientator O reads coverages of generated and human translations to generate a CDR score for each generated translation, which guides the discriminator D to differentiate good generated translations from bad ones. translationŷ by</p><formula xml:id="formula_3">CDR(ŷ|y, x) = 1 − |C ref \ C gen | |C ref |<label>(5)</label></formula><p>where C ref and C gen is the set of source words covered by human translation and generated translation, respectively. C ref \ C gen denotes the covered source words in C ref but not in C gen . We use C ref as the reference coverage to eliminate the effect of null-aligned source words which are not aligned to any target word. As seen, CDR(ŷ|y, x) is a number between 0 and 1, where 1 means "completely adequate translation" and 0 means "completely inadequate translation". Taking <ref type="figure">Figure 1</ref>(b) as an example, the CDR score is 1 − (7 − 4)/7 = 0.57.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>As shown in <ref type="figure">Figure 2</ref>, the proposed model consists of a generator, a discriminator, and an orientator.</p><p>Generator The generator G generates the translationŷ conditioned on the input sentence x. Because we need word alignments to calculate adequacy scores in terms of CDR, an attention-based NMT model is employed as the generator.</p><p>Orientator The orientator O reads the word alignments produced by NMT attention model when generating (or force decoding) the two translations and outputs an adequacy score for the generated translation in terms of the aforementioned CDR score. Then, the orientator is used to guide the discriminator to distinguish adequate translation from inadequate ones. Accordingly, adequate translations with higher CDR scores would contribute more to parameter tuning, as described in the following section.</p><p>Discriminator We employ a RNN-based discriminator to differentiate generated translation from human translation, given the input sentence. The discriminator reads the input sentence x and its translation (either y orŷ), and use two RNNs to summarize the two sentences individually. The concatenation of the two summarized representation vectors is fed into a fully-connected neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adequacy-Oriented Training</head><p>In order to train the system efficiently and effectively, we employ a periodical training strategy, which is commonly used in adversarial training <ref type="bibr" target="#b6">(Goodfellow et al. 2014;</ref><ref type="bibr" target="#b26">Wu et al. 2017</ref>). Specifically, we optimize two networks with two objective functions and periodically freeze the parameters of each network during training.</p><p>Train Generator and Freeze Discriminator Following <ref type="bibr" target="#b26">Wu et al. (2017)</ref>, we use the REINFORCE algorithm <ref type="bibr" target="#b23">(Williams 1992)</ref> to back-propagate the error signals from D to G, given the discretely generatedŷ from G. The objective of the generator is to maximize the expected reward:</p><formula xml:id="formula_4">L = E (x,ŷ)∈G θ [D(x,ŷ)]<label>(6)</label></formula><p>whose gradient is</p><formula xml:id="formula_5">θ = E (x,ŷ)∈G θ [D(x,ŷ) θ log G θ (ŷ|x)]<label>(7)</label></formula><p>The gradient is approximated by a sample from G using the REINFORCE algorithm <ref type="bibr" target="#b23">(Williams 1992)</ref>:</p><formula xml:id="formula_6">θ ≈ˆ θ = D(x,ŷ) θ log G θ (ŷ|x)<label>(8)</label></formula><p>where θ log G(ŷ|x) is the standard NMT gradient which is calculated by the maximum likelihood estimation. Therefore, the final update function for the generator is:</p><formula xml:id="formula_7">θ = θ − ηˆ θ<label>(9)</label></formula><p>where the η is the learning rate. Based on the update function, when the D(x,ŷ) is large (i.e., ideally, the generated translationŷ has a high adequacy score) , the larger reward the NMT model will get, and thus parameters are updated more based on the adequate training instance (x,ŷ).</p><p>Train Discriminator Oriented by Adequacy and Freeze Generator Ideally, a good translationŷ should be assigned a high adequacy score D(x,ŷ) and thus contribute more to updating the generator. Therefore, we expect the discriminator to not only differentiate generated translations from human translations but also distinguish bad generated translations from good ones. Therefore, a new objective of discriminator is to assign a precise score for each generated translation, which is consistent with their adequacy score:</p><formula xml:id="formula_8">min D |CDR(ŷ|x, y) − D(x,ŷ)| 2<label>(10)</label></formula><p>where CDR(ŷ|x, y) is the coverage difference ratio ofŷ. As seen, a well trained discriminator would assign a distinct score to each generated translation, which can better measure its adequacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>This work is related to modeling translation as policy gradient and adequacy modeling. For the former, we take minimum risk training, reinforcement learning and adversarial learning as representative strategies.</p><p>Minimum Risk Training In response to the exposure bias and word-level loss problems of MLE training, <ref type="bibr" target="#b17">Shen et al. (2016)</ref> minimize the expected loss in terms of evaluation metrics on the training data. Our simplified model is analogous to their MRT model, if we directly use CDR as the reward to update parameters:</p><formula xml:id="formula_9">θ = CDR(ŷ|x, y)) θ log G θ (ŷ|x)<label>(11)</label></formula><p>The simplified model differs in that (1) we use adequacyoriented metric (i.e., CDR) while they use sequence-level BLEU, and <ref type="formula">(2)</ref> we only need to sample one candidate to calculate reinforcement reward while they generate multiple samples to calculate the expected risk. In addition, our discriminator gives a smoother and dynamically-updated objective compared with directly using the adequacy-oriented metric, because the latter is highly sensitive to the slight coverage difference <ref type="bibr" target="#b9">(Koehn and Knowles 2017)</ref>.</p><p>Reinforcement Learning Recent work shows that maximum likelihood training could be sub-optimal due to the different conditions between training and test modes <ref type="bibr" target="#b15">Ranzato et al. 2016)</ref>. In order to address the exposure bias and the loss which does not operate at the sequence level, <ref type="bibr" target="#b15">Ranzato et al. (2016)</ref> employ the REINFORCE algorithm <ref type="bibr" target="#b23">(Williams 1992)</ref> to decide whether or not tokens from a sampled prediction could contribute to a high taskspecific score (e.g., BLEU). <ref type="bibr" target="#b0">Bahdanau et al. (2017)</ref> use the actor-critic method from reinforcement learning to directly optimize a task-specific score.</p><p>Adversarial Learning Recently, adversarial learning <ref type="bibr" target="#b6">(Goodfellow et al. 2014</ref>) has been successfully applied to neural machine translation <ref type="bibr" target="#b26">(Wu et al. 2017;</ref><ref type="bibr" target="#b3">Cheng et al. 2018</ref>). In the adversarial framework, NMT models generally serve as the generator which defines the policy to generate the target sentence y given the source sentence x. A discriminator tries to distinguish the translation resultŷ = G(x) from the human-generated one y, given the source sentence x.</p><p>If we remove the orientator O, our model is roll-backed to the adversarial NMT, and the training objective of the discriminator D is rewritten as</p><formula xml:id="formula_10">max D {log D(x, y) + log(1 − D(x,ŷ))}<label>(12)</label></formula><p>The goal of the discriminator is try to maximize the likelihood of human translation D(x, y) to 1 and minimize that of generated translation D(x,ŷ) to 0. As seen, the discriminator uses a binary classification by uniformly treating all generated translations as negative examples (i.e., labeling "0") and all human translations as positive examples (i.e., labeling "1"), regardless of the quality of the generated translations. However, intuitively, high-quality translations and low-quality translations should be treated differently by the discriminator, otherwise, inaccurate reward signals would be propagated back to the generator. In our proposed architecture, this problem can be alleviated by replacing the simple binary outputs with the more informative adequacy-oriented metric CDR, which is calculated by directly comparing generated and human translations.</p><p>Adequacy Modeling Inadequate translation problem is a commonly-cited weakness of NMT models <ref type="bibr" target="#b20">(Tu et al. 2016)</ref>. A number of recent efforts have explored ways to alleviate this problem. For example, <ref type="bibr" target="#b20">Tu et al. (2016)</ref> and <ref type="bibr" target="#b12">Mi et al. (2016)</ref>  Our approach is complementary to theirs since they model the adequacy learning at the word-level inside the generator (i.e., NMT models), while we model it at the sequencelevel outside the generator. We take the representative coverage mechanism <ref type="bibr" target="#b20">(Tu et al. 2016)</ref> as another stronger baseline model for its simplicity and efficiency, and experimental results show that our model can further improve performance.</p><p>In the context of adequacy-oriented training,  introduce an auxiliary objective to measure the adequacy of translation candidates, which is calculated by reconstructing generated translations back to the original inputs. Benefiting from the flexible framework of reinforcement training, we are able to directly compare generated translations with human translations and define a more straightforward metric, i.e., CDR to measure adequacy of generated sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Setup</head><p>We conduct experiments on the widely-used Chinese (Zh) ⇒English (En) and German (De) ⇔English (En) translation tasks. For Zh⇒En translation, the training corpus contains 1.25M sentence pairs extracted from LDC corpora. NIST 2002 (MT02) dataset is the validation set and the test data consists of NIST 2003 (MT03), NIST2004 (MT04), NIST 2005 (MT05) and NIST 2006(MT06). For De⇔En translation, to compare with the results reported by previous work <ref type="bibr" target="#b17">(Shen et al. 2016;</ref><ref type="bibr" target="#b0">Bahdanau et al. 2017;</ref><ref type="bibr" target="#b26">Wu et al. 2017;</ref><ref type="bibr" target="#b22">Vaswani et al. 2017)</ref>, we use both the IWSLT 2014 and WMT 2014 data. The former contains 153K sentence pairs and the latter consists of 4.56M sentence pairs. The 4-gram NIST BLEU score <ref type="bibr" target="#b13">(Papineni et al. 2002)</ref> is used as the evaluation metric and sign-test <ref type="bibr" target="#b4">(Collins, Koehn, and Kučerová 2005)</ref> is employed to test statistical significance.</p><p>For training all neural models, we set the vocabulary size to 30K for Zh⇒En, for IWSLT 2014 De⇒En, we follow the preprocessing procedure as used in <ref type="bibr" target="#b15">Ranzato et al. (2016)</ref>   <ref type="table">Table 1</ref>: Evaluation of translation performance on Zh⇒En translation. "D" denotes discriminator and "O" denotes orientator. "MRT" indicates minimum risk training <ref type="bibr" target="#b17">(Shen et al. 2016)</ref>, and "D CNN " indicates adversarial training with a CNN-based discriminator <ref type="bibr" target="#b26">(Wu et al. 2017)</ref>. "# Para." denotes the number of parameters, and "Speed" denotes the training speed (words/second). " †" and " ‡" indicate statistically significant difference (p &lt; 0.05 and p &lt; 0.01 respectively) from the corresponding baseline.</p><p>and for WMT 2014 En⇒De, preprocessing method described in <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref> is borrowed. We pre-train the discriminator on translation samples produced by the pre-trained generator. After that, the discriminator and the generator are trained together, and the generator is updated by the REINFORCE algorithm mentioned above. We also follow the training tips mentioned in <ref type="bibr" target="#b17">Shen et al. (2016)</ref> and <ref type="bibr" target="#b26">Wu et al. (2017)</ref>. The hyper-parameter α which could control the sharpness of the generator distribution in our system is 1e-4, which could also be regarded as a baseline to reduce the variance of the REINFORCE algorithm. We also randomly choose 50% minibatches trained with our objective function and the other with the MLE principle. In MRT training strategy <ref type="bibr" target="#b17">(Shen et al. 2016)</ref>, the sample size is 25, the hyper-parameter α is 5e-3 and the loss function is negative smoothed sentence-level BLEU. We validate our models on two representative model architectures, namely RNNSEARCH and TRANSFORMER. For the RNNSEARCH model, mini-batch size is 80, the word-embedding dimension is 620, and the hidden layer size is 1000. We use a neural coverage model for RNNSEARCH-COVERAGE and the dimensionality of coverage vector is 100. The baseline models are trained for 15 epochs, which are used as the initial generator in the proposed framework. For the TRANSFORMER model, we implement our proposed approach on top of an open source toolkit THMUT . Configurations in <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref> are used to train the baseline models. <ref type="table">Table 1</ref> lists the results of various translation models on Zh⇒En corpus. As seen, all advanced systems significantly outperform the baseline system (i.e., RNNSEARCH), although there are still considerable differences among different variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chinese-English Translation Task</head><p>Architectures of Discriminator (Rows 3-4) We evaluate two architectures for the discriminator. The CNN-based discriminator is composed of two convolution layers with 3 × 3 window, two max-pooling layers with 2 × 2 window and one softmax layer. The feature map size is 10 and the feedforward hidden size is 20. The RNN-based discriminator consists of two two-layer RNN encoders with 32 LSTM units and a fully-connected neural network with 32 units. We find that the RNN discriminator achieves similar performance with its CNN counterpart (37.59 vs. 37.54), while has a faster training speed (1.2K vs. 1.0K words/second). The main reason is that the CNN-based discriminator requires high computation and space cost to utilize multiple layers with convolution and pooling from a large input matrix.</p><p>Adequacy Metrics for Orientator (Rows 5-7) As aforementioned, the CDR score can be directly used as a reward to update the parameters, which is in analogy to the MRT <ref type="bibr" target="#b17">(Shen et al. 2016</ref>) except that we use 1-best sample while they use n-best samples. For comparison, we also used the word-level BLEU score (Row 5) and character-level CHRF3 score <ref type="bibr" target="#b14">(Popović 2015</ref>) (Row 6) as the rewards.</p><p>As seen, this strategy consistently improves translation performance, without introducing any new parameters. The extra computation cost is mainly from generating translation sentence and force decoding the human translation with the NMT model. We find that CDR not only outperforms its 1best counterpart "O BLEU " and "O CHRF3 ", but also surpasses "MRT BLEU " using 25 samples. We attribute this to the fact that CDR can better estimate the adequacy of the translation, which is the key problem of NMT models, and go beyond the the simple low-level n-gram matching measured by BLEU and CHRF3.</p><p>Combining Them Together (Row 8) By combining advantages of both reinforcement learning and adequacyoriented objective, our model achieves the best performance, which is 1.66 BLEU points better than the baseline "RNNSEARCH", up to 0.98 BLEU points better than using single component and significantly improve the performance of "MRT BLEU " model. One more observation can be made. "+D+O" outperforms its "+O" counterpart (e.g., 8 vs. 7), which confirms our claim that the discriminator gives a   smoother and dynamically-updated score than directly using the calculated one.</p><p>Working with Coverage Model (Rows 11-12) <ref type="bibr" target="#b20">Tu et al. (2016)</ref> propose a coverage model to indicate whether a source word is translated or not, which alleviates the inadequate translation problem of NMT models. We argue that our model is complementary to theirs, because we model the adequacy learning outside the generator by using an additional adequacy-oriented discriminator, while they model it inside the generator. Experimental results validate our hypothesis: the proposed approach further improves performance by 0.58 BLEU points over the coverage-augmented model RNNSEARCH-COVERAGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English-German Translation Tasks</head><p>To compare with previous work of applying reinforcement learning for NMT <ref type="bibr" target="#b15">(Ranzato et al. 2016;</ref><ref type="bibr" target="#b0">Bahdanau et al. 2017;</ref><ref type="bibr" target="#b24">Wiseman and Rush 2016;</ref><ref type="bibr" target="#b26">Wu et al. 2017)</ref>, we first conduct experiments on IWSLT 2014 De⇒En translation task. As listed in <ref type="table" target="#tab_4">Table 2</ref>, we reproduce the results of adversarial training reported by <ref type="bibr">Wu et al. (2017) (27.24 vs. 26.98</ref>  <ref type="table">Table 4</ref>: Adequacy scores on randomly selected 100 sentences on Zh⇒En task, which are measured by CDR and human evaluation ("MAN").</p><p>our models.</p><p>We also evaluate our model on the recently proposed TRANSFORMER model <ref type="bibr" target="#b22">(Vaswani et al. 2017</ref>) on WMT 2014 En⇒De corpus. As shown in <ref type="table" target="#tab_5">Table 3</ref>, our models significantly improve performances in all cases. Combining with previous results, our model consistently improve translation performance across various language pairs and NMT architectures, demonstrating the effectiveness and universality of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>To better understand our models, we conduct extensive analyses on the Zh⇒En translation task.</p><p>Adequacy Evaluation To better evaluate the adequacy, we randomly choose 100 sentences from the test set, and ask two human evaluators to judge the quality of generated translations. Five scales have been set up, i.e., {1, 2, 3, 4, 5}, where "1" means that it is irrelevant between the source sentence and the translation sentence, and "5" means that from semantic and syntactic aspect, the translation sentence and the source sentence is completely equivalent. <ref type="table">Table 4</ref> lists the results of human evaluation and the proposed CDR score. First, our models consistently improve the translation adequacy under both human evaluation and the CDR score, indicating that the proposed approaches indeed alleviate the inadequate translation problem. Second, the relative improvement on CDR is consistent with that on subjective evaluation. The Pearson Correlation Coefficient between CDR and manual evaluation score is 0.64, indicat- ing that the proposed CDR is a reasonable metric to measure translation adequacy.</p><p>Length Analysis We group sentences of similar lengths and compute both the BLEU score and CDR score for each group, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The four length spans contain 1386, 2284, 1285, and 498 sentences, respectively. From the perspective of the BLEU score, the proposed model (i.e., "+D+O") outperforms RNNSEARCH in all length segments. In contrast, using discriminator only (i.e., "+D") outperforms RNNSEARCH in most cases, except long sentences (i.e., &gt; 45). One possible reason is that it is difficult for the discriminator to differentiate generated translations from human translations for long source sentences, thus the generator cannot learn well about these instances due to the "mistaken" rewards from the discriminator. Accordingly, using the CDR score (i.e., "+O") alleviates this problem by providing a sequence-level score, which better estimates the adequacy of the translations. The final model combines the advantages of both a smoother and dynamically-updated objective from the discriminator ("+D"), and a more accurate objective specifically designed for the translation task from the orientator ("+O").</p><p>The CDR scores for all models degrade when the length of source sentence increases. This is mainly due to that inadequate translation problem is more serious on longer sentences for NMT models <ref type="bibr" target="#b20">(Tu et al. 2016)</ref>. The adversarial model (i.e., "+D") improves CDR scores while the improvement degrades faster with the increase of sentence length. However, our proposed approach consistently improves CDR performance in all length segments. <ref type="bibr" target="#b9">Koehn and Knowles (2017)</ref> point out that the attention model does not always corre- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the Discriminator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OURS</head><p>Chairman of AOL time warner announced tonight that he will resign in May, he said this is the best thing for the company.</p><p>(CDR: 0.88; BLEU: 58.82) <ref type="figure">Figure 4</ref>: Example translations on Zh⇒En test set.</p><p>spond to word alignment and may considerably diverge. Accordingly, the attention matrix-based CDR score may not always correctly reflect the adequacy of generation sentences. However, our discriminator is able to give a smoother and dynamically-updated objective, and thus could provide more accurate adequacy scores of generation sentences. From the above quantitative and qualitative results, the discriminator indeed leads to better performance (i.e., "+D+O" vs. "+O").</p><p>Case Study To better understand the advantage of our proposed model, we show a translation case in <ref type="figure">Figure 4</ref>. Specially, we provide a Zh⇒En example with two translation results from the RNNSearch and Adequacy-NMT models respectively, as well as the corresponding CDR and BLEU scores. We emphasize on their different parts with bold fonts which lead to different translation quality. As seen, the latter part of the source sentence is not translated by the RNNSearch model while our proposed model correct this mistake. Accordingly, our model improves both CDR and BLEU scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we propose a novel learning approach for RLbased NMT models, which integrates into the policy gradient with an adequacy-oriented reward designed specifically for translation. The proposed approach combines the advantages of both sequence-level training of reinforcement learning, as well as a more accurately estimated reward by considering the translation adequacy in terms of coverage difference ratio (CDR). Experimental results on different language pairs show that our proposed approach not only significantly outperforms standard NMT models, but also further improves performance over those using the policy gradient and the adequacy-oriented reward individually. In addition, the proposed approach is also complementary to the coverage models <ref type="bibr" target="#b20">(Tu et al. 2016)</ref>, because the two models aim to alleviate the inadequate translation problem from two different perspectives (i.e., sequence-level vs. word-level). Future directions include validating our approach on other architectures such as CNN-based NMT models <ref type="bibr" target="#b5">(Gehring et al. 2017</ref>) and improved TRANSFORMER models <ref type="bibr" target="#b16">(Shaw, Uszkoreit, and Vaswani 2018;</ref><ref type="bibr" target="#b18">Shen et al. 2018)</ref>, as well as combining with other advanced techniques in reinforcement learning and adversarial learning <ref type="bibr" target="#b0">(Bahdanau et al. 2017;</ref><ref type="bibr" target="#b27">Yu et al. 2017;</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>employ coverage vector as a lexical-level indicator to indicate whether a source word is translated or not. Zheng et al. (2018) and Meng et al. (2018) move one step further and directly model translated and untranslated source contents by operating on the attention context vector. He et al. (2017) use a prediction network to estimate the future cost of translating the uncovered source words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>BLEU and CDR scores of the translations with respect to the input lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input ⼆⼗六岁 男司机 被困 车内 ，由 到场 的 消防员 救出 。REFThe 26-year-old driver was trapped inside the car after the impact and was rescued by the firemen.NMT The 26-year-old driver was trapped inside the vehicle.</figDesc><table><row><cell></cell><cell>REF</cell></row><row><cell></cell><cell>NMT</cell></row><row><cell cols="2">(a) Example of human (REF) and generated (NMT) translations.</cell></row><row><cell cols="2">REF</cell></row><row><cell cols="2">NMT</cell></row><row><cell>Input</cell><cell></cell></row><row><cell>REF</cell><cell>The 26-year-old driver was trapped inside the car after the impact and was rescued by the firemen.</cell></row><row><cell cols="2">NMT The 26-year-old driver was trapped inside the vehicle.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparing with previous works of applying reinforcement learning for NMT on IWSLT 2014 De⇒En translation task. " †" and " ‡" indicate statistically significant difference (p &lt; 0.05 and p &lt; 0.01 respectively) from the RNNSEARCH model.</figDesc><table><row><cell>Model</cell><cell>BLEU</cell></row><row><cell>GNMT + RL (Wu et al. 2016)</cell><cell>26.30</cell></row><row><cell>ConvS2S (Gehring et al. 2017)</cell><cell>26.43</cell></row><row><cell cols="2">Transformer (Base) (Vaswani et al. 2017) 27.3</cell></row><row><cell>Transformer (Big) (Vaswani et al. 2017)</cell><cell>28.4</cell></row><row><cell>TRANSFORMER-BASE</cell><cell>27.30</cell></row><row><cell>+ OCDR</cell><cell>27.80</cell></row><row><cell>+ DRNN + OCDR</cell><cell>28.01  †</cell></row><row><cell>TRANSFORMER-BIG</cell><cell>28.35</cell></row><row><cell>+ OCDR</cell><cell>28.63</cell></row><row><cell>+ DRNN + OCDR</cell><cell>28.99  †</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: BLEU scores on WMT 2014 En⇒De testset us-</cell></row><row><cell>ing the state-of-the-art TRANSFORMER model. ' †" indicates</cell></row><row><cell>statistically significant difference (p &lt; 0.05) from the corre-</cell></row><row><cell>sponding TRANSFORMER baseline model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>said that his move is the best interest of the company.</head><label></label><figDesc>Input 美国 线上 时代华纳 董事长 凯斯 今晚 宣布 他 将 在 五 ⽉ 辞职 , 他 说 , 此 举 「 对 公司 ⽽ ⾔ 是 最 好 的 」 。 REF Steve case chairman of AOL time warner announced tonight that he will resign in May, he BASE The chairman of the board of time warner of the united states announced tonight that he will resign in May. (CDR: 0.56; BLEU: 36.18)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For generated translations, we directly use the attention probability distributions from decoding procedure; for human translations, we obtain attention distributions by force decoding the target sentences with the same NMT model.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An actorcritic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards robust neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kučerová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decoding with value networks for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural machine translation with key-value memory-augmented attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Coverage embedding models for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">chrf: character n-gram f-score for automatic mt evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-Attention with Relative Position Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DiSAN: directional self-attention network for RNN/CNN-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving neural machine translation with conditional sequence generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Adversarial neural machine translation</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SeqGAN: sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling past and future for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xinyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06415</idno>
	</analytic>
	<monogr>
		<title level="m">THUMT: An open source toolkit for neural machine translation</title>
		<imprint>
			<publisher>TACL</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
