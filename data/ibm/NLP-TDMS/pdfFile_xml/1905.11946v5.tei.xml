<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
						</author>
						<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.</p><p>To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https: //github.com/tensorflow/tpu/tree/ master/models/official/efficientnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scaling up ConvNets is widely used to achieve better accuracy. For example, ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref> can be scaled up from ResNet-18 to ResNet-200 by using more layers; Recently, GPipe    EfficientNet-B7 Top1 Acc. #Params ResNet-152 <ref type="bibr" target="#b8">(He et al., 2016)</ref> 77.8% 60M EfficientNet-B1 79.1% 7.8M ResNeXt-101 <ref type="bibr" target="#b44">(Xie et al., 2017)</ref> 80.9% 84M EfficientNet-B3 81.6% 12M SENet  82.7% 146M NASNet-A  82.7% 89M EfficientNet-B4 82.9% 19M GPipe  84.3% 556M EfficientNet-B7 84.3% 66M † Not plotted <ref type="figure">Figure 1</ref>. Model Size vs. ImageNet Accuracy. All numbers are for single-crop, single-model. Our EfficientNets significantly outperform other ConvNets. In particular, EfficientNet-B7 achieves new state-of-the-art 84.3% top-1 accuracy but being 8.4x smaller and 6.1x faster than GPipe. EfficientNet-B1 is 7.6x smaller and 5.7x faster than ResNet-152. Details are in <ref type="table" target="#tab_4">Table 2 and 4.</ref> time larger. However, the process of scaling up ConvNets has never been well understood and there are currently many ways to do it. The most common way is to scale up Con-vNets by their depth <ref type="bibr" target="#b8">(He et al., 2016)</ref> or width <ref type="bibr" target="#b46">(Zagoruyko &amp; Komodakis, 2016)</ref>. Another less common, but increasingly popular, method is to scale up models by image resolution . In previous work, it is common to scale only one of the three dimensions -depth, width, and image size. Though it is possible to scale two or three dimensions arbitrarily, arbitrary scaling requires tedious manual tuning and still often yields sub-optimal accuracy and efficiency.</p><p>In this paper, we want to study and rethink the process of scaling up ConvNets. In particular, we investigate the central question: is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency? Our empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with constant ratio. Based on this observation, we propose a simple yet effective compound scaling method. Unlike conventional practice that arbitrary scales these factors, our method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2 N times more computational resources, then we can simply increase the network depth by α N , width by β N , and image size by γ N , where α, β, γ are constant coefficients determined by a small grid search on the original small model. <ref type="figure">Figure 2</ref> illustrates the difference between our scaling method and conventional methods.</p><p>Intuitively, the compound scaling method makes sense because if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. In fact, previous theoretical <ref type="bibr" target="#b33">(Raghu et al., 2017;</ref><ref type="bibr" target="#b26">Lu et al., 2018)</ref> and empirical results <ref type="bibr" target="#b46">(Zagoruyko &amp; Komodakis, 2016)</ref> both show that there exists certain relationship between network width and depth, but to our best knowledge, we are the first to empirically quantify the relationship among all three dimensions of network width, depth, and resolution.</p><p>We demonstrate that our scaling method work well on existing MobileNets <ref type="bibr" target="#b12">(Howard et al., 2017;</ref><ref type="bibr" target="#b37">Sandler et al., 2018)</ref> and ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref>. Notably, the effectiveness of model scaling heavily depends on the baseline network; to go even further, we use neural architecture search <ref type="bibr" target="#b50">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b43">Tan et al., 2019)</ref> to develop a new baseline network, and scale it up to obtain a family of models, called EfficientNets. <ref type="figure">Figure 1</ref> summarizes the ImageNet performance, where our EfficientNets significantly outperform other ConvNets. In particular, our EfficientNet-B7 surpasses the best existing GPipe accuracy , but using 8.4x fewer parameters and running 6.1x faster on inference. Compared to the widely used ResNet-50 <ref type="bibr" target="#b8">(He et al., 2016)</ref>, our EfficientNet-B4 improves the top-1 accuracy from 76.3% to 83.0% (+6.7%) with similar FLOPS. Besides ImageNet, EfficientNets also transfer well and achieve state-of-the-art accuracy on 5 out of 8 widely used datasets, while reducing parameters by up to 21x than existing ConvNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>ConvNet Accuracy: Since AlexNet <ref type="bibr" target="#b22">(Krizhevsky et al., 2012)</ref> won the 2012 ImageNet competition, ConvNets have become increasingly more accurate by going bigger: while the 2014 ImageNet winner GoogleNet  achieves 74.8% top-1 accuracy with about 6.8M parameters, the 2017 ImageNet winner SENet  achieves 82.7% top-1 accuracy with 145M parameters. Recently, GPipe  further pushes the state-of-the-art ImageNet top-1 validation accuracy to 84.3% using 557M parameters: it is so big that it can only be trained with a specialized pipeline parallelism library by partitioning the network and spreading each part to a different accelerator. While these models are mainly designed for ImageNet, recent studies have shown better ImageNet models also perform better across a variety of transfer learning datasets <ref type="bibr" target="#b19">(Kornblith et al., 2019)</ref>, and other computer vision tasks such as object detection <ref type="bibr" target="#b8">(He et al., 2016;</ref><ref type="bibr" target="#b43">Tan et al., 2019)</ref>. Although higher accuracy is critical for many applications, we have already hit the hardware memory limit, and thus further accuracy gain needs better efficiency.</p><p>ConvNet Efficiency: Deep ConvNets are often overparameterized. Model compression <ref type="bibr" target="#b10">He et al., 2018;</ref><ref type="bibr" target="#b45">Yang et al., 2018</ref>) is a common way to reduce model size by trading accuracy for efficiency. As mobile phones become ubiquitous, it is also common to handcraft efficient mobile-size ConvNets, such as SqueezeNets <ref type="bibr" target="#b17">(Iandola et al., 2016;</ref><ref type="bibr" target="#b6">Gholami et al., 2018)</ref>, MobileNets <ref type="bibr" target="#b12">(Howard et al., 2017;</ref><ref type="bibr" target="#b37">Sandler et al., 2018)</ref>, and ShuffleNets <ref type="bibr" target="#b27">Ma et al., 2018)</ref>. Recently, neural architecture search becomes increasingly popular in designing efficient mobile-size ConvNets <ref type="bibr" target="#b43">(Tan et al., 2019;</ref><ref type="bibr" target="#b2">Cai et al., 2019)</ref>, and achieves even better efficiency than hand-crafted mobile ConvNets by extensively tuning the network width, depth, convolution kernel types and sizes. However, it is unclear how to apply these techniques for larger models that have much larger design space and much more expensive tuning cost. In this paper, we aim to study model efficiency for super large ConvNets that surpass state-of-the-art accuracy. To achieve this goal, we resort to model scaling.</p><p>Model Scaling: There are many ways to scale a Con-vNet for different resource constraints: ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref> can be scaled down (e.g., ResNet-18) or up (e.g., ResNet-200) by adjusting network depth (#layers), while WideResNet <ref type="bibr" target="#b46">(Zagoruyko &amp; Komodakis, 2016)</ref> and Mo-bileNets <ref type="bibr" target="#b12">(Howard et al., 2017)</ref> can be scaled by network width (#channels). It is also well-recognized that bigger input image size will help accuracy with the overhead of more FLOPS. Although prior studies <ref type="bibr" target="#b33">(Raghu et al., 2017;</ref><ref type="bibr" target="#b23">Lin &amp; Jegelka, 2018;</ref><ref type="bibr" target="#b38">Sharir &amp; Shashua, 2018;</ref><ref type="bibr" target="#b26">Lu et al., 2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Compound Model Scaling</head><p>In this section, we will formulate the scaling problem, study different approaches, and propose our new scaling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>A ConvNet Layer i can be defined as a function:</p><formula xml:id="formula_0">Y i = F i (X i ), where F i is the operator, Y i is output tensor, X i is input tensor, with tensor shape H i , W i , C i 1 ,</formula><p>where H i and W i are spatial dimension and C i is the channel dimension. A ConvNet N can be represented by a list of composed layers:</p><formula xml:id="formula_1">N = F k ... F 2 F 1 (X 1 ) = j=1...k F j (X 1 ).</formula><p>In practice, ConvNet layers are often partitioned into multiple stages and all layers in each stage share the same architecture: for example, ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref> has five stages, and all layers in each stage has the same convolutional type except the first layer performs down-sampling. Therefore, we can define a ConvNet as:</p><formula xml:id="formula_2">N = i=1...s F Li i X Hi,Wi,Ci<label>(1)</label></formula><p>where F Li i denotes layer F i is repeated L i times in stage i, H i , W i , C i denotes the shape of input tensor X of layer 1 For the sake of simplicity, we omit batch dimension.</p><p>i. <ref type="figure">Figure 2</ref>(a) illustrate a representative ConvNet, where the spatial dimension is gradually shrunk but the channel dimension is expanded over layers, for example, from initial input shape 224, 224, 3 to final output shape 7, 7, 512 .</p><p>Unlike regular ConvNet designs that mostly focus on finding the best layer architecture F i , model scaling tries to expand the network length (L i ), width (C i ), and/or resolution (H i , W i ) without changing F i predefined in the baseline network. By fixing F i , model scaling simplifies the design problem for new resource constraints, but it still remains a large design space to explore different L i , C i , H i , W i for each layer. In order to further reduce the design space, we restrict that all layers must be scaled uniformly with constant ratio. Our target is to maximize the model accuracy for any given resource constraints, which can be formulated as an optimization problem:</p><formula xml:id="formula_3">max d,w,r Accuracy N (d, w, r) s.t. N (d, w, r) = i=1...sF d·Li i X r·Ĥi,r·Ŵi,w·Ĉi Memory(N ) ≤ target memory FLOPS(N ) ≤ target flops (2)</formula><p>where w, d, r are coefficients for scaling network width, depth, and resolution;F i ,L i ,Ĥ i ,Ŵ i ,Ĉ i are predefined parameters in baseline network (see <ref type="table" target="#tab_3">Table 1</ref> as an example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scaling Dimensions</head><p>The main difficulty of problem 2 is that the optimal d, w, r depend on each other and the values change under different resource constraints. Due to this difficulty, conventional methods mostly scale ConvNets in one of these dimensions:</p><formula xml:id="formula_4">Depth (d d d):</formula><p>Scaling network depth is the most common way used by many ConvNets <ref type="bibr" target="#b8">(He et al., 2016;</ref><ref type="bibr" target="#b15">Huang et al., 2017;</ref><ref type="bibr" target="#b40">Szegedy et al., 2015;</ref>. The intuition is that deeper ConvNet can capture richer and more complex features, and generalize well on new tasks. However, deeper networks are also more difficult to train due to the vanishing gradient problem <ref type="bibr" target="#b46">(Zagoruyko &amp; Komodakis, 2016)</ref>. Although several techniques, such as skip connections <ref type="bibr" target="#b8">(He et al., 2016)</ref> and batch normalization <ref type="bibr" target="#b18">(Ioffe &amp; Szegedy, 2015)</ref>, alleviate the training problem, the accuracy gain of very deep network diminishes: for example, ResNet-1000 has similar accuracy as ResNet-101 even though it has much more layers. <ref type="figure" target="#fig_1">Figure  3</ref> (middle) shows our empirical study on scaling a baseline model with different depth coefficient d, further suggesting the diminishing accuracy return for very deep ConvNets.</p><p>Width (w w w): Scaling network width is commonly used for small size models <ref type="bibr" target="#b12">(Howard et al., 2017;</ref><ref type="bibr" target="#b37">Sandler et al., 2018</ref>; Bigger networks with larger width, depth, or resolution tend to achieve higher accuracy, but the accuracy gain quickly saturate after reaching 80%, demonstrating the limitation of single dimension scaling. Baseline network is described in <ref type="table" target="#tab_3">Table 1.</ref> Tan et al., 2019) 2 . As discussed in <ref type="bibr" target="#b46">(Zagoruyko &amp; Komodakis, 2016)</ref>, wider networks tend to be able to capture more fine-grained features and are easier to train. However, extremely wide but shallow networks tend to have difficulties in capturing higher level features. Our empirical results in <ref type="figure" target="#fig_1">Figure 3</ref> (left) show that the accuracy quickly saturates when networks become much wider with larger w.</p><p>Resolution (r r r): With higher resolution input images, Con-vNets can potentially capture more fine-grained patterns. Starting from 224x224 in early ConvNets, modern Con-vNets tend to use 299x299 <ref type="bibr" target="#b41">(Szegedy et al., 2016)</ref> or 331x331  for better accuracy. Recently, GPipe  achieves state-of-the-art ImageNet accuracy with 480x480 resolution. Higher resolutions, such as 600x600, are also widely used in object detection ConvNets <ref type="bibr" target="#b24">Lin et al., 2017)</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> (right) shows the results of scaling network resolutions, where indeed higher resolutions improve accuracy, but the accuracy gain diminishes for very high resolutions (r = 1.0 denotes resolution 224x224 and r = 2.5 denotes resolution 560x560).</p><p>The above analyses lead us to the first observation:</p><p>Observation 1 -Scaling up any dimension of network width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Compound Scaling</head><p>We empirically observe that different scaling dimensions are not independent. Intuitively, for higher resolution images, we should increase network depth, such that the larger receptive fields can help capture similar features that include more pixels in bigger images. Correspondingly, we should also increase network width when resolution is higher, in 2 In some literature, scaling number of channels is called "depth multiplier", which means the same as our width coefficient w.  order to capture more fine-grained patterns with more pixels in high resolution images. These intuitions suggest that we need to coordinate and balance different scaling dimensions rather than conventional single-dimension scaling.</p><p>To validate our intuitions, we compare width scaling under different network depths and resolutions, as shown in <ref type="figure" target="#fig_3">Figure  4</ref>. If we only scale network width w without changing depth (d=1.0) and resolution (r=1.0), the accuracy saturates quickly. With deeper (d=2.0) and higher resolution (r=2.0), width scaling achieves much better accuracy under the same FLOPS cost. These results lead us to the second observation:</p><p>Observation 2 -In order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling.</p><p>In fact, a few prior work <ref type="bibr" target="#b35">Real et al., 2019)</ref> have already tried to arbitrarily balance network width and depth, but they all require tedious manual tuning.</p><p>In this paper, we propose a new compound scaling method, which use a compound coefficient φ to uniformly scales network width, depth, and resolution in a principled way:</p><formula xml:id="formula_5">depth: d = α φ width: w = β φ resolution: r = γ φ s.t. α · β 2 · γ 2 ≈ 2 α ≥ 1, β ≥ 1, γ ≥ 1<label>(3)</label></formula><p>where α, β, γ are constants that can be determined by a small grid search. Intuitively, φ is a user-specified coefficient that controls how many more resources are available for model scaling, while α, β, γ specify how to assign these extra resources to network width, depth, and resolution respectively. Notably, the FLOPS of a regular convolution op is proportional to d, w 2 , r 2 , i.e., doubling network depth will double FLOPS, but doubling network width or resolution will increase FLOPS by four times. Since convolution ops usually dominate the computation cost in ConvNets, scaling a ConvNet with equation 3 will approximately increase total FLOPS by α · β 2 · γ 2 φ . In this paper, we constraint α · β 2 · γ 2 ≈ 2 such that for any new φ, the total FLOPS will approximately 3 increase by 2 φ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EfficientNet Architecture</head><p>Since model scaling does not change layer operatorsF i in baseline network, having a good baseline network is also critical. We will evaluate our scaling method using existing ConvNets, but in order to better demonstrate the effectiveness of our scaling method, we have also developed a new mobile-size baseline, called EfficientNet.</p><p>Inspired by <ref type="bibr" target="#b43">(Tan et al., 2019)</ref>, we develop our baseline network by leveraging a multi-objective neural architecture search that optimizes both accuracy and FLOPS. Specifically, we use the same search space as <ref type="bibr" target="#b43">(Tan et al., 2019)</ref>, and use ACC(m)×[F LOP S(m)/T ] w as the optimization goal, where ACC(m) and F LOP S(m) denote the accuracy and FLOPS of model m, T is the target FLOPS and w=-0.07 is a hyperparameter for controlling the trade-off between accuracy and FLOPS. Unlike <ref type="bibr" target="#b43">(Tan et al., 2019;</ref><ref type="bibr" target="#b2">Cai et al., 2019)</ref>, here we optimize FLOPS rather than latency since we are not targeting any specific hardware device. Our search produces an efficient network, which we name EfficientNet-B0. Since we use the same search space as <ref type="bibr" target="#b43">(Tan et al., 2019)</ref>, the architecture is similar to Mnas-3 FLOPS may differ from theoretical value due to rounding.  <ref type="bibr" target="#b43">Tan et al., 2019)</ref>, to which we also add squeeze-and-excitation optimization .</p><p>Starting from the baseline EfficientNet-B0, we apply our compound scaling method to scale it up with two steps:</p><p>• STEP 1: we first fix φ = 1, assuming twice more resources available, and do a small grid search of α, β, γ based on Equation 2 and 3. In particular, we find the best values for EfficientNet-B0 are α = 1.2, β = 1.1, γ = 1.15, under constraint of α · β 2 · γ 2 ≈ 2.</p><p>• STEP 2: we then fix α, β, γ as constants and scale up baseline network with different φ using Equation 3, to obtain EfficientNet-B1 to B7 (Details in <ref type="table" target="#tab_4">Table 2</ref>).</p><p>Notably, it is possible to achieve even better performance by searching for α, β, γ directly around a large model, but the search cost becomes prohibitively more expensive on larger models. Our method solves this issue by only doing search once on the small baseline network (step 1), and then use the same scaling coefficients for all other models (step 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we will first evaluate our scaling method on existing ConvNets and the new proposed EfficientNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Scaling Up MobileNets and ResNets</head><p>As a proof of concept, we first apply our scaling method to the widely-used MobileNets <ref type="bibr" target="#b12">(Howard et al., 2017;</ref><ref type="bibr" target="#b37">Sandler et al., 2018)</ref> and ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref>. <ref type="table" target="#tab_5">Table 3</ref> shows the ImageNet results of scaling them in different ways. Compared to other single-dimension scaling methods, our compound scaling method improves the accuracy on all these models, suggesting the effectiveness of our proposed scaling method for general existing ConvNets.   84.3% 97.0% 557M 8.4x --</p><p>We omit ensemble and multi-crop models , or models pretrained on 3.5B Instagram images <ref type="bibr" target="#b28">(Mahajan et al., 2018)</ref>.   <ref type="bibr" target="#b44">(Xie et al., 2017)</ref> 77.8% 11B EfficientNet-B1 79.1% 0.7B ResNeXt-101 <ref type="bibr" target="#b44">(Xie et al., 2017)</ref> 80.9% 32B EfficientNet-B3 81.6% 1.8B SENet  82.7% 42B NASNet-A  80.7% 24B EfficientNet-B4 82.9% 4.2B AmeobaNet-C <ref type="bibr" target="#b4">(Cubuk et al., 2019)</ref> 83.5% 41B EfficientNet-B5 83.6% 9.9B <ref type="figure">Figure 5</ref>. FLOPS vs. ImageNet Accuracy -Similar to <ref type="figure">Figure 1</ref> except it compares FLOPS rather than model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ImageNet Results for EfficientNet</head><p>We train our EfficientNet models on ImageNet using similar settings as <ref type="bibr" target="#b43">(Tan et al., 2019)</ref>: RMSProp optimizer with decay 0.9 and momentum 0.9; batch norm momentum 0.99; Geo-Mean (4.7x) (9.6x) † GPipe  trains giant models with specialized pipeline parallelism library. ‡ DAT denotes domain adaptive transfer learning . Here we only compare ImageNet-based transfer learning results. Transfer accuracy and #params for NASNet , Inception-v4 <ref type="bibr" target="#b42">(Szegedy et al., 2017)</ref>, <ref type="bibr">ResNet-152 (He et al., 2016)</ref> are from <ref type="bibr" target="#b19">(Kornblith et al., 2019)</ref>. weight decay 1e-5; initial learning rate 0.256 that decays by 0.97 every 2.4 epochs. We also use SiLU (Swish-1) activation <ref type="bibr" target="#b34">(Ramachandran et al., 2018;</ref><ref type="bibr" target="#b5">Elfwing et al., 2018;</ref><ref type="bibr" target="#b11">Hendrycks &amp; Gimpel, 2016)</ref>, AutoAugment <ref type="bibr" target="#b4">(Cubuk et al., 2019)</ref>, and stochastic depth <ref type="bibr" target="#b14">(Huang et al., 2016)</ref> with survival probability 0.8. As commonly known that bigger models need more regularization, we linearly increase dropout <ref type="bibr" target="#b39">(Srivastava et al., 2014)</ref> ratio from 0.2 for EfficientNet-B0 to 0.5 for B7. We reserve 25K randomly picked images from the training set as a minival set, and perform early stopping on this minival; we then evaluate the earlystopped checkpoint on the original validation set to report the final validation accuracy. <ref type="table" target="#tab_4">Table 2</ref> shows the performance of all EfficientNet models that are scaled from the same baseline EfficientNet-B0. Our EfficientNet models generally use an order of magnitude fewer parameters and FLOPS than other ConvNets with similar accuracy. In particular, our EfficientNet-B7 achieves 84.3% top1 accuracy with 66M parameters and 37B FLOPS, being more accurate but 8.4x smaller than the previous best GPipe . These gains come from both better architectures, better scaling, and better training settings that are customized for EfficientNet. <ref type="figure">Figure 1</ref> and <ref type="figure">Figure 5</ref> illustrates the parameters-accuracy and FLOPS-accuracy curve for representative ConvNets, where our scaled EfficientNet models achieve better accuracy with much fewer parameters and FLOPS than other ConvNets. Notably, our EfficientNet models are not only small, but also computational cheaper. For example, our EfficientNet-B3 achieves higher accuracy than ResNeXt-101 <ref type="bibr" target="#b44">(Xie et al., 2017)</ref> using 18x fewer FLOPS.</p><p>To validate the latency, we have also measured the inference latency for a few representative CovNets on a real CPU as shown in <ref type="table" target="#tab_6">Table 4</ref>, where we report average latency over 20 runs. Our EfficientNet-B1 runs 5.7x faster than the widely used ResNet-152, while EfficientNet-B7 runs about 6.1x faster than GPipe   <ref type="figure">Figure 7</ref>. Class Activation Map (CAM) <ref type="bibr" target="#b49">(Zhou et al., 2016)</ref> for Models with different scaling methods-Our compound scaling method allows the scaled model (last column) to focus on more relevant regions with more object details. Model details are in <ref type="table" target="#tab_10">Table 7</ref>.  <ref type="bibr" target="#b21">(Krizhevsky &amp; Hinton, 2009)</ref> 50,000 10,000 100 Birdsnap <ref type="bibr" target="#b0">(Berg et al., 2014)</ref> 47,386 2,443 500 Stanford Cars <ref type="bibr" target="#b20">(Krause et al., 2013)</ref> 8,144 8,041 196 Flowers <ref type="bibr" target="#b31">(Nilsback &amp; Zisserman, 2008)</ref> 2,040 6,149 102 FGVC Aircraft <ref type="bibr" target="#b29">(Maji et al., 2013)</ref> 6,667 3,333 100 Oxford-IIIT Pets <ref type="bibr" target="#b32">(Parkhi et al., 2012)</ref> 3,680 3,369 37 Food-101 <ref type="bibr" target="#b1">(Bossard et al., 2014)</ref> 75,750 25,250 101</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Transfer Learning Results for EfficientNet</head><p>We have also evaluated our EfficientNet on a list of commonly used transfer learning datasets, as shown in <ref type="table" target="#tab_9">Table  6</ref>. We borrow the same training settings from <ref type="bibr" target="#b19">(Kornblith et al., 2019)</ref> and , which take ImageNet pretrained checkpoints and finetune on new datasets. <ref type="table" target="#tab_7">Table 5</ref> shows the transfer learning performance: (1) Compared to public available models, such as NASNet-A  and Inception-v4 <ref type="bibr" target="#b42">(Szegedy et al., 2017)</ref>, our Ef-ficientNet models achieve better accuracy with 4.7x average (up to 21x) parameter reduction.</p><p>(2) Compared to stateof-the-art models, including DAT ) that dynamically synthesizes training data and GPipe  that is trained with specialized pipeline parallelism, our EfficientNet models still surpass their accuracy in 5 out of 8 datasets, but using 9.6x fewer parameters <ref type="figure" target="#fig_4">Figure 6</ref> compares the accuracy-parameters curve for a variety of models. In general, our EfficientNets consistently achieve better accuracy with an order of magnitude fewer parameters than existing models, including ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref>, DenseNet <ref type="bibr" target="#b15">(Huang et al., 2017)</ref>, Inception <ref type="bibr" target="#b42">(Szegedy et al., 2017)</ref>, and NASNet .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>To disentangle the contribution of our proposed scaling method from the EfficientNet architecture, <ref type="figure" target="#fig_5">Figure 8</ref> compares the ImageNet performance of different scaling meth-  ods for the same EfficientNet-B0 baseline network. In general, all scaling methods improve accuracy with the cost of more FLOPS, but our compound scaling method can further improve accuracy, by up to 2.5%, than other singledimension scaling methods, suggesting the importance of our proposed compound scaling.</p><p>In order to further understand why our compound scaling method is better than others, <ref type="figure">Figure 7</ref> compares the class activation map <ref type="bibr" target="#b49">(Zhou et al., 2016)</ref> for a few representative models with different scaling methods. All these models are scaled from the same baseline, and their statistics are shown in <ref type="table" target="#tab_10">Table 7</ref>. Images are randomly picked from ImageNet validation set. As shown in the figure, the model with compound scaling tends to focus on more relevant regions with more object details, while other models are either lack of object details or unable to capture all objects in the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we systematically study ConvNet scaling and identify that carefully balancing network width, depth, and resolution is an important but missing piece, preventing us from better accuracy and efficiency. To address this issue, we propose a simple and highly effective compound scaling method, which enables us to easily scale up a baseline Con-vNet to any target resource constraints in a more principled way, while maintaining model efficiency. Powered by this compound scaling method, we demonstrate that a mobilesize EfficientNet model can be scaled up very effectively, surpassing state-of-the-art accuracy with an order of magnitude fewer parameters and FLOPS, on both ImageNet and five commonly used transfer learning datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Scaling Up a Baseline Model with Different Network Width (w), Depth (d), and Resolution (r) Coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Scaling Network Width for Different Baseline Networks. Each dot in a line denotes a model with different width coefficient (w). All baseline networks are from Table 1. The first baseline network (d=1.0, r=1.0) has 18 convolutional layers with resolution 224x224, while the last baseline (d=2.0, r=1.3) has 36 layers with resolution 299x299.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Model Parameters vs. Transfer Learning Accuracy -All models are pretrained on ImageNet and finetuned on new datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Scaling Up EfficientNet-B0 with Different Methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>achieved 84.3% Ima-geNet top-1 accuracy by scaling up a baseline model four 1 Google Research, Brain Team, Mountain View, CA. Correspondence to: Mingxing Tan &lt;tanmingxing@google.com&gt;. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>EfficientNet-B0 baseline network -Each row describes a stage i withLi layers, with input resolution Ĥ i,Ŵi and output channelsĈi. Notations are adopted from equation 2.</figDesc><table><row><cell>Stage</cell><cell>Operator</cell><cell cols="3">Resolution #Channels #Layers</cell></row><row><cell cols="5">iF iĤi ×Ŵ iĈiLi</cell></row><row><cell>1</cell><cell>Conv3x3</cell><cell>224 × 224</cell><cell>32</cell><cell>1</cell></row><row><cell>2</cell><cell>MBConv1, k3x3</cell><cell>112 × 112</cell><cell>16</cell><cell>1</cell></row><row><cell>3</cell><cell>MBConv6, k3x3</cell><cell>112 × 112</cell><cell>24</cell><cell>2</cell></row><row><cell>4</cell><cell>MBConv6, k5x5</cell><cell>56 × 56</cell><cell>40</cell><cell>2</cell></row><row><cell>5</cell><cell>MBConv6, k3x3</cell><cell>28 × 28</cell><cell>80</cell><cell>3</cell></row><row><cell>6</cell><cell>MBConv6, k5x5</cell><cell>14 × 14</cell><cell>112</cell><cell>3</cell></row><row><cell>7</cell><cell>MBConv6, k5x5</cell><cell>14 × 14</cell><cell>192</cell><cell>4</cell></row><row><cell>8</cell><cell>MBConv6, k3x3</cell><cell>7 × 7</cell><cell>320</cell><cell>1</cell></row><row><cell>9</cell><cell>Conv1x1 &amp; Pooling &amp; FC</cell><cell>7 × 7</cell><cell>1280</cell><cell>1</cell></row><row><cell cols="5">Net, except our EfficientNet-B0 is slightly bigger due to</cell></row><row><cell cols="5">the larger FLOPS target (our FLOPS target is 400M). Ta-</cell></row><row><cell cols="5">ble 1 shows the architecture of EfficientNet-B0. Its main</cell></row><row><cell cols="5">building block is mobile inverted bottleneck MBConv (San-</cell></row><row><cell cols="2">dler et al., 2018;</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>EfficientNet Performance Results on ImageNet<ref type="bibr" target="#b36">(Russakovsky et al., 2015)</ref>. All EfficientNet models are scaled from our baseline EfficientNet-B0 using different compound coefficient φ in Equation 3. ConvNets with similar top-1/top-5 accuracy are grouped together for efficiency comparison. Our scaled EfficientNet models consistently reduce parameters and FLOPS by an order of magnitude (up to 8.4x parameter reduction and up to 16x FLOPS reduction) than existing ConvNets. Acc. Top-5 Acc. #Params Ratio-to-EfficientNet #FLOPs Ratio-to-EfficientNet</figDesc><table><row><cell cols="2">Model Top-1 EfficientNet-B0 77.1%</cell><cell>93.3%</cell><cell>5.3M</cell><cell>1x</cell><cell>0.39B</cell><cell>1x</cell></row><row><cell>ResNet-50 (He et al., 2016)</cell><cell>76.0%</cell><cell>93.0%</cell><cell>26M</cell><cell>4.9x</cell><cell>4.1B</cell><cell>11x</cell></row><row><cell>DenseNet-169 (Huang et al., 2017)</cell><cell>76.2%</cell><cell>93.2%</cell><cell>14M</cell><cell>2.6x</cell><cell>3.5B</cell><cell>8.9x</cell></row><row><cell>EfficientNet-B1</cell><cell>79.1%</cell><cell>94.4%</cell><cell>7.8M</cell><cell>1x</cell><cell>0.70B</cell><cell>1x</cell></row><row><cell>ResNet-152 (He et al., 2016)</cell><cell>77.8%</cell><cell>93.8%</cell><cell>60M</cell><cell>7.6x</cell><cell>11B</cell><cell>16x</cell></row><row><cell>DenseNet-264 (Huang et al., 2017)</cell><cell>77.9%</cell><cell>93.9%</cell><cell>34M</cell><cell>4.3x</cell><cell>6.0B</cell><cell>8.6x</cell></row><row><cell>Inception-v3 (Szegedy et al., 2016)</cell><cell>78.8%</cell><cell>94.4%</cell><cell>24M</cell><cell>3.0x</cell><cell>5.7B</cell><cell>8.1x</cell></row><row><cell>Xception (Chollet, 2017)</cell><cell>79.0%</cell><cell>94.5%</cell><cell>23M</cell><cell>3.0x</cell><cell>8.4B</cell><cell>12x</cell></row><row><cell>EfficientNet-B2</cell><cell>80.1%</cell><cell>94.9%</cell><cell>9.2M</cell><cell>1x</cell><cell>1.0B</cell><cell>1x</cell></row><row><cell>Inception-v4 (Szegedy et al., 2017)</cell><cell>80.0%</cell><cell>95.0%</cell><cell>48M</cell><cell>5.2x</cell><cell>13B</cell><cell>13x</cell></row><row><cell>Inception-resnet-v2 (Szegedy et al., 2017)</cell><cell>80.1%</cell><cell>95.1%</cell><cell>56M</cell><cell>6.1x</cell><cell>13B</cell><cell>13x</cell></row><row><cell>EfficientNet-B3</cell><cell>81.6%</cell><cell>95.7%</cell><cell>12M</cell><cell>1x</cell><cell>1.8B</cell><cell>1x</cell></row><row><cell>ResNeXt-101 (Xie et al., 2017)</cell><cell>80.9%</cell><cell>95.6%</cell><cell>84M</cell><cell>7.0x</cell><cell>32B</cell><cell>18x</cell></row><row><cell>PolyNet (Zhang et al., 2017)</cell><cell>81.3%</cell><cell>95.8%</cell><cell>92M</cell><cell>7.7x</cell><cell>35B</cell><cell>19x</cell></row><row><cell>EfficientNet-B4</cell><cell>82.9%</cell><cell>96.4%</cell><cell>19M</cell><cell>1x</cell><cell>4.2B</cell><cell>1x</cell></row><row><cell>SENet (Hu et al., 2018)</cell><cell>82.7%</cell><cell>96.2%</cell><cell>146M</cell><cell>7.7x</cell><cell>42B</cell><cell>10x</cell></row><row><cell>NASNet-A (Zoph et al., 2018)</cell><cell>82.7%</cell><cell>96.2%</cell><cell>89M</cell><cell>4.7x</cell><cell>24B</cell><cell>5.7x</cell></row><row><cell>AmoebaNet-A (Real et al., 2019)</cell><cell>82.8%</cell><cell>96.1%</cell><cell>87M</cell><cell>4.6x</cell><cell>23B</cell><cell>5.5x</cell></row><row><cell>PNASNet (Liu et al., 2018)</cell><cell>82.9%</cell><cell>96.2%</cell><cell>86M</cell><cell>4.5x</cell><cell>23B</cell><cell>6.0x</cell></row><row><cell>EfficientNet-B5</cell><cell>83.6%</cell><cell>96.7%</cell><cell>30M</cell><cell>1x</cell><cell>9.9B</cell><cell>1x</cell></row><row><cell>AmoebaNet-C (Cubuk et al., 2019)</cell><cell>83.5%</cell><cell>96.5%</cell><cell>155M</cell><cell>5.2x</cell><cell>41B</cell><cell>4.1x</cell></row><row><cell>EfficientNet-B6</cell><cell>84.0%</cell><cell>96.8%</cell><cell>43M</cell><cell>1x</cell><cell>19B</cell><cell>1x</cell></row><row><cell>EfficientNet-B7</cell><cell>84.3%</cell><cell>97.0%</cell><cell>66M</cell><cell>1x</cell><cell>37B</cell><cell>1x</cell></row><row><cell>GPipe</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Scaling Up MobileNets and ResNet.</figDesc><table><row><cell>Model</cell><cell cols="2">FLOPS Top-1 Acc.</cell></row><row><cell>Baseline MobileNetV1 (Howard et al., 2017)</cell><cell>0.6B</cell><cell>70.6%</cell></row><row><cell>Scale MobileNetV1 by width (w=2)</cell><cell>2.2B</cell><cell>74.2%</cell></row><row><cell>Scale MobileNetV1 by resolution (r=2)</cell><cell>2.2B</cell><cell>72.7%</cell></row><row><cell>compound scale (d d d=1.4, w w w=1.2, r r r=1.3)</cell><cell>2.3B</cell><cell>75.6%</cell></row><row><cell>Baseline MobileNetV2 (Sandler et al., 2018)</cell><cell>0.3B</cell><cell>72.0%</cell></row><row><cell>Scale MobileNetV2 by depth (d=4)</cell><cell>1.2B</cell><cell>76.8%</cell></row><row><cell>Scale MobileNetV2 by width (w=2)</cell><cell>1.1B</cell><cell>76.4%</cell></row><row><cell>Scale MobileNetV2 by resolution (r=2)</cell><cell>1.2B</cell><cell>74.8%</cell></row><row><cell>MobileNetV2 compound scale</cell><cell>1.3B</cell><cell>77.4%</cell></row><row><cell>Baseline ResNet-50 (He et al., 2016)</cell><cell>4.1B</cell><cell>76.0%</cell></row><row><cell>Scale ResNet-50 by depth (d=4)</cell><cell>16.2B</cell><cell>78.1%</cell></row><row><cell>Scale ResNet-50 by width (w=2)</cell><cell>14.7B</cell><cell>77.7%</cell></row><row><cell>Scale ResNet-50 by resolution (r=2)</cell><cell>16.4B</cell><cell>77.5%</cell></row><row><cell>ResNet-50 compound scale</cell><cell>16.7B</cell><cell>78.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Inference Latency Comparison -Latency is measured with batch size 1 on a single core of Intel Xeon CPU E5-2690.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Imagenet Top-1 Accuracy (%)</cell><cell>76 78 80 84 82</cell><cell>B3 B0</cell><cell cols="4">NASNet-A EfficientNet-B6 AmeobaNet-A ResNet-152 Inception-ResNet-v2 Xception B5 ResNet-50 B4 DenseNet-201 ResNet-152</cell><cell>AmoebaNet-C SENet ResNeXt-101 Top1 Acc. FLOPS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>74</cell><cell cols="2">Inception-v2 NASNet-A</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-34</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20 FLOPS (Billions) 25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell></row><row><cell></cell><cell>Acc. @ Latency</cell><cell></cell><cell>Acc. @ Latency</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-152</cell><cell>77.8% @ 0.554s</cell><cell>GPipe</cell><cell>84.3% @ 19.0s</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">EfficientNet-B1 78.8% @ 0.098s EfficientNet-B7</cell><cell>84.4% @ 3.1s</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Speedup</cell><cell>5.7x</cell><cell>Speedup</cell><cell>6.1x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>EfficientNet Performance Results on Transfer Learning Datasets. Our scaled EfficientNet models achieve new state-of-theart accuracy for 5 out of 8 datasets, with 9.6x fewer parameters on average.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Comparison to best public-available results</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Comparison to best reported results</cell></row><row><cell></cell><cell>Model</cell><cell>Acc.</cell><cell>#Param</cell><cell>Our Model</cell><cell>Acc.</cell><cell>#Param(ratio)</cell><cell>Model</cell><cell>Acc.</cell><cell>#Param</cell><cell>Our Model</cell><cell>Acc.</cell><cell>#Param(ratio)</cell></row><row><cell>CIFAR-10</cell><cell>NASNet-A</cell><cell>98.0%</cell><cell>85M</cell><cell cols="2">EfficientNet-B0 98.1%</cell><cell>4M (21x)</cell><cell cols="2">† Gpipe 99.0%</cell><cell>556M</cell><cell cols="2">EfficientNet-B7 98.9%</cell><cell>64M (8.7x)</cell></row><row><cell>CIFAR-100</cell><cell>NASNet-A</cell><cell>87.5%</cell><cell>85M</cell><cell cols="2">EfficientNet-B0 88.1%</cell><cell>4M (21x)</cell><cell cols="2">Gpipe 91.3%</cell><cell>556M</cell><cell cols="2">EfficientNet-B7 91.7%</cell><cell>64M (8.7x)</cell></row><row><cell>Birdsnap</cell><cell cols="2">Inception-v4 81.8%</cell><cell>41M</cell><cell cols="2">EfficientNet-B5 82.0%</cell><cell>28M (1.5x)</cell><cell cols="2">GPipe 83.6%</cell><cell>556M</cell><cell cols="2">EfficientNet-B7 84.3%</cell><cell>64M (8.7x)</cell></row><row><cell>Stanford Cars</cell><cell cols="2">Inception-v4 93.4%</cell><cell>41M</cell><cell cols="2">EfficientNet-B3 93.6%</cell><cell>10M (4.1x)</cell><cell cols="2">‡ DAT 94.8%</cell><cell>-</cell><cell cols="2">EfficientNet-B7 94.7%</cell><cell>-</cell></row><row><cell>Flowers</cell><cell cols="2">Inception-v4 98.5%</cell><cell>41M</cell><cell cols="2">EfficientNet-B5 98.5%</cell><cell>28M (1.5x)</cell><cell>DAT</cell><cell>97.7%</cell><cell>-</cell><cell cols="2">EfficientNet-B7 98.8%</cell><cell>-</cell></row><row><cell>FGVC Aircraft</cell><cell cols="2">Inception-v4 90.9%</cell><cell>41M</cell><cell cols="2">EfficientNet-B3 90.7%</cell><cell>10M (4.1x)</cell><cell>DAT</cell><cell>92.9%</cell><cell>-</cell><cell cols="2">EfficientNet-B7 92.9%</cell><cell>-</cell></row><row><cell>Oxford-IIIT Pets</cell><cell cols="2">ResNet-152 94.5%</cell><cell>58M</cell><cell cols="2">EfficientNet-B4 94.8%</cell><cell>17M (5.6x)</cell><cell cols="2">GPipe 95.9%</cell><cell>556M</cell><cell cols="2">EfficientNet-B6 95.4%</cell><cell>41M (14x)</cell></row><row><cell>Food-101</cell><cell cols="2">Inception-v4 90.8%</cell><cell>41M</cell><cell cols="2">EfficientNet-B4 91.5%</cell><cell>17M (2.4x)</cell><cell cols="2">GPipe 93.0%</cell><cell>556M</cell><cell cols="2">EfficientNet-B7 93.0%</cell><cell>64M (8.7x)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>, suggesting our EfficientNets are indeed fast on real hardware.</figDesc><table><row><cell>original image</cell><cell>baseline model</cell><cell>deeper (d=4)</cell><cell>wider (w=2)</cell><cell>higher resolution (r=2) compound scaling</cell></row><row><cell>bakeshop</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>maze</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Transfer Learning Datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Train Size Test Size #Classes</cell></row><row><cell>CIFAR-10 (Krizhevsky &amp; Hinton, 2009)</cell><cell>50,000</cell><cell>10,000</cell><cell>10</cell></row><row><cell>CIFAR-100</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Scaled Models Used in Figure 7.</figDesc><table><row><cell>Model</cell><cell cols="2">FLOPS Top-1 Acc.</cell></row><row><cell>Baseline model (EfficientNet-B0)</cell><cell>0.4B</cell><cell>77.3%</cell></row><row><cell>Scale model by depth (d=4)</cell><cell>1.8B</cell><cell>79.0%</cell></row><row><cell>Scale model by width (w=2)</cell><cell>1.8B</cell><cell>78.9%</cell></row><row><cell>Scale model by resolution (r=2)</cell><cell>1.9B</cell><cell>79.1%</cell></row><row><cell>Compound Scale (d d d=1.4, w w w=1.2, r r r=1.3)</cell><cell>1.8B</cell><cell>81.1%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Since 2017, most research papers only report and compare ImageNet validation accuracy; this paper also follows this convention for better comparison. In addition, we have also verified the test accuracy by submitting our predictions on the 100k test set images to http://image-net.org; results are in <ref type="table">Table 8</ref>. As expected, the test accuracy is very close to the validation accuracy. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large-scale fine-grained visual categorization of birds. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Birdsnap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Food-101-mining discriminative components with random forests. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Proxylessnas</surname></persName>
		</author>
		<title level="m">Direct neural architecture search on target task and hardware. ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1610" to="02357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<title level="m">Learning augmentation policies from data. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Squeezenext</surname></persName>
		</author>
		<title level="m">Hardware-aware neural network design. ECV Workshop at CVPR&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mask r-cnn. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobilenets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Deep networks with stochastic depth. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpipe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07233</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Squeezenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Alexnet-level accuracy with 50x fewer parameters and &lt;0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do better imagenet models transfer better? CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Collecting a large-scale dataset of fine-grained cars. Second Workshop on Fine-Grained Visual Categorizatio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Resnet with one-neuron hidden layers is a universal approximator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="6172" to="6181" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive neural architecture search. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The expressive power of neural networks: A view from the width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00932</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Domain adaptive transfer learning with specialist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07056</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICVGIP</title>
		<imprint>
			<biblScope unit="page" from="722" to="729" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cats and dogs. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<title level="m">On the expressive power of deep neural networks. ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On the expressive power of overlapping architectures of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Going deeper with convolutions. CVPR</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2818" to="2826" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<title level="m">Platform-aware neural architecture search for mobile. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Netadapt</surname></persName>
		</author>
		<title level="m">Platform-aware neural network adaptation for mobile applications. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polynet</surname></persName>
		</author>
		<title level="m">A pursuit of structural diversity in very deep networks. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3900" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2921" to="2929" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
