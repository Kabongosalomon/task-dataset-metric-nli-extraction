<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SMPR: Single-Stage Multi-Person Pose Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huixin</forename><surname>Miao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixun</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risheng</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">SMPR: Single-Stage Multi-Person Pose Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing multi-person pose estimators can be roughly divided into two-stage approaches (top-down and bottom-up approaches) and one-stage approaches. The two-stage methods either suffer high computational redundancy for additional person detectors or group keypoints heuristically after predicting all the instancefree keypoints. The recently proposed single-stage methods do not rely on the above two extra stages but have lower performance than the latest bottom-up approaches. In this work, a novel single-stage multi-person pose regression, termed SMPR, is presented. It follows the paradigm of dense prediction and predicts instance-aware keypoints from every location. Besides feature aggregation, we propose better strategies to define positive pose hypotheses for training which all play an important role in dense pose estimation. The network also learns the scores of estimated poses. The pose scoring strategy further improves the pose estimation performance by prioritizing superior poses during non-maximum suppression (NMS). We show that our method not only outperforms existing single-stage methods and but also be competitive with the latest bottom-up methods, with 70.2 AP and 77.5 AP75 on the COCO test-dev pose benchmark. Code is available at https://github.com/cmdi-dlut/SMPR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Multi-person pose estimation from a single image aims to identify all the person instances and detect the keypoints of each person simultaneously. It is a challenging task in computer vision, and widely used in motion recognition <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b27">[28]</ref>, person Re-ID <ref type="bibr" target="#b12">[13]</ref>, and pedestrian tracking <ref type="bibr" target="#b32">[33]</ref>, etc.</p><p>Previous neural network methods are mostly twostage based, which are roughly divided into top-down and bottom-up approaches. The top-down methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b23">[24]</ref> employ object detectors to identify all the persons and then detect their keypoints individually. The bottom-up methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b20">[21]</ref> detect all instance-free keypoints first and then group them into person instances heuristically. On the one hand, these two-stage strategies achieve higher <ref type="figure">Fig. 1</ref>: Superior pose hypotheses identification is vital for dense pose regression. "baseline" and "baseline*" means initial pose regression with different positive poses selection strategies. "refinement" and "refinements*" means pose refinement after feature aggregation with different positive poses selection strategies. Only classification score is used in NMS for the above comparisons. "pose scoring" means that predicated pose scores are also utilized in NMS. The comparison are done on COCO minival. performance by dividing the challenge into smaller sub problems which are easier to be solved. On the other hand, they impose additional computational overheads for the computation is separated or not fully shared. They also introduce some essential obstacles. For example, the top-down methods' running time heavily depends on the number of persons in the image and there is no clear semantic connection between keypoints when grouping them into instances in the bottom-up methods.</p><p>Very recently, a single-stage paradigm is proposed <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref> to overcome the aforementioned limitations. These methods estimate all the instanceaware keypoints of an input image in a compact and efficiency manner. Single-stage pose estimation seems similar with single-stage object detection <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>. They all regress K 2D points from each feature location using one single feature vector of that location. However predicting K keypoints, K = 17 in COCO <ref type="bibr" target="#b15">[16]</ref> dataset, from a feature vector is harder than estimating 2 corners of a bounding box from the feature vector. Because key-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Overview of the proposed SMPR. SMPR adopts a FPN backbone. We only draw the afterwards pipeline of one scale of FPN feature maps for clear illustration. There are 5 heads for the feature maps from level 3 to level 7. Each head has two subnets: one pose regression and the other for location classification which labels the locations on the feature maps with "person" or "not person". The regression subnet has three branches aiming for initial pose estimation (location + offsets 1), final pose regression (location + offsets 1 + offsets 2) and scoring. In the head for feature level 3, a heatmap prediction subnet is included to assist training. points usually not lie in proximity of the location which needs larger receptive field and keypoints own much more geometric variations than box corners. Modules based on hierarchical keypoints representation <ref type="bibr" target="#b19">[20]</ref> or feature aggregation by deformable convolution (DCN) <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref> are designed to improve the initial keypoints regression by locating more informative features lying closer to the keypoints. It is evident that predicting the keypoints using multiple nearby feature vectors, for example, regressing the wrist keypoint with a feature vector around the wrist, is much easier than using a single feature vector far away from them. But they still perform less well than the latest bottom-up methods.</p><p>There are two problems in current single-state pose estimators. The first problem is that including false positive poses in regression losses, and the second is that superior pose may be suppressed in NMS using classification scores. <ref type="figure">Fig. 1</ref> show that positive pose hypothesis selection, feature aggregation and pose scoring are all important for dense pose regression. The two problems can actually be improved using one common perspective, i.e. identifying positive/superior poses. Following the perspective, we propose a single-stage multi-person pose regression network in an anchor-free way, called SMPR. It estimates all the instance-aware keypoints of an input image in a compact and efficiency manner, see <ref type="figure">Fig. 2</ref>. SMPR adopts feature pyramid networks (FPN) <ref type="bibr" target="#b13">[14]</ref> and has 5 heads for the feature maps from level 3 (downsampling ratio of 8) to level 7 (downsampling ratio of 128). Each head contains multiple branches for classification and pose regression, etc. We regress one initial pose for each feature location of current feature level and refine the pose after feature aggregation, and the two regressions are both supervised by positive poses. We adopt three positive pose identification strategies for the initial pose regression, the final pose regression and the NMS step. Considering that the initial pose regression from single feature location is not trusty, we select a pose as positive if its feature location is inside a shrunk minimal enclosing bounding box of the corresponding pose, which means that the used feature is very close to the box's center. When refining the initial regression, OKS (Object Keypoint Similarity) of the refined poses are adopted to select positives since the refined poses are far superior than the initial estimation. Finally, we also learn a OKS score for NMS in view that the classification score is not very relevant to the quality of a pose. Our SMPR, with the three superior pose identification strategies, outperforms other single-stage pose estimators. It also shows competitive performance with the latest bottom-up pose estimators <ref type="bibr" target="#b3">[4]</ref>. The contributions of this paper are summarized as follows:</p><p>• We propose two positive pose identification strategies during training which improve the performance of single-stage pose estimation. • We present a pose scoring module to address the problem of sorting pose hypotheses in NMS. It explores a new direction for improving the performance of pose estimators. • We propose an anchor-free single-stage network for multi-person pose estimation, which reduces the computational redundancy. It is end-to-end trainable and does not require additional pose anchors or handcrafted keypoints grouping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Top-down Methods. In the top-down methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>, person detectors is first employed to generate bounding boxes for each person instance in an image. Then, the regions in the bounding boxes are cropped and scaled to perform singleperson pose estimation. Mask RCNN <ref type="bibr" target="#b8">[9]</ref> proposes to utilize extracted features instead of the original image to improve efficiency. In order to focus on regressing difficult keypoints and improve accuracy further, CPN <ref type="bibr" target="#b2">[3]</ref> proposes a cascaded pyramid network. From the perspective of feature encoding, HRNet <ref type="bibr" target="#b23">[24]</ref> proposes a high-resolution network to maintain high-resolution feature representations, which achieves the state-of-theart performance. Top-down methods have better performance, but they cannot achieve computing sharing. Moreover, they are highly dependent on the performance of person detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-up Methods.</head><p>The bottom-up methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b3">[4]</ref> first detect all instance-free keypoints and then group them together. For grouping, Openpose <ref type="bibr" target="#b1">[2]</ref> proposes to utilize part affinity fields to establish connections between keypoints, and then uses the greedy algorithm to combine the corresponding keypoints with the highest scores. Associative Embedding <ref type="bibr" target="#b17">[18]</ref> proposes to group keypoints by generating tags for each keypoint, and those with the same tag belong to the same person. Based on this grouping strategy, HigherHRNet <ref type="bibr" target="#b3">[4]</ref> proposes to use the fusion of multi-resolution heatmaps to implement keypoints detection. Bottom-up methods can share calculations, so they are faster. But grouping is heuristic, which makes the model difficult to train. Their performance are usually lower than the top-down methods.</p><p>Single-Stage Methods.</p><p>The single-stage methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref> are proposed very recently to overcome the aforementioned difficulties. The structured pose representation is introduced, which represents the keypoints of each person instance as a root location with K offsets. Hence these methods predict instanceaware keypoints directly from estimated root locations <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b31">[32]</ref> or follow the dense predication strategy <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, i.e. predicate poses from each location of the feature maps. To regress a pose from one root location accurately, SPM <ref type="bibr" target="#b19">[20]</ref> adapts an hierarchical strategy. CenterNet <ref type="bibr" target="#b31">[32]</ref> estimates individual keypoints using standard bottom-up approach and assign the individual keypoints to their closest person instance indicated by a root location and K offsets. The dense pose estimators <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref> employ DCN <ref type="bibr" target="#b4">[5]</ref> to refine the initial estimation from each location. Point-set anchors <ref type="bibr" target="#b29">[30]</ref> provides 27 pose anchors per location as the initial pose estimation and DirectPose <ref type="bibr" target="#b25">[26]</ref> and our SMPR adopt anchor-free approach. The performance of DirectPose is poor even with a delicate feature aggregation mechanism. Although SPM and point-set anchors outperform many state-ofthe-art bottom-up methods, they are not competitive with HigherHRNet <ref type="bibr" target="#b3">[4]</ref>. Our SMPR outperforms previous onestage methods by a large margin (more than 7.2 points AP than DirectPose and 1.5 points AP than Point-set Anchors with smaller model size). In this section, we first introduce the instance-aware keypoints representation. SMPR is designed based on the representation and its network architecture are discussed next. Then we show how to predicate the initial poses, align features with keypoints' position and refine the initial poses using the aligned features. We observe that selection of positive poses is vital for pose regression. Hence two different positive selection strategies are detailed when introducing the initial pose predication and the final pose refinement. Finally, we illustrate the proposed pose scoring module (PSM) which solves the inconsistency between classification scores and quality of predicated poses. In each subsection, the relationship to and differences from existing single-stage pose estimators are also discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Instance-aware Pose Representation</head><p>Poses are conventionally represented as</p><formula xml:id="formula_0">P = {P 1 i , P 2 i , ..., P K i } N i=1 ,<label>(1)</label></formula><p>where N is the number of persons in the image, K is the number of keypoints, and P j i = (x j i , y j i ) denotes coordinates of the jth keypoint from person i. SMPR estimates poses in an image in a dense way, i.e. predicating one pose from each location. We need an instance-aware pose representation aims to unify the location information of person instance and keypoints, which can be defined as</p><formula xml:id="formula_1">P = {(x c i , y c i ), (∆x 1 i , ∆y 1 i ), (∆x 2 i , ∆y 2 i ), ..., (∆x K i , ∆y K i ), } N i=1 , (2) where (x c i , y c i )</formula><p>represents the coordinates of a location for predicating the initial keypoints of the ith person. Then the jth keypoint of the ith person can be expressed as (x j i , y j i ) = (x c i + ∆x j i , y c i + ∆y j i ). ∆x j i and ∆y j i represent the offsets. The structured pose representation (SPR) in <ref type="bibr" target="#b19">[20]</ref> is also offsets based. Note that there are two differences between SPR and our representation. In SPR, an auxiliary root joint (x r i , y r i ) is introduced to denote the person position which is learnt in their network, and multiple person may share the same root joint theoretically. In SMPR, the location (x c i , y c i ) is known and not shared. It is assigned to person i if it locates in the bounding box of that person. If it locates in the overlap of multiple bounding boxes we assign it to the person with the smallest bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Initial Pose Regression</head><p>The network architecture of SMPR is an extention of the anchor-free object detector RepPoints <ref type="bibr" target="#b30">[31]</ref> with new branches, adapted modules and specified training strategies for pose detection. We use ResNet <ref type="bibr" target="#b9">[10]</ref> or HRNet <ref type="bibr" target="#b23">[24]</ref> as the backbone network, and then utilize the feature pyramid network (FPN) <ref type="bibr" target="#b13">[14]</ref> to produce 5  <ref type="figure">Fig. 2</ref> and its detail architecture can be found in <ref type="figure" target="#fig_0">Fig. 3</ref>. Each head contains two subnets aiming at pose regression and location classification, respectively. Classification labels the locations on the feature maps with "person" or "not person". The regression subnet has three branches aiming for initial pose regression (location + offsets 1), final pose regression (location + offsets 1 + offsets 2) and OKS scoring. On each feature location, we estimate an initial pose using the single feature vector of the location. Then a feature aggregation module is designed to align the keypoints with more feature vectors located around each keypoint. We predicate the final poses, OKS score and classification score from the aligned features. The numbers of output channels of the two regression branches are both 2K (K represents the number of keypoints). A heatmap prediction subnet <ref type="bibr" target="#b27">[28]</ref> is included only in the head for feature maps from scale 3 to assist training.</p><p>We regress one initial pose for each feature location of the current feature level. The initial pose is expressed as K offsets, stored in offsets 1, plus the feature location. The regression is supervised by the ground truth poses. We observe that supervising regression from all the locations reduce the network's performance and only positive samples, i.e. superior poses, should be supervised. Superior poses can be measured by OKS. But the initial poses are far from precise since only one feature vector of current location is used and the location is distant from the predicated keypoints. Hence we prefer to identify positives by the relative position of current locations and the nearest pseudo bounding box, which is the minimal bounding box of ground truth keypoints. Specifically, if a location is inside a shrunk area of a pseudo box, we take the predication from it as a positive. The side length of the shrunk boxes for different feature levels are (8, 16, 32, 64, 128) * 1.5 respectively. Previous single-stage estimators <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref> actually all have a step for the initial keypoints predication. <ref type="bibr" target="#b19">[20]</ref> predicates partial keypoints closer to the root joint. <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref> and we adopt similar strategies. But <ref type="bibr" target="#b29">[30]</ref> employs 27 pose point-set anchors per location and we depend on anchorfree regression. <ref type="bibr" target="#b25">[26]</ref> also follows the anchor-free way but the initial predication is not supervised. We find that the supervised regression using positives determined by the shrunk pseudo boxes yields to higher performance than using positives determined by the pseudo boxes, see <ref type="figure" target="#fig_1">Fig.  4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Feature Aggregation</head><p>The initial predicated poses are inferior, see top row of <ref type="figure" target="#fig_2">Fig. 5</ref>. Because it is difficult to decode all the keypoints of a person through a single feature vector of current location, especially for the keypoints far away from the receptive field of the location. DCN is employed in <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref> to learn shape invariant features, i.e. extract more feature vectors closer to the keypoints. In principle, superior poses can be decoded from these features. We follow the simple strategy of <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b29">[30]</ref>. The initial keypoints field of 34 channels are converted to a offset field, offset d in <ref type="figure" target="#fig_0">Fig. 3</ref>, of 18 channels by 1x1 convolution. Feature aggregation is thus implemented by 3x3 DCN with the specified offset field. Then superior poses can be learnt from the same location using more aligned feature vectors as shown in bottom row of <ref type="figure" target="#fig_2">Fig. 5</ref>. We also utilize the operation to enhance the classification features simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Pose Refinement</head><p>We can regress superior poses based on the aligned features. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, the jth keypoint of person  i is expressed as (x j i , y j i ) = (x c i + ∆x j i +∆x j i , y c i + ∆y j i +∆y j i ), (3) where ∆ and∆ are stored in the predicated offsets 1 and offsets 2. Although the refined poses are superior, compared with the initial poses, there are still many inferior poses. As stated above, identifying positives is vital for supervised pose regression. We compute the OKS of all the refined poses and the poses with OKS higher than a threshold are selected as positives. The threshold is set to 0.5 empirically, see <ref type="figure" target="#fig_3">Fig. 6</ref>. The poses with lower OKS are filtered out. OKS is defined in <ref type="bibr" target="#b15">[16]</ref> as:</p><formula xml:id="formula_2">OKS = i e −d 2 i 2s 2 κ 2 i δ(v i &gt; 0) i [δ(v i &gt; 0)] ,<label>(4)</label></formula><p>where d i is the Euclidean distance between each groundtruth and predicted keypoint, v i is the visibility flag of the keypoint, s is the object scale and κ i is a per-keypoint constant that controls falloff. It is proved by experiments that the performance is improved by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Pose Scoring</head><p>After the final pose regression, the predictions from all feature levels are merged and NMS is employed as postprocessing. Usually, the classification score is used in NMS to select the best poses from the dense estimation. However, the pose quality, measured by OKS, is usually not well correlated with classification score, see <ref type="figure" target="#fig_4">Fig. 7</ref> and 8. Huang etc. <ref type="bibr" target="#b10">[11]</ref> also meet similar case in instance segmentation. The performance is improved by learning the quality of the predicted instance masks. Hence we design a pose scoring module (PSM) to learn the quality of the predicated poses. It is a branch of the pose regression subnet. The predicated pose scores are more correlated with OKS, see <ref type="figure" target="#fig_4">Fig. 7 and 8</ref>. In NMS, the product of the classification score and the pose score is used as the confidence of each final pose. Then superior poses are preserved. Experimental results show that the performance can be improved with PSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Loss Functions</head><p>The loss function of the entire network is as follows: <ref type="bibr" target="#b4">(5)</ref> where λ 1 = λ 5 = 1, λ 2 = 4, λ 3 = 0.05 and λ 4 = 0.1. L reg * means L1 loss for the regression branch and L P SM is the binary cross entropy (BCE) loss for PSM. Since the training samples on each feature map are mostly negative samples, there is a serious imbalance problem. L cls and L hm are both focal loss functions <ref type="bibr" target="#b14">[15]</ref> for the classification branch and the heatmap branch, respectively.</p><formula xml:id="formula_3">L = λ 1 L cls + λ 2 L hm + λ 3 L reginitial + λ 4 L regrefined + λ 5 L P SM ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation</head><p>We present experimental results on the large-scale benchmark COCO dataset <ref type="bibr" target="#b15">[16]</ref>. Following the common practice <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, we use trainval35k split (57k images) for training, minival split (5k images) for our ablation studies and test-dev split (20k images) to report our main results. The Average Precision (AP) based on OKS is used as metric. When testing, NMS <ref type="bibr" target="#b0">[1]</ref> with a threshold of 0.3 is employed.</p><p>Unless specified, ablation studies are conducted with ResNet-50 <ref type="bibr" target="#b9">[10]</ref> and FPN <ref type="bibr" target="#b13">[14]</ref>. The network is trained with the stochastic gradient descent (SGD) optimizer over 4 GPU with a mini-batch of 32 images. The initial learning rate is set to 0.02. Weight decay and momentum are set as 0.0001 and 0.9, respectively. Specifically, the model is trained for 30 epochs and the initial learning rate is divided by 10 at epochs 25 and 28. We initialize the backbone network with the weights pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref>, and initialize the newly added layers as <ref type="bibr" target="#b14">[15]</ref>. When training, the input image is resized to have a shorter side of 800 and a longer side less or equal to 1333, and then it is randomly horizontally flipped with probability being 0.5. Finally, it is randomly cropped into 800 x 800 patches.</p><p>B. Ablation Experiments 1) Baseline: First, we conduct the experiments through regressing the initial poses directly to implement multi-person pose estimation, where the samples in the pseudo bounding box are defined as positives. As shown in Tab. I, the performance is poor (43.3 AP). Because it is hard to regress K keypoints from a single feature vector, especially when its location is far away from the center of the person. Hence the performance is improved when only the samples inside the shrunk pseudo boxes   "baseline" and "baseline*" mean training initial pose regression using the samples inside the pseudo bounding box or the shrunk box. "+ refine" and "+ refine*" correspond to refine the initial poses after feature aggregation with all or superior pose hypotheses as positives. "+ PSM" means that predicated pose scores are also used in NMS. "+ heatmap" means using heatmap to assist training. '-FPN + PAFPN' means using PAFPN instead of FPN to enhance communication between features of different scales.</p><p>are taken as positives (from 43.3 AP to 49.5 AP), which is denoted as "baseline*".</p><p>2) Refinement: As shown in Tab. I, there is a gap between AP 50 and AP 75 of "baseline*", which means that the initial estimation is not accurate but acceptable when measured by a lower OKS. To overcome this issue, feature aggregation is utilized to collect more features close to the keypoints to refine the initial estimation. Then the performance is improved by a large margin (from 49.5 AP to 54.2 AP). But the model has higher AR and relatively low AP, 63.2 vs. 54.2, which means that it can detect person in the image, but it is still difficult to regress keypoints' precise locations. Therefore, we use a OKS threshold, set to 0.5 in this paper, to make the network focus on refine superior initial pose hypotheses.  57.0 AP.</p><p>3) PSM: As described before, pose quality is usually not well correlated with classification scores. Hence we design PSM to score the predicated poses. Using the product of the classification score and the predicted pose score as the confidence in NMS to improve the performance from 57.0 AP to 58.5 AP. We also conduct a upper limit experiment. Using the ground truth OKS as the confidence of the predicated poses in NMS yields to higher performance, 68.8 AP in Tab. II. It shows that the performance of multi-person pose estimators can be improved by a very large margin by designing a more powerful pose scoring module. 4) Additional Discussion: As shown in Tab. I, after using the heatmap with downsampling ratio being 8 (i.e. from scale 3) for auxiliary supervision during training <ref type="bibr" target="#b25">[26]</ref>, the performance is improved by 2.3 AP. Note that the heatmap predication is not used during inference. In order to boost information flow and enhance communication between features of different scales, we replaced FPN <ref type="bibr" target="#b13">[14]</ref> with PAFPN <ref type="bibr" target="#b16">[17]</ref> to obtain slightly better performance (from 60.8 AP to 61.6 AP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State-of-the-art Methods</head><p>We compare SMPR with other state-of-the-art multiperson pose estimators on test-dev split of COCO benchmark, see Tab. III. We increase the number of epochs from 30 to 100 and divide the initial learning rate by 10 at epochs 80 and 90. With multi-scale testing,  1) Comparison with Top-down Methods: SMPR outperforms the classic top-down method -Mask-RCNN (64.7 AP vs. 62.7 AP). Our model is still behind some latest top-down methods, since they employ an additional person detector to identify people and resolve the inconsistency of human scales. But if the detected boxes overlap, the calculation is redundant and slow. However, the inference time of SMPR is independent of the number of person in the input image.</p><p>2) Comparison with Bottom-up Methods: SMPR outperforms the state-of-the-art bottom-up methods, such as CMU-Pose, AE and PersonLab. It also achieves better performance than the latest bottom-up method High-erHRNet <ref type="bibr" target="#b3">[4]</ref>, 68.2 vs. 66.4 AP, using the same backbone HRNet-w32 in single-scale testing. Besides that, our performance with HRNet-w32 is also competitive with HigherHRNet with HRNet-w48 in multi-scale testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Comparison with Single-stage Methods:</head><p>SMPR performs far better than most of the single-stage methods, such as DirectPose <ref type="bibr" target="#b25">[26]</ref>, CenterNet <ref type="bibr" target="#b31">[32]</ref> and SPM <ref type="bibr" target="#b19">[20]</ref>. SMPR with the backbone HRNet-w32 even outperforms the latest single-stage pose estimator <ref type="bibr" target="#b29">[30]</ref> with the backbone HRNet-w48, 70.2 vs. 68.7 AP, in multi-scale testing. Note that SMPR is anchor free and Point-Set Anchors need 27 carefully chosen pose anchors per location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we present a single-stage multi-person regression network. The proposed network is anchor-free and end-to-end trainable. It estimates multiple instanceaware keypoints in constant inference time. Two positive identification strategies are proposed to improve the network's performance. We also learn to score the predicted poses to boost the performance further. Experiments demonstrate that the proposed method outperforms many bottom-up and top-down methods, and all the singlestage pose estimators. It achieves 70.2 AP and 89.7 AP50 on the COCO test-dev pose benchmark, which is even competitive with the latest bottom-up methods.</p><p>One interesting future direction would be to improve the pose scoring module. It would also be interesting to identify positives in a differentiable approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>The head architecture of SMPR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Pose hypotheses from locations inside the pseudo boxes (top row) or shrunk pseudo boxes (bottom row) are selected as positives during the initial pose regression, which leads to 43.3 or 49.5 AP on COCO minival. Locations of positives are represented by green or red dots in the top row or bottom row respectively. feature pyramid levels from scale 3 (downsampling ratio of 8) to scale 7 (downsampling ratio of 128). Thus there are 5 shared heads for the features of 5 scales. The ground truth poses are assigned to different feature levels according to their scales, and each head is responsible for predicating poses of corresponding scale. The head is illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of poses predicated from the same locations before and after feature aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Performance of pose refinement using positives selected by different OKS thresholds on COCO minival.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Examples of the inconsistency between classification scores and quality of predicated poses, i.e. OKS. The predicated OKS is more correlated with the poses' quality. The green and blue poses represents the ground truth and predicted poses respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Visualizations of predicated classification scores (left), predicated OKS (right) and their multiplication (middle) vs the ground truth OKS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>APAP 50 AP 75 AP M AP L AR</figDesc><table><row><cell cols="2">baseline</cell><cell></cell><cell>43.3</cell><cell>72.7</cell><cell>45.8</cell><cell>38.6</cell><cell>51.0</cell><cell>54.7</cell></row><row><cell cols="2">baseline*</cell><cell></cell><cell>49.5</cell><cell>78.3</cell><cell>53.9</cell><cell>44.3</cell><cell>57.3</cell><cell>59.2</cell></row><row><cell cols="2">+ refine</cell><cell></cell><cell>54.2</cell><cell>80.6</cell><cell>59.4</cell><cell>48.3</cell><cell>62.9</cell><cell>63.2</cell></row><row><cell cols="2">+ refine*</cell><cell></cell><cell>57.0</cell><cell>83.0</cell><cell>62.6</cell><cell>50.8</cell><cell>65.8</cell><cell>64.1</cell></row><row><cell cols="2">+ PSM</cell><cell></cell><cell>58.5</cell><cell>82.9</cell><cell>64.3</cell><cell>52.3</cell><cell>67.1</cell><cell>64.8</cell></row><row><cell cols="2">+ heatmap</cell><cell></cell><cell>60.8</cell><cell>84.5</cell><cell>67.3</cell><cell>54.4</cell><cell>69.9</cell><cell>67.0</cell></row><row><cell>-</cell><cell>FPN</cell><cell>+</cell><cell>61.6</cell><cell>85.0</cell><cell>67.1</cell><cell>55.6</cell><cell>70.2</cell><cell>67.7</cell></row><row><cell cols="2">PAFPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Ablation experiments on COCO minival.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>It further improves the performance from 54.2 AP to AP AP 50 AP 75 AP M AP L</figDesc><table><row><cell>+ PSM</cell><cell>58.5 82.9</cell><cell>64.3</cell><cell>52.3</cell><cell>67.1</cell></row><row><cell cols="2">Gt Scoring 68.8 87.9</cell><cell>75.7</cell><cell>64.5</cell><cell>75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>The upper limit experiment for PSM. "+ PSM" and "Gt Scoring" means using predicated OKS or ground truth OKS in NMS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>MethodBackbone AP AP 50 AP 75 AP M AP L</figDesc><table><row><cell></cell><cell cols="2">Top-down Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask-RCNN [9]</cell><cell>ResNet-50</cell><cell>62.7</cell><cell>87.0</cell><cell>68.4</cell><cell>57.4</cell><cell>71.1</cell></row><row><cell>HRNet [24]</cell><cell>HRNet-w48</cell><cell>75.5</cell><cell>92.5</cell><cell>83.3</cell><cell>71.9</cell><cell>81.5</cell></row><row><cell></cell><cell cols="2">Bottom-up Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CMU-Pose [2]</cell><cell>3CM-3PAF (102)</cell><cell>61.8</cell><cell>84.9</cell><cell>67.5</cell><cell>57.1</cell><cell>68.2</cell></row><row><cell>AE [18]</cell><cell>Hourglass-4 stacked*</cell><cell>63.0</cell><cell>85.7</cell><cell>68.9</cell><cell>58.0</cell><cell>70.4</cell></row><row><cell>PersonLab [21]</cell><cell>ResNet-152</cell><cell>66.5</cell><cell>88.0</cell><cell>72.6</cell><cell>62.4</cell><cell>72.3</cell></row><row><cell>HigherHRNet [4]</cell><cell>HRNet-w32</cell><cell>66.4</cell><cell>87.5</cell><cell>72.8</cell><cell>61.2</cell><cell>74.2</cell></row><row><cell>HigherHRNet [4]</cell><cell>HRNet-w48*</cell><cell>70.5</cell><cell>89.3</cell><cell>77.2</cell><cell>66.6</cell><cell>75.8</cell></row><row><cell></cell><cell cols="2">Single-stage Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Comparison with state-of-the-art methods on MS COCO test-dev dataset. "*": using multi-scale testing. SMPR achieve 67.1 AP and 70.2 AP with ResNet-50 and HRNet-w32, respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3 conv, 256 3 3 conv, 256 3 3 conv, 256 3 3 conv, 256 3 3 conv, 2K 1 conv, 18 3 3 dconv, 256 Offset_d Offset_2 3 3 conv, 2K 3 3 dconv, 256 3 3 conv, 1 3 3 conv, 256 3 3 conv, 256 3 3 conv, 256 3 3 conv, 256 3 3 conv, 1 3 3 conv, 128 3 3 conv, 128 3 3 conv, 17 Oks score Class score</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-nmsimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multiperson pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3582" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6409" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6951" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lcr-net++: Multiperson 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Articulated part-based model for joint object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Directpose: Direct end-to-end multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07451</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rgb-dbased human motion recognition with deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="118" to="139" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Point-set anchors for object detection, instance segmentation and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Apt: Accurate outdoor pedestrian tracking with smartphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2508" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
