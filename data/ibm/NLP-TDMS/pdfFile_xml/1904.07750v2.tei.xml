<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Co-Separating Sounds of Visual Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
							<email>rhgao@cs.utexas.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
							<email>grauman@cs.utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">UT Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Co-Separating Sounds of Visual Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel. Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of "true" mixed sounds. We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos. Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate videolevel audio tracks for each source training pair. Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training. We obtain state-of-the-art results on visuallyguided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.</p><p>where M V 1 and M V 2 are the ground-truth spectrogram ratio masks for the two videos, respectively. Namely,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-modal perception is important to capture the richness of real-world sensory data for objects, scenes, and events. The sounds made by objects, whether actively generated or incidentally emitted, offer valuable signals about their physical properties and spatial locations-the cymbals crash on stage, the bird tweets up in the tree, the truck revs down the block, the silverware clinks in the drawer.</p><p>Objects often generate sounds while coexisting or interacting with other surrounding objects. Thus, rather than observe them in isolation, we hear them intertwined with sounds coming from other sources. Likewise, a realistic video records the various objects with a single audio channel that mixes all their acoustic frequencies together. Automatically separating the sounds of each object in a video is of great practical interest, with applications including audio denoising, audio-visual video indexing, instrument equalization, audio event remixing, and dialog following.</p><p>Whereas traditional methods assume access to multiple cello violin consistently identifiable guitar guitar <ref type="figure">Figure 1</ref>: We propose a co-separation training objective to learn audio-source separation from unlabeled video containing multiple sound sources. Our approach learns to associate consistent sounds to similar-looking objects across pairs of training videos. Then, given a single novel video, it returns a separate sound track for each object. Image credit: <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>. microphones or carefully supervised clean audio samples <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b9">9]</ref>, recent methods tackle the audio(-visual) source separation problem using a "mix-and-separate" paradigm to train deep neural networks in a self-supervised manner <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b52">52]</ref>. Namely, such methods randomly mix audio/video clips, and the learning objective is to recover the original unmixed signals. For example, one can create "synthetic cocktail parties" that mix clean speech with other sounds <ref type="bibr" target="#b8">[8]</ref>, add pseudo "off-screen" human speakers to other real videos <ref type="bibr" target="#b31">[31]</ref>, or superimpose audio from clips of musical instruments <ref type="bibr" target="#b52">[52]</ref>.</p><p>There are two key limitations with this current training strategy. First, it implicitly assumes that the original real training videos are dominated by single-source clips containing one primary sound maker. However, gathering a large number of such clean "solo" recordings is impractical and will be difficult to scale beyond particular classes like human speakers and musical instruments. Second, it implicitly assumes that the sources in a recording are independent. However, it is precisely the correlations between real sound sources (objects) that make the source separation problem most challenging at test time. Such correlations can go uncaptured by the artificially mixed training clips.</p><p>Towards addressing these shortcomings, we introduce a new strategy for learning to separate audio sources. Our key insight is a novel co-separation training objective that learns from naturally occurring multi-source videos. <ref type="bibr" target="#b1">1</ref> During training, our co-separation network considers pairs of training videos and, rather than simply separate their artificially mixed soundtracks, it must also generate audio tracks that are consistently identifiable at the object level across all training samples. In particular, using noisy object detections from the unlabeled training video, we devise a loss requiring that within an individual training video, each separated audio track should be distinguishable as its proper object. For example, when two training instances both contain a guitar plus other instruments, there is pressure to make the separated guitar tracks consistently identifiable. See <ref type="figure">Fig. 1</ref>.</p><p>We call our idea "co-separation" as a loose analogy to image co-segmentation <ref type="bibr" target="#b38">[38]</ref>, whereby jointly segmenting two related images can be easier than segmenting them separately, since it allows disentangling a shared foreground object from differently cluttered backgrounds. Note, however, that our co-separation operates during training only; unlike co-segmentation, at test time our method performs separation on an individual video input.</p><p>Our method design offers the following advantages. First, co-separation allows training with "in the wild" sound mixes. It has the potential to benefit from the variability and richness of unlabeled multi-source video. Second, it enhances the supervision beyond "mix-and-separate". By enforcing separation within a single video at the object-level, our approach exposes the learner to natural correlations between sound sources. Finally, objects with similar appearance from different videos can partner with each other to separate their sounds jointly, thereby regularizing the learning process. In this way, our method is able to learn well from multi-source videos, and it can successfully separate an object sound in a test video even if the object has never been observed individually during training.</p><p>We experiment on three benchmark datasets and demonstrate the advantages discussed above. Our approach yields state-of-the-art results on separation and denoising. Most notably, it outperforms the prior methods and baselines by a large margin when learning from noisy AudioSet <ref type="bibr" target="#b14">[14]</ref> videos. Overall co-separation is a promising direction to learn audio-visual separation from multi-source videos. <ref type="bibr" target="#b1">1</ref> Throughout, we use "multi-source video" as shorthand for video containing multiple sounds in its single-channel audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Audio-Only Source Separation Audio source separation has a rich history in signal processing. While many methods assume audio captured by multiple microphones, some tackle the "blind" separation problem with singlechannel audio <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b9">9]</ref>, most recently with deep learning <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b44">44]</ref>. Mix-and-separate style training is now commonly used for audio-only source separation to create artificial training examples <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b50">50]</ref>. Our approach adapts the mix-and-separate idea. However, different from all of the above, we leverage visual object detection to guide sound source separation. Furthermore, as discussed above, our co-separation framework is more flexible in terms of training data and can generalize to multi-source videos.</p><p>Audio-Visual Source Separation Early methods for audio-visual source separation focus on mutual information <ref type="bibr" target="#b10">[10]</ref>, subspace analysis <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b34">34]</ref>, matrix factorization <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b39">39]</ref>, and correlated onsets <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b27">27]</ref>. Recent methods leverage deep learning for separating speech <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b11">11]</ref>, musical instruments <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b51">51]</ref>, and other objects <ref type="bibr" target="#b12">[12]</ref>. Similar to the audio-only methods, almost all use a "mixand-separate" training paradigm to perform video-level separation by artificially mixing training videos. In contrast, we perform source separation at the object level to explicitly model sounds coming from visual objects, and our model enforces separation within a video during training.</p><p>Most related to our work are the "sound of pixels" (SoP) <ref type="bibr" target="#b52">[52]</ref> and multi-instance learning (AV-MIML) <ref type="bibr" target="#b12">[12]</ref> approaches. AV-MIML <ref type="bibr" target="#b12">[12]</ref> also focuses on learning object sound models from unlabeled video, but its two-stage method relies on NMF to perform separation, which limits its performance and practicability. Furthermore, whereas AV-MIML simply uses image classification to obtain weak labels on video frames, our approach detects localized objects and our end-to-end network learns visual object representations in concert with the audio streams. SoP <ref type="bibr" target="#b52">[52]</ref> outputs a sound for each pixel, whereas we predict sounds for visual objects with the help of a pre-trained object detector. More importantly, SoP works best when clean solo videos are available to perform video-level "mix-andseparate" training. Our method instead disentangles mixed sounds of objects within an individual training video, allowing more flexible training with multi-source data (though unlike <ref type="bibr" target="#b52">[52]</ref> we do require an object detection step).</p><p>Localizing Sounds in Video Frames Localization entails identifying the pixels where the sound of a video comes from, but not separating the audio <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b45">45]</ref>. Different from all these methods, our goal is to separate the sounds of multiple objects from a single-channel signal. We localize potential sound sources via object detection, and use the localized object regions to guide the separation learning process.</p><p>Generating Sounds from Video Sound generation methods synthesize a sound track from a visual input <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b6">6]</ref>. Given both visual input and monaural audio, recent methods generate spatial (binaural or ambisonic) audio <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b30">30]</ref>. Unlike any of the above, our work aims to separate an existing real audio track, not synthesize plausible new sounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our approach leverages localized object detection to visually guide audio source separation. We first formalize our object-level audio-visual source separation task (Sec. 3.1). Then we introduce our framework for learning object sound models from unlabeled video and our CO-SEPARATION deep network architecture (Sec. 3.2). Finally, we present our training criteria and inference procedures (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Given an unlabeled video clip V with accompanying audio x(t), we denote the set of N objects detected in the video frames as V = {O 1 , . . . , O N }. We treat each object as a potential sound source, and x(t) = ∑ N n=1 s n (t) is the observed single-channel linear mixture of these sources, where s n (t) are time-discrete signals responsible for each object. Our goal of object-level audio-visual source separation is to separate the sound s n (t) for each object O n from x(t).</p><p>Following <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b8">8]</ref>, we start with the commonly adopted "mix-and-separate" idea to selfsupervise source separation. Given two training videos V 1 and V 2 with corresponding audios x 1 (t) and x 2 (t), we use a pre-trained object detector to find objects in both videos. Then, we mix the audios of the two videos and obtain the mixed signal x m (t) = x 1 (t) + x 2 (t). The mixed audio x m (t) is transformed into a magnitude spectrogram X M ∈ R F×N + consisting of F frequency bins and N short-time Fourier transform (STFT) <ref type="bibr" target="#b15">[15]</ref> frames, which encodes the change of a signal's frequency and phase content over time.</p><p>Our learning objective is to separate the sound each object makes from x m (t) conditioned on the localized object regions. For example, <ref type="figure" target="#fig_4">Fig. 3</ref> illustrates a scenario of mixing two videos V 1 and V 2 with two objects O 1 , O 2 detected in V 1 and one object O 3 detected in V 2 . The goal is to separate s 1 (t), s 2 (t), and s 3 (t) for objects O 1 , O 2 , and O 3 from the mixture signal x m (t), respectively. To perform separation, we predict a spectrogram mask M n for each object. We use real-valued ratio masks and obtain the predicted magnitude spectrogram by soft masking the mixture spectrogram: X n = X M × M n . Finally, we use the inverse short-time Fourier transform (ISTFT) <ref type="bibr" target="#b15">[15]</ref> to reconstruct the waveform sound for each object source.</p><p>Going beyond video-level mix-and-separation, the key insight of our approach is to simultaneously enforce separation within a single video at the object level. This enables  our method to learn object sound models even from multisource training videos. Our new co-separation framework can capture the correlations between sound sources and is able to learn from noisy Web videos, as detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Co-Separation Framework</head><p>Next we present our CO-SEPARATION training framework and our network architecture to perform separation. Object Detection Firstly, we train an object detector for a vocabulary of C objects. In general, this detector should cover any potential sound-making object categories that may appear in training videos. Our implementation uses the Faster R-CNN <ref type="bibr" target="#b36">[36]</ref> object detector with a ResNet-101 <ref type="bibr" target="#b17">[17]</ref> backbone trained with Open Images <ref type="bibr" target="#b26">[26]</ref>. For each unlabeled training video, we use the pre-trained object detector to automatically 2 find objects in all video frames. Then, we gather all object detections across frames to obtain a videolevel pool of objects. See Supp. for details. Audio-Visual Separator We use the detected object regions to guide the source separation process. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates our audio-visual separator network that performs audio-visual feature aggregation and source separation. A related design for multi-modal feature fusion is also used in <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref> for audio spatialization and separation. However, unlike those models, our separator network combines the visual features of a localized object region and the audio features of the mixed audio to predict a magnitude spectrogram mask for source separation.</p><p>The network takes a detected object region and the mixed audio signal as input, and separates the portion of the sound responsible for the object. We use a ResNet-18 network to extract visual features after the 4 th ResNet block with size (H/32) × (W /32) × D, where H, W, D denote the frame and channel dimensions. We then pass the visual feature through a 1 × 1 convolution layer to reduce the channel di-   mension, and use a fully-connected layer to obtain an aggregated visual feature vector.</p><p>On the audio side, we adopt a U-NET <ref type="bibr" target="#b37">[37]</ref> style network for its effectiveness in dense prediction tasks, similar to <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b13">13]</ref>. The network takes the magnitude spectrogram X M as input and passes it through a series of convolution layers to extract an audio feature of dimension (T /128) × (F/128) × D. We replicate the visual feature vector (T /128) × (F/128) times, tile them to match the audio feature dimension, and then concatenate the audio and visual feature maps along the channel dimension. Then a series of up-convolutions are performed on the concatenated audio-visual feature map to generate a multiplicative spectrogram mask M. We find spectrogram masks to work better than direct prediction of spectrograms or raw waveforms for source separation, confirming reports in <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b13">13]</ref>. The separated spectrogram for the input object is obtained by multiplying the mask and the spectrogram of the mixed audio: X = X M × M. Finally, ISTFT is applied to the spectrogram to produce the separated real-time signal. Co-Separation Our proposed CO-SEPARATION framework first detects objects in a pair of videos, then mixes their audios at the video level, and finally separates the sounds for each detected object class. As shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, for each video pair, we randomly sample a high confidence object window for each class detected in either video, and use the localized object region to guide audio source separation using the audio-visual separator network. For each object O n , we predict a mask M n , and then generate the corresponding magnitude spectrogram.</p><p>Let V 1 and V 2 denote the set of objects for the two videos. We want to separate the sounds of their corresponding objects from the audio mixture of V 1 and V 2 . For each video, summing up the separated sounds of all objects should ideally reconstruct the audio signal for that video. Namely,</p><formula xml:id="formula_0">x 1 (t) = |V 1 | ∑ i s i (t) and x 2 (t) = |V 2 | ∑ i s i (t),<label>(1)</label></formula><p>where |V 1 | and |V 2 | are the number of detected objects for V 1 and V 2 . For simplicity of notation, we defer presenting how we handle background sounds (those unattributable to detected objects) until later in this section. Because we are operating in the frequency domain, the above relationship will only hold approximately due to phase interference.</p><p>As an alternative, we approximate Eq. (1) by enforcing the following relationship on the predicted magnitude spectrograms:</p><formula xml:id="formula_1">X V 1 ≈ |V 1 | ∑ i X i and X V 2 ≈ |V 2 | ∑ i X i ,<label>(2)</label></formula><p>where X V 1 and X V 2 are the magnitude spectrograms for x 1 (t) and x 2 (t). Therefore, we minimize the following coseparation loss over the separated magnitude spectrograms:</p><formula xml:id="formula_2">L co-separation spect = || |V 1 | ∑ i=1 X i − X V 1 || 1 + || |V 2 | ∑ i=1 X i − X V 2 || 1 ,<label>(3)</label></formula><p>which approximates to minimizing the following loss function over their predicted ratio masks:</p><p>In practice, we find that computing the loss over masks (vs. spectograms) makes the network easier to learn. We hypothesize that the sigmoid after the last layer of the audiovisual separator bounds the masks, making them more constrained and structured compared to spectrograms. In short, the proposed co-separation loss provides supervision to the network to only separate the audio portion responsible for the input visual object, so that the corresponding audios for each of the pair of input videos can be reconstructed.</p><p>In addition to the co-separation loss that enforces separation, we also introduce an object-consistency loss for each predicted audio spectrogram. The intuition is that if the sources are well-separated, the predicted "category" of the separated spectrogram should be consistent with the category of the visual object that initially guides its separation. Specifically, for the predicted spectrogram of each object, we introduce another ResNet-18 audio classifier 3 that targets the weak labels of the input visual objects. We use the following cross-entropy loss:</p><formula xml:id="formula_3">L object-consistency = 1 |V 1 | + |V 2 | |V 1 |+|V 2 | ∑ i=1 C ∑ c=1 −y i,c log(p i,c ),<label>(6)</label></formula><p>where C is the number of classes, y i,c is a binary indicator on whether c is the correct class for predicted spectrogram X i , and p i,c is the predicted probability for class c. We stress that these audio "classes" are discovered during training; we have no pre-trained sound models for different objects.</p><p>Not all sounds in a video will be attributable to a visually detected object. To account for ambient sounds, offscreen sounds, and noise, we incorporate a C + 1 st "adaptable" audio class, as follows. During training, we pair each video with a visual scene feature in addition to the detected objects from the pre-trained object detector. Then an additional mask M adapt responsible for the scene context is also predicted in Eq. (4) for both V 1 and V 2 to be optimized jointly. This step arms the network with the flexibility to assign noise or unrelated sounds to the "adaptable" class, leading to cleaner separation for sounds of the detected visual objects. These adaptable objects (ideally ambient sounds, noise, etc.) are collectively designated as having the "extra" C + 1 st audio label. The separated spectrograms for these adaptable objects are also trained to match their category label by the object-consistency loss in Eq. <ref type="bibr" target="#b6">(6)</ref>.</p><p>Putting it all together, during training the network needs to discover separations for the multi-source videos that 1) minimize the co-separation loss, such that the two source videos' object sounds reassemble to produce their original video-level audio tracks, respectively, while also 2) minimizing the object consistency loss, such that separated sounds for any instances of the same visual object are reliably identifiable as that sound. We stress that our model achieves the latter without any pre-trained audio model and without any single-source audio examples for the object class. The object consistency loss only knows that sameobject sounds should be similar after training the networknot what any given object is expected to sound like.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference</head><p>We minimize the following combined loss function and train our network end to end:</p><formula xml:id="formula_4">L = L co-separation mask + λ L object-consistency ,<label>(7)</label></formula><p>where λ is the weight for the object-consistency loss. We use per-pixel L1 loss for the co-separation loss, and weight the gradients by the magnitude of the spectrogram of the mixed audio. The network uses the weighted gradients to perform back-propagation, thereby emphasizing predictions on more informative parts of the spectrogram.</p><p>During testing, our model takes a single realistic multisource video to perform source separation. Similarly, we first detect objects in the video frames by using the pretrained object detector. For each detected object class, we use the most confident object region(s) as the visual input to separate the portion of the sound responsible for this object category from its accompanying audio. We use a sliding window approach to process videos segment by segment with a small hop size, and average the audio predictions on all overlapping parts.</p><p>We perform audio-visual source separation on video clips of 10s, and we pool all the detected objects in the video frames. Therefore, our approach assumes that each detected object within this period of 10s is a potential sound source, although it may only sound in some of the frames. For objects that are detected but do not make sound at all, we treat it as learning noise and expect our deep network to adapt by learning from large-scale training videos. We leave it as future work to explicitly model silent visual objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We now validate our approach for audio-visual source separation and compare to existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>MUSIC This MIT dataset contains YouTube videos crawled with keyword queries <ref type="bibr" target="#b52">[52]</ref>. It contains 685 untrimmed videos of musical solos and duets, with 536 solo videos and 149 duet videos. The dataset is relatively clean and collected for the purpose of training audio-visual source separation models. It includes 11 instrument categories: accordion, acoustic guitar, cello, clarinet, erhu, flute, saxophone, trumpet, tuba, violin and xylophone. Following the authors' public dataset file of video IDs, we hold out the first/second video in each category as validation/test data, and the rest as training data. We split all videos into 10s clips during both training and testing, for a total of 8,928/259/269 train/val/test clips, respectively.</p><p>AudioSet-Unlabeled AudioSet <ref type="bibr" target="#b14">[14]</ref> consists of challenging 10s video clips, many of poor quality and containing a variety of sound sources. Following <ref type="bibr" target="#b12">[12]</ref>, we filter the dataset to extract video clips of 15 musical instruments. We use the videos from the "unbalanced" split for training, and videos from the "balanced" split as validation/test data, for a total of 113,756/456/456 train/val/test clips, respectively.</p><p>AudioSet-SingleSource A dataset assembled by <ref type="bibr" target="#b12">[12]</ref> of AudioSet videos containing only a single sounding object. We use the 15 videos (from the "balanced" split) of musical instruments for evaluation only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AV-Bench</head><p>This dataset contains the benchmark videos (Violin Yanni, Wooden Horse, and Guitar Solo) used in previous studies <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b34">34]</ref> on visually-guided audio denoising.</p><p>On both MUSIC and AudioSet, we compose the test sets following standard practice <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b12">12]</ref>-by mixing the audio from two single-source videos. This ensures the ground truth separated sounds are known for quantitative evaluation. There are 550 and 105 such test pairings for MUSIC and AudioSet, respectively (the result of pairwise mixing 10 random clips per the 15 classes for MUSIC and pairwise mixing all 15 clips for AudioSet). For qualitative results (Supp.), we apply our method to real multi-source test videos. In either case, we train our method with multisource videos, as specified below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our CO-SEPARATION deep network is implemented in PyTorch. For all experiments, we sub-sample the audio at 11kHz, and the input audio sample is approximately 6s long. STFT is computed using a Hann window size of 1022 and a hop length of 256, producing a 512 × 256 Time-Frequency audio representation. The spectrogram is then re-sampled on a log-frequency scale to obtain a T × F magnitude spectrogram of T = 256, F = 256. The settings are the same as <ref type="bibr" target="#b52">[52]</ref> for fair comparison.</p><p>Our object detector is trained on images of C = 15 object categories from the Open Images dataset <ref type="bibr" target="#b26">[26]</ref>. We filter out low confidence object detections for each video, and keep the top two 4 detected categories. See Supp. for details. During co-separation training, we randomly sample 64 pairs of videos for each batch. We sample a confident object detection for each class as its input visual object, paired with a random scene image sampled from the ADE dataset <ref type="bibr" target="#b53">[53]</ref> as the adaptable object. The object window is resized to 256 × 256, and a randomly cropped 224 × 224 region is used as the input to the network. We use horizontal flipping, color and intensity jittering as data augmentation. λ is set to 0.05 in Eq. <ref type="bibr" target="#b7">(7)</ref>. The network is trained using an Adam optimizer with weight decay 1 × 10 −4 with the starting learning rate set to 1 × 10 −4 . We use a smaller starting learning rate of 1 × 10 −5 for the ResNet-18 visual feature extractor because it is pre-trained on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results on Source Separation</head><p>We compare to the following baselines:</p><p>• Sound-of-Pixels <ref type="bibr" target="#b52">[52]</ref>: We use the authors' publicly available code 5 to train 1-frame based models with ratio masks for fair comparison. Default settings are used for other hyperparameters.</p><p>• AV-Mix-and-Separate: A "mix-and-separate" baseline using the same audio-visual separation network as our model to do video-level separation. We use multilabel hinge loss to enforce video-level consistency, i.e., the class of each separated spectrogram should agree with the objects present in that training video.</p><p>• AV-MIML <ref type="bibr" target="#b12">[12]</ref>: An existing audio-visual source separation method that uses audio bases learned from unlabeled videos to supervise an NMF separation process. The audio bases are learned from a deep multiinstance multi-label (MIML) learning network. We use the results reported in <ref type="bibr" target="#b12">[12]</ref> for AudioSet and AV-Bench; the authors do not report results in SDR and do not report results for MUSIC.</p><p>• NMF-MFCC <ref type="bibr" target="#b43">[43]</ref>: An off-the-shelf audio-only method that performs NMF based source separation using Mel frequency cepstrum coefficients (MFCC). This non-learned baseline is a good representation of a well established pipeline for audio-only source separation <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>• AV-Loc <ref type="bibr" target="#b34">[34]</ref>, JIVE <ref type="bibr" target="#b28">[28]</ref>, Sparse CCA <ref type="bibr" target="#b25">[25]</ref>: We use results reported in <ref type="bibr" target="#b12">[12]</ref> to compare to these methods for the audio denoising benchmark AV-Bench. We use the widely used mir eval library <ref type="bibr" target="#b35">[35]</ref> to evaluate the source separation and report the standard metrics: Signal-to-Distortion Ration (SDR), Signal-to-Interference Ratio (SIR), and Signal-to-Artifact Ratio (SAR). Separation Results. <ref type="table" target="#tab_1">Tables 1 and 2</ref> show the results for the MUSIC and AudioSet datasets, respectively.  when training only on solo videos, it fails to make use of the additional duets, and its performance degrades when training on the multi-source videos. In contrast, our method actually improves when trained on a combination of solos and duets, achieving its best performance. This experiment highlights precisely the limitation of the mix-and-separate training paradigm when presented with multi-source training videos, and it demonstrates that our co-separation idea can successfully overcome that limitation.</p><p>Our method also outperforms all baselines, including <ref type="bibr" target="#b52">[52]</ref>, when training on solos. Our better accuracy versus the AV-Mix-and-Separate baseline and <ref type="bibr" target="#b52">[52]</ref> shows that our object-level co-separation idea is essential. The NMF-MFCC baseline can only return ungrounded separated signals. Therefore, we evaluate both possible matchings and take its best results (to the baseline's advantage). Also, our gains are similar even if we give <ref type="bibr" target="#b52">[52]</ref> the advantage of temporal pooling over 3 frames. Overall our method achieves large gains, and also has the benefit of matching the separated sounds to semantically meaningful visual objects in the video. <ref type="table" target="#tab_1">Table 2</ref> shows the results when training on AudioSet-Unlabeled and testing on mixes of AudioSet-SingleSource. Our method outperforms all prior methods and the baselines by a large margin on this challenging dataset. It demonstrates that our framework can better learn from the noisy and less curated "in the wild" videos of AudioSet, which contains many multi-source videos. See Supp. for additional results on removing the limit of two objects per video.</p><p>Next we devise an experiment to test explicitly how well our method can learn to separate sound for objects it has not   <ref type="table" target="#tab_3">Table 3</ref> shows the results. We can see that although our system is not trained on any guitar solos, it can learn better from multi-source videos that contain guitar and other sounds. Our method consistently performs well on all three combinations, while <ref type="bibr" target="#b52">[52]</ref> performs well only on the violin+guitar mixture. We hypothesize the reason is that it can learn by mixing the large quantity of violin solos and the guitar solo moments within the duets to perform separation, but it fails to disentangle other sound source correlations. Our method scores worse in terms of SAR, which again measures artifacts, but not separation quality. See Supp. for additional experiments where we train only on duets as well as an ablation study to isolate the impact of each loss term. Denoising Results. As a side product of our audio-visual source separation system, we can also use our model to perform visually-guided audio denoising. As mentioned in Sec. 3.3, we use an additional scene image to capture ambient/unseen sounds and noise, etc. Therefore, given a test video with noise, we can use the top detected visual object in the video to guide our system to separate out the noise. <ref type="table">Table 4</ref> shows the results on AV-Bench <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b12">12]</ref>. Though our method learns only from unlabeled video and does not explicitly model the low-rank nature of noise as in <ref type="bibr" target="#b34">[34]</ref>, we obtain state-of-the-art performance on 2 of the 3 videos. The method of <ref type="bibr" target="#b34">[34]</ref> uses motion in manually segmented regions, which may help on Guitar Solo, where the hand's motion strongly correlates with the sound.   10 10 10 10 1010 1010 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 1010 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 <ref type="table" target="#tab_0">10  10  11 11   11   11  12   13   13 13   13  13   13   13   13   13   13   13   13 13  13  13  13   13   1313   13  13  13   13   13   13  13   13   13   13  13   13  13  13  13   13  13   13   13   13   13  13  13   13   13   13  13   13 13   13  13   13   13   13   13   13   13   13   13  13   13  13   13   13   13 13   13 13 13  13   13  13   13   13   13  13   13  13   13   13   13   13  13  13   13  13   13   13   13   13   13   13   13  13  13   13 13   13   13   13  13   13  13   13   13   13  13   13  13   13   13   13   13  13   13   13  13   1313   13   13  13  13 13   13   13   13  13   13   13   13   13   13   13  13   13 13   13   13   13   13  13  13   13   13  13  13   13   13   13   13   13  13   13 13   13   13   13  13   13   13   13   13   13  13   13   13  13   13  13  13  13   13   13   13   13   13   13   13 13   13   13   13   13   13  13   13   13   13   13  13 13  13   13   13 13   13   13   13   13   13   13   1313   13   13   13   13   13  13   13   13   13  13   13   13 13  13   13  13   13   13  13  13   13  13   13   13   13   13   13 13  13   13  13   13  13   13   13   13  13   13   13   13  13   13   13  13  13   13   13  13   13   13  13   13  13   13  13   13  13   13  13   13   13   13   13  13   13   13   13   13   13   13  13   13  13   13   13  13  13  13   13   13 13   13   13   13   13  13   13   13  13  13   13   13  13  13   13   13   13  13  13   13 13   13   13  13   13   13  13   13   13  13   13   13   13   13  13   13   13   13   13   13   13  13   13  13 13  13  13  13   13   13 13   13   13   13   13   13   13  13   13  13   13   13   13  13  13   13   13   13  13  13  13 13   13   13   13   13   13  13   13   13  13  13   13   13   13   13   13  13   13  13   13   13  13   13   13   13  13   13   13   13  13  13   13   13   13   13  13  13  13 13  13  13   13  13   13  13   13   13   13   13   13   13   13   13   13   13   13  13   13  13   13   13   13  13   13   13   13   13   13   13  13  13  13   13   13  13   13   13   13   13   13   13  13   13   13  13  13   13   13  13  13   13  13   13  13   13   13   13   13   13   13   13   13  13  13   13   13   13  13   13  13  13  13   13   13   13   13   13   13 13   13   13  13   13  13  13   13   13   13   13  13   13   13  13   13   13  13   13   13   13   13   13   13   13   13   13   13   13   13   13  13  13  13   13   13   13   13 13  13   13   13  13   13   13  13   13   13   13  13   13   13   13   13   13   13  13   13   13   13   13  13  13  13   13   13 13   13   13   13   13 13   13   13   13   13   13   13   13   13  13  13  13  13  13   13   13   13  13 13  13   13   13   13  13  13   13   13   13   13  13   13   13   13  13  13   13   13   13  13   13   13   13   13   13   13   13   13   13   13  13  13  13  13  13  13   13   13   1313   13   13   13  13   13   13  13  13   13   13   13   13 13   13  13   13  13  13   13   13   13  13 13   13   13 13  13   13   13   13  13  13   13   13  13  13   13   13  13 13  13  13   13  13   13  13   13   13   13   13   13   13   13   13  13  13   13   13   13  13  13   13  13   13   13  13  13  13  13   13   13   13 13   13   13 13   13  13   1313   13  13   13  13   13  13   13   13  13   13   13   13  13   13   13   13   13   13  13   13  13   13   13   13   13   13   13   13   13   13   13  13   13  13   13   13   13   13  13  13   13  13   13   13   13   13   13   13   13   13  13   13 13   13   13   13   13   13   13   13  13   13   13  13   13   13   13 13   13  13   13   13   13   13   13  13  13  13  13  13   13   13   13  13  13   13   13   13   13   13   13   13   13   13   13   13   13  13   13   13   13   13   13   13   13   13   13   13  13   13   13   13  13   13 13   13 13   13  13   13  13   13  13   13   13 13   13   13   13  13   13   13   13   13   13  13  13  13  13   13   13  13   13   13   13  13  13   13   13   13   13   13   13  13 13   13   13   13  13   13 13   13  13   13   13  13 13  13  13   13 13  13   13   13   13  13  13   13  13   13   13  13   13   13  13  13   13   13  13  13   13   13  13   13   13  13   13  13   13   13   13   13   13   13   13   13   13 13  13   13   13   13   13   13   13   13   13  13   13   13   13   13  13  13   13  13   13   13   13  13   13   13   13  13  13   13  13  13   13   13   13  13 13   13   13   13  13   13   13   13   13   13   13   13   13  13   13  13  13  13   13   1313   13   13   13   13 13   13  13  13   13   13   13   13   13  13   13   13   13   13  13   13  13   13  13  13   13  13   13 13   13   13  13   13   13  13   13   13   13   13  13   13   13  13   13   13  13   13  13   13   13  13  13   13  13   13   13 13  13   13   13   13   13   13   13  13   13  13  13  13  13   13   13  13   13   13   13   13   13   13  13  13  13   13  13   13   13   13   13  13   13   13  13  13 13   13  13   13   13   13   13   13   13   13  13   13  13   13  13  13   13  13   13   13   13   13   13   13   13   13   13   13  13   13   13  13  13  13   13  13   13   13   13   13  13 13  13 13   13   13 13  13   13  13   13   13   13   13   13  13   13   13  13  13  13   13   13  13  13   13  13   13  13   13   13 13   13  13  13   13   13   13   13   13  13  13  13   13  13   13  13   13   13   13  13   13 13   13   13   13  13   13  13   13   13 13 13  13   13   13 13 13   13   13   13   13  13   13   13  13  13   13  13   13   13   13  13   13   13   13  13   13  13   13   13   13 13   13  13   13   13   13  13  13   13   13   13   13   13  13   13   13   13   13  13   13 13   13  13   13   13   13   13  13  13   13   13  13  13   13   13   13 13  13  13   13  13   13   13   13  13  13  13   13   13   13   13   13   13  13   13  13   13  13  13   13  13   13  13   13   13  13 13   13   13  13   13   13   13  13 13   13   13   13   13  13   13   13   13   13   13   13   13   13   13   13   13 13   13 13   13   13  13   13   13   13 13   13   13  13  13   13   13   13   13  13  13   13   13   13   13  13   1313   13   13  13   13   13   13   13   13   13   13   13   13 13 13   13   13  13  13   13   13   13   13 13   13  13   13   13  13   13   13   13   13  13   13   13   13   13   13  13   13   13 13   13   13   13   13  13  13  13 13   13   13   13   13   13   13   13  13   13   13  13 13  13  13   13   13   13   13  13  13  13  13 13   13  13   13   13  13  13  13  13   13   13  13   13  13   13   13   13   13   13   13   13   13   13  13 13   13   13   13  13   13  13  13   13   13   13   13   13   13   13  13  13   13   13   13  13   13   13 13   13   13   13   13   13  13 13   13  13  13  13   13   13   13   13  13   13   13   13   13   13   13   13  13  13   13   13   13   13   13  13  13  13  13  13  13   13   13  13  13   13  13   13   13   13   13   13   13  13  13   13  13  13   13   13  13   13   13   13  13  13   13   13 13  13  13   13   13  13   13   13   13   13   13   13  13  13 13   13  13   13  13  13   13  13  13  13  13  13   13  13   13   13   13  13   13   13  13   13  13 13   13   13   13  13   13   1313   13   13   13   13  13   13  13   13  13  13   13   13   13  13   13  13   13  13 13   13   13   13   13   13  13  13  13   13   13  13   13  13   13   13   13  13   13  13   13   13  13 13   13   13   13   13  13 13   13   13  13 13  13   13   13  13  13  13   13   13   13   13  13   13   13  13   13  13   13   13   13   13   13  13   13   13   13  13   13  13  13   13   13  13   13   13  13   13   13  13   13  13  13   13  13   13   13   13   13  13  13   13  13   13   13  13  13  13  13   13   13   13   13   13   13   13   13   13   13   13   13  13   13   13   13   13   13   13   13   13   13   13   13  13   13   13   13  13  13  13   13  13   13   13   13   13  13   13   13   13  13   13   13  13   13   13  13   13   13   13   13   13  13  13   13   13   13   13   13   13   13  13   13   13   13   13   13   13   13   13   13   13  13   13  13   13  13   13   13   13  13   13   13   13 13   13   13   13   13   13   13  13   13  13   13   13  13  13   13   13   13   13  13   13   13  13   13   13  13   13   13  13 13  13   13   13   13   13   13  13   13   13   13   13   13  13   13   13  13   13   13  13   13   13  13   13  13   13   13  13  13   13   13  13   13   13   13   13   13   13 13   13  13   13  13  13   13  13 13  13  13   13   13   13   13   13  13 13   13   13  13   13   13  13   13   13   13   13   13   13   13   13   13   13   13   13  13   13   13 13   13  13 13  13   13  13   13   13   13  13  13   13   13   13   13   13   13   13  13   13   13   13   13   13   13   13   13  13   13   13  13  13   13   13  13   13   13   13   13  13  13   13  13  13   13   13   13   13   13   13  13   13  13   13 13   13   13   13   13   13   13   13   13  13   13  13  13   13   13  13  13  13   13   13   13   13   13   13  13   13  13   13   13   13  13   13  13   13  13  13   13   13   13   13  13   13  1313   13  13   13   13  13  13   13  13   13   13   13  13   13  13  13   13   13 13   13   13   13   13   13  13   13   13   13   13   13   13   13   13   13   13   13  13   13 13   13  13  13   13  13  13  13  13  13   13   13  13   13  13  13   13   13   13  13   13   13   13   13   13   13  13   13  13  13   13  13  13  13  13  13  13   13</ref>    <ref type="table">Table 4</ref>: Visually-assisted audio denoising on AV-Bench, in terms of NSDR (in dB, higher is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>Audio-Visual Separation Video Examples. Our video results (see Supp.) show qualitative separation results. We use our system to discover and separate object sounds for realistic multi-source videos. They lack ground truth, but the results can be manually inspected for quality.</p><p>Learned Audio Embedding. To visualize that our CO-SEPARATION network has indeed learned to separate sounds of visual objects, <ref type="figure" target="#fig_7">Fig. 4</ref> displays a t-SNE <ref type="bibr" target="#b29">[29]</ref> embedding of the discovered sounds for various input objects in 20K Au-dioSet clips. We use the features extracted at the last layer of the ResNet-18 audio classifier as the audio representa-Harp Guitar Accordion <ref type="figure">Figure 5</ref>: Top object proposals according to our discovered audio classifier. Last column shows typical failure cases.</p><p>tion for the separated spectrograms. The sounds our method learned from multi-source videos tend to cluster by object category, demonstrating that the separator discovers sounds characteristic of the corresponding objects.</p><p>Using Discovered Sounds to Detect Objects. Finally, we use our trained audio-visual source separation network for visual object discovery using 912 noisy unseen videos from AudioSet. Given the pool of videos, we generate object region proposals using Selective Search <ref type="bibr" target="#b46">[46]</ref>. Then we pass these region proposals to our network together with the audio of its accompanying video, and retrieve the visual proposals that achieve the highest audio classification scores according to our object consistency loss. <ref type="figure">Fig. 5</ref> shows the top retrieved proposals for several categories after removing duplicates from the same video. We can see that our method has learned a good mapping between the visual and audio modalities; the best visual object proposals usually best activate the audio classifier. The last column shows failure cases where the wrong object is detected with high confidence. They usually come from objects of similar texture or shape, like the stripes on the man's t-shirt and the shadow of the harp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented an object-level audio-visual source separation framework that associates localized object regions in videos to their characteristic sounds. Our CO-SEPARATION approach can leverage noisy object detections as supervision to learn from large-scale unlabeled videos. We achieve state-of-the-art results on visually-guided audio source separation and audio denoising. As future work, we plan to explore spatio-temporal object proposals and incorporate object motion to guide separation, which may especially benefit object sounds with similar frequencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our audio-visual separator network takes a mixed audio signal and a detected object from its accompanying video as input, and performs joint audio-visual analysis to separate the portion of sound responsible for the input object region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2</head><label></label><figDesc>No manual object annotations are used for co-separation train-/testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Co-separation training pipeline: our object-level co-separation framework first automatically detects objects in a pair of videos, then mixes the audios at the video-level, and separates the sounds for each visual object. The network is trained by minimizing the combination of the co-separation and object-consistency losses defined in Sec. 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Embedding of separated sounds in AudioSet visualized with t-SNE in two ways: (top) categories are color-coded, and (bottom) visual objects are shown at their sound's embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Table 1 :</head><label>11</label><figDesc>presents results on MUSIC as a function of the training source: single-source videos (solo) or multi-source videos (solo + duet). Our method consistently outperforms all baselines in separation accuracy, as captured by the SDR and SIR metrics.<ref type="bibr" target="#b6">6</ref> While the SoP method<ref type="bibr" target="#b52">[52]</ref> works well Average audio source separation results on a held out MUSIC test set. We show the performance of our method and the baselines when training on only single-source videos (solo) and multi-source videos (solo + duet). NMF-MFCC is non-learned, so its results do not vary across training sets. Higher is better for all metrics. Note that SDR and SIR capture separation accuracy; SAR captures only the absence of artifacts (and hence can be high even if separation is poor). Standard error is approximately 0.2 for all metrics.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Single-Source</cell><cell>Multi-Source</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SDR SIR SAR SDR SIR SAR</cell></row><row><cell cols="3">Sound-of-Pixels [52]</cell><cell>7.30 11.9 11.9</cell><cell>6.05 9.81 12.4</cell></row><row><cell cols="3">AV-Mix-and-Separate</cell><cell>3.16 6.74 8.89</cell><cell>3.23 7.01 9.14</cell></row><row><cell></cell><cell cols="2">NMF-MFCC [43]</cell><cell>0.92 5.68 6.84</cell><cell>0.92 5.68 6.84</cell></row><row><cell cols="4">CO-SEPARATION (Ours) 7.38 13.7 10.8</cell><cell>7.64 13.8 11.3</cell></row><row><cell></cell><cell cols="3">SDR SIR SAR</cell></row><row><cell>Sound-of-Pixels [52]</cell><cell cols="3">1.66 3.58 11.5</cell></row><row><cell>AV-MIML [12]</cell><cell>1.83</cell><cell>-</cell><cell>-</cell></row><row><cell>AV-Mix-and-Separate</cell><cell cols="3">1.68 3.30 12.2</cell></row><row><cell>NMF-MFCC [43]</cell><cell cols="3">0.25 4.19 5.78</cell></row><row><cell cols="4">CO-SEPARATION (Ours) 4.26 7.07 13.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average separation results on AudioSet test set. Standard error is approximately 0.3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Toy experiment to demonstrate learning to separate sounds for objects never heard individually during training.</figDesc><table /><note>observed individually during training. We train our model and the best baseline [52] on the following four categories: violin solo, saxophone solo, violin+guitar duet, and vio- lin+saxophone duet, and test by randomly mixing and sep- arating violin, saxophone, and guitar test solo clips.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The ResNet-18 audio classifier is ImageNet pre-trained to accelerate convergence, but not pre-trained for audio classification. Our co-separation training aims to automatically discover the audio classes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This is the number of objects detected in most training videos; relaxing this limit does not change the overall results (see Supp.).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/hangzhaomit/Sound-of-Pixels 6 Note that SAR measures the artifacts present in the separated signal, but not the separation accuracy. So, a less well-separated signal can achieve high(er) SAR values. In fact, naively copying the original input twice (i.e., doing no separation at all) results in SAR ≈ 80 in our setting.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: Thanks to Dongguang You for help with experiments setup, and Yu-Chuan Su, Tushar Nagarajan, Santhosh Ramakrishnan and Xingyi Zhou for helpful discussions and reading paper drafts. UT Austin is supported in part by DARPA Lifelong Learning Machines.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>9 9 9 9 9 9 9 99 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 99 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 99 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 <ref type="table">9   9  12   12   12  12   12   12   12   12   12   12   12   12   12   12  12  12  12   12   12  12 12  12   12   12   12   12  12  12  12   12  12   12   12   12   12   12   12  12  12  12  12  12  12   12   12   12  12 12  12  12   12  12  12  12   12   12   12  12   12   12  12   12   12   12  12  12   12   12   12   12   12  12   12  12  12   12  12   12   12  12   12  12  12   12  12   12  12  12   12  12   12  12  12  12  12   12  12  12   12   12   12  12  12  12 12  12   12   12   12   12   12   12  12   12   12   12   12  12   12   12   12   12   12   12   12  12  12  12  12   12   12   12  12  12  12  12  12  12   12   12   12  12  12  12  12   12   12   12   12  12   12   12  12  12   12   12   12  12  12   12   12   12  12   12   12  12  12  12  12   12  12  12  12 12  12   12  12  12  12   12 12  12  12  12  12   12   12   12  12  12   12   12  12  12  12   12  12</ref>   <ref type="table">15  15  15   15   15   15   15   15   15  15   15   15  15   15   15   15  15   15  15   15   15   15   15  15  15   15   15   15   15   15   15   15   15   15   15   15  15  15  15   15   15   15   15  15   15   15   15   15  15   15   15   15   15  15  15   15   15   15   15   15   15   15   15   15   15   15   15   15   15   15   15   15  15   15  15   15   15   15 15   15   15   15  15   15   15   15   15   15   15  15   15   15  15  15   15  15  15   15  15   15   15   15  15  15   15  15   15   15  15  15   15  15  15   15  15   15  15   15   15   15  15   15   15   15   15   15  15   15  15   15  1515   15   15   1515   15   15   15   15 15   15   15   15   15   15   15  15   15  15   15   15  15  15   15   15   15   15   15   15   15   15 15   15   15   15  15  15   15   15  15   15   15 15  15  15   15   15   15   15  15   15   15   15   15   15  15  15   15   15   15  15  15   15   15   15   15   15   15   15  15  15  15   15   15   15   15   15  15   15  15  15  15   15  15   15   15   15 15   15   15  15   15   15   15  15   15   15   15  15  15   15   15   15   15   15  15   15   15   15   15  15   15   15   15  15   15   15   15   15  15   15   15   15   15  15  15   15   15  15   15   15  15   15  15   15   15   15   15   15   15  15  15   15  15   15  15 15  15   15  15 15   15   15   15  15   15   15  15   15   15   15   15  15   15   15  15   15  15   15   15  15  15   15   15  15   15   15   15  15   15   15  15 15  15   15   15 15   15   15   15   15   15   15   15   15  15   15   15   15   15   15  15  15  15   15   15  15   15   15 15   15   15   15   15   15  15   15 15 15  15   15  15  15  15   15   15 15  15   15   15   15  15  15  15   15   15  15   15   15  15   15 15   15   15   15  15  15   15   15  15   15   15  15   15   15 15   15   15   15   15   15   15 15  15  1515  15  15   15   15   15   15  15   15   15   15   15   15  15   15   15   15   15   15   15   15   15   15   15   15   15   15   15   15   15 15   15   15   15   15  15   15  15  15   15   15   15   15   15  15   15   15  15   15  15  15   15  15  15  15  15  15   15  15   15   15  15   15  15  15   15   15   15  15   15   15  15   15   15  15   15   15  15   15   15   15  15   15  15   15   15  15 15   15   15   15   15   15  15  15   15   15   15  15   15   15   15   15   15   15   15  15   15   15 15   15   15   15   15   15  15   15   15  15   15   15  15   15   15   15  15  15  15   15  15  15  15   15   15   15 15  15   15  15  15 15  15  15   15  15   15   15  15  15   15   15   15  15  15  15  15   15   15   15   15   15   15   15   15   15   15   15   15   15   15   15   15   15   15   15  15   15   15  15   15   15  15 15   15   15  15   15   15  15  15   15   15   15  15   15  15   15  15  15   15 15   15  15   15   15   15   15   15   15 15  15  15   15 15  15   15 15  15   15   15  15   15  15   15  15  15  15  15   15  15 15   15  15   15  15   15   15   15   15   15  15   15  15   15   15  15   15   15 15   15  15   15   15   15  15   15   15   15   15   15   15   15  15  15   15   15  15  15   15   15 15   15  15   15  15   15   15   15   15  15   15  15   15   15  15   15 15   15  15  15   15   15  15  15  15   15  15  15  15   15  15   15   15   15   15  15  15  15   15  15   15   15  15   15   15   15   15   15   15   15   15   15   15  15   15   15   15   15   15   15   15 15   15   15  15   15   15  15  15  15   15   15  15   15   15   15  15   15  15   15   15  15   15   15   15   15   15   15   15   15  15   15  15   15  15   15   15   15  15   15  15   15   15  15   15  15   15   15 15   15   15   15   15   15  15   15   15   15   15   15 15  15   15   15   15   15  15   15   15   15   15  15  15   15  15   15  15   15  15   15   15 15   15  15   15   15   15   15  15   15   15  15   15   15   15   15  15   15  15   15   15   15   15   15   15   15  15   15   15  15   15  15   15   15 15  15   15   15  15  15  15   15   15   15   15  15   15  15   15 15  15   15   15  15  15 15   15   15  15   15  15  15   15   15   15  15   15   15   15   15   15  15   15   15   15   15  15   15   15   15   15   15   15   15  1515  15   15  15   15   15  15   15   15  15   15   15   15  15   15   15   15   15   15   15   15  15  15  15 15  15  15   15   15   15   15  15  15   15   15   15  15 15  15   15   15   15   15   15   15   15   15   15   15   15  15   15   15   15  15  15   15   15   15   15   15   15  15   15   15  15   15   15   15   15  15   15  15   15  15  15   15   15  15  15   15   15   15   15   15   15  15   15   15  15   15   15   15   15   15   15  15   15  15   15   15  15   15   15  15   15  15   15  15   15   15   15 15   15   15   15   15   15  15   15   15  15   15  15   15   15   15  15   15   15   15  15  15   15   15  15   15   15   15 15  15   15   15   15   15   15 15  15   15   15   15  15  15   15   15   15  15  15   15   15   15  15   15  15  15   15  15   15   15   15   15   15   15  15   15   15   15  15   15  15   15   15  15  15   15   15 15   15   15  15   15   15   15   15   15  15 15  15   15   15   15   15   15   15   15   15  15   15   15   15  15   15  15   15  15   15   15  15  15  15   15   15   15  15   15   15   15   15  15   15  15   15   15   15   15   15  15   15   15   15   15  15  15  15   15   15  15  15  15   15   15   15  15   15  15   15   15   15  15  15   15   15  15  15 15  15 15  15   15   15   15   15   15   15   15  15   15  15  15   15   15  15  15  15  15  15   15   15  15  15   15   15  15   15   15  15   15   15   15   15  15  15   15  15   15   15  15   15   15   15   15   15  15  15  15   15   15   15   15   15   15  15  15   15  15  15   15   15   15   15   15   15   15  15   15   15  15   15  15  15 15   15   15   15   15   15  15 15   15  15   15   15 15  15   15   15  15   15   15   15</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Trombone</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<publisher>Horn</publisher>
		</imprint>
	</monogr>
	<note>Flute</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bryan lucas band -acoustic guitar and cello wedding ceremony music</title>
		<ptr target="https://www.youtube.com/watch?v=_oRFIsHn2ek.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Shape of my heart violin and guitar wedding ceremony sydney manly q station</title>
		<ptr target="https://www.youtube.com/watch?v=u12_qzOriVg.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joon Son Chung, and Andrew Zisserman. The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Harmony in motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zohar</forename><surname>Barzelay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep cross-modal audio-visual generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">on Thematic Workshops of ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Prediction-driven computational auditory scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Patrick Whittlesey</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinatan</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Nonnegative matrix factorization with the itakura-saito divergence: With application to music analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Louis</forename><surname>Durrieu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning joint statistical models for audiovisual fusion and segregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>John W Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul A</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaph</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to separate object sounds by watching unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">2.5d visual sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and humanlabeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nmf-based blind source separation using a linear predictive coding error clustering criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Audio vision: Using audio-visual synchrony to locate sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint optimization of masks and deep recurrent neural networks for monaural source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nmf-based environmental sound source separation using time-variant gain features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Innami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Kasai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Mathematics with Applications</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clustering nmf basis functions using shifted nmf for monaural sound source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derry</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Coyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Rickard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pixels that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Kidron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhyanesh</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/openimages/web/index.html" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">See and listen: Score-informed association of sound tracks to players in chamber music performance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Joint and individual variation explained (jive) for integrated analysis of multiple data types. The annals of applied statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Lock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hoadley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Stephen Marron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nobel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne. JMLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised generation of spatial audio for 360 • video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nono</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Langlois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visually indicated sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Edward H Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Motion informed audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeel</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slim</forename><surname>Essid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Ozerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">K</forename><surname>Ngoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Audio-visual object localization and separation using lowrank and sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">mir eval: A transparent implementation of common mir metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two multimodal approaches for single microphone source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farnaz</forename><surname>Sedighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massoud</forename><surname>Babaie-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Rivet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jutten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arda</forename><surname>Senocak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep karaoke: Extracting vocals from musical mixtures using a convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Audio/visual independent components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Casey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Independent Component Analysis and Signal Separation</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Source-filter based clustering for monaural blind source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Spiertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volker Gnann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Digital Audio Effects</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial semi-supervised audio source separation applied to singing voice extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Audio-visual event localization in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sound source separation using sparse coding with temporal continuity objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Computer Music Conference</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Monaural sound source separation by nonnegative matrix factorization with temporal continuity and sparseness criteria. IEEE transactions on audio, speech, and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The sound of motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The sound of pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visual to sound: Generating natural sound for videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
