<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
							<email>haotan@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="hu">
<div xmlns="http://www.tei-c.org/ns/1.0"> arXiv:1908.07490v3 [cs.CL]  </div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pretrained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pretrained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR 2 , and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pretraining strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vision-and-language reasoning requires the understanding of visual contents, language seman-tics, and cross-modal alignments and relationships. There has been substantial past works in separately developing backbone models with better representations for the single modalities of vision and of language. For visual-content understanding, people have developed several backbone models <ref type="bibr" target="#b31">(Simonyan and Zisserman, 2014;</ref><ref type="bibr" target="#b34">Szegedy et al., 2015;</ref><ref type="bibr" target="#b10">He et al., 2016)</ref> and shown their effectiveness on large vision datasets <ref type="bibr" target="#b4">(Deng et al., 2009;</ref><ref type="bibr" target="#b21">Lin et al., 2014;</ref><ref type="bibr" target="#b18">Krishna et al., 2017)</ref>. Pioneering works <ref type="bibr" target="#b8">(Girshick et al., 2014;</ref><ref type="bibr" target="#b38">Xu et al., 2015)</ref> also show the generalizability of these pretrained (especially on ImageNet) backbone models by fine-tuning them on different tasks. In terms of language understanding, last year, we witnessed strong progress towards building a universal backbone model with large-scale contextualized language model pre-training <ref type="bibr" target="#b25">(Peters et al., 2018;</ref><ref type="bibr" target="#b26">Radford et al., 2018;</ref><ref type="bibr" target="#b5">Devlin et al., 2019)</ref>, which has improved performances on various tasks <ref type="bibr" target="#b27">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b36">Wang et al., 2018)</ref> to significant levels. Despite these influential singlemodality works, large-scale pretraining and finetuning studies for the modality-pair of vision and language are still under-developed.</p><p>Therefore, we present one of the first works in building a pre-trained vision-and-language crossmodality framework and show its strong performance on several datasets. We name this framework "LXMERT: Learning Cross-Modality Encoder Representations from Transformers" (pronounced: 'leksmert'). This framework is modeled after recent BERT-style innovations while further adapted to useful cross-modality scenarios. Our new cross-modality model focuses on learning vision-and-language interactions, especially for representations of a single image and its descriptive sentence. It consists of three Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. In order to better learn the cross-modal alignments between vision and language, we next pre-train our model with five diverse representative tasks: (1) masked crossmodality language modeling, (2) masked object prediction via RoI-feature regression, (3) masked object prediction via detected-label classification, (4) cross-modality matching, and (5) image question answering. Different from single-modality pre-training (e.g., masked LM in BERT), this multi-modality pre-training allows our model to infer masked features either from the visible elements in the same modality, or from aligned components in the other modality. In this way, it helps build both intra-modality and cross-modality relationships.</p><p>Empirically, we first evaluate LXMERT on two popular visual question-answering datasets, VQA <ref type="bibr" target="#b1">(Antol et al., 2015)</ref> and <ref type="bibr">GQA (Hudson and Manning, 2019)</ref>. Our model outperforms previous works in all question categories (e.g., Binary, Number, Open) and achieves state-of-the-art results in terms of overall accuracy. Further, to show the generalizability of our pre-trained model, we fine-tune LXMERT on a challenging visual reasoning task, Natural Language for Visual Reasoning for Real (NLVR 2 ) <ref type="bibr" target="#b32">(Suhr et al., 2019)</ref>, where we do not use the natural images in their dataset for our pre-training, but fine-tune and evaluate on these challenging, real-world images. In this setup, we achieve a large improvement of 22% absolute in accuracy (54% to 76%, i.e., 48% relative error reduction) and 30% absolute in consistency (12% to 42%, i.e., 34% relative error reduction). Lastly, we conduct several analysis and ablation studies to prove the effectiveness of our model components and diverse pre-training tasks by removing them or comparing them with their alternative options. Especially, we use several ways to take the existing BERT model and its variants, and show their ineffectiveness in vision-andlanguage tasks, which overall proves the need of our new cross-modality pre-training framework. We also present several attention visualizations for the different language, object-relationship, and cross-modality encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Architecture</head><p>We build our cross-modality model with selfattention and cross-attention layers following the recent progress in designing natural language pro-cessing models (e.g., transformers <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>). As shown in <ref type="figure">Fig. 1</ref>, our model takes two inputs: an image and its related sentence (e.g., a caption or a question). Each image is represented as a sequence of objects, and each sentence is represented as a sequence of words. Via careful design and combination of these self-attention and cross-attention layers, our model is able to generate language representations, image representations, and cross-modality representations from the inputs. Next, we describe the components of this model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input Embeddings</head><p>The input embedding layers in LXMERT convert the inputs (i.e., an image and a sentence) into two sequences of features: word-level sentence embeddings and object-level image embeddings. These embedding features will be further processed by the latter encoding layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word-Level Sentence Embeddings</head><p>A sentence is first split into words {w 1 , . . . , w n } with length of n by the same WordPiece tokenizer <ref type="bibr" target="#b37">(Wu et al., 2016)</ref> in <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>. Next, as shown in <ref type="figure">Fig. 1</ref>, the word w i and its index i (w i 's absolute position in the sentence) are projected to vectors by embedding sub-layers, and then added to the index-aware word embeddings:</p><formula xml:id="formula_0">w i = WordEmbed (w i ) u i = IdxEmbed (i) h i = LayerNorm (ŵ i +û i )</formula><p>Object-Level Image Embeddings Instead of using the feature map output by a convolutional neural network, we follow <ref type="bibr" target="#b0">Anderson et al. (2018)</ref> in taking the features of detected objects as the embeddings of images. Specifically, the object detector detects m objects {o 1 , . . . , o m } from the image (denoted by bounding boxes on the image in <ref type="figure">Fig. 1</ref>). Each object o j is represented by its position feature (i.e., bounding box coordinates) p j and its 2048-dimensional region-of-interest (RoI) feature f j . Instead of directly using the RoI feature f j without considering its position p j in <ref type="bibr" target="#b0">Anderson et al. (2018)</ref>, we learn a position-aware embedding v j by adding outputs of 2 fully-connected layers:  <ref type="figure">Figure 1</ref>: The LXMERT model for learning vision-and-language cross-modality representations. 'Self' and 'Cross' are abbreviations for self-attention sub-layers and cross-attention sub-layers, respectively. 'FF' denotes a feed-forward sub-layer.</p><formula xml:id="formula_1">f j = LayerNorm (W F f j + b F ) p j = LayerNorm (W P p j + b P ) v j = f j +p j /2<label>(1)</label></formula><formula xml:id="formula_2">N R ⇥ N L ⇥ N X ⇥</formula><p>In addition to providing spatial information in visual reasoning, the inclusion of positional information is necessary for our masked object prediction pre-training task (described in Sec. 3.1.2). Since the image embedding layer and the following attention layers are agnostic to the absolute indices of their inputs, the order of the object is not specified. Lastly, in Equation 1, the layer normalization is applied to the projected features before summation so as to balance the energy of the two different types of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoders</head><p>We build our encoders, i.e., the language encoder, the object-relationship encoder, and the crossmodality encoder, mostly on the basis of two kinds of attention layers: self-attention layers and crossattention layers. We first review the definition and notations of attention layers and then discuss how they form our encoders.</p><p>Background: Attention Layers Attention layers <ref type="bibr" target="#b2">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b38">Xu et al., 2015)</ref> aim to retrieve information from a set of context vectors {y j } related to a query vector x. An attention layer first calculates the matching score a j between the query vector x and each context vector y j . Scores are then normalized by softmax:</p><formula xml:id="formula_3">a j = score(x, y j ) α j = exp(a j )/ k exp(a k )</formula><p>The output of an attention layer is the weighted sum of the context vectors w.r.t. the softmaxnormalized score: Att X→Y (x, {y j }) = j α j y j . An attention layer is called self-attention when the query vector x is in the set of context vectors {y j }. Specifically, we use the multi-head attention following Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Modality Encoders</head><p>After the embedding layers, we first apply two transformer encoders <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>, i.e., a language encoder and an object-relationship encoder, and each of them only focuses on a single modality (i.e., language or vision). Different from BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, which applies the transformer encoder only to language inputs, we apply it to vision inputs as well (and to crossmodality inputs as described later below). Each layer (left dashed blocks in <ref type="figure">Fig. 1</ref>) in a singlemodality encoder contains a self-attention ('Self') sub-layer and a feed-forward ('FF') sub-layer, where the feed-forward sub-layer is further composed of two fully-connected sub-layers. We take N L and N R layers in the language encoder and the object-relationship encoder, respectively. We add a residual connection and layer normalization (annotated by the '+' sign in <ref type="figure">Fig. 1</ref>) after each sublayer as in <ref type="bibr" target="#b35">Vaswani et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modality Encoder</head><p>Each cross-modality layer (the right dashed block in <ref type="figure">Fig. 1)</ref> in the crossmodality encoder consists of two self-attention sub-layers, one bi-directional cross-attention sublayer, and two feed-forward sub-layers. We stack (i.e., using the output of k-th layer as the input of (k+1)-th layer) N X these cross-modality layers in our encoder implementation. Inside the k-th layer, the bi-directional cross-attention sub-layer ('Cross') is first applied, which contains two unidirectional cross-attention sub-layers: one from language to vision and one from vision to language. The query and context vectors are the outputs of the (k-1)-th layer (i.e., language features + RoI  </p><formula xml:id="formula_4">h k i = CrossAtt L→R h k−1 i ,{v k−1 1 , . . . , v k−1 m } v k j = CrossAtt R→L v k−1 j ,{h k−1 1 , . . . , h k−1 n }</formula><p>The cross-attention sub-layer is used to exchange the information and align the entities between the two modalities in order to learn joint crossmodality representations. For further building internal connections, the self-attention sub-layers ('Self') are then applied to the output of the crossattention sub-layer:</p><formula xml:id="formula_5">h k i = SelfAtt L→L ĥ k i , {ĥ k 1 , . . . ,ĥ k n } ṽ k j = SelfAtt R→R v k j , {v k 1 , . . . ,v k m }</formula><p>Lastly, the k-th layer output {h k i } and {v k j } are produced by feed-forward sub-layers ('FF') on top of {ĥ k i } and {v k j }. We also add a residual connection and layer normalization after each sub-layer, similar to the single-modality encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Output Representations</head><p>As shown in the right-most part of <ref type="figure">Fig. 1</ref>, our LXMERT cross-modality model has three outputs for language, vision, and cross-modality, respectively. The language and vision outputs are the feature sequences generated by the cross-modality encoder. For the cross-modality output, following the practice in <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>, we append a special token [CLS] (denoted as the top yellow block in the bottom branch of <ref type="figure">Fig. 1</ref>) before the sentence words, and the corresponding feature vector of this special token in language feature sequences is used as the cross-modality output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pre-Training Strategies</head><p>In order to learn a better initialization which understands connections between vision and language, we pre-train our model with different modality pre-training tasks on a large aggregated dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-Training Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Language Task: Masked</head><p>Cross-Modality LM</p><p>On the language side, we take the masked crossmodality language model (LM) task. As shown in the bottom branch of <ref type="figure" target="#fig_0">Fig. 2</ref>, the task setup is almost same to BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>: words are randomly masked with a probability of 0.15 and the model is asked to predict these masked words. In addition to BERT where masked words are predicted from the non-masked words in the language modality, LXMERT, with its cross-modality model architecture, could predict masked words from the vision modality as well, so as to resolve ambiguity. For example, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, it is hard to determine the masked word 'carrot' from its language context but the word choice is clear if the visual information is considered. Hence, it helps building connections from the vision modality to the language modality, and we refer to this task as masked cross-modality LM to emphasize this difference. We also show that loading BERT parameters into LXMERT will do harm to the pre-training procedure in Sec. 5.1 since BERT can perform relatively well in the language modality without learning these crossmodality connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Vision Task: Masked Object Prediction</head><p>As shown in the top branch of <ref type="figure" target="#fig_0">Fig. 2</ref>, we pretrain the vision side by randomly masking ob-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Split</head><p>Images Sentences (or Questions) jects (i.e., masking RoI features with zeros) with a probability of 0.15 and asking the model to predict proprieties of these masked objects. Similar to the language task (i.e., masked cross-modality LM), the model can infer the masked objects either from visible objects or from the language modality. Inferring the objects from the vision side helps learn the object relationships, and inferring from the language side helps learn the crossmodality alignments. Therefore, we perform two sub-tasks: RoI-Feature Regression regresses the object RoI feature f j with L2 loss, and Detected-Label Classification learns the labels of masked objects with cross-entropy loss. In the 'Detected-Label Classification' sub-task, although most of our pre-training images have object-level annotations, the ground truth labels of the annotated objects are inconsistent in different datasets (e.g., different number of label classes). For these reasons, we take detected labels output by Faster R-CNN <ref type="bibr" target="#b28">(Ren et al., 2015)</ref>. Although detected labels are noisy, experimental results show that these labels contribute to pre-training in Sec. 5.3.</p><formula xml:id="formula_6">COCO-Cap VG-Cap VQA GQA VG-QA All MS COCO -VG 72K 361K - 387K - - 0.75M MS COCO ∩ VG 51K 256K 2.54M 271K 515K 724K 4.30M VG -MS COCO 57K - 2.85M - 556K 718K 4.13M All 180K 617K 5.39M 658K 1.07M 1.44M 9.18M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Cross-Modality Tasks</head><p>As shown in the middle-rightmost part of <ref type="figure" target="#fig_0">Fig. 2</ref>, to learn a strong cross-modality representation, we pre-train the LXMERT model with 2 tasks that explicitly need both language and vision modalities.</p><p>Cross-Modality Matching For each sentence, with a probability of 0.5, we replace it with a mismatched 2 sentence. Then, we train a classifier to predict whether an image and a sentence match each other. This task is similar to 'Next Sentence Prediction' in BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>.</p><p>Image Question Answering (QA) In order to enlarge the pre-training dataset (see details in <ref type="bibr">2</ref> We take a sentence from another image as the mismatched sentence. Although the sentence and the image still have chance to match each other, this probability is very low. Sec. 3.2), around 1/3 sentences in the pre-training data are questions about the images. We ask the model to predict the answer to these imagerelated questions when the image and the question are matched (i.e., not randomly replaced in the cross-modality matching task). We show that pre-training with this image QA leads to a better cross-modality representation in Sec. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-Training Data</head><p>As shown in <ref type="table">Table.</ref> 1, we aggregate pre-training data from five vision-and-language datasets whose images come from MS COCO <ref type="bibr" target="#b21">(Lin et al., 2014)</ref> or Visual Genome <ref type="bibr" target="#b18">(Krishna et al., 2017)</ref>. Besides the two original captioning datasets, we also aggregate three large image question answering (image QA) datasets: VQA v2.0 <ref type="bibr" target="#b1">(Antol et al., 2015)</ref>, GQA balanced version (Hudson and Manning, 2019), and VG-QA <ref type="bibr" target="#b42">(Zhu et al., 2016)</ref>. We only collect train and dev splits in each dataset to avoid seeing any test data in pre-training. We conduct minimal pre-processing on the five datasets to create aligned image-and-sentence pairs. For each image question answering dataset, we take questions as sentences from the image-and-sentence data pairs and take answers as labels in the image QA pre-training task (described in <ref type="figure">Sec. 3.1.3)</ref>. This provides us with a large aligned vision-andlanguage dataset of 9.18M image-and-sentence pairs on 180K distinct images. In terms of tokens, the pre-training data contain around 100M words and 6.5M image objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-Training Procedure</head><p>We pre-train our LXMERT model on the large aggregated dataset (discussed in Sec. 3.2) via the pretraining tasks (Sec. 3.1). The details about the data splits are in the Appendix. The input sentences are split by the WordPiece tokenizer <ref type="bibr" target="#b37">(Wu et al., 2016)</ref> provided in BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>  2015) which is pre-trained on Visual Genome (provided by <ref type="bibr" target="#b0">Anderson et al. (2018)</ref>). We do not fine-tune the Faster R-CNN detector and freeze it as a feature extractor. Different from detecting variable numbers of objects in <ref type="bibr" target="#b0">Anderson et al. (2018)</ref>, we consistently keep 36 objects for each image to maximize the pre-training compute utilization by avoiding padding. For the model architecture, we set the numbers of layers N L , N X , and N R to 9, 5, and 5 respectively. 3 More layers are used in the language encoder to balance the visual features extracted from 101-layer Faster R-CNN.</p><p>The hidden size 768 is the same as BERT BASE . We pre-train all parameters in encoders and embedding layers from scratch (i.e., model parameters are randomly initialized or set to zero). We also show results of loading pre-trained BERT parameters in Sec. 5.1. LXMERT is pre-trained with multiple pre-training tasks and hence multiple losses are involved. We add these losses with equal weights. For the image QA pre-training tasks, we create a joint answer table with 9500 answer candidates which roughly cover 90% questions in all three image QA datasets. We take Adam (Kingma and Ba, 2014) as the optimizer with a linear-decayed learning-rate schedule <ref type="bibr" target="#b5">(Devlin et al., 2019</ref>) and a peak learning rate at 1e − 4. We train the model for 20 epochs (i.e., roughly 670K 4 optimization steps) with a batch size of 256. We only pre-train with image QA task (see Sec. 3.1.3) for the last 10 epochs, because this task converges faster and empirically needs a smaller learning rate. The whole 3 If we count a single modality layer as one half crossmodality layer, the equivalent number of cross-modality layers is (9 + 5)/2 + 5 = 12, which is same as the number of layers in BERTBASE. <ref type="bibr">4</ref> For comparison, ResNet on ImageNet classification takes 600K steps and BERT takes 1000K steps. pre-training process takes 10 days on 4 Titan Xp.</p><p>Fine-tuning Fine-tuning is fast and robust. We only perform necessary modification to our model with respect to different tasks (details in Sec. 4.2). We use a learning rate of 1e − 5 or 5e − 5, a batch size of 32, and fine-tune the model from our pretrained parameters for 4 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup and Results</head><p>In this section, we first introduce the datasets that are used to evaluate our LXMERT framework and empirically compare our single-model results with previous best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluated Datasets</head><p>We use three datasets for evaluating our LXMERT framework: VQA v2.0 dataset <ref type="bibr" target="#b9">(Goyal et al., 2017)</ref>, GQA (Hudson and Manning, 2019), and NLVR 2 . See details in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>On VQA and GQA, we fine-tune our model from the pre-trained snapshot without data augmentation (analysis in Sec. 5.2). When training GQA, we only take raw questions and raw images as inputs and do not use other supervisions (e.g., functional programs and scene graphs). Since each datum in NLVR 2 has two natural images img 0 , img 1 and one language statement s, we use LXMERT to encode the two image-statement pairs (img 0 , s) and (img 1 , s), then train a classifier based on the concatenation of the two cross-modality outputs. More details in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Empirical Comparison Results</head><p>We compare our single-model results with previous best published results on VQA/GQA teststandard sets and NLVR 2 public test set. Be-sides previous state-of-the-art (SotA) methods, we also show the human performance and imageonly/language-only results when available.</p><p>VQA The SotA result is BAN+Counter in <ref type="bibr" target="#b16">Kim et al. (2018)</ref>, which achieves the best accuracy among other recent works: MFH , Pythia <ref type="bibr" target="#b15">(Jiang et al., 2018)</ref>, DFAF <ref type="bibr" target="#b6">(Gao et al., 2019a)</ref>, and Cycle-Consistency <ref type="bibr" target="#b30">(Shah et al., 2019)</ref>. 5 LXMERT improves the SotA overall accuracy ('Accu' in <ref type="table" target="#tab_4">Table 2</ref>) by 2.1% and has 2.4% improvement on the 'Binary'/'Other' question sub-categories. Although LXMERT does not explicitly take a counting module as in BAN+Counter, our result on the counting-related questions ('Number') is still equal or better. 6 GQA The GQA (Hudson and Manning, 2019) SotA result is taken from BAN <ref type="bibr" target="#b16">(Kim et al., 2018)</ref> on the public leaderbaord. Our 3.2% accuracy gain over the SotA GQA method is higher than VQA, possibly because GQA requires more visual reasoning. Thus our framework, with novel encoders and cross-modality pre-training, is suitable and achieves a 4.6% improvement on opendomain questions ('Open' in <ref type="table" target="#tab_4">Table 2</ref>). 7 NLVR 2 NLVR 2 <ref type="bibr" target="#b32">(Suhr et al., 2019</ref>) is a challenging visual reasoning dataset where some existing approaches <ref type="bibr" target="#b13">(Hu et al., 2017;</ref><ref type="bibr" target="#b24">Perez et al., 2018)</ref> fail, and the SotA method is 'MaxEnt' in <ref type="bibr" target="#b32">Suhr et al. (2019)</ref>. The failure of existing methods (and our model w/o pre-training in Sec. 5.1) indicates that the connection between vision and language may not be end-to-end learned in a complex vision-and-language task without largescale pre-training. However, with our novel pretraining strategies in building the cross-modality connections, we significantly improve the accuracy ('Accu' of 76.2% on unreleased test set 'Test-U', in <ref type="table" target="#tab_4">Table 2</ref>) by 22%. Another evaluation metric consistency measures the proportion of unique sentences for which all related image pairs 8 are 5 These are state-of-the-art methods at the time of our EMNLP May 21, 2019 submission deadline. Since then, there have been some recently updated papers such as MCAN <ref type="bibr" target="#b40">(Yu et al., 2019b)</ref>, MUAN <ref type="bibr" target="#b39">(Yu et al., 2019a)</ref>, and MLI <ref type="bibr" target="#b7">(Gao et al., 2019b)</ref>  correctly predicted. Our LXMERT model improves consistency ('Cons') to 42.1% (i.e., by 3.5 times). 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we analyze our LXMERT framework by comparing it with some alternative choices or by excluding certain model components/pre-training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">BERT versus LXMERT</head><p>BERT <ref type="bibr" target="#b5">(Devlin et al., 2019</ref>) is a pre-trained language encoder which improves several language tasks. As shown in <ref type="table" target="#tab_6">Table 3</ref>, we discuss several ways to incorporate a BERT BASE pre-trained model for vision-language tasks and empirically compare it with our LXMERT approach. Although our full model achieves accuracy of 74.9% on NLVR 2 , all results without LXMERT pretraining is around 22% absolute lower.</p><p>BERT+BUTD Bottom-Up and Top-Down (BUTD) attention <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref> method encodes questions with GRU <ref type="bibr" target="#b3">(Chung et al., 2015)</ref>, then attends to object RoI features {f j } to predict the answer. We apply BERT to BUTD by replacing its GRU language encoder with BERT. As shown in the first block of  enhance BERT+BUTD with our novel positionaware object embedding (in Sec. 2.1) and crossmodality layers (in Sec. 2.2). As shown in the second block of <ref type="table" target="#tab_6">Table 3</ref>, the result of 1 crossmodality layer is better than BUTD, while stacking more cross-modality layers further improves it. However, without our cross-modality pretraining (BERT is language-only pre-trained), results become stationary after adding 3 crossattention layers and have a 3.4% gap to our full LXMERT framework (the last bold row in <ref type="table" target="#tab_6">Table 3</ref>).</p><p>BERT+LXMERT We also try loading BERT parameters 10 into LXMERT, and use it in model training (i.e., without LXMERT pre-training) or in pre-training. We show results in the last block of <ref type="table">Table.</ref> 3. Compared to the 'from scratch' (i.e., model parameters are randomly initialized) approach, BERT improves the fine-tuning results but it shows weaker results than our full model. Empirically, pre-training LXMERT initialized with BERT parameters has lower (i.e., better) pretraining loss for the first 3 pre-training epochs but was then caught up by our 'from scratch' approach. A possible reason is that BERT is already pre-trained with single-modality masked language model, and thus could do well based only on the language modality without considering the connection to the vision modality (as discussed in Sec. 3.1.1).  Pre-training versus Data Augmentation Data augmentation (DA) is a technique which is used in several VQA implementations <ref type="bibr" target="#b0">(Anderson et al., 2018;</ref><ref type="bibr" target="#b16">Kim et al., 2018;</ref><ref type="bibr" target="#b15">Jiang et al., 2018)</ref>. It increases the amount of training data by adding questions from other image QA datasets. Our LXMERT framework instead uses multiple QA datasets in pre-training and is fine-tuned only on one specific dataset. Since the overall amounts of data used in pre-training and DA are similar, we thus can fairly compare these two strategies, and results show that our QA pre-training approach outperforms DA. We first exclude the QA task in our pre-training and show the results of DA finetuning. As shown in <ref type="table">Table.</ref> 4 row 1, DA finetuning decreases the results compared to non-DA fine-tuning in row 2. Next, we use DA after QApre-training (row 3) and DA also drops the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Vision Pre-training tasks</head><p>We analyze the effect of different vision pretraining tasks in <ref type="table" target="#tab_10">Table 5</ref>. Without any vision tasks in pre-training (i.e., only using the language and cross-modality pre-training tasks), the results (row 1 of <ref type="table" target="#tab_10">Table 5</ref>) are similar to BERT+3 CrossAtt in <ref type="table" target="#tab_6">Table 3</ref>. The two visual pre-training tasks (i.e., RoI-feature regression and detected-label classification) could get reasonable results (row 2 and row 3) on their own, and jointly pre-training with these two tasks achieves the highest results (row 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Visualizing LXMERT Behavior</head><p>In the appendix, we show the behavior of LXMERT by visualizing its attention graphs in the language encoder, object-relationship encoder, and cross-modality encoder, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Model Architecture Our model is closely related to three ideas: bi-directional attention, Transformer, and BUTD. <ref type="bibr" target="#b23">Lu et al. (2016)</ref> applies bi-directional attention to the vision-and-language tasks while its concurrent work BiDAF <ref type="bibr" target="#b29">(Seo et al., 2017)</ref> adds modeling layers in solving reading comprehension. Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> is first used in machine translation, we utilize it as our single-modality encoders and design our cross-modality encoder based on it. BUTD <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref> embeds images with the object RoI features, we extend it with object positional embeddings and object relationship encoders.</p><p>Pre-training After ELMo <ref type="bibr" target="#b25">(Peters et al., 2018)</ref>, GPT <ref type="bibr" target="#b26">(Radford et al., 2018)</ref>, and BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> show improvements in language understanding tasks with large-scale pre-trained language model, progress has been made towards the cross-modality pre-training. XLM <ref type="bibr" target="#b19">(Lample and Conneau, 2019</ref>) learns the joint cross-lingual representations by leveraging the monolingual data and parallel data. VideoBert <ref type="bibr" target="#b33">(Sun et al., 2019)</ref> takes masked LM on the concatenation of language words and visual tokens, where the visual tokens are converted from video frames by vector quantization. However, these methods are still based on a single transformer encoder and BERTstype token-based pre-training, thus we develop a new model architecture and novel pre-training tasks to satisfy the need of cross-modality tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recent works since our EMNLP submission</head><p>This version of our paper (and all current results) was submitted to EMNLP 11 and was used to participate in the VQA and GQA challenges in May 2019. Since our EMNLP submission, a few other useful preprints have recently been released (in August) on similar cross-modality pre-training directions: ViLBERT <ref type="bibr" target="#b22">(Lu et al., 2019)</ref> and Visual-BERT <ref type="bibr" target="#b20">(Li et al., 2019)</ref>. Our LXMERT methods differs from them in multiple ways: we use a more detailed, multi-component design for the crossmodality model (i.e., with an object-relationship encoder and cross-modality layers) and we employ additional, useful pre-training tasks (i.e., RoIfeature regression and image question answering). These differences result in the current best performance (on overlapping reported tasks): a margin of 1.5% accuracy on VQA 2.0 and a margin of 9% accuracy on NLVR 2 (and 15% in consistency).</p><p>LXMERT is also the only method which ranks in the top-3 on both the VQA and GQA challenges among more than 90 teams. We provide a detailed analysis to show how these additional pre-training tasks contribute to the fine-tuning performance in Sec. 5.2 and Sec. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a cross-modality framework, LXMERT, for learning the connections between vision and language. We build the model based on Transfermer encoders and our novel crossmodality encoder. This model is then pre-trained with diverse pre-training tasks on a large-scale dataset of image-and-sentence pairs. Empirically, we show state-of-the-art results on two image QA datasets (i.e., VQA and GQA) and show the model generalizability with a 22% improvement on the challenging visual reasoning dataset of NLVR 2 . We also show the effectiveness of several model components and training methods via detailed analysis and ablation studies.</p><p>VQA The goal of visual question answering (VQA) <ref type="bibr" target="#b1">(Antol et al., 2015)</ref> is to answer a natural language question related to an image. We take VQA v2.0 dataset <ref type="bibr" target="#b9">(Goyal et al., 2017)</ref> which reduces the answer bias compared to VQA v1.0. The dataset contains an average of 5.4 questions per image and the total amount of questions is 1.1M.</p><p>GQA The task of GQA (Hudson and Manning, 2019) is same as VQA (i.e., answer single-image related questions), but GQA requires more reasoning skills (e.g., spatial understanding and multistep inference). 22M questions in the dataset are generated from ground truth image scene graph to explicitly control the question quality.</p><p>NLVR 2 Since the previous two datasets are used in pre-training for increasing the amount of pretraining data to a certain scale, we evaluate our LXMERT framework on another challenging visual reasoning dataset NLVR 2 where all the sentences and images are not covered in pre-training. Each datum in NLVR 2 contains two related natural images and one natural language statement. The task is to predict whether the statement correctly describes these two images or not. NLVR 2 has 86K, 7K, 7K data in training, development, and test sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of NLVR 2 Fine-tuning</head><p>Each datum in NLVR 2 consists of a two-image pair (img 0 , img 1 ), one statement s, and a ground truth label y * indicating whether the statement correctly describe the two images. The task is to predict the label y given the images and the statement.</p><p>To use our LXMERT model on NLVR 2 , we concatenate the cross-modality representations of the two images and then build the classifier with GeLU activation <ref type="bibr" target="#b11">(Hendrycks and Gimpel, 2016)</ref>. Suppose that LXMERT(img, sent) is the singlevector cross-modality representation, the predicted probability is:</p><formula xml:id="formula_7">x 0 = LXMERT(img 0 , s) x 1 = LXMERT(img 1 , s) z 0 = W 0 [x 0 ; x 1 ] + b 0 z 1 = LayerNorm GeLU(z 0 ) prob = σ(W 1 z 1 + b 1 )</formula><p>where σ is sigmoid function. The model is optimized by maximizing the log-likelihood, which is equivalent to minimize the binary cross entropy loss: </p><formula xml:id="formula_8">L = -y * log prob − (1 − y * ) log(1 − prob) C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Training Details of 'BERT versus LXMERT'</head><p>When training with BERT only, we train each experiments for 20 epochs with a batch size 64/128 since it was not pre-trained on these crossmodality datasets. The learning rate is set to 1e−4 instead of 5e − 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Visualizing LXMERT Behavior</head><p>In this section, we show the behavior of LXMERT by visualizing its attention graphs in the language encoder, object-relationship encoder, and crossmodality encoder, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Language Encoder</head><p>In <ref type="figure">Fig. 3</ref>, we reveal that the LXMERT language encoder has similar behaviour as the original BERT encoder, by using the same sentence "Is it warm enough for him to be wearing shorts?" as the input to both models. LXMERT's attention graphs (in <ref type="figure">Fig. 3(a, c)</ref>) are extracted from the pretrained LXMERT without fine-tuning on a specific task. BERT's attention graphs (in <ref type="figure">Fig. 3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b,</head><p>Is it warm enough for him to be wearing shorts ?</p><p>What colors are the pole the horse is jumping over? <ref type="figure">Figure 5</ref>: Attention graphs in LXMERT's crossmodality encoder showing that the attention focus on pronouns (marked in pink), nouns (marked in blue), and articles (marked in red). d)) come from <ref type="bibr" target="#b12">Hoover et al. (2019)</ref>. <ref type="bibr">13</ref> We find that both the second LXMERT layer <ref type="figure">(Fig. 3(a)</ref>) and third BERT layer ( <ref type="figure">Fig. 3(b)</ref>) point to the next words while both the fourth LXMERT layer <ref type="figure">(Fig. 3(c)</ref>) and fourth BERT layer ( <ref type="figure">Fig. 3(d)</ref>) point to the previous words, thus showing the similar behaviour of the two encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Object-Relationship Encoder</head><p>In <ref type="figure" target="#fig_1">Fig. 4</ref>, we visualize the attention graph of the first layer in LXMERT's object-relationship encoder. We only highlight the objects with the highest attention scores while the other objects are mostly not attended to. We manually build the connections between objects (marked as yellow lines in <ref type="figure" target="#fig_1">Fig. 4(b)</ref>) according to the attention graph. These connections faithfully draw a scene graph of the figure, which indicates that the objectrelationship encoder might be learning a reasonably good network of the relationships between objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Cross-Modality Encoder</head><p>In <ref type="figure">Fig. 5</ref>, we visualize the attention in LXMERT's cross-modality encoder to reveal the connections between objects and words. We find that the attention focuses on nouns and pronouns as shown in the top figure of <ref type="figure">Fig. 5</ref> because they are the most informative words in current vision-and-language tasks. However, for non-plural nouns (as shown in the bottom example in <ref type="figure">Fig. 5)</ref>, the attention will focus on the articles. Although we do not specifically design for this behavior, we think that articles are possibly serving as special tokens (e.g., <ref type="bibr">[CLS]</ref>, <ref type="bibr">[SEP]</ref> in BERT), thus providing unified target entries for the attention layers. Next, we are also looking at how to utilize pre-training tasks which directly capture pairwise noun-noun and noun-verb relationships between the images and text sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Pre-training in LXMERT. The object RoI features and word tokens are masked. Our five pre-training tasks learn the feature representations based on these masked inputs. Special tokens are in brackets and classification labels are in braces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>12 https://cs.stanford.edu/people/dorarad/gqa/evaluate.html(a) LXMERT 2 nd Lang-layer (b) BERT 3 rd Layer (c) LXMERT 4 th Lang-layer (d) BERT 4 th LayerFigure 3: Attention graphs reveal similar behavior in the LXMERT language encoder (a, c) and in the original BERT encoder (b, d). Fig. a &amp; b show the attention pointing to next words whileFig. c &amp; dshow the attention pointing to previous words. The attention graph (a) and its recovered scene graph (b) in the first layer of LXMERT's objectrelationship encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Amount of data for pre-training. Each image has multiple sentences/questions. 'Cap' is caption. 'VG' is Visual Genome. Since MS COCO and VG share 51K images, we list it separately to ensure disjoint image splits.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Binary NumberOther Accu Binary Open Accu Cons Accu</figDesc><table><row><cell>Method</cell><cell></cell><cell>VQA</cell><cell></cell><cell></cell><cell></cell><cell>GQA</cell><cell></cell><cell cols="2">NLVR 2</cell></row><row><cell>Human</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.2</cell><cell>87.4</cell><cell>89.3</cell><cell>-</cell><cell>96.3</cell></row><row><cell>Image Only</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.1</cell><cell>1.74</cell><cell cols="3">17.8 7.40 51.9</cell></row><row><cell>Language Only</cell><cell>66.8</cell><cell>31.8</cell><cell>27.6</cell><cell>44.3</cell><cell>61.9</cell><cell>22.7</cell><cell cols="3">41.1 4.20 51.1</cell></row><row><cell>State-of-the-Art</cell><cell>85.8</cell><cell>53.7</cell><cell>60.7</cell><cell>70.4</cell><cell>76.0</cell><cell>40.4</cell><cell cols="3">57.1 12.0 53.5</cell></row><row><cell>LXMERT</cell><cell>88.2</cell><cell>54.2</cell><cell>63.1</cell><cell>72.5</cell><cell>77.8</cell><cell>45.0</cell><cell cols="3">60.3 42.1 76.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>. The ob-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">jects are detected by Faster R-CNN (Ren et al.,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Test-set results. VQA/GQA results are reported on the 'test-standard' splits and NLVR 2 results are reported on the unreleased test set ('Test-U'). The highest method results are in bold. Our LXMERT framework outperforms previous (comparable) state-of-the-art methods on all three datasets w.r.t. all metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. MCAN (VQA challenge version) uses stronger mixture of detection features and achieves 72.8% on VQA 2.0 test-standard. MUAN achieves 71.1% (compared to our 72.5%). Each statement in NLVR 2 is related to multiple image pairs in order to balance the dataset answer distribution.</figDesc><table><row><cell>Method</cell><cell cols="3">VQA GQA NLVR 2</cell></row><row><cell>LSTM + BUTD</cell><cell>63.1</cell><cell>50.0</cell><cell>52.6</cell></row><row><cell>BERT + BUTD</cell><cell>62.8</cell><cell>52.1</cell><cell>51.9</cell></row><row><cell>BERT + 1 CrossAtt</cell><cell>64.6</cell><cell>55.5</cell><cell>52.4</cell></row><row><cell>BERT + 2 CrossAtt</cell><cell>65.8</cell><cell>56.1</cell><cell>50.9</cell></row><row><cell>BERT + 3 CrossAtt</cell><cell>66.4</cell><cell>56.6</cell><cell>50.9</cell></row><row><cell>BERT + 4 CrossAtt</cell><cell>66.4</cell><cell>56.0</cell><cell>50.9</cell></row><row><cell>BERT + 5 CrossAtt</cell><cell>66.5</cell><cell>56.3</cell><cell>50.9</cell></row><row><cell>Train + BERT</cell><cell>65.5</cell><cell>56.2</cell><cell>50.9</cell></row><row><cell>Train + scratch</cell><cell>65.1</cell><cell>50.0</cell><cell>50.9</cell></row><row><cell>Pre-train + BERT</cell><cell>68.8</cell><cell>58.3</cell><cell>70.1</cell></row><row><cell cols="2">Pre-train + scratch 69.9</cell><cell>60.0</cell><cell>74.9</cell></row></table><note>6 Our result on VQA v2.0 'test-dev' is 72.4%.7 Our result on GQA 'test-dev' is 60.0%.8</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Dev-set accuracy of using BERT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table. 3, results of BERT encoder is comparable to LSTM encoder.</figDesc><table><row><cell>Method</cell><cell cols="3">VQA GQA NLVR 2</cell></row><row><cell>1. P20 + DA</cell><cell>68.0</cell><cell>58.1</cell><cell>-</cell></row><row><cell>2. P20 + FT</cell><cell>68.9</cell><cell>58.2</cell><cell>72.4</cell></row><row><cell cols="2">3. P10+QA10 + DA 69.1</cell><cell>59.2</cell><cell>-</cell></row><row><cell cols="2">4. P10+QA10 + FT 69.9</cell><cell>60.0</cell><cell>74.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BERT+CrossAtt Since BUTD only takes the</cell></row><row><cell></cell><cell></cell><cell></cell><cell>raw RoI features {f j } without considering the ob-ject positions {p j } and object relationships, we</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Dev-set accuracy showing the importance of the image-QA pre-training task. P10 means pretraining without the image-QA loss for 10 epochs while QA10 means pre-training with the image-QA loss. DA and FT mean fine-tuning with and without Data Augmentation, resp.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Dev-set accuracy of different vision pre-</cell></row><row><cell>training tasks. 'Feat' is RoI-feature regression; 'Label'</cell></row><row><cell>is detected-label classification.</cell></row><row><cell>comparing it with its alternative: data augmenta-</cell></row><row><cell>tion.</cell></row><row><cell>Pre-training w/ or w/o Image QA To fairly</cell></row><row><cell>compare with our original pre-training procedure</cell></row><row><cell>(10 epochs w/o QA + 10 epochs w/ QA, details in</cell></row><row><cell>Sec. 3.3) , we pre-train LXMERT model without</cell></row><row><cell>image QA task for 20 epochs. As shown in Ta-</cell></row><row><cell>ble 4 rows 2 and 4, pre-training with QA loss im-</cell></row><row><cell>proves the result on all three datasets. The 2.1%</cell></row><row><cell>improvement on NLVR 2 shows the stronger rep-</cell></row><row><cell>resentations learned with image-QA pre-training,</cell></row><row><cell>since all data (images and statements) in NLVR 2</cell></row><row><cell>are not used in pre-training.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Training, Validation, and Testing Splits Although the captions and questions of the MS COCO test sets are available, we exclude all of them to make sure that testing images are not seen in pre-training.Fine-tuning For training and validating VQA v2.0, we take the same split convention as in our LXMERT pre-training. The data related to images in LXMERT mini-validation set is used to validate model performance and the rest of the data in train+val are used in fine-tuning. We test our model on the VQA v2.0 'test-dev' and 'teststandard' splits. For GQA fine-tuning, we follow the suggestions in official GQA guidelines 12 to take testdev as our validation set and fine-tune our model on the joint train + validation sets. We test our GQA model on GQA 'test-standard' split. The images in NLVR 2 are not from either MS COCO or Visual Genome, we thus keep using the original split: fine-tune on train split, validate the model choice on val split, and test on the public ('Test-P') and unreleased ('Test-U') test splits.</figDesc><table><row><cell>We carefully split each dataset to ensure that</cell></row><row><cell>all testing images are not involved in any pre-</cell></row><row><cell>training or fine-tuning steps. Our data splits for</cell></row><row><cell>each dataset and reproducible code are available</cell></row><row><cell>at https://github.com/airsplay/lxmert.</cell></row><row><cell>LXMERT Pre-Traininig Since MS COCO has</cell></row><row><cell>a relative large validation set, we sample a set</cell></row><row><cell>of 5k images from the MS COCO validation set</cell></row><row><cell>as the mini-validation (minival) set. The rest of</cell></row><row><cell>the images in training and validation sets (i.e.,</cell></row><row><cell>COCO training images, COCO validation images</cell></row><row><cell>besides minival, and all the other images in Visual</cell></row><row><cell>Genome) are used in pre-training.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Published at EMNLP 2019. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">These are the unreleased test set ('Test-U') results. On the public test set ('Test-P'), LXMERT achieves 74.5% Accu and 39.7% Cons.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Since our language encoder is same as BERTBASE, except the number of layers (i.e., LXMERT has 9 layers and BERT has 12 layers), we load the top 9 BERT-layer parameters into the LXMERT language encoder.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">EMNLP deadline was on May 21, 2019, and the standard ACL/EMNLP arxiv ban rule was in place till the notification date of August 12, 2019.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">exBERT demo<ref type="bibr" target="#b12">(Hoover et al., 2019)</ref> is available at http://exbert.net/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful comments. This work was supported by ARO-YIP Award #W911NF-18-1-0336, and awards from Google, Facebook, Salesforce, and Adobe. The views, opinions, and/or findings contained in this article are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of the funding agency. We also thank Alane Suhr for evaluation on NLVR 2 .</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Evaluated Datasets Description</head><p>We use three datasets for evaluating our LXMERT framework.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and intermodality attention flow for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-modality latent interaction network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04289</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bk0MRI5lg" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">exbert: A visual analysis tool to explore learned representations in transformers models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05276</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gqa: a new dataset for compositional question answering over real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09956</idno>
		<title level="m">Pythia v0. 1: the winning entry to the vqa challenge 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1564" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m">Crosslingual language model pretraining</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cycle-consistency for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01766</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">353</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multimodal unified attention networks for vision-and-language interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04107</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5947" to="5959" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
