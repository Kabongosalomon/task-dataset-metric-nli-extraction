<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Saliency for Object Segmentation from Image Level Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><surname>Seong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics ‡ Amsterdam Machine Learning Lab Saarland Informatics</orgName>
								<orgName type="institution">Campus University of Amsterdam Saarbrücken</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country>Germany, the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics ‡ Amsterdam Machine Learning Lab Saarland Informatics</orgName>
								<orgName type="institution">Campus University of Amsterdam Saarbrücken</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country>Germany, the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics ‡ Amsterdam Machine Learning Lab Saarland Informatics</orgName>
								<orgName type="institution">Campus University of Amsterdam Saarbrücken</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country>Germany, the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics ‡ Amsterdam Machine Learning Lab Saarland Informatics</orgName>
								<orgName type="institution">Campus University of Amsterdam Saarbrücken</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country>Germany, the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics ‡ Amsterdam Machine Learning Lab Saarland Informatics</orgName>
								<orgName type="institution">Campus University of Amsterdam Saarbrücken</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country>Germany, the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<email>mfritz@mpi-inf.mpg.deschiele@mpi-inf.mpg.de†</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics ‡ Amsterdam Machine Learning Lab Saarland Informatics</orgName>
								<orgName type="institution">Campus University of Amsterdam Saarbrücken</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country>Germany, the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics ‡ Amsterdam Machine Learning Lab Saarland Informatics</orgName>
								<orgName type="institution">Campus University of Amsterdam Saarbrücken</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country>Germany, the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata@uva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics ‡ Amsterdam Machine Learning Lab Saarland Informatics</orgName>
								<orgName type="institution">Campus University of Amsterdam Saarbrücken</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country>Germany, the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics ‡ Amsterdam Machine Learning Lab Saarland Informatics</orgName>
								<orgName type="institution">Campus University of Amsterdam Saarbrücken</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country>Germany, the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Saliency for Object Segmentation from Image Level Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There have been remarkable improvements in the semantic labelling task in the recent years. However, the state of the art methods rely on large-scale pixel-level annotations. This paper studies the problem of training a pixel-wise semantic labeller network from image-level annotations of the present object classes. Recently, it has been shown that high quality seeds indicating discriminative object regions can be obtained from image-level labels. Without additional information, obtaining the full extent of the object is an inherently ill-posed problem due to co-occurrences. We propose using a saliency model as additional information and hereby exploit prior knowledge on the object extent and image statistics. We show how to combine both information sources in order to recover 80% of the fully supervised performance -which is the new state of the art in weakly supervised training for pixel-wise semantic labelling. The code is available at https://goo.gl/KygSeb.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic image labelling provides rich information about scenes, but comes at the cost of requiring pixel-wise labelled training data. The accuracy of convnet-based models correlates strongly with the amount of available training data. Collection and annotation of data have become a bottleneck for progress. This problem has raised interest in exploring partially supervised data or different means of supervision, which represents different tradeoffs between annotation efforts and yields in terms of supervision signal for the learning task. For tasks like semantic segmentation there is a need to investigate the minimal supervision to reach the quality comparable to the fully supervised case.</p><p>A reasonable starting point considers that all training  images have image-level labels to indicate the presence or absence of the classes of interest. The weakly supervised learning problem can be seen as a specific instance of learning from constraints <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b46">47]</ref>. Instead of explicitly supervising the output, the available labels provide a constraint on the desired output. If an image label is absent, no pixel in the image should take that label; if an image label is present at least in one pixel the image must take that label. However, the objects of interest are rarely single pixel. Thus to enforce larger output regions size, shape, or appearance priors are commonly employed (either explicitly or implicitly). Another reason for exploiting priors, is the fact that the task is fundamentally ambiguous. Strongly co-occurring categories (such as train and rails, sculls and oars, snowbikes and snow) cannot be separated without additional information. Because additional information is needed to solve the task, previous work have explored different avenues, including class-specific size priors <ref type="bibr" target="#b30">[31]</ref>, crawling additional images <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b45">46]</ref>, or requesting corrections from a human judge <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Despite these efforts, the quality of the current best results on the task seems to level out at ∼ 75% of the fully supervised case. Therefore, we argue that additional in-formation sources have to be explored to complement the image level label supervision -in particular addressing the inherent ambiguities of the task. In this work, we propose to exploit class-agnostic saliency as a new ingredient to train for class-specific pixel labelling; and show new state of the art results on Pascal VOC 2012 semantic labelling with image label supervision.</p><p>We decompose the problem of object segmentation from image labels into two separate ones: finding the object location (any point on the object), and finding the object's extent. Finding the object extent can be equivalently seen as finding the background area in an image.</p><p>For object location we exploit the fact that image classifiers are sensitive to the discriminative areas of an image. Thus training using the image labels enables to find high confidence points over the objects classes of interest (we call these "object seeds"), as well as high confidence regions for background. A classifier however will struggle to delineate the fine details of an object instance, since these might not be particularly discriminative.</p><p>For finding the object extent, we exploit the fact that a large portion of photos aim at capturing a subject. Using class-agnostic object saliency we can find the segment corresponding to some of the detected object seeds. Albeit saliency is noisy, it provides information delineating the object extent beyond what seeds can indicate. Our experiment show that this is an effective source of additional information. Our saliency model is itself trained from bounding box annotations only. At no point of our pipeline accurate pixel-wise annotations are used.</p><p>In this paper we provide an analysis of the factors that influence the seeds generation, explore the utility of saliency for the task, and report best known results both when using image labels only and image labels with additional data. In summary, our contributions are:</p><p>• Propose an effective method for combining seed and saliency for weakly supervised semantic segmentation. Our method achieves the best performance among the known works that utilise image level supervision with or without additional external data.</p><p>• Compare recent seed methods side by side, and analyse the importance of saliency towards the final quality. §3 presents our overall architecture, §4 investigates suitable object seeds, and §5 describes how we use saliency to guide the convnet training. Finally §6 discusses the experimental setup, and presents our key results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The last years have seen a renewed interest on weakly supervised training. For semantic labelling, different forms of supervision have been explored: image labels <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b17">18]</ref>, points <ref type="bibr" target="#b2">[3]</ref>, scribbles <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b23">24]</ref>, and bounding boxes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16]</ref>. In this work we focus on image labels as the main form of supervision.</p><p>Object seeds. Multiple works have considered using a trained classifier (from image level labels) to find areas of the image that belong to a given class, without necessarily enforcing to cover the full object extent (high precision, low recall). Starting from simple strategies such as "probing classifier with different image areas occluded" <ref type="bibr" target="#b49">[50]</ref>, or back-propagating the class score gradient on the image <ref type="bibr" target="#b40">[41]</ref>; significantly more involved strategies have been proposed, mainly by modifying the back-propagation strategy <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b39">40]</ref>, or by solving a per-image optimization problem <ref type="bibr" target="#b5">[6]</ref>. All these strategies provide some degree of empirical success but lack a clear theoretical justification, and tend to have rather noisy outputs. Another approach considers modifying the classifier training procedure so as to have it generate object masks as byproduct of a forward-pass. This can be achieved by adding a global a max-pooling <ref type="bibr" target="#b32">[33]</ref> or mean-pooling layer <ref type="bibr" target="#b53">[54]</ref> in the last stages of the classifier. In this work we provide an empirical comparison of existing seeders, and explore variants of the mean-pooling approach <ref type="bibr" target="#b53">[54]</ref> ( §4).</p><p>Pixel labelling from image level supervision. Initial work approached this problem by adapting multiple-instance learning <ref type="bibr" target="#b31">[32]</ref> and expectation-maximization techniques <ref type="bibr" target="#b29">[30]</ref>, to the semantic labelling case. Without additional priors only poor results are obtained. Using superpixels to inform about the object shape helps <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47]</ref> and so does using priors on the object size <ref type="bibr" target="#b30">[31]</ref>. <ref type="bibr" target="#b17">[18]</ref> carefully uses CRFs to propagate the seeds across the image during training, while <ref type="bibr" target="#b35">[36]</ref> exploits segment proposals for this. Most methods compared propose each a new procedure to train a semantic labelling convnet. One exception is <ref type="bibr" target="#b39">[40]</ref> which fuses at test time guided back-propagation <ref type="bibr" target="#b42">[43]</ref> at multiple convnet layers to generate class-wise heatmaps. They do this over a convnet trained for classification. Being based on classifier, their output masks only partially capture the object extents, as reflected in the comparatively low performance <ref type="table" target="#tab_5">(table 3)</ref>. Recognizing the ill-posed nature of the problem, <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b36">[37]</ref> propose to collect user-feedback as additional information to guide the training of a segmentation convnet. The closest work to our approach is <ref type="bibr" target="#b45">[46]</ref>, which also uses saliency as a cue to improve weakly supervised semantic segmentation. There are however a number of differences. First, they use a curriculum learning to expose the segmentation convnet first with simple images, and later with more complex ones. We do not need such curriculum, yet reach better results. Second, they use a manually crafted classagnostic saliency method, while we use a deep learning based one (which provides better cues). Third, their training procedure uses ∼ 40k additional images of the classes of interest crawled from the web; we do not use such classspecific external data. Fourth, we report significantly better results, showing in better light the potential of saliency as additional information to guide weakly supervised semantic object labelling. The seminal work <ref type="bibr" target="#b44">[45]</ref> proposed to use "objectness" map from bounding boxes to guide the semantic segmentation. By using bounding boxes, these maps end up being diffuse; in contrast, our saliency map has sharp object boundaries, thus giving more precise guidance to the semantic labeller.</p><p>Detection boxes from image level supervision. Detecting object boxes from image labels has similar challenges as pixel labelling. The object location and extent need to be found. State of the art techniques for this task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b14">15]</ref> learn to re-score detection proposals using two stream architectures that once trained separate "objectness" scores from class scores. These architecture echo with our approach, where the seeds provide information about the class scores at each pixel (albeit with low recall for foreground classes), and the saliency output provides a per-pixel (class agnostic) "objectness" score.</p><p>Saliency. Image saliency has multiple connotations, it can refer to a spatial probability map of where a person might look first <ref type="bibr" target="#b47">[48]</ref>, a probability map of which object a person might look first <ref type="bibr" target="#b22">[23]</ref>, or a binary mask segmenting the one object a person is most likely to look first <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>. We employ the last definition in this paper. Note that this notion is class-agnostic, and refers more to the composition of the image, than the specific object category. Like most computer vision areas, hand-crafted methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8]</ref> have now been surpassed by convnet based approaches <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref> for object saliency. In this paper we use saliency as an ingredient: improved saliency models would lead to improved results for our method. We describe in §6.1 our saliency model design, trained itself in a weakly supervised fashion from bounding boxes.</p><p>Semantic labelling. Even when pixel-level annotations are provided (fully supervised case), the task of semantic labelling is far from being solved. Multiple convnet architectures have been proposed, including recurrent networks <ref type="bibr" target="#b33">[34]</ref>, encoder-decoders <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1]</ref>, up-sampling layers <ref type="bibr" target="#b26">[27]</ref>, using skip layers <ref type="bibr" target="#b1">[2]</ref>, or dilated convolutions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b48">49]</ref>, to name a few. Most of them build upon classification architectures such as VGG <ref type="bibr" target="#b41">[42]</ref> or ResNet <ref type="bibr" target="#b12">[13]</ref>. For comparison with previous work, our experiments are based on the popular DeepLab <ref type="bibr" target="#b6">[7]</ref> architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Guided Segmentation architecture</head><p>While previous work has explored sophisticated training losses or involved pipelines, we focus on saliency as an  We approach the image-level supervised semantic segmentation problem via a system with two modules (see figure 2), we name this architecture "Guided Segmentation". Given an image and image-level labels, the "guide labeller" module combines cues from a seeder ( §4) and saliency ( §5) sub-modules, producing a rough segmentation mask (the "guide"). Then a segmenter convnet is trained using the produced guide mask as supervision. In this architecture the segmentation convnet is trained in a fully-supervised procedure, using per pixel softmax cross-entropy loss.</p><p>In §4 and 5 we explain how we build our guide labeller, by first generating seeds (discriminative areas of objects of interest), and then extending them to better cover the full object extents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Finding goods seeds</head><p>There has been a recent burst of techniques for localising objects from a classifier. Some approaches rely on image gradients from a trained classifier <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>, while the others propose to train global average pooling (GAP) based classifiers <ref type="bibr" target="#b53">[54]</ref>. Although the classifier based localisation approach has a theoretical limitation that the training objective (image classification) does not match final goal (object locations), they have proved to be effective in practice.</p><p>In this section, we review the seeder techniques side by side and compare their empirical performances. We report empirical results on different GAP architectures <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">GAP</head><p>GAP, or global average pooling layer, can be inserted in the last or penultimate layer of a fully convolutional architecture, which produces a dense prediction, to turn it into a classifier. The resulting architecture is then trained with a classification loss, and at test time the activation maps before the global average pooling layer have been shown to contain localisation information <ref type="bibr">[?]</ref>.</p><p>In our analysis, we consider four different fully convolutional architectures with a GAP layer: GAP-LowRes, GAP-HighRes, GAP-DeepLab, and GAP-ROI. The architectural differences are summarised in table 1, and the full details are provided in the supplementary materials.  <ref type="bibr" target="#b53">[54]</ref> is essentially a fully convolutional version of VGG-16 <ref type="bibr" target="#b41">[42]</ref>. GAP-HighRes, inspired by <ref type="bibr" target="#b17">[18]</ref>, has 2 times higher output resolution than GAP-LowRes. GAP-DeepLab is a state of the art semantic segmenter Dee-pLab with a GAP layer over the dense score output. The main difference between GAP-HighRes and GAP-DeepLab is the presence of dilated convolutions. GAP-ROI is a variant of GAP-HighRes where we use the region of interest pooling to replace the sliding window convolutions in the last layers of VGG-16. GAP-ROI is identical to GAP-HighRes, except for a slight structural variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAP-LowRes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Empirical study</head><p>In this section, we empirically compare the seed methods side by side focusing on their utility for the final semantic segmentation task. Together with GAP methods discussed in the previous section, we consider the back-propagation family: Vanilla, Guided, and Excitation back-propagations <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>. We include the centre mean shape baseline that always outputs the average mask shape; it works as a lower bound on the localisation performance. Evaluation. We evaluate each method on the val set of the Pascal VOC 2012 <ref type="bibr" target="#b10">[11]</ref> segmentation benchmark. We plot the foreground and background precision-recall curves in <ref type="figure" target="#fig_4">figure 3</ref> . In the foreground case, we compute the mean precision and recall over the 20 Pascal categories. We define mean precision (mP) as a summary metric for localisation performance. It averages the foreground precision at 20% recall and the background precision at 80% recall; mP = Prec Fg@20% +Prec Bg@80% 2</p><p>. Intuitively, for the foreground region we only need a small discriminative region, as saliency will fill in the extent; we thus care about precision at ∼ 20% recall. On the other hand, background has more diverse appearance and usually takes a larger region; we thus care about precision at ∼ 80% recall. Since we care about both, we take the average (as for the mAP metric). This metric has shown a good correlation with the final performance in our preliminary experiments.</p><p>We measure the classification performance in the standard mean average precision (mAP) metric. Implementation details. We train all four GAP network variants for multi-label image classification over the trainaug set of Pascal VOC 2012. Full convnet training details are in the supplementary materials. At test time, we take the output per-class heatmaps before the GAP layer and normalise them by the maximal per-class scores.</p><p>For the back-propagation based methods, we obtain image (pseudo-)gradients from the VGG-16 <ref type="bibr" target="#b41">[42]</ref> classifier trained on the trainaug set of Pascal VOC 2012 (10 582 images in total). We take the maximal absolute gradient value across the RGB channels to generate a rough object mask (following <ref type="bibr" target="#b40">[41]</ref>); it is successively smoothed first with vanilla Gaussian kernel and then with dense CRF <ref type="bibr" target="#b18">[19]</ref>.</p><p>In both GAP and backprop variants, we mark pixels with all foreground class scores below τ as background; other pixels are marked according to the argmax foreground class.</p><p>Results. See <ref type="figure" target="#fig_4">figure 3</ref> for the precision-recall curves. GAP variants have overall greater precision than backprop variants at the same recall rate. We note that the Guided back-prop gives highest precision at a very low recall regime (∼ 5%), but the recall is too low to be useful. Among the GAP methods, GAP -HighRes and GAP-ROI give higher precisions over a wide range of recall. GAP-DeepLab shows a significantly lower quality than any other GAP variants.</p><p>Network matters for GAP. <ref type="table" target="#tab_0">Table 1</ref> shows detailed architectural comparisons and classification/localisation performances of the GAP variants. We observe that the network with higher resolution output has better localisation performance (80.7 mP for GAP-HighRes versus 76.5 mP for GAP-LowRes). Dilated convolutions significantly hurt the GAP performance (87.0 mP for GAP-HighRes versus 57.7 mP for GAP-DeepLab). The architectural choice matters a lot for the localisation performance. This contrasts with the classification performances (mAP), which are stable across design choices. Intriguingly, GAP-DeepLab is in fact the best classifier and the worst seeder at the same time; better design choices for classifiers do not lead to better seeders.</p><p>We use GAP-HighRes as the seeder module in the next sections. In <ref type="bibr" target="#b17">[18]</ref>, foreground and background seeds are handled via different mechanisms; in our experiments we treat all the non-foreground region as background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Finding the object extent</head><p>Having generated a set of seeds indicating discriminative object areas, the guide labeller needs to find the extent of the object instances ( §3).</p><p>Without any prior knowledge, it is very hard, if not impossible, to learn the extent of objects only from images and image-level labels. Image-level labels only convey information about commonly occurring patterns that are present in images with positive tags and absent in images with negative tags. The system is thus susceptible to strong inter-class co-occurrences (e.g. train with rail), as well as systematic part occlusions (e.g. feet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRF and CRFLoss.</head><p>A traditional approach to make labels match object boundaries is to solve a CRF inference problem <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref> over the image grid; where the pair-wise terms relate to the object boundaries. CRF can be applied at three stages: (1) on the seeds (crf-seed), (2) as a loss function during segmenter convnet training (crf-loss) <ref type="bibr" target="#b17">[18]</ref>, and (3) as a post-processing at test time (crf-postproc). We have experimented with multiple combinations of those (see supplementary materials section C).</p><p>Albeit some gains are observed, these are inconsistent. For example GAP-HighRes and GAP-ROI provide near identical classification and seeding performance (see <ref type="table" target="#tab_0">table  1</ref>), yet using the same CRF setup provides +13 mIoU percent points in one, but only +7 pp on the other. In comparison our saliency approach will provide +17 mIoU and +18 mIoU for these two networks respectively (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Saliency</head><p>We propose to use object saliency to extract information about the object extent. We work under the assumption that a large portion of the dataset are intentional photographies, which is the case for most datasets crawled from the web such as Pascal <ref type="bibr" target="#b10">[11]</ref> and Coco <ref type="bibr" target="#b24">[25]</ref>. If the image contains a single label "dog", chances are that the image is about a dog, and that the salient object of the image is a dog. We use a convnet based saliency estimator (detailed in §6.1) which adds the benefit of translation invariance. If two locally salient dogs appear in the image, both will be labelled as foreground.</p><p>When using saliency to guide semantic labelling at least two difficulties need to be handled. For one, saliency per-se does not segment object instances. In the example <ref type="figure">figure 4a</ref>, the person-bike is well segmented, but person and bike are not separated. Yet, the ideal Guide labeller <ref type="figure" target="#fig_3">(fig. 2</ref>) should give different labels to these two objects. The second difficulty, clearly visible in the examples of <ref type="figure">figure 4</ref>, is that the salient object might not belong to a category of interest (shirt instead of person in <ref type="figure">figure 4b</ref>) or that the method fails to identify any salient region at all (figure 4c).</p><p>We measure the saliency quality when compared to the ground truth foreground on Pascal VOC 2012 validation set. Albeit our convnet saliency model is better than handcrafted methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b51">52]</ref>, in the end only about 20% of images have reasonably good (IoU &gt; 0.6) foreground saliency quality. Yet, as we will see in §6, this bit of information is already helpful for the weakly supervised learning task.</p><p>Crucially, our saliency system is trained on images containing diverse objects (hundreds of categories), the object categories are treated as "unknown", and to ensure clean experiments we handicap the system by removing any instance of Pascal categories in the object saliency training set. Our saliency model captures a general notion of plausible foreground objects and background areas (more details in section 6.1).</p><p>On every Pascal training image, we obtain a classagnostic foreground/background binary mask from our saliency model, and high precision/low recall class-specific image labels from the seeds model (section 4). We want to combine them in such a way that seed signals are well propagated throughout the foreground saliency mask. We consider two baselines strategies to generate guide labels using saliency but no seeds (G 0 and G 1 ), and then discuss how we combine saliency with seeds (G 2 ). G 0 Random class assignment. Given a saliency mask, we assign all foreground pixels to a class randomly picked from the ground truth image labels. If a single "dog" label is present, then all foreground pixels are "dog". Two labels are present ("dog, cat"), then all pixels are either dog or cat. . Region R f g i will be labelled with the ground truth class with the greatest positive score difference before and after zeroing. G 2 Propagating seeds. Here, instead of assigning the label per connected component R f g i using a classifier, we instead use the seed labels. We also treat the seeds as a set of connected components (seed R s j ). Depending on how the seeds and the foreground regions intersect, we decide the label for each pixel in the guide labeller output.</p><p>Our fusion strategy uses five simple ideas. 1) We treat the seeds as reliable small size point predictors of each object instance, but that might leak outside of the object. 2) We assume the saliency might trigger on objects that are not part of the classes of interest. 3) A foreground connected component R f g i should take the label of the seed touching it, 4) If two (or more) seeds touch the same foreground component, then we want to propagate all the seed labels inside it. 5) When in doubt, mark as ignore. The details for the corner cases are provided in the supplementary material section E. <ref type="figure">Figure 6</ref> provides example results of the different guide strategies. For additional qualitative examples of seeds, saliency foreground, and generated labels, see <ref type="figure" target="#fig_6">figure 7</ref>. With our guide strategies G 0 , G 1 , and G 2 at hand, we now proceed to empirically evaluate them in §6.</p><p>6. Experiments §6 and 6.1 provide the details of the evaluation and our implementation. §6.2 compares our different guide strategies amongst each other, and §6.3 compares with previous work on weakly supervised semantic labelling from image-level labels.</p><p>Evaluation. We evaluate our image-level supervised semantic segmentation system on the PASCAL VOC 2012 segmentation benchmark <ref type="bibr" target="#b10">[11]</ref>. We report all the intermediate results on the val set (1 449 images) and only report the final system result on the test set (1 456 images). Evaluation metric is the standard mean intersection-over-union (mIoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation details</head><p>For training seeder and Segmenter networks, we use the ImageNet <ref type="bibr" target="#b9">[10]</ref> pretrained models for initialisation and finetune on the Pascal VOC 2012 trainaug set (10 582 images), an extension of the original train set (1 464 images) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. This is the same procedure used by previous work on fully <ref type="bibr" target="#b6">[7]</ref> and weakly supervised learning <ref type="bibr" target="#b17">[18]</ref>. Seeder. Results in tables 2 and 3 are obtained using GAP-HighRes (see §4), trained for image classification on the Pascal trainaug set. The test time foreground threshold τ is set to 0.2, following the previous literature <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b17">18]</ref>. G 1 Classifier. The guide labeller strategy G 1 uses an image classifier trained on Pascal trainaug set. We use the VGG-16 architecture <ref type="bibr" target="#b41">[42]</ref> with a multi-label loss. Saliency. Following <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref> we re-purpose a semantic labelling network for the task of class-agnostic saliency. We train the DeepLab-v2 ResNet <ref type="bibr" target="#b6">[7]</ref>   <ref type="figure">Figure 6</ref>: Guide labelling strategies example results. The image, its labels ("bicycle, chair"), seeds, and saliency map are their input. White overlay indicates "ignore" pixel label.</p><formula xml:id="formula_0">(b) Ground truth (c) Seed (d) Saliency (e) G 0 (f) G 1 (g) G 2</formula><p>saliency model to output pixel-wise masks, we follow <ref type="bibr" target="#b15">[16]</ref>. We generate segments from the MSRA boxes by applying grabcut over the average box annotation, and use these as supervision for the DeepLab model. The model is trained as a binary semantic labeller for foreground and background regions. The trained model generates masks like the ones shown in <ref type="figure">figure 5</ref>. Although having been trained with images with single salient objects, due to its convolutional nature the network can predict multiple salient regions in the Pascal images (as shown in <ref type="figure" target="#fig_6">figure 7)</ref>. At test time, the saliency model generates a heatmap of foreground probabilities. We threshold at 50% of the maximal foreground probability to generate the mask. Segmenter. For comparison with previous work we use the DeepLabv1-LargeFOV <ref type="bibr" target="#b6">[7]</ref> architecture as our segmenter convnet. The network is trained on Pascal trainaug set with 10 582 images, using the output of the guide labeller ( §2), which uses only the image itself and presence-absence tags of the 20 Pascal categories as supervision. The network is trained for 8k iterations. Following the standard DeepLab procedure, at test time we up-sample the output to the original image resolution and apply the dense CRF inference <ref type="bibr" target="#b18">[19]</ref>. Unless stated otherwise, we use the CRF parameters used for DeepLabv1-LargeFOV <ref type="bibr" target="#b6">[7]</ref>.</p><p>Additional training details and hyper-parameters are given in the supplementary materials section F. <ref type="table" target="#tab_3">Table 2</ref> compares different guide strategies G 0 , G 1 , G 2 , and oracle versions of G 2 . The first row shows the result of training our segmenter using the seeds directly as guide labels. This leads to poor quality (38.7 mIoU). The "Supervision" column shows recall and precision for foreground and background of the guide labels themselves (training data for the segmenter). We can see that the seeds alone have low recall for the foreground (37%). In comparison, using saliency only, G 0 reaches significantly better results, due to the higher foreground recall (52%), at a comparable precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ingredients study</head><p>Adding a classifier on top of the saliency (G 0 → G 1 ) provides only a negligible improvement <ref type="bibr">(45.8 → 46.2)</ref>. This can be attributed the fact that many Pascal images con-  tain only a single foreground class, and that the classifier might have difficulties recognizing the masked objects. Interestingly, when using a similar classifier to generate seeds instead of scoring the image (G 1 → G 2 ) we gain 5 pp (percent points, 46.2 → 51.2). This shows that the details of how a classifier is used can make a large difference. <ref type="table" target="#tab_3">Table 2</ref> also reports a saliency oracle case on top of G 2 . If we use the ground truth annotation to generate an ideal saliency mask, we see a significant improvement over G 2 (51.2 → 56.9). Thus, the quality of saliency is an important ingredient, and there is room for further gains. <ref type="table" target="#tab_5">Table 3</ref> compares our results with previous related work. We group results by methods that only use ImageNet pre-training and image-level labels (I, P, E; see legend table 3), and methods that use additional data or userinputs. Here our G 0 and G 2 results include a CRF postprocessing (crf-postproc). We also experimented with crf-loss but did not find a parameter set that provided improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head><p>We see that the guide strategies G 0 , which uses saliency and random ground-truth label, reaches competitive performance compared to methods using I+P only. This shows that saliency by itself is already a strong cue. Our guide strategy G 2 (which uses seeds and saliency) obtains the best reported results on this task <ref type="bibr" target="#b0">1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have addressed the problem of training a semantic segmentation convnet from image labels. Image labels alone can provide high quality seeds, or discriminative object regions, but learning the full object extents is a hard problem. We have shown that saliency is a viable option for feeding the object extent information. The proposed Guided Segmentation architecture ( §3), where the "guide labeller" combines cues from the seeds and saliency, can successfully train a segmentation convnet to achieve the state of the art performance. Our weakly supervised results reach 80% of the fully supervised case.</p><p>We expect that a deeper understanding of the seeder methods and improvements on the saliency model can lead to further improvements. sider these results comparable since they use the MCG scores <ref type="bibr" target="#b34">[35]</ref>, which are trained on the ground truth Pascal segments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Content</head><p>This document contains the following additional details:</p><p>• GAP -Architecture details for GAP-LowRes, GAP-HighRes, GAP-DeepLab, GAP-ROI.</p><p>-Training details.</p><p>-Qualitative results.</p><p>• CRF experiments and CRF parameters used.</p><p>• More qualitative results of our saliency model.</p><p>• Details of G 2 guide labeller rules, and more qualitative examples of G 0 , G 1 , and G 2 strategies.</p><p>• Convnet training details for Seeder, Classifier, and Segmenter networks.  <ref type="table" target="#tab_6">table 6</ref> for the details of the GAP networks used. For GAP-ROI, we insert the GAP layer after the final linear layer, instead of after the penultimate layer as suggested by <ref type="bibr" target="#b53">[54]</ref>. We note that the resulting functions are identical (and hence the forward and backward passes): GAP is a linear sum over the spatial dimensions, and the final layer performs a linear combination over the channel dimensions, so they can be swapped without changing the function. Also in practice, we find that there is only negligible difference in performance between the variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Training GAP</head><p>All GAP network variants are trained with stochastic gradient descent (SGD) with minibatch size 15, momentum 0.9, weight decay 5 × 10 −4 , and base learning rate 0.001, decreased by the factor of 10 at every 2 000 iterations. The training stops at 8 000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Qualitative examples</head><p>See <ref type="figure">figure 8</ref> for qualitative examples. We observe that GAP-LowRes, GAP-HighRes, and GAP-ROI show qualitatively similar results, while GAP-DeepLab has significantly low quality with repeating patterns in the output. The output suggests that the learned filters have repeating patterns modulo ≈ 12 output pixels, which is the width of the dilated filters in DeepLab-LargeFOV <ref type="bibr" target="#b6">[7]</ref> on conv5 features. <ref type="table">See table 4</ref> for the segmenter performance after applying different combinations of CRF units, crf-seed, crf-loss, and crf-postproc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CRF</head><p>Combination of crf-loss and crf-postproc on the GAP-HighRes seed gives 50.4 mIoU, giving 12.9 mIoU boost over the vanilla seed. However, we do not see such a gain when either the CRF parameters or the seed type is changed. When CRF parameters are changed from v 1 to v 2 , both of which are reasonable choices (see §C.1), we lose 5.2 mIoU. When the seed type is changed from GAP-HighRes to GAP-ROI, we lose 5.4 mIoU. The 12.9 mIoU boost thus seems fragile.</p><p>Our saliency-based model, on the other hand, gives a consistent ≥ 4 mIoU gain over the best CRF combination, regardless of the seed type used, showing superiority over CRF both in terms of performance and stability. It is possible to combine crf-loss and saliency, but our preliminary experiments show that it hurts the performance of the saliency-only case. Thus, crf-loss is excluded from our final model.</p><p>See <ref type="table">table 5</ref> for all the combinations considered in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. CRF parameters</head><p>Throughout the paper, we use the CRF parameters from the DeepLab-LargeFOV model <ref type="bibr" target="#b6">[7]</ref>, unless stated otherwise. The parameters are given by w (1) = 4, θ α = 121, and θ β = 5 for the appearance kernel, and w (2) = 3 and θ γ = 3 for the smoothness kernel, following the notation of equation 3 in <ref type="bibr" target="#b18">[19]</ref>. For compatibility, we always use the Potts model:</p><formula xml:id="formula_1">µ(x i , x j ) = 1 xi=xj .</formula><p>For some experiments, we also use parameters from <ref type="bibr" target="#b17">[18]</ref>, which uses w (1) = 10, θ α = 80, and θ β = 13 for the appearance kernel, and w (2) = 3 and θ γ = 3 for the smoothness kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Saliency</head><p>See <ref type="figure">figure 9</ref> for more examples of the MSRA training samples for our weakly supervised saliency model. Samples corresponding to Pascal categories are excluded from the training. See <ref type="figure" target="#fig_1">figure 10</ref> for qualitative examples of our saliency model on the Pascal images. We observe that the saliency model does fail in examples usually when the central salient object is not Pascal category, or when the scene is cluttered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. G 2 guide labeller algorithm</head><p>We introduce details of the algorithm for G 2 strategy of combining seed and saliency signals ( §5.3 of the main paper). As mentioned in the main paper, we follow five simple ideas:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Convnet training details</head><p>Saliency. The network is DeepLab-v2 ResNet, and follows the training procedure for DeepLab-v2 ResNet in <ref type="bibr" target="#b6">[7]</ref>. Segmenter. The network is DeepLab-v1, and is trained with stochastic gradient descent (SGD) with minibatch size 15, momentum 0.9, weight decay 5 × 10 −4 , and base learning rate 0.001, decreased by the factor of 10 at every 2 000 iterations. The training stops at 8 000 iterations. Classifiers. All classifiers discussed in the paper are VGG-16 trained with stochastic gradient descent (SGD) with minibatch size 40, momentum 0.9, weight decay 5 × 10 −4 , and base learning rate 0.001, decreased by the factor of 10 at every 5 000 iterations. The training stops at 30 000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Qualitative examples</head><p>See <ref type="figure" target="#fig_1">figure 12</ref> and 13 for more qualitative examples of the seeds, saliency, G 2 guide labeller output, and Guided Segmentation trained results on the training set. Seeds have high precision and low recall. The saliency foreground mask gives a pixel-wise class-agnostic object extent information. G 2 guide labeller combines both sources to generate an accurate class-wise guide labelling. The generated guide labelling can still be noisy especially if the quality of the saliency mask is low. However, the segmenter convnet averages out the noisy supervision to produce more precise predictions. CRF post-processing further refines the predictions.    <ref type="figure">figure 6</ref> in the main paper. Example results for three different guide labelling strategies, G 0 , G 1 , and G 2 . The image, its image labels, seeds, and saliency map are their input. White labels indicate "ignore" regions. Note that G 0 and G 1 give qualitatively similar results, while G 2 produces much more precise labelling by exploiting rich localisation information from the seeds. Examples are chosen at random. More qualitative examples of the different stages of the Guided Segmentation system on the training images. White labels are "ignore" regions. Seeds have high precision and low recall; combined with saliency foreground mask using G 2 guide labeller, object extents are recovered. The generated guide labelling can still be noisy; however, the segmenter convnet can average out the noise to produce more precise predictions. CRF post-processing further refines the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seeds</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>We train a semantic labelling network with (a) image-level labels and (b) saliency masks, to generate (c) a pixel-wise labelling of object classes at test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>High level Guided Segmentation architecture. effective prior knowledge, and thus keep our architecture simple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Precision-recall curves for different seeds. Foregrounds curves show the average precision and recall of the 20 foreground classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :G 1</head><label>451</label><figDesc>Example of our saliency map results on Pascal VOC 2012 data. Example of saliency results on its training data. We use MSRA box annotations to train a weakly supervised saliency model. Note that the MSRA subset employed does not contain Pascal categories. Per-connected component classification. Given a saliency mask, we split it in components, and assign a separate label for each component. The per-component labels are given using a full-image classifier trained using the image labels (classifier details in §6.1). Given a connected component mask R f g i (with pixel values 1: foreground, 0: background), we compute the classifier scores when feeding the original image (I), and when feeding an image with background zeroed (I R f g i )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative examples of the different stages of our system. Additional examples in the supplementary materials figures 12 and 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>•</head><label></label><figDesc>Additional qualitative examples like figure 7 in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Qualitative examples of GAP output for GAP-LowRes, GAP-HighRes, GAP-DeepLab, and GAP-ROI. Note that all of them, except for GAP-DeepLab, are qualitatively similar. For GAP-DeepLab, we observe repeating patterns of certain stride. Examples are chosen at random.Salient objects with boxesSaliency model result Salient objects with boxes Saliency model result Extension offigure 5in the main paper. Examples of saliency results on its training data. We use MSRA box annotations to train a weakly supervised saliency model. Note that the MSRA subset employed is not biased towards the Pascal categories. Examples are chosen at random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Extension of figure 4 in the main paper. Example of saliency results on Pascal images. We note that the saliency often fails when the central, salient objects are non-Pascal or when the scene is cluttered. Examples are chosen at random. Extension of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Extension of figure 7 in the main paper. Qualitative examples of the different stages of the Guided Segmentation system on the training images. White labels are "ignore" regions. Seeds have high precision and low recall; combined with saliency foreground mask using G 2 guide labeller, object extents are recovered. The generated guide labelling can still be noisy; however, the segmenter convnet can average out the noise to produce more precise predictions. CRF post-processing further refines the predictions. Extension of figure 7 in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Architectural comparisons among GAP variants together with classification (mAP) and localisation (mP; see text for details) performances. We compare the output resolution (high res.), use of the dilated convolutions (dil. conv.), and the region of interest pooling (ROI pool).</figDesc><table><row><cell>GAP</cell><cell cols="4">-LowRes -HighRes -ROI -DeepLab</cell></row><row><cell>high res. dil. conv. ROI pool</cell><cell>[54] % % %</cell><cell>[18] % %</cell><cell>%</cell><cell>[7] %</cell></row><row><cell>mP</cell><cell>76.5</cell><cell>80.7</cell><cell>80.8</cell><cell>57.7</cell></row><row><cell>mAP</cell><cell>88.0</cell><cell>87.0</cell><cell>87.2</cell><cell>92.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>over a subset of MSRA<ref type="bibr" target="#b25">[26]</ref>, a saliency dataset with class agnostic bounding box annotations. We constrain the training only to samples of non-Pascal categories. Thus, the saliency model does not leverage class specific features when Pascal images are fed. Out of 25k MSRA images, 11 041 remain after filtering.MRSA provides bounding boxes (from multiple annotators) of the main salient element of each image. To train the</figDesc><table><row><cell>(a) Image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different guide labeller variants.</figDesc><table /><note>Pascal VOC 2012 validation set results, without CRF post- processing. Fg/Bg P/R: are foreground/background preci- sion and recall of the guide labels. Discussion in §6.2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of state of the art methods, on Pascal VOC 2012 val. and test set. FS%: fully supervised percent.</figDesc><table><row><cell>loop micro-</cell></row><row><cell>annotations.</cell></row><row><cell>methods using saliency (STC) or using additional human</cell></row><row><cell>annotations (MicroAnno, CheckMask). Compared to a</cell></row><row><cell>fully supervised DeepLabv1 model, our results reach 80%</cell></row><row><cell>of the fully supervised quality.</cell></row></table><note>Ingredients: I: ImageNet classification pre-training, P: Pas- cal image level tags, P f ull : fully supervised case (pixel wise labels), En : n extra images with image level tags, S: sali- ency, Z: per-class size prior, µ: human-in-the-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Detailed architecture of the GAP networks. Triplets denote (channel, height, width) for the input and output data, and (output channel dim, kernel height, kernel width) for the layer parameters. st =stride and dil =width of dilated convolution, with default values 1 for both, unless otherwise stated.</figDesc><table><row><cell>Layers</cell><cell>VGG-16</cell><cell cols="2">GAP-LowRes GAP-HighRes</cell><cell>GAP-ROI</cell><cell>GAP-DeepLab</cell></row><row><cell></cell><cell>[42]</cell><cell>[54]</cell><cell>[18]</cell><cell></cell><cell>[7]</cell></row><row><cell cols="4">input (C, H, W) (3, 224, 224) (3, 321, 321) (3, 321, 321)</cell><cell>(3, 321, 321)</cell><cell>(3, 321, 321)</cell></row><row><cell>2 × conv1</cell><cell>(64, 3, 3)</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell></row><row><cell></cell><cell>pad = 1</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell></row><row><cell>pool1</cell><cell>(-, 2, 2)</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell><cell>(-, 3, 3)</cell></row><row><cell></cell><cell>pad = 0</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>st = 2, pad = 1</cell></row><row><cell>2 × conv2</cell><cell>(128, 3, 3)</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell></row><row><cell></cell><cell>pad = 1</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell></row><row><cell>pool2</cell><cell>(-, 2, 2)</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell><cell>(-, 3, 3)</cell></row><row><cell></cell><cell>pad = 0</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>st = 2, pad = 1</cell></row><row><cell>3 × conv3</cell><cell>(256, 3, 3)</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell></row><row><cell></cell><cell>pad = 1</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell></row><row><cell>pool3</cell><cell>(-, 2, 2)</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell><cell>(-, 3, 3)</cell></row><row><cell></cell><cell>pad = 0</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>st = 1, pad = 1</cell></row><row><cell>3 × conv4</cell><cell>(512, 3, 3)</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell></row><row><cell></cell><cell>pad = 1</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell></row><row><cell>pool4</cell><cell>(-, 2, 2)</cell><cell>Same as</cell><cell>None</cell><cell>None</cell><cell>(-, 3, 3)</cell></row><row><cell></cell><cell>pad = 0</cell><cell>VGG-16</cell><cell></cell><cell></cell><cell>st = 1, pad = 1</cell></row><row><cell>3 × conv5</cell><cell>(512, 3, 3)</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell><cell>(512, 3, 3)</cell></row><row><cell></cell><cell>pad = 1</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>dil = 2, pad = 2</cell></row><row><cell>pool5</cell><cell>(-, 2, 2)</cell><cell>None</cell><cell>None</cell><cell>ROI-pool</cell><cell>None</cell></row><row><cell></cell><cell>pad = 0</cell><cell></cell><cell></cell><cell>3 × 3 windows</cell><cell></cell></row><row><cell>fc6</cell><cell>(4096, 7, 7)</cell><cell>(1024, 3, 3)</cell><cell>(1024, 3, 3)</cell><cell>(1024, 1, 1)</cell><cell>(1024, 3, 3)</cell></row><row><cell></cell><cell>pad = 0</cell><cell>pad = 1</cell><cell>pad = 1</cell><cell>pad = 0</cell><cell>dil = 12, pad = 12</cell></row><row><cell>fc7</cell><cell>(4096, 1, 1)</cell><cell>None</cell><cell>(1024, 3, 3)</cell><cell>(1024, 1, 1)</cell><cell>(1024, 1, 1)</cell></row><row><cell></cell><cell>pad = 0</cell><cell></cell><cell>pad = 1</cell><cell>pad = 0</cell><cell>pad = 0</cell></row><row><cell>GAP</cell><cell>None</cell><cell>GAP</cell><cell>GAP</cell><cell>None (used after fc8)</cell><cell>GAP</cell></row><row><cell>fc8</cell><cell>(20, 1, 1)</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell><cell>Same as</cell></row><row><cell></cell><cell>pad = 0</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell><cell>VGG-16</cell></row><row><cell>output heatmap</cell><cell>(20, 1, 1)</cell><cell>(20, 21, 21)</cell><cell>(20, 41, 41)</cell><cell>(20, 41, 41)</cell><cell>(20, 41, 41)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"><ref type="bibr" target="#b35">[36]</ref> also reports 54.3 validation set results, however we do not con-</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the German Research Foundation (DFG CRC 1223).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. We treat seeds as reliable small size point predictors of each object instance.</p><p>2. We assume the saliency might trigger on objects that are not part of the classes of interest.</p><p>3. If a seed touches a connected component R f g i ,it should take the label of the seed.</p><p>4. If two (or more) seeds touch the same foreground component, then we want to propagate all the seed labels inside it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>When in doubt, mark as ignore.</p><p>The detailed procedure is given as follows.</p><p>We compute the set of connected components of the saliency foreground mask with area ≥ 1% of the image size, {R f g i } i , and similarly for the set of connected components of the seeds, {R s j } j . For each R f g i , we assign a ground truth label on it depending on how many foreground seed categories it intersects with:</p><p>• 0 category: R f g i is then either a false positive from the saliency (e.g. salient object that is not part of the classes of interest), or a false negative from the seeds. We don't commit to any of those cases by marking with "ignore" label.</p><p>• 1 category: R f g i is delineating the full extent of the instance for the seed. Put the class label from the seed.</p><p>• ≥ 2 categories: R f g i is a combination of instances from multiple classes. Use dense CRF inference inside R f g i , with unaries set by the seed(s), to assign precise pixel-wise labels in R f g i .</p><p>After assigning pixel-wise labels on each R f g i , we perform the following operations regarding the seed connected components R s j :</p><p>• When a seed R s j intersects with some R f g i , but is not strictly covered by R f g i , we put "ignore" labels on the seed region bleeding out of R f g i , assuming that the saliency mask provides a better delineation of the object.</p><p>• If a seed R s j touches two or more foreground regions, it will propagate its label to all of them.</p><p>• Whenever there is an isolated seed R s j not intersecting with any R f g i , we treat it as a reliable foreground prediction missed by saliency, and include it in the final guide labelling.</p><p>See <ref type="figure">figure 11</ref> for the qualitative examples of guide labelling strategies, G 0 , G 1 , and G 2 . Note that G 2 produces much more precise labelling with the access to rich localisation information from the seeds.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pixelnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06694</idno>
		<title level="m">Towards a general pixel-level architecture</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02106</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.4" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Weakly supervised semantic labelling and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07485</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving weaklysupervised object localization by micro-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. 2011. 4, 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional random fields: probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What makes a patch distinct</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kraehenbuehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00848</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Augmented feedback in semantic segmentation under image level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Built-in foreground/background prior for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S A</forename><surname>Akbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convexification of learning from constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shcherbatyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distinct class-specific saliency maps for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with a multi-image model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.03150</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to segment under various forms of weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Can saliency map models predict human egocentric visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Minimum barrier salient object detection at 80 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mȇch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision(ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
