<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Agnostic Lane Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hou-Yuenan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Agnostic Lane Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>FIRST YEAR REPORT 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lane detection is an important yet challenging task in autonomous driving, which is affected by many factors, e.g., light conditions, occlusions caused by other vehicles, irrelevant markings on the road and the inherent long and thin property of lanes. Conventional methods typically treat lane detection as a semantic segmentation task, which assigns a class label to each pixel of the image. This formulation heavily depends on the assumption that the number of lanes is pre-defined and fixed and no lane changing occurs, which does not always hold. To make the lane detection model applicable to an arbitrary number of lanes and lane changing scenarios, we adopt an instance segmentation approach, which first differentiates lanes and background and then classify each lane pixel into each lane instance. Besides, a multi-task learning paradigm is utilized to better exploit the structural information and the feature pyramid architecture is used to detect extremely thin lanes. Three popular lane detection benchmarks, i.e., TuSimple, CULane and BDD100K, are used to validate the effectiveness of our proposed algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Lane detection <ref type="bibr" target="#b0">[1]</ref> plays a pivotal role in autonomous driving because lanes could serve as significant cues for constraining the maneuver of vehicles on roads.</p><p>However, lane detection is challenging since it is affected by many factors, e.g., light conditions, occlusions caused by other vehicles, the existence of irrelevant markings on the road and the inherent long and thin property of lanes.</p><p>Conventional methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr">[4]</ref> usually utilize handcrafted features to extract lane segments and can perform quite well in the highway driving scenarios. However, these approaches need a good selection of features and have poor generalization ability. Therefore, they cannot be applied to scenarios with varying light conditions and road types. The emergence of deep learning has brought new insights into the task and Convolutional Neural Network (CNN) based methods begin to gain popularity <ref type="bibr" target="#b5">[8]</ref>, <ref type="bibr" target="#b8">[11]</ref>, <ref type="bibr">[5]</ref>, <ref type="bibr">[3]</ref>, <ref type="bibr" target="#b3">[6]</ref>. The inherent and automatic feature extracting ability of CNN eases the complex feature selection process and partially solves the generalization problems. However, the CNN-based methods perform sub-optimally in urban roads where the lane markings are ambiguous or the lanes are severely occluded. Several schemes have been proposed to handle lane detection in urban roads, e.g., performing message passing to better exploit structural information <ref type="bibr" target="#b8">[11]</ref> or utilizing vanishing points to guide the lane detection task <ref type="bibr" target="#b5">[8]</ref>. These methods can work to some extent but cannot fully solve the problem as they ignore the inherent relationship between the different entities in the driving scenarios. For instance, the areas within two neighbouring lanes (i.e., drivable areas and alternative areas <ref type="bibr" target="#b11">[14]</ref>) can serve as a strong indicator for the existence, shape and position of lanes. Besides, these models tend to fail when encountering an arbitrary number of lanes or lane changing since they model lane detection as the semantic segmentation task and each lane is assigned a pre-defined class. Failing to achieve real-time performance is also a drawback of these approaches <ref type="bibr" target="#b8">[11]</ref>, <ref type="bibr" target="#b5">[8]</ref>.</p><p>Therefore, in this study, we propose to use a multitask learning paradigm to better utilize the structural and contextual information of the driving scenarios. More <ref type="bibr">May</ref>  specifically, besides the traditional lane detection branch, we also borrow the rich structural information from the drivable area detection task and the lane point regression task. The feature pyramid architecture <ref type="bibr" target="#b6">[9]</ref> is also incorporated in our model to handle challenges of detecting extremely thin lanes. To fulfill the real-time requirement, we adopt the light-weight and efficient network, i.e.,</p><p>ENet <ref type="bibr" target="#b9">[12]</ref>, as our backbone. To detect conditions with unfixed number of lanes, we follow <ref type="bibr" target="#b7">[10]</ref> and divide the lane detection task into two sub-tasks. The first one is to generate the binary segmentation map which only differentiates the lanes and the background. The second sub-task is to classify the lane pixels into different lane instances (i.e., treat each lane as an instance). Three popular benchmarks, i.e., TuSimple <ref type="bibr" target="#b10">[13]</ref>, CULane <ref type="bibr" target="#b8">[11]</ref> and BDD100K <ref type="bibr" target="#b11">[14]</ref>, are selected to validate the effectiveness of our proposed algorithm. Since it is an on-going project, we only report preliminary experimental results on TuSimple and CULane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Lane detection is conventionally handled via using specialized and hand-crafted features to obtain lane segments. These segments are further grouped to get the final results <ref type="bibr" target="#b1">[2]</ref>, [4]. These methods are intuitive but have many shortcomings, e.g., requiring complex feature selection process, being lack of robustness and only applicable to relatively easy driving scenarios.</p><p>Recently, deep learning methods <ref type="bibr" target="#b5">[8]</ref>, <ref type="bibr" target="#b8">[11]</ref>, [5], <ref type="bibr">[3]</ref> have been proposed to ease the selection of handcrafted features as well as greatly improve the models' generalization ability. These approaches usually adopt the dense prediction formulation, i.e., treat lane detection as a semantic segmentation task, where each pixel in an image is assigned with a label to indicate whether it belongs to a lane or not. For example, Pan et al <ref type="bibr" target="#b8">[11]</ref> propose SCNN, which combines spatial cues with CNN, to generate multi-channel probability maps where the number of channels equals to the number of lanes.</p><p>However, these methods can only handle scenarios where the number of lanes is pre-defined and fixed, and they often fail when the vehicle is changing lanes. Another drawback is that these approaches could not achieve realtime performance, which impedes them from being used in the real world.</p><p>To overcome these shortcomings, we follow <ref type="bibr" target="#b7">[10]</ref> and model lane detection as an instance segmentation task.</p><p>More specifically, the lane detection task is divided into two sub-tasks. The first sub-task is generating a binary </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Drivable Area Detection</head><p>The target of the drivable area detection branch is to output a segmentation map, indicating which part of the road is drivable (we merge the original alternative areas into the drivable areas to provide denser targets). Standard cross-entropy loss is adopted to train this branch.</p><p>This branch aims at using the boundary of drivable areas to refine the binary segmentation result via providing more structural information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Lane Point Regression</head><p>The  </p><formula xml:id="formula_0">L var = 1 L L c=1 1 N c Nc i=1 [ µ c − x i 2 2 − δ v ] 2 + ,<label>(1)</label></formula><formula xml:id="formula_1">L dist = 1 L(L − 1) L c A =1 L c B =1,c A =c B [δ d − µ c A −µ c B 2 2 ] 2 + ,<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Clustering</head><p>The clustering branch is used to process the output of the lane pixel embedding branch. In the experiments, we set δ d &gt; 6δ v . Therefore, given the output of the lane pixel embedding branch, we can randomly select a pixel as the starting point, and then label all pixels whose distance from the selected pixel is smaller than 2δ v as the same instance. This process is continued until all the lane pixels are assigned to a specific lane instance. Note that this branch does not have any learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Training strategy</head><p>Currently, we adopt a two-stage training strategy. In the first stage, we fix the parameters of branch E and train the branch P, B and D. In the second stage, we fix the parameters of branch P, B and D and train branch E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we will first give a brief introduction to three datasets used for evaluation. Then, preliminary experimental results are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>Table I records the basic information of three lane detection datasets. Note that the last column of <ref type="table" target="#tab_1">Table I</ref> shows that TuSimple and CULane have no more than 5 lanes in a video frame while BDD100K typically has more than 8 lanes in a video frame. Besides, TuSimple is :</p><formula xml:id="formula_2">Accuracy = N pred N gt ,<label>(3)</label></formula><p>where N pred is the number of correctly predicted lane points and N gt is the number of ground-truth lane points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) CULane and BDD100K:</head><p>To judge whether a lane is correctly detected, we treat each lane as a line with fixed pixel width (30 for CULane and 8 for BDD100K) and compute the intersection-over-union (IoU) between labels and predictions. Predictions whose IoUs are larger than 0.5 are considered as true positives (TP). Then, we use F 1 − measure as the evaluation metric formulated as follows:</p><formula xml:id="formula_3">F 1 − measure = 2 × P recision × Recall P recision + Recall ,<label>(4)</label></formula><p>where P recision = T P T P +F P and Recall = T P T P +F N .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Lane detection model</head><p>We choose ENet <ref type="bibr" target="#b9">[12]</ref> as the backbone model (i.e., the encoder and decoder module in <ref type="figure" target="#fig_0">Fig. 1</ref>). Adam <ref type="bibr" target="#b4">[7]</ref> is selected as the optimizer to train our model with an initial learning rate of 5 × 10 −4 .   <ref type="bibr" target="#b8">[11]</ref>. For crossroad, only FP is shown. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Preliminary results on TuSimple and CULane</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>segmentation map which differentiates lanes and the background. The second sub-task is classifying each lane pixel into a lane instance. The light-weight network, i.e., ENet [12] is used as our backbone to achieve real-time performance. What's more, to utilize the structural and contextual information, we adopt a multi-task learning paradigm in which drivable area detection and lane point regression are incorporated into the original lane detection model. Moreover, the feature pyramid architecture is utilized to detect extremely thin lanes.III. METHODOLOGYIn this section, we will give a detailed explanation of our framework as shown inFig. 1. Our model is mainly composed of five components, i.e., the binary segmentation branch (B), the drivable area detection branch (D), the lane point regression branch (P), the lane pixel embedding branch (E) and the clustering branch (C). The encoder and decoder of the first four branches are the same but only the encoder is shared. A. Binary Segmentation The objective of the binary segmentation branch is to generate a binary segmentation map, indicating whether each pixel in the original image belongs to the lanes or May 10An overview of our agnostic lane detection model. not. Since the ground-truth lane labels of three datasets are all lane points, we generate the final targets by connecting lane points into lines (see the final targets inFig. 2). We use standard cross-entropy loss to train this branch. Besides, to solve the class imbalance of lane pixels and background pixels, the loss of background is multiplied by 0.4. Moreover, the feature pyramid architecture<ref type="bibr" target="#b6">[9]</ref> is adopted to detect extremely thin lanes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>objective of this branch is to regress the position of each lane points. Since the lane points are relatively sparse, we use an 11 x 11 kernel to smooth the original lane point maps to get the final targets of this branch.L 2 loss is used to train this branch. This branch aims at refining the output of the binary segmentation branch. D. Lane Pixel Embedding The input of this branch is the lane pixels extracted from the binary segmentation maps. We treat each lane in the image as an instance. The target of this branch is to classify the lane pixels into different lane instances. The core idea is that pixels belonging to the same lane instance should be close to each other while those belonging to different lane instances should be far from each other. We utilize the following equation to compute the clustering loss [10]:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>May 10 ,</head><label>10</label><figDesc>2019 DRAFTwhere L denotes the number of lanes, x i is the embedding of a pixel, N c is the number of elements in cluster c, µ c is the mean embedding of cluster c and [x] + = max(0, x). The first loss term L var is used to keep the distance between pixels belonging to the same lane instance closer than 2δ v . The second loss term L dist is used to keep the distance between different lane clusters farther than δ d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Typical video frames of TuSimple, CULane and BDD100K datasets. relatively easy while CULane and BDD100K are more challenging considering the total number of video frames and road types. B. Evaluation Criterion 1) TuSimple: In TuSimple dataset, we use the official metric (accuracy) as the evaluation criterion. Besides, false positive (F P ) and false negative (F N ) are also listed. The following is the equation to compute accuracy<ref type="bibr" target="#b10">[13]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>A brief description about three lane detection datasets.</figDesc><table><row><cell>Name</cell><cell># Frame</cell><cell>Train</cell><cell>Validation</cell><cell>Test</cell><cell>Resolution</cell><cell>Road Type</cell><cell># Lane ≤ 5 ?</cell></row><row><cell>TuSimple</cell><cell>6, 408</cell><cell>3, 626</cell><cell>-</cell><cell>2, 782</cell><cell>1280 × 720</cell><cell>highway</cell><cell>√</cell></row><row><cell>CULane</cell><cell cols="2">133, 235 88, 880</cell><cell>9, 675</cell><cell cols="3">34, 680 1640 × 590 urban, rural and highway</cell><cell>√</cell></row><row><cell>BDD100K</cell><cell>80, 000</cell><cell>70, 000</cell><cell>-</cell><cell cols="3">10, 000 1280 × 720 urban, rural and highway</cell><cell>×</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Performance of different algorithms onTuSimple testing set.</figDesc><table><row><cell>Algorithm</cell><cell>Accuracy</cell><cell>FP</cell><cell>FN</cell></row><row><cell>SCNN [11]</cell><cell>0.9653</cell><cell>0.0617</cell><cell>0.0180</cell></row><row><cell>LaneNet [10]</cell><cell>0.9638</cell><cell>0.0780</cell><cell>0.0244</cell></row><row><cell>EL-GAN [5]</cell><cell>0.9639</cell><cell>0.0412</cell><cell>0.0336</cell></row><row><cell>ENet (ours)</cell><cell>0.9629</cell><cell>0.0722</cell><cell>0.0218</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II</head><label>II</label><figDesc>The structure of the overall framework, the training strategy as well as the experiments are solely proposed and implemented by me. Besides, I thank Professor Chen Change Loy, Dr. Chunxiao Liu and Dr. Zheng Ma for their helpful discussions during the whole project.</figDesc><table><row><cell></cell><cell>shortcomings of previous methods, our agnostic lane</cell></row><row><cell></cell><cell>detection model is proposed, which utilizes a multi-task</cell></row><row><cell></cell><cell>learning paradigm and the feature pyramid architecture</cell></row><row><cell></cell><cell>to exploit structural and contextual information. We use</cell></row><row><cell></cell><cell>three popular benchmarks, i.e., TuSimple, CULane and</cell></row><row><cell></cell><cell>BDD100K, to validate the effectiveness of the proposed</cell></row><row><cell></cell><cell>algorithm. Preliminary experimental results have shown</cell></row><row><cell></cell><cell>that our model outperforms previous approaches in terms</cell></row><row><cell></cell><cell>of the running time efficiency and the number of pa-</cell></row><row><cell></cell><cell>rameters. However, this is still an on-going project and</cell></row><row><cell></cell><cell>more performance gains will be achieved via a good</cell></row><row><cell></cell><cell>deployment of different components and a more rational</cell></row><row><cell></cell><cell>training strategy.</cell></row><row><cell></cell><cell>VI. DECLARATIONS AND ACKNOWLEDGEMENT</cell></row><row><cell>records the performance of some baselines</cell><cell>I declare that this report is solely written by me</cell></row><row><cell>and our algorithm in the testing set of TuSimple. Since</cell><cell>without help from others. In addition, I am the main con-</cell></row><row><cell>TuSimple is relatively easy and our ENet model has</cell><cell>tributor to the techniques and methodology expounded in</cell></row><row><cell>much fewer parameters compared with SCNN (see Table</cell><cell>this report.</cell></row><row><cell>IV), the performance of our model is satisfying. Table</cell><cell></cell></row><row><cell>III records the performance of some baselines and our</cell><cell></cell></row><row><cell>algorithms in the testing set of CULane. As can be seen</cell><cell></cell></row><row><cell>in Table IV, in terms of the running time efficiency</cell><cell></cell></row><row><cell>and the number of parameters, our algorithm obviously</cell><cell></cell></row><row><cell>outperforms other baselines.</cell><cell></cell></row><row><cell>V. CONCLUSION</cell><cell></cell></row><row><cell>In this study, we first point out the value and main</cell><cell></cell></row><row><cell>challenges of the lane detection task. Then, the strengths</cell><cell></cell></row><row><cell>and weaknesses of both conventional and deep learn-</cell><cell></cell></row><row><cell>ing based methods are presented. To overcome the</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Performance (F 1 -measure) of different algorithms on CULane testing set. † indicates the results are copied from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>The running time and parameters of different algorithms on CULane testing set. Zhe Chen and Zijing Chen. Rbnet: A deep neural network for unified road and road boundary detection. In International Conference on Neural Information Processing, pages 677-687. Hendrik Deusch, Jürgen Wiest, Stephan Reuter, Magdalena Szczot, Marcus Konrad, and Klaus Dietmayer. A random finite set approach to multiple lane detection. In Intelligent Transportation Systems (ITSC), 2012 15th International IEEE Conference on, pages 270-275. IEEE, 2012. [5] Mohsen Ghafoorian, Cedric Nugteren, Nóra Baka, Olaf Booij, and Michael Hofmann. El-gan: Embedding loss driven generative adversarial networks for lane detection. arXiv preprint arXiv:1806.05525, 2018.</figDesc><table><row><cell>Indicator</cell><cell>ENet</cell><cell cols="3">ResNet-18 ResNet-101 SCNN</cell></row><row><cell>Running time (ms)</cell><cell>13.4</cell><cell>25.3</cell><cell>171.2</cell><cell>133.5</cell></row><row><cell>Parameter (M)</cell><cell>0.98</cell><cell>12.41</cell><cell>52.53</cell><cell>20.72</cell></row><row><cell>[3] Springer, 2017.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[4]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">May 10, 2019DRAFT</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gold: A parallel real-time stereo vision system for generic obstacle and lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Broggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="81" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A novel lane detection system with efficient ground truth generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monson</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="365" to="374" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Category ENet ResNet-18 VGG-16 † ReNet † DenseCRF † MRFNet † ResNet-50 †</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to steer by mimicking features from heterogeneous auxiliary networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02759</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tae-Hee Lee, Hyun Seok Hong, Seung-Hoon Han, and In So Kweon. Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namil</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1965" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05591</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06080</idno>
		<title level="m">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for realtime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<ptr target="http://benchmark.tusimple.ai/#/t/1.Accessed" />
		<title level="m">TuSimple</title>
		<imprint>
			<date type="published" when="2018-09-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vashisht</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
