<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ERNIE: Enhanced Representation through Knowledge Integration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
							<email>wangshuohuan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
							<email>liyukun01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
							<email>fengshikun01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
							<email>tianhao@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wuhua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ERNIE: Enhanced Representation through Knowledge Integration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT (Devlin et al., 2018), ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words. Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit. Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language representation pre-training <ref type="bibr" target="#b9">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b2">Devlin et al., 2018)</ref> has been shown effective for improving many natural language processing tasks such as named entity recognition, sentiment analysis, and question answering. In order to get reliable word representation, neural language models are designed to learn word cooccurrence and then obtain word embedding with unsupervised learning. The methods in Word2Vec <ref type="bibr" target="#b9">(Mikolov et al., 2013)</ref> and Glove <ref type="bibr" target="#b10">(Pennington et al., 2014)</ref> represent words as vectors, where similar words have similar word representations. These word representations provide an initialization for the word vectors in other deep learning models. Recently, lots of works such as Cove <ref type="bibr">(Mc-Cann et al., 2017)</ref>, <ref type="bibr">Elmo (Peters et al., 2018)</ref>, GPT <ref type="bibr" target="#b12">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> improved word representation via different strategies, which has been shown to be more effective for down-stream natural language processing tasks.</p><p>The vast majority of these studies model the representations by predicting the missing word only through the contexts. These works do not consider the prior knowledge in the sentence. For example, In the sentence " Harry Potter is a series of fantasy novels written by J. K. Rowling". Harry Potter is a novel name and J. K. Rowling is the writer. It is easy for the model to predict the missing word of the entity Harry Potter by word collocations inside this entity without the help of long contexts. The model cannot predict Harry Potter according to the relationship between Harry Potter and J. K. Rowling. It is intuitive that if the model learns more about prior knowledge, the model can obtain more reliable language representation.</p><p>In this paper, we propose a model called ERNIE (enhanced representation through knowledge integration) by using knowledge masking strategies. In addition to basic masking strategy, we use two kinds of knowledge strategies: phrase-level strategy and entity-level strategy. We take a phrase or a entity as one unit, which is usually composed of several words. All of the words in the same unit are masked during word representation training, instead of only one word or character being masked. In this way, the prior knowledge of phrases and entities are implicitly learned during the training procedure. Instead of adding the knowledge embedding directly, ERNIE implicitly learned the information about knowledge and longer semantic dependency, such as the relationship between entities, the property of a entity and the type of a event, to guide word embedding learning. This can make the model have better generalization and adaptability.</p><p>In order to reduce the training cost of the model, ERNIE is pre-trained on heterogeneous Chinese data, and then applied to 5 Chinese NLP tasks. ERNIE advances the state-of-the-art results on all of these tasks. An additional experiment on the cloze test shows that ERNIE has better knowledge inference capacity over other strong baseline methods.</p><p>Our Contribution are as follows:</p><p>(1) We introduce a new learning processing of language model which masking the units such as phrases and entities in order to implicitly learn both syntactic and semantic information from these units.</p><p>(2) ERNIE significantly outperforms the previous state-of-the art methods on various Chinese natural language processing tasks.</p><p>(3) We released the codes of ERNIE and pre-trained models, which are available in https://github.com/PaddlePaddle/ LARK/tree/develop/ERNIE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Context-independent Representation</head><p>Representation of words as continuous vectors has a long history. A very popular model architecture for estimating neural network language model (NNLM) was proposed in <ref type="bibr" target="#b0">(Bengio et al., 2003)</ref>, where a feed forward neural network with a linear projection layer and a non-linear hidden layer was used to learn the word vector representation.</p><p>It is effective to learn general language representation by using a large number of unlabeled data to pretrain a language model. Traditional methods focused on context-independent word embedding. Methods such as Word2Vec <ref type="bibr" target="#b9">(Mikolov et al., 2013)</ref> and Glove <ref type="bibr" target="#b10">(Pennington et al., 2014)</ref> take a large corpus of text as inputs and produces a word vectors, typically in several hundred dimensions. They generate a single word embedding representation for each word in the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context-aware Representation</head><p>However, a word can have completely different senses or meanings in the contexts. Skip-thought <ref type="bibr" target="#b4">(Kiros et al., 2015)</ref> proposed a approach for unsupervised learning of a generic, distributed sentence encoder. Cove <ref type="bibr" target="#b8">(McCann et al., 2017)</ref> show that adding these context vectors improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks. ULMFit <ref type="bibr" target="#b3">(Howard and Ruder, 2018)</ref> proposed an effective transfer learning method that can be applied to any task in NLP. ELMo <ref type="bibr" target="#b11">(Peters et al., 2018)</ref> generalizes traditional word embedding research along a different dimension. They propose to extract context-sensitive features from a language model. The GPT <ref type="bibr" target="#b12">(Radford et al., 2018)</ref> enhanced the context-sensitive embedding by adapting the Transformer.</p><p>BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> uses two different pretraining tasks for language modeling. BERT randomly masks a certain percentage of words in the sentences and learn to predict those masked words. Moreover, BERT learn to predict whether two sentences are adjacent. This task tries to model the relationship between two sentences which is not captured by traditional language models. Consequently, this particular pretraining scheme helps BERT to outperform state-of-the-art techniques by a large margin on various key NLP datasets such as GLUE <ref type="bibr" target="#b19">(Wang et al., 2018)</ref> and SQUAD <ref type="bibr" target="#b14">(Rajpurkar et al., 2016)</ref> and so on. Some other researchers try to add more information based on these models. MT-DNN <ref type="bibr" target="#b6">(Liu et al., 2019)</ref> combine pre-training learning and multi-task learning to improve the performances over several different tasks in GLUE <ref type="bibr" target="#b19">(Wang et al., 2018)</ref>. GPT-2 <ref type="bibr" target="#b13">(Radford et al., 2019)</ref> adds task information into the pre-training process and adapt their model to zero-shot tasks. XLM <ref type="bibr" target="#b5">(Lample and Conneau, 2019)</ref> adds language embedding to the pre-training process which achieved better results in cross-lingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Heterogeneous Data</head><p>Semantic encoder pre-trained on heterogeneous unsupervised data can improve the transfer learning performance. Universal sentence encoder  adopts heterogeneous training data drawn from Wikipedia, web news, web QA pages and discussion forum. Sentence encoder  based on response prediction benefits from query-response pair data drawn from Reddit conversation. XLM <ref type="bibr" target="#b5">(Lample and Conneau, 2019)</ref> introduce parallel corpus to BERT, which is trained jointly with masked language model task. With transformer model pre-trained on heterogeneous data, XLM shows great performance gain on supervise/unsupervised MT task and classification task.  <ref type="figure">Figure 1</ref>: The different masking strategy between BERT and ERNIE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We introduce ERNIE and its detailed implementation in this section. We first describe the model's transformer encoder,and then introduce the knowledge integration method in Section 3.2. The comparisons between BERT and ERNIE are shown visually in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformer Encoder</head><p>ERNIE use multi-layer Transformer <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref> as basic encoder like previous pretraning model such as GPT, BERT and XLM. The Transformer can capture the contextual information for each token in the sentence via selfattention, and generates a sequence of contextual embeddings.</p><p>For Chinese corpus, we add spaces around every character in the CJK Unicode range and use the WordPiece <ref type="bibr" target="#b20">(Wu et al., 2016)</ref> to tokenize Chinese sentences. For a given token, its input representation is constructed by summing the corresponding token, segment and position embeddings. The first token of every sequence is the special classification embedding([CLS]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge Integration</head><p>we use prior knowledge to enhance our pretrained language model. Instead of adding the knowledge embedding directly, we proposed a multi-stage knowledge masking strategy to integrate phrase and entity level knowledge into the Language representation. The different masking level of a sentence is described in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Basic-Level Masking</head><p>The first learning stage is to use basic level masking, It treat a sentence as a sequence of basic Language unit, for English, the basic language unit is word, and for Chinese, the basic language unit is Chinese Character. In the training process, We randomly mask 15 percents of basic language units, and using other basic units in the sentence as inputs, and train a transformer to predict the mask units. Based on basic level mask, we can obtain a basic word representation. Because it is trained on a random mask of basic semantic units, high level semantic knowledge is hard to be fully modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Phrase-Level Masking</head><p>The second stage is to employ phrase-level masking. Phrase is a small group of words or characters together acting as a conceptual unit. For English, we use lexical analysis and chunking tools to get the boundary of phrases in the sentences, and use some language dependent segmentation tools to get the word/phrase information in other language such as Chinese. In phrase-level mask stage, we also use basic language units as training input, unlike random basic units mask, this time we randomly select a few phrases in the sentence, mask and predict all the basic units in the same phrase. At this stage, phrase information is encoded into the word embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Entity-Level Masking</head><p>The third stage is entity-level masking. Name entities contain persons, locations, organizations, products, etc., which can be denoted with a proper K.</p><p>J.</p><p>[mask]</p><p>[mask]</p><p>[mask]</p><p>[mask]</p><p>[mask]</p><p>[mask]</p><p>[mask] by novels <ref type="bibr">[mask]</ref> [mask]</p><p>[mask] a a [mask] <ref type="figure">Figure 2</ref>: Different masking level of a sentence name. It can be abstract or have a physical existence. Usually entities contain important information in the sentences. As in the phrase masking stage, we first analyze the named entities in a sentence, and then mask and predict all slots in the entities. After three stage learninga word representation enhanced by richer semantic information is obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>ERNIE was chosen to have the same model size as BERT-base for comparison purposes. ERNIE uses 12 encoder layers, 768 hidden units and 12 attention heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Heterogeneous Corpus Pre-training</head><p>ERNIE adopts Heterogeneous corpus for pretraining. Following , we draw the mixed corpus Chinese Wikepedia, Baidu Baike, Baidu news and Baidu Tieba. The number of sentences are 21M, 51M, 47M, 54M. respectively. Baidu Baike contains encyclopedia articles written in formal languages, which is used as a strong basis for language modeling. Baidu news provides the latest information about movie names, actor names, football team names, etc. Baidu Tieba is an open discussion forum like Reddits, where each post can be regarded as a dialogue thread. Tieba corpus is used in our DLM task, which will be discussed in the next section.</p><p>We perform traditional-to-simplified conversion on the Chinese characters, and upper-to-lower conversion on English letters. We use a shared vocabulary of 17,964 unicode characters for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DLM</head><p>Dialogue data is important for semantic representation, since the corresponding query semantics of the same replies are often similar. ERNIE models the Query-Response dialogue structure on the DLM (Dialogue Language Model) task. As shown in figure 3, our method introduces dialogue embedding to identify the roles in the dialogue, which is different from that of universal sentence encoder . ERNIE's Dialogue embedding plays the same roles as token type embedding in BERT, except that ERNIE can also represent multi-turn conversations (e.g. QRQ, QRR, QQR, where Q and R stands for "Query" and "Response" respectively). Like MLM in BERT, masks are applied to enforce the model to predict missing words conditioned on both query and response. What's more, we generate fake samples by replacing the query or the response with a randomly selected sentence. The model is designed to judge whether the multi-turn conversation is real or fake.</p><p>The DLM task helps ERNIE to learn the implicit relationship in dialogues, which also enhances the model's ability to learn semantic representation. The model architecture of DLM task is compatible with that of the MLM task, thus it is pre-trained alternatively with the MLM task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Chinese NLP Tasks</head><p>ERNIE is applied to 5 Chinese NLP tasks, including natural language inference, semantic similarity, named entity recognition, sentiment analysis, and question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Natural Language Inference</head><p>The Cross-lingual Natural Language Inference (XNLI) corpus <ref type="bibr" target="#b6">(Liu et al., 2019</ref>) is a crowdsourced collection for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into 14 languages including Chinese. The labels contains contradiction, neutral and entailment. We follow the Chinese experiments in BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Semantic Similarity</head><p>The Large-scale Chinese Question Matching Corpus (LCQMC) <ref type="bibr" target="#b7">(Liu et al., 2018)</ref> aims at identifying whether two sentences have the same intention. Each pair of sentences in the dataset is associated with a binary label indicating whether the two sentences share the same intention, and the task can be formalized as predicting a binary label. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Name Entity Recognition</head><p>The MSRA-NER dataset is designed for named entity recognition, which is published by Microsoft Research Asia. The entities contains several types including person name, place name, organization name and so on. This task can be seen as a sequence labeling task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Sentiment Analysis</head><p>ChnSentiCorp (Song-bo) is a dataset which aims at judging the sentiment of a sentence. It includes comments in several domains such as hotels, books and electronic computers. the goal of this task is to judge whether the sentence is positive or negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Retrieval Question Answering</head><p>The goal of NLPCC-DBQA dataset ( http: //tcci.ccf.org.cn/conference/ 2016/dldoc/evagline2.pdf) is to select answers of the corresponding questions. The evaluation methods on this dataset include MRR (Voorhees, 2001) and F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiment results</head><p>The test results on 5 Chinese NLP tasks are presented in <ref type="table" target="#tab_3">Table 1</ref>. It can be seen that ERNIE outperforms BERT on all tasks, creating new stateof-the-art results on these Chinese NLP tasks. For the XNLI, MSRA-NER, ChnSentiCorp and nlpccdbqa tasks, ERNIE obtains more than 1% absolute accuracy improvement over BERT. The gain of ERNIE is attributed to its knowledge integration strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>To better understand ERNIE, we perform ablation experiments over every strategy of ERNIE in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Effect of Knowledge Masking Strategies</head><p>We sample 10% training data from the whole corpus to verify the effectiveness of the knowledge masking strategy. Results are presented in <ref type="table" target="#tab_4">Table  2</ref>. We can see that adding phrase-level mask to the baseline word-level mask can improve the performance of the model. Based on this, we add the entity-level masking strategythe performance of the model is further improved. In addition. The results also show that with 10 times larger size of the pre-training dataset, 0.8% performance gain is achieved on XNLI test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Effect of DLM</head><p>Ablation study is also performed on the DLM task. we use 10% of all training corpus with different proportions to illustrate the contributions of DLM task on XNLI develop set. we pre-train ERNIE from scratch on these datasets, and report average result on XNLI task from 5 random restart of fine-tuning. Detail experiment setting and develop set result is presented in <ref type="table" target="#tab_5">Table 3</ref>, We can see that 0.7%/1.0% of improvement in develop/test accuracy is achieved on this DLM task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Cloze Test</head><p>To verify ERNIE's knowledge learning ability, We use several Cloze test samples <ref type="bibr" target="#b16">(Taylor, 1953)</ref> to examine the model. In the experiment, the name entity is removed from the paragraphs and the model need to infer what it is. Some cases are show in <ref type="figure">Figure 4</ref>. We compared the predictions of BERT and ERNIE. In case 1, BERT try to copy the name appeared in the context while ERNIE remembers the knowl-    <ref type="figure">Figure 4</ref>: Cloze test edge about relationship mentioned in the article. In cases 2 and Case 5, BERT can successfully learn the patterns according to the contexts, therefore correctly predicting the named entity type but failing to fill in the slot with the correct entity. on the contrary, ERNIE can fill in the slots with the correct entities. In cases 3, 4, 6, BERT fills in the slots with several characters related to sentences, but it is hard to predict the semantic concept. ERNIE predicts correct entities except case 4. Although ERNIE predicts the wrong entity in Case 4, it can correctly predict the semantic type and fills in the slot with one of an Australian city. In summary, these cases show that ERNIE performs better in context-based knowledge reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presents a novel method to integrate knowledge into pre-training language model. Experiments on 5 Chinese language processing tasks show that our method outperforms BERT over all of these tasks. We also confirmed that both the knowledge integration and pre-training on heterogeneous data enable the model to obtain better language representation. In future we will integrate other types of knowledge into semantic representation models, such as using syntactic parsing or weak supervised signals from other tasks. In addition We will also validate this idea in other languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Results on 5 major Chinese NLP tasks</figDesc><table><row><cell>Task</cell><cell>Metrics</cell><cell>Bert dev test</cell><cell>dev</cell><cell>ERNIE</cell><cell>test</cell></row><row><cell>XNLI</cell><cell cols="5">accuracy 78.1 77.2 79.9 (+1.8) 78.4 (+1.2)</cell></row><row><cell>LCQMC</cell><cell cols="5">accuracy 88.8 87.0 89.7 (+0.9) 87.4 (+0.4)</cell></row><row><cell>MSRA-NER</cell><cell>F1</cell><cell cols="4">94.0 92.6 95.0 (+1.0) 93.8 (+1.2)</cell></row><row><cell cols="6">ChnSentiCorp accuracy 94.6 94.3 95.2 (+0.6) 95.4 (+1.1)</cell></row><row><cell>nlpcc-dbqa</cell><cell>mrr F1</cell><cell cols="4">94.7 94.6 95.0 (+0.3) 95.1 (+0.5) 80.7 80.8 82.3 (+1.6) 82.7 (+1.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">: XNLI performance with different masking strategy and dataset size</cell></row><row><cell cols="2">pre-train dataset size mask strategy</cell><cell cols="2">dev Accuracy test Accuracy</cell></row><row><cell>10% of all</cell><cell>word-level(chinese character)</cell><cell>77.7%</cell><cell>76.8%</cell></row><row><cell>10% of all</cell><cell>word-level&amp;phrase-level</cell><cell>78.3%</cell><cell>77.3%</cell></row><row><cell>10% of all</cell><cell cols="2">word-level&amp;phrase-leve&amp;entity-level 78.7%</cell><cell>77.6%</cell></row><row><cell>all</cell><cell cols="2">word-level&amp;phrase-level&amp;entity-level 79.9 %</cell><cell>78.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>_____ married Cecilia Cheung. They had two sons, the older one is Zhenxuan Xie and the younger one is Zhennan Xie. Chinese gods and demons, which reaching the peak of ancient Romantic novels. It is also known as the four classical works of China with Romance of the Three Kingdoms, Water Margin and Dream of Red Mansions.</figDesc><table><row><cell></cell><cell cols="2">: XNLI finetuning performance with DLM</cell><cell></cell><cell></cell></row><row><cell></cell><cell>corpus proportion(10% of all training data)</cell><cell cols="3">dev Accuracy test Accuracy</cell></row><row><cell></cell><cell>Baike(100%)</cell><cell>76.5%</cell><cell>75.9%</cell><cell></cell></row><row><cell></cell><cell>Baike(84%) / news(16%)</cell><cell>77.0%</cell><cell>75.8%</cell><cell></cell></row><row><cell></cell><cell cols="2">Baike(71.2%)/ news(13%)/ forum Dialogue(15.7%) 77.7%</cell><cell>76.8%</cell><cell></cell></row><row><cell cols="2">No Text</cell><cell>Predict by ERNIE</cell><cell>Predict by BERT</cell><cell>Answer</cell></row><row><cell>1</cell><cell>2006年9月，________与张柏芝结婚，两人婚后育有两儿子--大儿子Lucas谢振</cell><cell>谢霆锋</cell><cell>谢振轩</cell><cell>谢霆锋</cell></row><row><cell></cell><cell>轩，小儿子Quintus谢振南；</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">In September 2006, Tingfeng Xie</cell><cell>Zhenxuan Xie</cell><cell>Tingfeng Xie</cell></row><row><cell>2</cell><cell>戊戌变法，又称百日维新，是________ 、梁启超等维新派人士通过光绪帝进行</cell><cell>康有为</cell><cell>孙世昌</cell><cell>康有为</cell></row><row><cell></cell><cell>的一场资产阶级改良。</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>The Reform Movement of 1898, also known as the Hundred-Day Reform,</cell><cell>Youwei Kang</cell><cell>Shichang Sun</cell><cell>Youwei Kang</cell></row><row><cell></cell><cell>was a bourgeois reform carried out by the reformists such as ____</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>and Qichao Liang through Emperor Guangxu.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>高血糖则是由于________分泌缺陷或其生物作用受损，或两者兼有引起。糖尿病</cell><cell>胰岛素</cell><cell>糖糖内</cell><cell>胰岛素</cell></row><row><cell></cell><cell>时长期存在的高血糖，导致各种组织，特别是眼、肾、心脏、血管、神经的慢</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>性损害、功能障碍。</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Hyperglycemia is caused by defective ______ secretion or impaired</cell><cell>Insulin</cell><cell>(Not a word in</cell><cell>Insulin</cell></row><row><cell></cell><cell>biological function, or both. Long-term hyperglycemia in diabetes</cell><cell></cell><cell>Chinese)</cell><cell></cell></row><row><cell></cell><cell>leads to chronic damage and dysfunction of various tissues,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>especially eyes, kidneys, heart, blood vessels and nerves.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>澳大利亚是一个高度发达的资本主义国家，首都为________。作为南半球经济最</cell><cell>墨尔本</cell><cell>墨悉本</cell><cell>堪培拉</cell></row><row><cell></cell><cell>发达的国家和全球第12大经济体、全球第四大农产品出口国，其也是多种矿产</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>出口量全球第一的国家。</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Australia is a highly developed capitalist country with _______ as</cell><cell>Melbourne</cell><cell cols="2">(Not a city name) Canberra</cell></row><row><cell></cell><cell>its capital. As the most developed country in the Southern</cell><cell></cell><cell></cell><cell>(the capital</cell></row><row><cell></cell><cell>Hemisphere, the 12th largest economy in the world and the fourth</cell><cell></cell><cell></cell><cell>of</cell></row><row><cell></cell><cell>largest exporter of agricultural products in the world, it is also</cell><cell></cell><cell></cell><cell>Australia)</cell></row><row><cell></cell><cell>the world's largest exporter of various minerals.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>________是中国神魔小说的经典之作，达到了古代长篇浪漫主义小说的巅峰，与</cell><cell>西游记</cell><cell>《小》</cell><cell>西游记</cell></row><row><cell></cell><cell>《三国演义》《水浒传》《红楼梦》并称为中国古典四大名著。</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">___________is a classic novel of The Journey to</cell><cell>(Not a word in</cell><cell>The Journey</cell></row><row><cell></cell><cell></cell><cell>the West</cell><cell>Chinese)</cell><cell>to the West</cell></row><row><cell>6</cell><cell>相对论是关于时空和引力的理论，主要由________创立。</cell><cell>爱因斯坦</cell><cell>卡尔斯所</cell><cell>爱因斯坦</cell></row><row><cell></cell><cell>Relativity is a theory about space-time and gravity, which was</cell><cell>Einstein</cell><cell>(Not a word in</cell><cell>Einstein</cell></row><row><cell></cell><cell>founded by _________.</cell><cell></cell><cell>Chinese)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Universal sentence encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m">Crosslingual language model pretraining</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lcqmc: A large-scale chinese question matching corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1952" to="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6294" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/languageunder-standingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan Song-Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chnsenticorp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">cloze procedure: A new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Bulletin</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overview of the trec 2001 question answering track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amapreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning semantic textual similarity from conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pilar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Heming Ge, Yun Hsuan Sung, and Brian Strope</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
