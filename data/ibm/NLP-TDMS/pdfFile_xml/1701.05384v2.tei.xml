<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FusionSeg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Dutt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jain</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
							<email>bxiong@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
							<email>grauman@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FusionSeg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an end-to-end learning framework for segmenting generic objects in videos. Our method learns to combine appearance and motion information to produce pixel level segmentation masks for all prominent objects. We formulate the task as a structured prediction problem and design a two-stream fully convolutional neural network which fuses together motion and appearance in a unified framework. Since large-scale video datasets with pixel level segmentations are lacking, we show how to bootstrap weakly annotated videos together with existing image recognition datasets for training. Through experiments on three challenging video segmentation benchmarks, our method substantially improves the state-of-the-art results for segmenting generic (unseen) objects. Code and pretrained models are available on the project website.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In video object segmentation, the task is to separate out foreground objects from the background across all frames. This entails computing dense pixel level masks for foreground objects, regardless of the object's category-i.e., learned object-specific models must not be assumed. A resulting foreground object segment is a spatio-temporal tube delineating object boundaries in both space and time. This fundamental problem has a variety of applications, including high level vision tasks such as activity and object recognition, as well as graphics areas such as post production video editing and rotoscoping.</p><p>In recent years, video object segmentation has received significant attention, with great progress on fully automatic algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, propagation methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, and interactive methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. We are interested in the fully automated setup, where the system processes the video directly * Both authors contributed equally to this work without any human involvement. Forgoing manual annotations could scale up the processing of video data, yet it remains a very challenging problem. Automatic algorithms not only need to produce accurate space-time boundaries for any generic object but also need to handle challenges like occlusions, shape changes, and camera motion.</p><p>While appearance alone drives segmentation in images, videos provide a rich and complementary source of information in form of object motion. It is natural to expect that both appearance and motion should play a key role in successfully segmenting objects in videos. However, existing methods fall short of bringing these complementary sources of information together in a unified manner.</p><p>In particular, today motion is employed for video segmentation in two main ways. On the one hand, the propagation or interactive techniques strongly rely on appearance information stemming from human-drawn outlines on frames in the video. Here motion is primarily used to either propagate information or enforce temporal consistency in the resulting segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. On the other hand, fully automatic methods strongly rely on motion to seed the segmentation process by locating possible moving objects. Once a moving object is detected, appearance is primarily used to track it across frames <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Such methods can fail if the object(s) are static or when there is significant camera motion. In either paradigm, results suffer because the two essential cues are treated only in a sequen-tial or disconnected way.</p><p>We propose an end-to-end trainable model that draws on the respective strengths of generic (non-category-specific) object appearance and motion in a unified framework. Specifically, we develop a novel two-stream fully convolutional deep segmentation network where individual streams encode generic appearance and motion cues derived from a video frame and its corresponding optical flow. These individual cues are fused in the network to produce a final object versus background pixel-level binary segmentation for each video frame. The proposed network segments both static and moving objects in new videos without any human involvement.</p><p>Declaring that motion should assist in video segmentation is non-controversial, and indeed we are certainly not the first to inject motion into video segmentation, as noted above. However, thus far the sum is not much greater than its parts. We contend that this is because the signal from motion is adequately complex such that rich learned models are necessary to exploit it. For example, a single object may display multiple motions simultaneously, background and camera motion can intermingle, and even small-magnitude motions should be informative.</p><p>To learn the rich signals, sufficient training data is needed. However, no large-scale video datasets with pixellevel segmentations exist. Our second contribution is to address this practical issue. We propose a solution that leverages readily available image segmentation annotations together with weakly annotated video data to train our model.</p><p>Our results show the reward of learning from both signals in a unified framework: a true synergy, often with substantially stronger results than what we can obtain from either one alone-even if they are treated with an equally sophisticated deep network. We significantly advance the state-of-the-art for fully automatic video object segmentation on multiple challenging datasets. In some cases, the proposed method even outperforms existing methods that require manual intervention on the target video. In summary our key contributions are:</p><p>• the first end-to-end trainable framework for producing pixel level foreground object segmentation in videos.</p><p>• state-of-the-art on multiple datasets, improving over many reported results in the literature and strongly outperforming simpler applications of optical flow, and</p><p>• a means to train a deep pixel-level video segmentation model with access to only weakly labeled videos and strongly labeled images, with no explicit assumptions about the categories present in either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Automatic methods Fully automatic or unsupervised video segmentation methods assume no human input on the video. They can be grouped into two broad categories. First we have the supervoxel methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> which oversegment the video into space-time blobs with cohesive appearance and motion. Their goal is to generate mid-level video regions useful for downstream processing, whereas ours is to produce space-time tubes which accurately delineate object boundaries. Second we have the fully automatic methods that generate thousands of "object-like" space-time segments <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. While useful in accelerating object detection, it is not straightforward to automatically select the most accurate one when a single hypothesis is desired. Methods that do produce a single hypothesis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> strongly rely on motion to identify the objects, either by seeding appearance models with moving regions or directly reasoning about occlusion boundaries using optical flow. This limits their capability to segment static objects in video. In comparison, our method is fully automatic, produces a single hypothesis, and can segment both static and moving objects.</p><p>Human-guided methods Semi-supervised label propagation methods accept human input on a subset of frames, then propagate it to the remaining frames <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. In a similar vein, interactive video segmentation methods leverage a human in the loop to provide guidance or correct errors, e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr">32]</ref>. Since the human pinpoints the object of interest, these methods typically focus more on learning object appearance from the manual annotations. Motion is primarily used to propagate information or enforce temporal smoothness. In the proposed method, both motion and appearance play an equally important role, and we show their synergistic combination results in a much better segmentation quality. Moreover, our method is fully automatic and uses no human involvement to segment a novel video.</p><p>Category-specific semantic segmentation State-of-theart semantic segmentation techniques for images rely on fully convolutional deep learning architectures that are endto-end trainable [33, <ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b32">35,</ref><ref type="bibr" target="#b33">36]</ref>. These deep learning based methods for segmenting images have seen rapid advances in recent years. Unfortunately, video segmentation has not seen such rapid progress. We hypothesize that the lack of large-scale human segmented video segmentation benchmarks is a key bottleneck. Recent video benchmarks like Cityscapes <ref type="bibr" target="#b34">[37]</ref> are valuable, but 1) it addresses categoryspecific segmentation, and 2) thus far methods competing on it process each frame independently, treating it like multiple image segmentation tasks. In contrast, we aim to segment generic objects in video, whether or not they appear in training data. Furthermore, our idea to leverage weakly labeled video for training opens a path towards training deep segmentation models that fuse spatial and temporal cues.  Deep learning with motion Deep learning for combining motion and appearance in videos has proven to be useful in several other computer vision tasks such as video classification <ref type="bibr" target="#b36">[39,</ref><ref type="bibr" target="#b37">40]</ref>, action recognition <ref type="bibr" target="#b38">[41,</ref><ref type="bibr" target="#b39">42]</ref>, object tracking <ref type="bibr" target="#b40">[43,</ref><ref type="bibr" target="#b41">44,</ref><ref type="bibr" target="#b42">45]</ref> and even computation of optical flow <ref type="bibr" target="#b43">[46]</ref>. While we take inspiration from these works, we are the first to present a deep framework for segmenting objects in videos in a fully automatic manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our goal is to segment generic objects in video, independent of the object categories they belong to, and without any manual intervention. We pose the problem as a dense labeling task: given a sequence of video frames [I 1 , I 2 , ..., I N ], we want to infer either "object" or "background" for each pixel in each frame, to output a sequence of binary maps [S 1 , S 2 , ..., S N ]. We propose a solution based on a convolutional neural network.</p><p>First we segment generic objects based on appearance only from individual frames (Sec. 3.1). Then we use the appearance model to generate initial pixel-level annotations in training videos, and bootstrap strong annotations to train a model from motion (Sec. 3.2). Finally, we fuse the two streams to perform video segmentation (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Appearance Stream</head><p>Building on our "pixel objectness" method <ref type="bibr" target="#b44">[47]</ref>, we train a deep fully convolutional network to learn a model of generic foreground appearance. The main idea is to pretrain for object classification, then re-purpose the network to produce binary object segmentations by fine-tuning with relatively few pixel-labeled foreground masks. Pixel objectness uses the VGG architecture <ref type="bibr" target="#b45">[48]</ref> and transforms its fully connected layers into convolutional layers. The resulting network possesses a strong notion of objectness, making it possible to identify foreground regions of more than 3,000 object categories despite seeing ground truth masks for only 20 during training.</p><p>We take this basic idea and upgrade its implementation for our work. In particular, we adapt the image classi-fication model ResNet-101 <ref type="bibr" target="#b35">[38,</ref><ref type="bibr" target="#b46">49]</ref> by replacing the last two groups of convolution layers with dilated convolution layers to increase feature resolution. This results in only an 8× reduction in the output resolution instead of a 32× reduction in the output resolution in the original ResNet model. In order to improve the model's ability to handle both large and small objects, we replace the classification layer of ResNet-101 with four parallel dilated convolutional layers with different sampling rates to explicitly account for object scale. Then we fuse the prediction from all four parallel layers by summing all the outputs. The loss is the sum of cross-entropy terms over each pixel position in the output layer, where ground truth masks consist of only two labels-object foreground or background. We train the model using the Caffe implementation of <ref type="bibr" target="#b46">[49]</ref>. The network takes a video frame of arbitrary size and produces an objectness map of the same size. See <ref type="figure" target="#fig_1">Fig. 2</ref> (top stream).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motion Stream</head><p>Our complete video segmentation architecture consists of a two-stream network in which parallel streams for appearance and motion process the RGB and optical flow images, respectively, then join in a fusion layer (see <ref type="figure" target="#fig_1">Fig. 2</ref>).</p><p>The direct parallel to the appearance stream discussed above would entail training the motion stream to map optical flow maps to video frame foreground maps. However, an important practical catch to that solution is training data availability. While ground truth foreground image segmentations are at least modestly available, datasets for video object segmentation masks are small-scale in deep learning terms, and primarily support evaluation. For example, Segtrack-v2 <ref type="bibr" target="#b6">[7]</ref>, a commonly used benchmark dataset for video segmentation, contains only 14 videos with 1066 labeled frames. DAVIS <ref type="bibr" target="#b47">[50]</ref> contains only 50 sequences with 3455 labeled frames. None contain enough labeled frames to train a deep neural network. Semantic video segmentation datasets like CamVid <ref type="bibr" target="#b48">[51]</ref> or Cityscapes <ref type="bibr" target="#b34">[37]</ref> are somewhat larger, yet limited in object diversity due to a focus on street scenes and vehicles. A good training source for our task would have ample frames with human-drawn segmentations on a wide variety of foreground objects, and would show a good mix of static and moving objects. No such large-scale dataset exists and creating one is non-trivial.</p><p>We propose a solution that leverages readily available image segmentation annotations together with weakly annotated video data to train our model. In brief, we temporarily decouple the two streams of our model, and allow the appearance stream to hypothesize likely foreground regions in frames of a large video dataset annotated only by bounding boxes. Since appearance alone need not produce perfect segmentations, we devise a series of filtering stages to generate high quality estimates of the true foreground. These instances bootstrap pre-training of the optical flow stream, then the two streams are joined to learn the best combination from minimal human labeled training videos.</p><p>More specifically, given a video dataset with bounding boxes labeled for each object, <ref type="bibr" target="#b0">1</ref> we ignore the category labels and map the boxes alone to each frame. Then, we apply the appearance stream, thus far trained only from images labeled by their foreground masks, to compute a binary segmentation for each frame.</p><p>Next we deconflict the box and segmentation in each training frame. First, we refine the binary segmentation by setting all the pixels outside the bounding box(es) as background. Second, for each bounding box, we check if the the smallest rectangle that encloses all the foreground pixels overlaps with the bounding box by at least 75%. Otherwise we discard the segmentation. Third, we discard regions where the box contains more than 95% pixels labeled as foreground, based on the prior that good segmentations are rarely a rectangle, and thus probably the true foreground spills out beyond the box. Finally, we eliminate segments where object and background lack distinct optical flow, so our motion model can learn from the desired cues. Specifically, we compute the frame's optical flow using <ref type="bibr" target="#b49">[52]</ref> and convert it to an RGB flow image <ref type="bibr" target="#b50">[53]</ref>. If the 2-norm between a) the average value within the bounding box and b) the average value in a box whose height and width are twice the original size exceeds 30, the frame and filtered segmentation are added to the training set. See <ref type="figure">Fig. 3</ref> for visual illustration of these steps.</p><p>To recap, bootstrapping from the preliminary appearance model, followed by bounding box pruning, bounding box tests, and the optical flow test, we can generate accurate per-pixel foreground masks for thousands of diverse moving objects-for which no such datasets exist to date. Note that by eliminating training samples with these filters, we aim to reduce label noise for training. However, at test time our system will be evaluated on standard benchmarks for which each frame is manually annotated (see Sec. 4).</p><p>With this data, we now turn to training the motion stream. Analogous to our strong generic appearance model, <ref type="figure">Figure 3</ref>: Procedure to generate (pseudo)-ground truth segmentations. We first apply the appearance model to obtain initial segmentations (second row, with object segment in green) and then prune by setting pixels outside bounding boxes as background (third row). Then we apply the bounding box test (fourth row, yellow bounding box is ground truth and blue bounding box is the smallest bounding box enclosing the foreground segment) and optical flow test (fifth row) to determine whether we add the segmentation to the motion stream's training set or discard it. Best viewed in color.</p><p>we also want to train a strong generic motion model that can segment foreground objects purely based on motion. We use exactly the same network architecture as the appearance model (see <ref type="figure" target="#fig_1">Fig. 2</ref>). Our motion model takes only optical flow as the input and is trained with automatically generated pixel level ground truth segmentations. In particular, we convert the raw optical flow to a 3-channel (RGB) colorcoded optical flow image <ref type="bibr" target="#b50">[53]</ref>. We use this color-coded optical flow image as the input to the motion network. We again initialize our network with pre-trained weights from ImageNet classification <ref type="bibr" target="#b51">[54]</ref>. Representing optical flow using RGB flow images allows us to leverage the strong pretrained initializations as well as maintain symmetry in the appearance and motion arms of the network.</p><p>An alternative solution might forgo handing the system optical flow, and instead input two raw consecutive RGB frames. However, doing so would likely demand more training instances in order to discover the necessary cues. Another alternative would directly train the joint model that combines both motion and appearance, whereas we first "pre-train" each stream to make it discover convolutional features that rely on appearance or motion alone, followed by a fusion layer (below). Our design choices are rooted in avoiding bias in training our model. Since the (pseudo) ground truth comes from the initial appearance network, training jointly from the onset is liable to bias the network to exploit appearance at the expense of motion. By feeding the motion model with only optical flow, we ensure our motion stream learns to segment objects from motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fusion Model</head><p>The final processing in our pipeline joins the outputs of the appearance and motion streams, and aims to leverage a whole that is greater than the sum of its parts. We now describe how to train the joint model using both streams.</p><p>An object segmentation prediction is reliable if 1) either appearance or motion model alone predicts the object segmentation with very strong confidence or 2) their combination together predicts the segmentation with high confidence. This motivates the structure of our joint model. We implement the idea by creating three independent parallel branches: 1) We apply a 1×1 convolution layer followed by a RELU to the output of the appearance model 2)</p><p>We apply a 1×1 convolution layer followed by a RELU to the output of the motion model 3) We replicate the structure of first and second branches and apply element-wise multiplication on their outputs. The element-wise multiplication ensures the third branch outputs confident predictions of object segmentation if and only if both appearance model and motion model have strong predictions. We finally apply a layer that takes the element-wise maximum to obtain the final prediction. See <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>As discussed above, we do not fuse the two streams in an early stage because we want them both to have strong independent predictions. Another advantage of our approach is we only introduce six additional parameters in each 1×1 convolution layer, for a total of 24 trainable parameters. We can then train the fusion model with very limited annotated video data, without overfitting. In the absence of large volumes of video segmentation training data, precluding a complete end-to-end training, our strategy of decoupling the individual streams and training works very well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Datasets and metrics: We evaluate our method on three challenging video object segmentation datasets: DAVIS <ref type="bibr" target="#b47">[50]</ref>, YouTube-Objects <ref type="bibr" target="#b52">[55,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b53">56]</ref> and Segtrack-v2 <ref type="bibr" target="#b6">[7]</ref>. To measure accuracy we use the standard Jaccard score, which computes the intersection over union overlap (IoU) between the predicted and ground truth object segmentations. The three datasets are:</p><p>• DAVIS <ref type="bibr" target="#b47">[50]</ref>: the latest and most challenging video object segmentation benchmark consisting of 50 high quality video sequences of diverse object categories with 3, 455 densely annotated, pixel-accurate frames. The videos are unconstrained in nature and contain challenges such as occlusions, motion blur, and appearance changes. Only the prominent moving objects are annotated in the ground-truth. • YouTube-Objects <ref type="bibr" target="#b52">[55,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b53">56]</ref>: consists of 126 challenging web videos from 10 object categories with more than 20,000 frames and is commonly used for evaluating video object segmentation. We use the subset defined in <ref type="bibr" target="#b53">[56]</ref> and the ground truth provided by <ref type="bibr" target="#b13">[14]</ref> for evaluation. • SegTrack-v2 <ref type="bibr" target="#b6">[7]</ref>: one of the most common benchmarks for video object segmentation consisting of 14 videos with a total of 1, 066 frames with pixel-level annotations. For videos with multiple objects with individual ground-truth segmentations, we treat them as a single foreground for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines:</head><p>We compare with several state-of-the-art methods for each dataset as reported in the literature. Here we group them together based on whether they can operate in a fully automatic fashion (automatic) or require a human in the loop (semi-supervised) to do the segmentation:</p><p>• Automatic methods: Automatic video segmentation methods do not require any human involvement to segment new videos. Depending on the dataset, we compare with the following state of the art methods: FST <ref type="bibr" target="#b7">[8]</ref>, KEY <ref type="bibr" target="#b3">[4]</ref>, NLC <ref type="bibr" target="#b8">[9]</ref> and COSEG <ref type="bibr" target="#b25">[26]</ref>. All use some form of unsupervised motion or objectness cues to identify foreground objects followed by postprocessing to obtain space-time object segmentations. • Semi-supervised methods: Semi-supervised methods bring a human in the loop. They have some knowledge about the object of interest which is exploited to obtain the segmentation (e.g., a manually annotated first frame). We compare with the following state-of-the-art methods: HVS <ref type="bibr" target="#b0">[1]</ref>, HBT <ref type="bibr" target="#b54">[57]</ref>, FCP <ref type="bibr" target="#b19">[20]</ref>, IVID <ref type="bibr" target="#b18">[19]</ref>, HOP <ref type="bibr" target="#b13">[14]</ref>, and BVS <ref type="bibr" target="#b29">[30]</ref>. The methods require different amounts of human annotation to operate, e.g. HOP, BVS, and FCP make use of manual complete object segmentation in the first frame to seed the method; HBT requests a bounding box around the object of interest in the first frame; HVS, IVID require a human to constantly guide the algorithm whenever it fails.</p><p>Note that our method requires human annotated data only during training. At test time it operates in a fully automatic fashion. Thus, given a new video, we require equal effort as the automatic methods, and less effort than the semisupervised methods. Apart from these comparisons, we also examine some natural baselines and variants of our method:</p><p>• Flow-thresholding (Flow-Th): To examine the effectiveness of motion alone in segmenting objects, we adaptively threshold the optical flow in each frame using the flow magnitude. Specifically, we compute the mean and standard deviation from the L2 norm of flow magnitude and use "mean+unit std." as the threshold.   which normalizes the flow by applying a saliency detection method <ref type="bibr" target="#b55">[58]</ref> to the flow image itself. We use average thresholding to obtain the segmentation.</p><p>• Appearance model (Ours-A): To quantify the role of appearance in segmenting objects, we obtain segmentations using only the appearance stream of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Motion model (Ours-M):</head><p>To quantify the role of motion, we obtain segmentations using only the motion stream of our model.</p><p>• Joint model (Ours-Joint): Our complete joint model that learns to combine both motion and appearance together to obtain the final object segmentation.</p><p>Implementation details: To train the appearance stream, we rely on the PASCAL VOC 2012 segmentation dataset <ref type="bibr" target="#b56">[59]</ref> and use a total of 10,582 training images with binary object vs. background masks (see <ref type="bibr" target="#b44">[47]</ref> for more details). As weak bounding box video annotations, we use the ImageNet-Video dataset <ref type="bibr" target="#b51">[54]</ref>. This dataset comes with a total of 3,862 training videos from 30 object categories with 866,870 labeled object bounding boxes from over a million frames. Post refinement using our ground truth generation procedure (see Sec. 3.2), we are left with 84,929 frames with good pixel segmentations 2 which are then used to train our motion model. For training the joint model we use a held-out set for each dataset. We train each stream for a total of 20,000 iterations, use "poly" learning rate policy (power = 0.9) with momentum (0.9) and weight decay (0.0005). No post-processing is applied on the segmentations obtained from our networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality of training data:</head><p>To ascertain that the quality of training data we automatically generate for training our motion stream is good, we first compare it with a small amount of human annotated ground truth. We randomly select 100 frames that passed both the bounding box and optical flow tests, and collect human-drawn segmentations on Amazon MTurk. We first present crowd workers a frame with a bounding box labeled for each object, and then ask them to draw the detailed segmentation for all objects within the bounding boxes. Each frame is labeled by three crowd workers and the final segmentation is obtained by majority <ref type="bibr" target="#b1">2</ref> Available for download on our project website.</p><p>vote on each pixel. The results indicate that our strategy to gather pseudo-ground truth is effective. On the 100 labeled frames, Jaccard overlap with the human-drawn ground truth is 77.8 (and 70.2 before pruning with bounding boxes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative evaluation:</head><p>We now present the quantitative comparisons of our method with several state-of-the-art methods and baselines, for each of the three datasets in turn.</p><p>DAVIS dataset: <ref type="table" target="#tab_2">Table 1</ref> shows the results, with some of the best performing methods taken from the benchmark results <ref type="bibr" target="#b47">[50]</ref>. Our method outperforms all existing methods on this dataset and significantly advances state-of-the-art. Our method is significantly better than simple flow baselines. This supports our claim that even though motion contains a strong signal about foreground objects in videos, it is not straightforward to simply threshold optical flow and obtain those segmentations. A data-driven approach that learns to identify motion patterns indicative of objects as opposed to backgrounds or camera motion is required. The appearance and motion variants of our method themselves result in a very good performance. The performance of the motion variant is particularly impressive, knowing that it has no information about object's appearance and purely relies on the flow signal. When combined together, the joint model results in a significant improvement, with an absolute gain of up to 11% over individual streams.</p><p>Our method is also significantly better than fully automatic methods, which typically rely on motion alone to identify foreground objects. This illustrates the benefits of a unified combination of both motion and appearance. Most surprisingly, our method significantly outperforms even the state-of-the-art semi-supervised techniques, which require substantial human annotation on every video they process. The main motivation behind bringing a human in the loop is to achieve higher accuracies than fully automated methods, yet in this case, our proposed fully automatic method outperforms the best human-in-the-loop algorithms by a significant margin. For example, the BVS <ref type="bibr" target="#b29">[30]</ref> method-which is the current best performing semi-supervised method and requires the first frame of the video to be manually segmented-achieves an overlap score of 66.5%. Our method significantly outperforms it with an overlap score of 71.51%, yet uses no human involvement.</p><p>YouTube-Objects dataset: In <ref type="table" target="#tab_4">Table 2</ref> we see a similarly YouTube-Objects dataset (126 videos)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Flow-Th Flow-Sal FST <ref type="bibr" target="#b7">[8]</ref> COSEG <ref type="bibr" target="#b25">[26]</ref> HBT <ref type="bibr" target="#b54">[57]</ref> HOP <ref type="bibr" target="#b13">[14]</ref> IVID <ref type="bibr" target="#b18">[19]</ref>    <ref type="table">Table 3</ref>: Video object segmentation results on Segtrack-v2. We show the average accuracy over all 14 videos. Our method outperforms several state-of-the art methods, including the ones which actually require human annotation during segmentation. The best performing methods grouped by whether they require human-in-the-loop or not during segmentation are highlighted in bold. * For NLC results are averaged over 12 videos as reported in their paper <ref type="bibr" target="#b8">[9]</ref>. Metric: Jaccard score, higher is better. Please see supp. for per video results.</p><p>strong result on the YouTube-Objects dataset. Our method again outperforms the flow baselines and all the automatic methods by a significant margin. The publicly available code for NLC <ref type="bibr" target="#b8">[9]</ref> runs successfully only on 9% of the YouTube dataset (1725 frames); on those, its jaccard score is 43.64%. Our proposed model outperforms it by a significant margin of 25%. Even among human-in-the-loop methods, we outperform all methods except IVID <ref type="bibr" target="#b18">[19]</ref>. However, IVID <ref type="bibr" target="#b18">[19]</ref> requires a human to consistently track the segmentation performance and correct whatever mistakes the algorithm makes. This can take up to minutes of annotation time for each video. Our method uses zero human involvement but still performs competitively.</p><p>It is also important to note that this dataset shares categories with the PASCAL segmentation benchmark which is used to train our appearance stream. Accordingly, we observe that the appearance stream itself results in the overall best performance. Moreover, this dataset has a mix of static and moving objects which explains the relatively weaker performance of our motion model alone. Overall the joint model works similarly well as appearance alone, however our ablation study (see <ref type="table">Table 4</ref>) where we rank test frames by their amount of motion, shows that our jointmodel is stronger for moving objects. In short, our joint model outperforms our appearance model on moving objects, while our appearance model is sufficient for the most static frames. Whereas existing methods tend to suffer in one extreme or the other, our method handles both well.  <ref type="table">Table 4</ref>: Ablation study for YouTube-Objects dataset: Performance of our appearance and joint models on frames with most (left) and least (right) motion.</p><p>Segtrack-v2 dataset: In <ref type="table">Table 3</ref>, our method outperforms all semi-supervised and automatic methods except NLC <ref type="bibr" target="#b8">[9]</ref> on Segtrack. While our approach significantly outperforms NLC <ref type="bibr" target="#b8">[9]</ref> on the DAVIS dataset, NLC is exceptionally strong on this dataset. Our relatively weaker performance could be due to the low quality and resolution of the Segtrack-v2 videos, making it hard for our network based model to process them. Nonetheless, our joint model still provides a significant boost over both our appearance and motion models, showing it again realizes the synergy of motion and appearance in a serious way.</p><p>Qualitative evaluation: <ref type="figure" target="#fig_3">Fig. 4</ref> shows qualitative results. The top half shows visual comparisons between different components of our method including the appearance, motion, and joint models. We also show the optical flow image that was used as an input to the motion stream. These images help reveal the complexity of learned motion signals.</p><p>In the bear example, the flow is most salient only on the bear's head, still our motion stream alone is able to segment the bear completely. The boat, car, and sail examples show that even when the flow is noisy-including strong NLC <ref type="bibr" target="#b8">[9]</ref> FCP <ref type="bibr" target="#b19">[20]</ref> Ours-Joint Ours-Joint The bottom half of <ref type="figure" target="#fig_3">Fig. 4</ref> shows visual comparisons between our method and state-of-the-art automatic <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and semi-supervised <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref> methods. The automatic methods have a very weak notion about object's appearance; hence they completely miss parts of objects <ref type="bibr" target="#b8">[9]</ref> or cannot disambiguate the objects from background <ref type="bibr" target="#b7">[8]</ref>. Semi-supervised methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>, which rely heavily on the initial humansegmented frame to learn about object's appearance, start to fail as time elapses and the object's appearance changes considerably. In contrast, our method successfully learns to combine generic cues about object motion and appearance, segmenting much more accurately across all frames even in very challenging videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We presented a new approach for learning to segment generic objects in video that 1) achieves deeper synergy between motion and appearance and 2) addresses practical challenges in training a deep network for video segmentation. Results show sizeable improvements over many existing methods-in some cases, even those requiring human intervention. In future work we plan to explore extensions that could permit individuation of multiple touching foreground objects, as well as ways to incorporate human intervention intelligently into our framework.</p><p>Video examples, code &amp; pre-trained models available at: http://vision.cs.utexas.edu/projects/ fusionseg/ Acknowledgements: This research is supported in part by ONR YIP N00014-12-1-0754.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>Per-video results for DAVIS and Segtrack-v2: <ref type="table" target="#tab_8">Table 5</ref> shows the per video results for the 50 videos from the DAVIS dataset. <ref type="table" target="#tab_2">Table 1</ref> in the main paper summarizes these results over all 50 videos. We compare with several semi-supervised and fully automatic baselines. Our method outperforms the per-video best fully automatic and semisupervised baseline in 25 out of 50 videos. <ref type="table">Table 6</ref> shows the per video results for the 14 videos from the Segtrack-v2 dataset. <ref type="table">Table 3</ref> in the main paper summarizes these results over all 14 videos. Our method outperforms the per-video best fully automatic method in 5 out of 14 cases. Our method also outperforms the semisupervised HVS <ref type="bibr" target="#b0">[1]</ref>    <ref type="table">Table 6</ref>: Video object segmentation results on Segtrack-v2. We show the results for all 14 videos. <ref type="table">Table 3</ref> in the main paper summarizes these results over all 14 videos. Our method outperforms several state-of-the art methods, including the ones which actually require human annotation during segmentation. For NLC results are averaged over 12 videos as reported in their paper <ref type="bibr" target="#b8">[9]</ref>. The best performing methods grouped by whether they require human-in-the-loop or not during segmentation are highlighted in bold. Metric: Jaccard score, higher is better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We show color-coded optical flow images (first row) and video segmentation results (second row) produced by our joint model. Our proposed end-to-end trainable model simultaneously draws on the respective strengths of generic object appearance and motion in a unified framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Network structure for our model. Each convolutional layer except the first 7× 7 convolutional layer and our fusion blocks is a residual block<ref type="bibr" target="#b35">[38]</ref>, adapted from ResNet-101. We show reduction in resolution at top of each box and the number of stacked convolutional layers in the bottom of each box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>•</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results: The top half shows examples from our appearance, motion, and joint models along with the flow image which was used as an input to the motion network. The bottom rows show visual comparisons of our method with automatic and semi-supervised baselines (best viewed on pdf and see text for the discussion). Videos of our segmentation results are available on the project website. flow on the background-our motion model is able to learn about object shapes and successfully suppresses the background. The rhino and train examples show cases where the appearance model fails but when combined with the motion stream, the joint model produces accurate segmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Video object segmentation results on DAVIS dataset. We show the average accuracy over all 50 videos. Our method outperforms several state-of-the art methods, including the ones which actually require human annotations during segmentation. The best performing methods grouped by whether they require human-in-the-loop or not during segmentation are highlighted in bold. Metric: Jaccard score, higher is better. Please see supp. for per video results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Video object segmentation results on YouTube-Objects dataset. We show the average performance for each of the 10 categories from the dataset. The final row shows an average over all the videos. Our method outperforms several state-of-the art methods, including the ones which actually require human annotation during segmentation. The best performing methods grouped by whether they require human-in-the-loop or not during segmentation are highlighted in bold. Metric: Jaccard score, higher is better.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Segtrack-v2 dataset (14 videos)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="10">Flow-Th Flow-Sal FST [8] KEY [4] NLC [9] HBT [57] HVS [1] Ours-A Ours-M Ours-Joint</cell></row><row><cell>Human in loop?</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell>Avg. IoU</cell><cell>37.77</cell><cell>27.04</cell><cell>53.5</cell><cell>57.3</cell><cell>80 *</cell><cell>41.3</cell><cell>50.8</cell><cell>56.88</cell><cell>53.04</cell><cell>61.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>method in 8 out of 14 cases.</figDesc><table><row><cell></cell><cell cols="7">DAVIS: Densely Annotated Video Segmentation dataset (50 videos)</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="9">FST [8] KEY [4] NLC [9] HVS [1] FCP [20] BVS [30] Ours-A Ours-M Ours-Joint</cell></row><row><cell>Human in loop?</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell>Bear</cell><cell>89.8</cell><cell>89.1</cell><cell>90.7</cell><cell>93.8</cell><cell>90.6</cell><cell>95.5</cell><cell>91.52</cell><cell>86.30</cell><cell>90.66</cell></row><row><cell>Blackswan</cell><cell>73.2</cell><cell>84.2</cell><cell>87.5</cell><cell>91.6</cell><cell>90.8</cell><cell>94.3</cell><cell>89.54</cell><cell>61.71</cell><cell>81.10</cell></row><row><cell>Bmx-Bumps</cell><cell>24.1</cell><cell>30.9</cell><cell>63.5</cell><cell>42.8</cell><cell>30</cell><cell>43.4</cell><cell>38.77</cell><cell>26.42</cell><cell>32.97</cell></row><row><cell>Bmx-Trees</cell><cell>18</cell><cell>19.3</cell><cell>21.2</cell><cell>17.9</cell><cell>24.8</cell><cell>38.2</cell><cell>34.67</cell><cell>37.08</cell><cell>43.54</cell></row><row><cell>Boat</cell><cell>36.1</cell><cell>6.5</cell><cell>0.7</cell><cell>78.2</cell><cell>61.3</cell><cell>64.4</cell><cell>63.80</cell><cell>59.53</cell><cell>66.35</cell></row><row><cell>Breakdance</cell><cell>46.7</cell><cell>54.9</cell><cell>67.3</cell><cell>55</cell><cell>56.7</cell><cell>50</cell><cell>14.22</cell><cell>61.80</cell><cell>51.10</cell></row><row><cell>Breakdance-Flare</cell><cell>61.6</cell><cell>55.9</cell><cell>80.4</cell><cell>49.9</cell><cell>72.3</cell><cell>72.7</cell><cell>54.87</cell><cell>62.09</cell><cell>76.21</cell></row><row><cell>Bus</cell><cell>82.5</cell><cell>78.5</cell><cell>62.9</cell><cell>80.9</cell><cell>83.2</cell><cell>86.3</cell><cell>80.38</cell><cell>77.70</cell><cell>82.70</cell></row><row><cell>Camel</cell><cell>56.2</cell><cell>57.9</cell><cell>76.8</cell><cell>87.6</cell><cell>73.4</cell><cell>66.9</cell><cell>76.39</cell><cell>74.19</cell><cell>83.56</cell></row><row><cell>Car-Roundabout</cell><cell>80.8</cell><cell>64</cell><cell>50.9</cell><cell>77.7</cell><cell>71.7</cell><cell>85.1</cell><cell>74.84</cell><cell>84.75</cell><cell>90.15</cell></row><row><cell>Car-Shadow</cell><cell>69.8</cell><cell>58.9</cell><cell>64.5</cell><cell>69.9</cell><cell>72.3</cell><cell>57.8</cell><cell>88.38</cell><cell>81.03</cell><cell>89.61</cell></row><row><cell>Car-Turn</cell><cell>85.1</cell><cell>80.6</cell><cell>83.3</cell><cell>81</cell><cell>72.4</cell><cell>84.4</cell><cell>90.67</cell><cell>83.92</cell><cell>90.23</cell></row><row><cell>Cows</cell><cell>79.1</cell><cell>33.7</cell><cell>88.3</cell><cell>77.9</cell><cell>81.2</cell><cell>89.5</cell><cell>87.96</cell><cell>82.22</cell><cell>86.82</cell></row><row><cell>Dance-Jump</cell><cell>59.8</cell><cell>74.8</cell><cell>71.8</cell><cell>68</cell><cell>52.2</cell><cell>74.5</cell><cell>10.32</cell><cell>64.22</cell><cell>61.16</cell></row><row><cell>Dance-Twirl</cell><cell>45.3</cell><cell>38</cell><cell>34.7</cell><cell>31.8</cell><cell>47.1</cell><cell>49.2</cell><cell>46.23</cell><cell>55.39</cell><cell>70.42</cell></row><row><cell>Dog</cell><cell>70.8</cell><cell>69.2</cell><cell>80.9</cell><cell>72.2</cell><cell>77.4</cell><cell>72.3</cell><cell>90.41</cell><cell>81.90</cell><cell>88.92</cell></row><row><cell>Dog-Agility</cell><cell>28</cell><cell>13.2</cell><cell>65.2</cell><cell>45.7</cell><cell>45.3</cell><cell>34.5</cell><cell>68.94</cell><cell>67.88</cell><cell>73.36</cell></row><row><cell>Drift-Chicane</cell><cell>66.7</cell><cell>18.8</cell><cell>32.4</cell><cell>33.1</cell><cell>45.7</cell><cell>3.3</cell><cell>46.13</cell><cell>44.14</cell><cell>59.86</cell></row><row><cell>Drift-Straight</cell><cell>68.3</cell><cell>19.4</cell><cell>47.3</cell><cell>29.5</cell><cell>66.8</cell><cell>40.2</cell><cell>67.24</cell><cell>69.08</cell><cell>81.06</cell></row><row><cell>Drift-Turn</cell><cell>53.3</cell><cell>25.5</cell><cell>15.4</cell><cell>27.6</cell><cell>60.6</cell><cell>29.9</cell><cell>85.09</cell><cell>72.09</cell><cell>86.30</cell></row><row><cell>Elephant</cell><cell>82.4</cell><cell>67.5</cell><cell>51.8</cell><cell>74.2</cell><cell>65.5</cell><cell>85</cell><cell>86.18</cell><cell>77.51</cell><cell>84.35</cell></row><row><cell>Flamingo</cell><cell>81.7</cell><cell>69.2</cell><cell>53.9</cell><cell>81.1</cell><cell>71.7</cell><cell>88.1</cell><cell>44.46</cell><cell>63.80</cell><cell>75.67</cell></row><row><cell>Goat</cell><cell>55.4</cell><cell>70.5</cell><cell>1</cell><cell>58</cell><cell>67.7</cell><cell>66.1</cell><cell>84.11</cell><cell>74.99</cell><cell>83.09</cell></row><row><cell>Hike</cell><cell>88.9</cell><cell>89.5</cell><cell>91.8</cell><cell>87.7</cell><cell>87.4</cell><cell>75.5</cell><cell>82.54</cell><cell>58.30</cell><cell>76.90</cell></row><row><cell>Hockey</cell><cell>46.7</cell><cell>51.5</cell><cell>81</cell><cell>69.8</cell><cell>64.7</cell><cell>82.9</cell><cell>66.03</cell><cell>44.89</cell><cell>70.05</cell></row><row><cell>Horsejump-High</cell><cell>57.8</cell><cell>37</cell><cell>83.4</cell><cell>76.5</cell><cell>67.6</cell><cell>80.1</cell><cell>71.09</cell><cell>54.10</cell><cell>64.93</cell></row><row><cell>Horsejump-Low</cell><cell>52.6</cell><cell>63</cell><cell>65.1</cell><cell>55.1</cell><cell>60.7</cell><cell>60.1</cell><cell>70.23</cell><cell>55.20</cell><cell>71.20</cell></row><row><cell>Kite-Surf</cell><cell>27.2</cell><cell>58.5</cell><cell>45.3</cell><cell>40.5</cell><cell>57.7</cell><cell>42.5</cell><cell>47.71</cell><cell>18.54</cell><cell>38.98</cell></row><row><cell>Kite-Walk</cell><cell>64.9</cell><cell>19.7</cell><cell>81.3</cell><cell>76.5</cell><cell>68.2</cell><cell>87</cell><cell>52.65</cell><cell>39.35</cell><cell>49.00</cell></row><row><cell>Libby</cell><cell>50.7</cell><cell>61.1</cell><cell>63.5</cell><cell>55.3</cell><cell>31.6</cell><cell>77.6</cell><cell>67.70</cell><cell>35.34</cell><cell>58.48</cell></row><row><cell>Lucia</cell><cell>64.4</cell><cell>84.7</cell><cell>87.6</cell><cell>77.6</cell><cell>80.1</cell><cell>90.1</cell><cell>79.93</cell><cell>49.18</cell><cell>77.31</cell></row><row><cell>Mallard-Fly</cell><cell>60.1</cell><cell>58.5</cell><cell>61.7</cell><cell>43.6</cell><cell>54.1</cell><cell>60.6</cell><cell>74.62</cell><cell>42.64</cell><cell>68.46</cell></row><row><cell>Mallard-Water</cell><cell>8.7</cell><cell>78.5</cell><cell>76.1</cell><cell>70.4</cell><cell>68.7</cell><cell>90.7</cell><cell>83.34</cell><cell>25.31</cell><cell>79.43</cell></row><row><cell>Motocross-Bumps</cell><cell>61.7</cell><cell>68.9</cell><cell>61.4</cell><cell>53.4</cell><cell>30.6</cell><cell>40.1</cell><cell>83.78</cell><cell>56.56</cell><cell>77.15</cell></row><row><cell>Motocross-Jump</cell><cell>60.2</cell><cell>28.8</cell><cell>25.1</cell><cell>9.9</cell><cell>51.1</cell><cell>34.1</cell><cell>80.43</cell><cell>59.02</cell><cell>77.50</cell></row><row><cell>Motorbike</cell><cell>55.9</cell><cell>57.2</cell><cell>71.4</cell><cell>68.7</cell><cell>71.3</cell><cell>56.3</cell><cell>28.67</cell><cell>45.71</cell><cell>41.15</cell></row><row><cell>Paragliding</cell><cell>72.5</cell><cell>86.1</cell><cell>88</cell><cell>90.7</cell><cell>86.6</cell><cell>87.5</cell><cell>17.68</cell><cell>60.76</cell><cell>47.42</cell></row><row><cell>Paragliding-Launch</cell><cell>50.6</cell><cell>55.9</cell><cell>62.8</cell><cell>53.7</cell><cell>57.1</cell><cell>64</cell><cell>58.88</cell><cell>50.34</cell><cell>57.00</cell></row><row><cell>Parkour</cell><cell>45.8</cell><cell>41</cell><cell>90.1</cell><cell>24</cell><cell>32.2</cell><cell>75.6</cell><cell>79.39</cell><cell>58.51</cell><cell>75.81</cell></row><row><cell>Rhino</cell><cell>77.6</cell><cell>67.5</cell><cell>68.2</cell><cell>81.2</cell><cell>79.4</cell><cell>78.2</cell><cell>77.56</cell><cell>83.03</cell><cell>87.52</cell></row><row><cell>Rollerblade</cell><cell>31.8</cell><cell>51</cell><cell>81.4</cell><cell>46.1</cell><cell>45</cell><cell>58.8</cell><cell>63.27</cell><cell>57.73</cell><cell>69.01</cell></row><row><cell>Scooter-Black</cell><cell>52.2</cell><cell>50.2</cell><cell>16.2</cell><cell>62.4</cell><cell>50.4</cell><cell>33.7</cell><cell>36.07</cell><cell>62.18</cell><cell>68.47</cell></row><row><cell>Scooter-Gray</cell><cell>32.5</cell><cell>36.3</cell><cell>58.7</cell><cell>43.3</cell><cell>48.3</cell><cell>50.8</cell><cell>73.22</cell><cell>61.69</cell><cell>73.40</cell></row><row><cell>Soapbox</cell><cell>41</cell><cell>75.7</cell><cell>63.4</cell><cell>68.4</cell><cell>44.9</cell><cell>78.9</cell><cell>49.70</cell><cell>53.24</cell><cell>62.57</cell></row><row><cell>Soccerball</cell><cell>84.3</cell><cell>87.9</cell><cell>82.9</cell><cell>6.5</cell><cell>82</cell><cell>84.4</cell><cell>29.27</cell><cell>73.56</cell><cell>79.72</cell></row><row><cell>Stroller</cell><cell>58</cell><cell>75.9</cell><cell>84.9</cell><cell>66.2</cell><cell>59.7</cell><cell>76.7</cell><cell>63.91</cell><cell>54.40</cell><cell>66.55</cell></row><row><cell>Surf</cell><cell>47.5</cell><cell>89.3</cell><cell>77.5</cell><cell>75.9</cell><cell>84.3</cell><cell>49.2</cell><cell>88.78</cell><cell>73.00</cell><cell>88.41</cell></row><row><cell>Swing</cell><cell>43.1</cell><cell>71</cell><cell>85.1</cell><cell>10.4</cell><cell>64.8</cell><cell>78.4</cell><cell>73.75</cell><cell>59.41</cell><cell>74.05</cell></row><row><cell>Tennis</cell><cell>38.8</cell><cell>76.2</cell><cell>87.1</cell><cell>57.6</cell><cell>62.3</cell><cell>73.7</cell><cell>76.88</cell><cell>47.19</cell><cell>70.75</cell></row><row><cell>Train</cell><cell>83.1</cell><cell>45</cell><cell>72.9</cell><cell>84.6</cell><cell>84.1</cell><cell>87.2</cell><cell>42.50</cell><cell>80.33</cell><cell>75.56</cell></row><row><cell>Avg. IoU</cell><cell>57.5</cell><cell>56.9</cell><cell>64.1</cell><cell>59.6</cell><cell>63.1</cell><cell>66.5</cell><cell>64.69</cell><cell>60.18</cell><cell>71.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Video object segmentation results on DAVIS dataset. We show the results for all 50 videos.Table 1in the main paper summarizes these results over all 50 videos. Our method outperforms several state-of-the art methods, including the ones which actually require human annotation during segmentation. The best performing methods grouped by whether they require human-in-the-loop or not during segmentation are highlighted in bold. Metric: Jaccard score, higher is better.[32] B. L. Price, B. S. Morse, and S. Cohen, "Livecut: Learningbased interactive video segmentation by evaluation of multiple propagated cues," in ICCV, 2009. 2 [33] H. Noh, S. Hong, and B. Han, "Learning deconvolution network for semantic segmentation," in ICCV, 2015. 2Segtrack-v2 dataset (14 videos) Methods FST [8] KEY [4] NLC [9] HVS [1] Ours-A Ours-M Ours-Joint Human in loop?</figDesc><table><row><cell></cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell>birdfall2</cell><cell>17.50</cell><cell>49.00</cell><cell>74.00</cell><cell>57.40</cell><cell>6.94</cell><cell>55.50</cell><cell>38.01</cell></row><row><cell cols="2">bird of paradise 81.83</cell><cell>92.20</cell><cell>-</cell><cell>86.80</cell><cell>49.82</cell><cell>62.46</cell><cell>69.91</cell></row><row><cell>bmx</cell><cell>67.00</cell><cell>63.00</cell><cell>79.00</cell><cell>35.85</cell><cell>59.53</cell><cell>55.12</cell><cell>59.08</cell></row><row><cell>cheetah</cell><cell>28.00</cell><cell>28.10</cell><cell>69.00</cell><cell>21.60</cell><cell>71.15</cell><cell>36.00</cell><cell>59.59</cell></row><row><cell>drift</cell><cell>60.50</cell><cell>46.90</cell><cell>86.00</cell><cell>41.20</cell><cell>82.18</cell><cell>80.03</cell><cell>87.64</cell></row><row><cell>frog</cell><cell>54.13</cell><cell>0.00</cell><cell>83.00</cell><cell>67.10</cell><cell>54.86</cell><cell>52.88</cell><cell>57.03</cell></row><row><cell>girl</cell><cell>54.90</cell><cell>87.70</cell><cell>91.00</cell><cell>31.90</cell><cell>81.07</cell><cell>43.57</cell><cell>66.73</cell></row><row><cell>hummingbird</cell><cell>52.00</cell><cell>60.15</cell><cell>75.00</cell><cell>19.45</cell><cell>61.50</cell><cell>60.86</cell><cell>65.19</cell></row><row><cell>monkey</cell><cell>65.00</cell><cell>79.00</cell><cell>71.00</cell><cell>61.90</cell><cell>86.42</cell><cell>58.95</cell><cell>80.46</cell></row><row><cell>monkeydog</cell><cell>61.70</cell><cell>39.60</cell><cell>78.00</cell><cell>43.55</cell><cell>39.08</cell><cell>24.36</cell><cell>32.80</cell></row><row><cell>parachute</cell><cell>76.32</cell><cell>96.30</cell><cell>94.00</cell><cell>69.10</cell><cell>24.86</cell><cell>59.43</cell><cell>51.58</cell></row><row><cell>penguin</cell><cell>18.31</cell><cell>9.27</cell><cell>-</cell><cell>74.45</cell><cell>66.20</cell><cell>45.09</cell><cell>71.25</cell></row><row><cell>soldier</cell><cell>39.77</cell><cell>66.60</cell><cell>83.00</cell><cell>66.50</cell><cell>83.70</cell><cell>48.37</cell><cell>69.82</cell></row><row><cell>worm</cell><cell>72.79</cell><cell>84.40</cell><cell>81.00</cell><cell>34.70</cell><cell>29.13</cell><cell>59.94</cell><cell>50.63</cell></row><row><cell>Avg. IoU</cell><cell>53.5</cell><cell>57.3</cell><cell>80 *</cell><cell>50.8</cell><cell>56.88</cell><cell>53.04</cell><cell>61.40</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We rely on ImageNet Video data, which contains 3862 videos and 30 diverse objects. See Sec. 4.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Streaming Hierarchical Video Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video segmentation with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum weight cliques with mutex constraints for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video Segmentation by Tracking Many Figure-Ground Segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tracking as repeated figure/ground segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion coherent tracking with multi-label mrf optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining self training and active learning for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Active frame selection for label propagation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Jots: Joint online tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="585" to="594" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video object cut and paste</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="595" to="600" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video snapcut: Robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust video segment proposals with painless occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatiotemporal object detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic cosegmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Occlusion boundary detection and figure/ground assignment from optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to find object boundaries using motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Label propagation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks,&quot; in ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.70622" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeptrack: Learning discriminative feature representations by convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks,&quot; in ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pixel objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05349</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Beyond pixels: exploring new representations and applications for motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Ph.D. dissertation, Citeseer</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Discriminative segment annotation in weakly labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Hough-based tracking of non-rigid objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Saliency detection via absorbing markov chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
