<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPH INFOCLUST: LEVERAGING CLUSTER-LEVEL NODE INFORMATION FOR UNSUPERVISED GRAPH REPRESENTATION LEARNING A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costas</forename><surname>Mavromatis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
							<email>karypis@umn.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GRAPH INFOCLUST: LEVERAGING CLUSTER-LEVEL NODE INFORMATION FOR UNSUPERVISED GRAPH REPRESENTATION LEARNING A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised (or self-supervised) graph representation learning is essential to facilitate various graph data mining tasks when external supervision is unavailable. The challenge is to encode the information about the graph structure and the attributes associated with the nodes and edges into a low dimensional space. Most existing unsupervised methods promote similar representations across nodes that are topologically close. Recently, it was shown that leveraging additional graph-level information, e.g., information that is shared among all nodes, encourages the representations to be mindful of the global properties of the graph, which greatly improves their quality. However, in most graphs, there is significantly more structure that can be captured, e.g., nodes tend to belong to (multiple) clusters that represent structurally similar nodes. Motivated by this observation, we propose a graph representation learning method called Graph InfoClust (GIC), that seeks to additionally capture cluster-level information content. These clusters are computed by a differentiable K-means method and are jointly optimized by maximizing the mutual information between nodes of the same clusters. This optimization leads the node representations to capture richer information and nodal interactions, which improves their quality. Experiments show that GIC outperforms state-of-art methods in various downstream tasks (node classification, link prediction, and node clustering) with a 0.9% to 6.1% gain over the best competing approach, on average.</p><p>Motivated by the fact that GNN encoders already preserve similarities between neighboring nodes, Deep Graph Infomax (DGI) [4] adopts a self-supervision approach that maximizes the mutual information (MI) between the representation of each node and the global graph representation, which is obtained by averaging the representations of the nodes in</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph structured data naturally emerge in various real-world applications. Such examples include social networks, citation networks, and biological networks. The challenge, from a data representation perspective, is to encode the high-dimensional, non-Euclidean information about the graph structure and the attributes associated with the nodes and edges into a low dimensional embedding space. The learned embeddings (a.k.a. representations) can then be used for various tasks, e.g., node classification, link prediction, community detection, and data visualization. In this paper, we focus on unsupervised representation learning methods that estimate node embeddings without using any labeled data but instead employ various self-supervision approaches. These methods eliminate the need to develop task-specific graph representation models, eliminate the cost of acquiring labeled datasets, and can lead to better representations by using large unlabeled datasets.</p><p>Many self-supervision approaches ensure that nodes that are close to each other, both in terms of the graph's topology and in terms of their available features, are also close in the embedding space. This is achieved by employing a contrastive loss between pairs of nearby and pairs of distant nodes; e.g., DeepWalk <ref type="bibr" target="#b0">[1]</ref>, and GraphSAGE <ref type="bibr" target="#b1">[2]</ref>. Another self-supervision approach focuses on reconstructing the existing edges of the graph based on the embedding similarity of the incident nodes; e.g., GAE/VGAE <ref type="bibr" target="#b2">[3]</ref>. Because preserving neighbor similarities is desired, many of these methods use graph neural network (GNN) encoders, that insert an additional inductive bias that nodes share similarities with their neighbors.  <ref type="bibr" target="#b6">[7]</ref> of the representations learned by DGI and GIC (using 7 clusters) on the CORA citation network <ref type="bibr" target="#b7">[8]</ref>.</p><p>the graph. This encourages the computed node embeddings to be mindful of the global properties of the graph, which improves the representations' quality. DGI has shown to estimate superior node representations <ref type="bibr" target="#b3">[4]</ref> and is considered to be among the best unsupervised node representation learning approaches.</p><p>By maximizing the mutual information between the representation of a node and that of the global summary, DGI obtains node representations that capture the information content of the entire graph. However, in most graphs, there is significantly more structure that can be captured. For example, nodes tend to belong to (multiple) clusters that represent topologically near-by nodes as well as nodes have similar structural roles but are topologically distant from each other. In such cases, methods that simultaneously maximize the mutual information between the representation of a node and the summary representation of the different clusters that this node belongs to, including that of the entire graph, will allow the node representations to capture the information content of these clusters and thus encode richer structural information.</p><p>Motivated by this observation, we developed Graph InfoClust (GIC), an unsupervised representation learning method that learns node representations by simultaneously maximizing the mutual information (MI) with respect to the graph-level summary as well as cluster-level summaries. The graph-level summary is obtained by averaging all node representations and the cluster-level summaries by a differentiable K-means clustering <ref type="bibr" target="#b4">[5]</ref> of the node representations. The optimization of these summaries is achieved by a noise-contrastive objective, which uses discriminator functions, that discriminate between real and fake samples, as a proxy for estimating and maximizing the MI. The joint computation and optimization of the summaries, promotes both graph-level and cluster-level information and properties to the node representations, which improves their quality. For example, as illustrated in <ref type="figure" target="#fig_0">Fig 1,</ref> GIC leads to representations that better separate the same-labeled nodes over the representations computed by DGI-the silhouette score (SIL) <ref type="bibr" target="#b5">[6]</ref> (see Section 5) of GIC is 0.257 compared to DGI's 0.222.</p><p>We evaluated GIC on seven standard datasets using node classification, link prediction, and clustering as the downstream tasks. Our experiments show that in all but two of the dataset-task combinations, GIC performs better than the best competing approach and its average improvement over DGI is 0.9, 2.6, and 15.5 percentage points for node classification, link prediction, and clustering, respectively. These results demonstrate that by leveraging cluster summaries, GIC is able to improve the quality of the estimated representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notation, Definitions, and Problem Statement</head><p>We denote vectors by bold lower-case letters and they are assumed to be column vectors (e.g., x). We also denote matrices by bold upper-case letters (e.g., X). Symbol := is used in definition statements. If a matrix X consists of row vectors x 1 , . . . , x N , it is denoted as X := [x 1 , . . . , x N ].</p><p>A graph G that consists of N nodes is defined as G := {V, E}, where V := {n 1 , . . . , n N } is the vertex set and E is the corresponding edge set. Connectivity is captured by the N × N adjacency matrix A ∈ R N ×N , where [A] i,j = 1 if (n i , n j ) ∈ E, and [A] i,j = 0 otherwise. Each node n i ∈ V is also associated with a feature vector x i ∈ R F . All feature vectors are collected in the feature matrix X := [x 1 , . . . , x N ], where X ∈ R N ×F .</p><p>Let h i ∈ R F be an F -dimensional node embedding vector for each n i ∈ V. Let H := [h 1 , . . . , h N ], such that H ∈ R N ×F is the node embedding matrix of G. The goal of node representation learning is to learn H such that it preserves both G's structural information A and its feature information X. Once learned, H can be used as a single input feature matrix for downstream tasks such as node classification, link prediction, and clustering. The problem of node representation learning is equivalent to learning an encoder function, f : R N ×N × R N ×F − → R N ×F , that takes the adjacency matrix A and the feature matrix X as input and generates node representations H, namely, H = f (A, X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Non-GNN-based approaches. Early unsupervised approaches relied on matrix factorization techniques, derived by classic dimensionality reduction <ref type="bibr" target="#b8">[9]</ref>. They use the adjacency matrix and generate embeddings by factorizing it so that similar nodes have similar embeddings, e.g., Graph factorization <ref type="bibr" target="#b9">[10]</ref>, GraRep <ref type="bibr" target="#b10">[11]</ref>, and HOPE <ref type="bibr" target="#b11">[12]</ref>. Matrix factorization methods use deterministic measures for node similarity, thus probabilistic models were introduced to offer stochasticity. DeepWalk <ref type="bibr" target="#b0">[1]</ref> and node2vec <ref type="bibr" target="#b12">[13]</ref> optimize embeddings to encode the statistics of random walks; nodes have similar embeddings if they tend to co-occur on short random walks over the graph. Similar approaches include LINE <ref type="bibr" target="#b13">[14]</ref> and HARP <ref type="bibr" target="#b14">[15]</ref>. Common limitations of the aforementioned methods is that they are inherently transductive; they need to perform additional optimization rounds to generate embeddings for unseen nodes in the training phase. Generalizing machine learning models (i.e., neural networks) to the graph domain, notably with deep auto-encoders and graph neural networks (GNN), overcame these limitations and made representation learning applicable to large-scale evolving graphs. Deep auto-encoder approaches, such as DNGR <ref type="bibr" target="#b15">[16]</ref> and SDNE <ref type="bibr" target="#b16">[17]</ref>, use multi-layer perceptron encoders to extract node embeddings and multi-layer perceptron decoders to preserve the graph topology.</p><p>GNN-based approaches. Graph neural networks (GNN) are machine learning models specifically designed for node embeddings, which operate on local neighborhoods to extract embeddings. GNNs generate node embeddings by learning how to repeatedly aggregate neighbors' information. Different GNNs have been developed to promote certain graph properties <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> and primarily differ on how they combine neighbors' information (a survey in <ref type="bibr" target="#b22">[23]</ref>). As a result, successful unsupervised methods used GNN encoders to capture topology information. GAE and VGAE <ref type="bibr" target="#b2">[3]</ref> use a GNN encoder to generate node embeddings and a simple decoder to reconstruct the adjacency matrix. ARGVA <ref type="bibr" target="#b23">[24]</ref> follows a similar schema with VGAE, but learns the data distribution in an adversarial manner <ref type="bibr" target="#b24">[25]</ref>. Independently developed, GraphSAGE <ref type="bibr" target="#b25">[26]</ref> employs a GNN encoder and a random walk based objective to optimize the node embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Graph Infomax</head><p>The aforementioned methods share the general idea that nodes that are close in the input data (graph structure or graph structure plus features), should also be as close as possible in the embedding space. Motivated by the fact that GNNs already account for neighborhood information (i.e., are already local-biased), the work of Deep Graph Infomax (DGI) <ref type="bibr" target="#b3">[4]</ref> (and inspirational to us) employs an alternative loss function that encourages node embeddings to be mindful of the global structural properties.</p><p>The basic idea is to train the GNN-encoder f GN N to maximize the mutual information (MI) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> between node (fine-grain) representations, i.e., H = f GN N (A, X), and a global representation (summary of all representations). This encourages the encoder to prefer the information that is shared across all nodes. If some specific information, e.g., noise, is present in some neighborhoods only, this information would not increase the MI and thus, would not be preferred to be encoded.</p><p>Maximizing the precise value of mutual information is intractable, thus, a Jensen-Shannon MI estimator is often used <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, which maximizes MI's lower bound. The Jensen-Shannon-based estimator acts like a standard binary cross-entropy (BCE) loss, whose objective maximizes the expected log-ratio of the samples from the joint distribution (positive examples) and the product of marginal distributions (negative examples). The positive examples are pairings of s with h i of the real input graph G := (A, X), but the negatives are pairings of s withh i , which are obtained from a fake/corrupted input graphG := (Ã,X) withH = f GN N (Ã,X). Then, a discriminator D 1 : R F × R F → R is used to assign higher scores to the positive examples than the negatives, as in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>The Jensen-Shannon-based BCE objective is expressed as This objective is DGI's main contribution, which leads to superior node representations <ref type="bibr" target="#b3">[4]</ref>. As a result, DGI is considered to be among the best unsupervised node representation learning approaches.</p><formula xml:id="formula_0">L 1 = N i=1 E (X,A) log D 1 (h i , s) + N i=1 E (X,Ã) log 1 − D 1 (h i , s) ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Graph InfoClust (GIC)</head><p>Graph InfoClust relies on a framework similar to DGI's to optimize the embedding space so that it contains additional cluster-level information content. The novel idea is to learn node representations by maximizing the mutual information between (i) node (fine-grain) representations and the global graph summary, and (ii) node (fine-grain) representations and corresponding cluster (coarse-grain) summaries. This enables the embeddings to be mindful of various structural properties and avoids the pitfall of optimizing the embeddings based on a single vector. We explain this with <ref type="figure" target="#fig_1">Figure 2</ref>, and show that GIC leads to better representations than DGI, especially when we limit the dimensions F of the embeddings (its capacity), and thus, the amount of information that can be encoded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of GIC</head><p>GIC's overall framework is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Mutual information is estimated and maximized through discriminator functions that discriminate between positive samples from a real input and negative samples from a fake inputG ( <ref type="figure" target="#fig_2">Figure 3a</ref>), as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. Real node embeddings H and fake node embeddingsH are obtained by using a graph neural network (GNN) encoder f GN N as H = f GN N (A, X) andH = f GN N (Ã,X), respectively ( <ref type="figure" target="#fig_2">Figure 3b</ref>).</p><p>The global graph summary s ∈ R 1×F is obtained by averaging all nodes' representations, as in DGI <ref type="bibr" target="#b3">[4]</ref>. Cluster summaries µ k are obtained by, first, clustering fine-grain representations and, then, computing their summary (average of all nodes within a cluster). Suppose we want to optimize K cluster summaries, so that µ k ∈ R 1×F with k = 1, . . . , K <ref type="figure" target="#fig_2">(Figure 3c</ref>).</p><p>The optimization is achieved by maximizing the mutual information (MI) between nodes within a cluster. In order to estimate and maximize the MI, we compute z i ∈ R 1×F which represents the corresponding cluster summary of each node n i , based on the cluster it belongs to. Then, we can simply maximize the MI between h i and z i of each node. A discriminator D K : R F × R F → R is used as a proxy for estimating the MI by assigning higher scores to positive examples than negatives, as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref>. We obtain positive examples by pairing h i with z i from the real graph and negatives by pairingh i with z i from the fake graph. The proposed objective term is given by</p><formula xml:id="formula_1">(d) (c) Real + Fake − Encoder f (a) A, X A,X (b) H H s µ 1 · · · µ K</formula><formula xml:id="formula_2">L K = N i=1 E (X,A) log D K (h i , z i ) + N i=1 E (X,Ã) log 1 − D K (h i , z i<label>(2)</label></formula><p>and GIC's overall objective is given by</p><formula xml:id="formula_3">L = αL 1 + (1 − α)L K ,<label>(3)</label></formula><p>where α ∈ [0, 1] controls the relative importance of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Fake input. When the input is a single graph, we opt to corrupt the graph by row-shuffling the original features X as X := shuffle([x 1 , x 2 , . . . , x N ]) andÃ := A as proposed by <ref type="bibr" target="#b3">[4]</ref> (see <ref type="figure" target="#fig_2">Figure 3a</ref>). In the case of multiple input graphs, it may be useful to randomly sample a different graph from the training set as negative examples <ref type="bibr" target="#b3">[4]</ref>.</p><p>Cluster and graph summaries. The graph's summary s in Eq. (1) (and thus in Eq. <ref type="formula" target="#formula_3">(3)</ref>), is computed as</p><formula xml:id="formula_4">s = σ   1 N N i=1 h i   ,<label>(4)</label></formula><p>and is essentially the average of all node representations with a nonlinearity σ(·); here is the logistic sigmoid, which worked better in our case and in <ref type="bibr" target="#b3">[4]</ref>.</p><p>In Eq. (2), in order to compute z i for each node n i , we apply a weighted average of the summaries of the clusters to which node n i belongs to, as</p><formula xml:id="formula_5">z i = σ   K k=1 r ik µ k   ,<label>(5)</label></formula><p>where r ik is the degree that node n i is assigned to cluster k, and is a soft-assignment value (i.e., k r ik = 1, ∀i), and σ(·) is the logistic sigmoid nonlinearity.</p><p>The cluster summaries µ k , with k = 1, . . . , K, are obtained by a layer that implements a differentiable version of K-means clustering, as in ClusterNet <ref type="bibr" target="#b4">[5]</ref>. The ClusterNet layer updates the cluster centers in an end-to-end differentiable manner based on an input objective term, e.g., cluster modularity maximization. Here, we use Eq. (2) to optimize the clusters, and essentially maximize the intra-cluster mutual information. More precisely, the cluster centroids µ k are updated by optimizing Eq. (2) via an iterative process by alternately setting</p><formula xml:id="formula_6">µ k = i r ik h i i r ik k = 1, . . . , K<label>(6)</label></formula><p>and</p><formula xml:id="formula_7">r ik = exp(−β sim(h i , µ k )) k exp(−β sim(h i , µ k )) k = 1, . . . , K,<label>(7)</label></formula><p>where sim(·, ·) denotes a similarity function between two instances and β is an inverse-temperature hyperparameter; β → ∞ gives a binary value for each cluster assignment. All h i , z i , and r ik for each node n i , and thus µ k , are jointly optimized and the process is analogous to the typical K-means updates. It was shown that the approximate gradient with respect to the cluster centers can be calculated by unrolling a single (the last one) iteration of the forward-pass updates <ref type="bibr" target="#b4">[5]</ref>. This enables the final cluster output to be computed in an end-to-end fashion.</p><p>Discriminators. As the discriminator function D 1 , which is a proxy for estimating the MI between node representations and the graph summary, we use a bilinear scoring function, as proposed in <ref type="bibr" target="#b3">[4]</ref>, followed by a logistic sigmoid nonlinearity, which convert scores into probabilities, as</p><formula xml:id="formula_8">D 1 (h i , s) = σ(h T i Ws),<label>(8)</label></formula><p>with W being a learnable scoring matrix.</p><p>Moreover, we use an inner product similarity, followed by a logistic sigmoid nonlinearity σ(·), as the discriminator function D K for estimating the MI between node representations and their cluster summaries as</p><formula xml:id="formula_9">D K (h i , z i ) = σ(h T i z i ) .<label>(9)</label></formula><p>Here, we replace the bilinear scoring function used for D 1 by an inner product, since it dramatically reduces the memory requirements and worked better in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluated the performance of GIC using seven commonly used benchmark datasets, whose statistics can be found in the Supplementary Material. Briefly, CORA <ref type="bibr" target="#b7">[8]</ref>, CiteSeer <ref type="bibr" target="#b30">[31]</ref>, and PubMed <ref type="bibr" target="#b31">[32]</ref> are three citation networks, CoauthorCS and CoauthorPhysics <ref type="bibr" target="#b32">[33]</ref> are co-authorship graphs, and AmazonComputer and AmazonPhoto <ref type="bibr" target="#b32">[33]</ref> are segments of the Amazon co-purchase graph <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Node classification</head><p>The goal is to predict some (or all) of the nodes' labels based on a given (small) training set. In unsupervised methods, the learned node embeddings are passed to a downstream classifier, e.g., logistic regression. Following <ref type="bibr" target="#b32">[33]</ref>, we sample 20×#classes nodes as the train set, 30×#classes nodes as the validation set, and the remaining nodes are the test set. The sets are either uniformly drawn from each class (balanced sets) or randomly sampled (imbalanced sets). For the unsupervised methods, we use a logistic regression classifier, which is trained with a learning rate of 0.01 for 1k epochs with Adam SGD optimizer <ref type="bibr" target="#b34">[35]</ref> and Glorot initialization <ref type="bibr" target="#b35">[36]</ref>. The classification accuracy (Acc) is reported as a performance metric, which is the percentage of correctly classified instances (TP + TN)/(TP + TN + FP + FN) where TP, FN, FP, and TN represent the number of true positives, false negatives, false positives, and true negatives, respectively. The final results are averaged over 20 times. Following <ref type="bibr" target="#b32">[33]</ref>, we set the embedding dimensions F = 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Link prediction</head><p>In link prediction, some edges are hidden in the input graph and the goal is to predict the existence of these edges based on the computed embeddings. The probability of an edge between nodes i and j is given by σ(h T i h j ), where σ is the logistic sigmoid function. We follow the setup described in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>: 5% of edges and negative edges as validation set, 10% of edges and negative edges as test set, F = 16, and the results are averaged over 10 runs. We report the area under the ROC curve (AUC) score <ref type="bibr" target="#b36">[37]</ref>, which is equal to the probability that a randomly chosen edge is ranked higher than a randomly chosen negative edge, and the average precision (AP) score <ref type="bibr" target="#b37">[38]</ref>, which is the area under the precision-recall curve; here, precision is given by TP/(TP+FP) and recall by TP/(TP+FN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Clustering</head><p>In clustering, the goal is to cluster together related nodes (e.g., nodes that belong to the same class) without any label information. The computed embeddings are clustered into K = #classes clusters with K-means. The evaluation is provided by external labels, the same used for node classification. We report the classification accuracy (Acc), normalized mutual information (NMI), and average rand index (ARI) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. NMI is an information-theoretic metric, while ARI can be viewed as an accuracy metric which additionally penalizes incorrect decisions. We set F = 32, since most state-of-art competing approaches are invariant to the embedding size choice <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyper-parameter tuning and model selection</head><p>Encoder. As an encoder function f GNN we utilize a graph convolution network (GCN) <ref type="bibr" target="#b19">[20]</ref> with the following propagation rule at layer l</p><formula xml:id="formula_10">H (l+1) = σ D − 1 2ÂD − 1 2 H (l) Θ ,<label>(10)</label></formula><p>whereÂ = A + I N is the adjacency matrix with self-loops,D is the degree matrix ofÂ, i.e.,D ii = jÂ ij , Θ ∈ R F ×F is a learnable matrix, σ(·) denotes a nonlinear activation (here PReLU <ref type="bibr" target="#b41">[42]</ref>), and H (0) = X.</p><p>Parameter selection. We use one-layer GCN-encoder (l = 1), as suggested by <ref type="bibr" target="#b3">[4]</ref>, and as a similarity function in Eq. <ref type="formula" target="#formula_7">(7)</ref>, we employ the cosine similarity as suggested by <ref type="bibr" target="#b4">[5]</ref>, and we iterate the cluster updates in Eq. (6), Eq. <ref type="formula" target="#formula_7">(7)</ref> for 10 times. Since GIC's cluster updates are performed in the unit sphere (cosine similarity), we row-normalize the embeddings before the downstream task.</p><p>GIC's learnable parameters are initialized with Glorot initialization <ref type="bibr" target="#b35">[36]</ref> and the objective is optimized using the Adam SGD optimizer <ref type="bibr" target="#b34">[35]</ref> with a learning rate of 0.001. We train for a maximum of 2k epochs, but the training is terminated with early stopping if the training loss does not improve in 50 consecutive epochs. The model state is reset to the one with the best (lowest) training loss.</p><p>To study the effect of the hyperparameters, which are α, the regularization parameter of the two objective terms, β, that controls the softness of the clusters, and K, the number of clusters, an ablation study is first provided. Then, for each dataset-task pair, we perform model selection based on the validation set: We set α ∈ {0.25, 0.5, 0.75}, β = {10, 100} and K ∈ {32, 128} to train the model, and keep the parameters' triplet that achieved the best result on the validation set (accuracy metric for node classification, AUC for link prediction, and accuracy for clustering). After that, we continue with the same triplet for the rest runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Competing approaches</head><p>We compare the performance of GIC against fifteen unsupervised and six semi-supervised methods and variants; details can be found in the Supplementary Material. The results for all the competing methods, except DGI, were obtained directly from <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref>. For DGI, we report results based on the DGI implementation of Deep Graph Library <ref type="bibr" target="#b42">[43]</ref> for node classification, and the original DGI implementation for link prediction and clustering. Oftentimes, we refer to GIC with α = 1 in Eq. (3) as DGI, since it coincides with the original DGI model. Results are reported on the largest connected component (LCC) of the input graph. Semi-supervised methods: GCN <ref type="bibr" target="#b19">[20]</ref>, GAT <ref type="bibr" target="#b20">[21]</ref>, GraphSAGE (GS) <ref type="bibr" target="#b1">[2]</ref>, and MoNet <ref type="bibr" target="#b43">[44]</ref>.  For VGAE and ARGVA, we report their best performing variant for each dataset-metric pair: four and six total variants, respectively. For DGI only, we set F = 32 which greatly improves its results compared to F = 16.</p><p>6 Experimental results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Study</head><p>In <ref type="figure" target="#fig_3">Figure 4</ref>, we plot the t-SNE 2D projection <ref type="bibr" target="#b6">[7]</ref> of the learned node representations for the CORA dataset and F = 64, and use silhouette scores (SIL) <ref type="bibr" target="#b5">[6]</ref> to evaluate the results. SIL is the the mean silhouette coefficient of all nodes, where the silhouette coefficient of each node n i is a function of (i) the average distance between n i and all other nodes with the same label and (ii) the lowest average distance of n i to all nodes in any other label. Here, these distances are computed in the 2D projected space.</p><p>As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, parameter α (which controls the relative importance of the two objective terms) tends to lead the optimized cluster summaries to show a certain behavior: A larger α pushes these summaries far from the center of the embedding space, while α = 0 usually leads them to be near the center. For example, when comparing <ref type="figure" target="#fig_3">Figure 4c</ref> (K = 128) to <ref type="figure" target="#fig_0">Figure 4a (K = 128)</ref>, most cluster summaries are spread to the borders of the embedding space (which also achieves a better SIL score).</p><p>With alternating parameter β, the distance between two cluster summaries is affected. A large β, e.g., β = 100, makes these distances larger than a smaller one, e.g., β = 10. For example, when comparing <ref type="figure" target="#fig_3">Figure 4d</ref> (K = 128) to <ref type="figure" target="#fig_3">Figure 4c</ref> (K = 128), for β = 100, the cluster summaries are more well-located in the embedding space, while for β = 10, we observe that most of them coincide.</p><p>Although increasing K generally helps, e.g., <ref type="figure" target="#fig_3">Figure 4a</ref> (K = 128) compared to <ref type="figure" target="#fig_3">Figure 4a (K = 7)</ref>, the choice of K is not independent on the choice of α, β: <ref type="figure" target="#fig_3">Figure 4b</ref> is a counterexample that K = 7 achieved the best SIL score. Acc: accuracy, NMI: normalized mutual information, ARI: average rand index in percents (%). For VGAE and ARGVA, we report their best performing variant for each dataset-metric pair: four and six total variants, respectively. For DGI, we also provide results in parentheses for F = 512 (CORA and CiteSeer) and F = 128 (PubMed), as reference.</p><p>Generally, we found that setting α close to values of 0.5 achieves better results α = 0 or α = 1 (see <ref type="figure">Supplementary  Material)</ref>. Finally, we observe the same behaviour based on β and K values in larger datasets, i.e., PubMed, as shown in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison against competing approaches</head><p>Node classification. GIC against DGI for the task of node classification and we present the results in <ref type="table" target="#tab_0">Table 1</ref>. As we can see, GIC outperforms DGI in all datasets. In CORA and PubMed, GIC achieves a mean classification accuracy gain of more than 1%. In CiteSeer, CoauthorCS and CoauthorPhysics, the gain is slightly lower, but still more than 0.4%, on average. In AmazonComputers and AmazonPhoto, GIC performs significantly better than DGI with a gain of more than 2%, on average. Moreover, GIC's performance lies always in between the best and worst performing semi-supervised method. In all cases, GIC performs better than the worst performing semi-supervised method with a gain of more than 1.5% and as high as 4.3%. Finally, as it is later demonstrated, GIC achieves even higher gains over DGI in cases like clustering, where the downstream model does not have access to the labels.</p><p>Link prediction. <ref type="table" target="#tab_1">Table 2</ref> illustrates the benefits of GIC over DGI for link prediction tasks. GIC outperforms DGI in all three datasets, by around 3.5% in CORA, 1.5% in CiteSeer, and 2.5% in PubMed, even though GIC's embedding size is half of DGI's. GIC also outperforms VGAE and ARGVA, in CORA and CiteSeer by 1%-2% and 4.5%-5.5%, respectively. In PubMed, the performance of GIC and DGI is worse than that of VGAE and ARGVA. We believe this is because reconstruction-based methods overestimate the existing node links (they reconstruct the adjacency matrix), which favors datasets like PubMed (this is not the case for CORA and CiteSeer which have significantly more attributes to exploit).</p><p>Clustering. <ref type="table" target="#tab_2">Table 3</ref> illustrates GIC's performance for clustering. GIC performs better than other unsupervised methods in two out of three datasets, except for PubMed, where ARGVA works slightly better (however, we note that GIC outperforms five out of six ARGVA variants in this dataset). The gain over DGI is significantly large in all datasets, and can be as high as 15% to 18.5% for the NMI metric. GIC outperforms DGI in most metrics and datasets, even though DGI uses 16 times the embedding size of GIC (F = 512).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented Graph InfoClust (GIC), an unsupervised graph representation learning method which relies on leveraging cluster-level content. GIC identifies nodes with similar representations, clusters them together, and maximizes their mutual information. This enables us to improve the quality of node representations with richer content and obtain better results than existing approaches for tasks like node classification, link prediction, clustering, and data visualization. We evaluated the performance of GIC using seven commonly used benchmark datasets <ref type="table" target="#tab_3">(Table 4)</ref>.</p><p>CORA, CiteSeer, and PubMed <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> are three text classification datasets. Each dataset contains bag-of-words representation of documents as features and citation links between the documents as edges. The CORA dataset <ref type="bibr" target="#b7">[8]</ref> contains a number of machine-learning papers divided into one of seven classes while the CiteSeer dataset <ref type="bibr" target="#b30">[31]</ref> has six class labels. The PubMed dataset <ref type="bibr" target="#b31">[32]</ref> consists of scientific publications from the PubMed database pertaining to diabetes classified into three classes.</p><p>CoauthorCS and CoauthorPhysics <ref type="bibr" target="#b32">[33]</ref> are co-authorship graphs based on the Microsoft Academic Graph. Nodes are authors and edges represent co-authorship relations. Classes represent the most active fields of studies, and node features are the bag-of-words encoded paper keywords for each author's papers.</p><p>AmazonComputer and AmazonPhoto <ref type="bibr" target="#b32">[33]</ref> are segments of the Amazon co-purchase graph <ref type="bibr" target="#b33">[34]</ref>. Nodes represent items and are classified into product categories, with their features to be bag-of-words encoded product reviews. Edges indicate that two items are frequently bought together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Competing Approaches</head><p>We compare the performance of GIC against the following unsupervised methods and baselines.</p><p>• Deep Graph Infomax (DGI) <ref type="bibr" target="#b3">[4]</ref> • Variational Graph Auto-Encoders (GAE*/VGAE* and GAE/VGAE) <ref type="bibr" target="#b2">[3]</ref> • Adversarially Regularized Graph Autoencoder (ARGVA) <ref type="bibr" target="#b40">[41]</ref>, and its variants ARGA, ARGA-DG, ARGVA-DG, ARGA-AX, and ARGVA-AX</p><p>• DeepWalk <ref type="bibr" target="#b0">[1]</ref>, Text-Associated DeepWalk (TADW) <ref type="bibr" target="#b45">[46]</ref> • Deep Neural Network for Graph Representation (DNGR) <ref type="bibr" target="#b15">[16]</ref> • Spectral Clustering <ref type="bibr" target="#b44">[45]</ref> Representative GNN unsupervised methods that rely on both structure and features include GAE/VGAE, which are well-known reconstruction-based methods, and ARGVA and its variants, which additionally learn the data distribution in an adversarial fashion. GAE*/VGAE*, Spectral Clustering, DeepWalk, and DNRG rely only on graph structure. TADW is a version of DeepWalk that additionally accounts for features.</p><p>To evaluate how GIC's unsupervised representations compared to semi-supervised approaches that use the labels during representation learning, we used the following semi-supervised approaches.</p><p>• Graph Convolutional Network (GCN) <ref type="bibr" target="#b19">[20]</ref> • Graph Attention Network (GAT) <ref type="bibr" target="#b20">[21]</ref> • Mixture Model Network (MoNet) <ref type="bibr" target="#b43">[44]</ref> • GraphSAGE (GS) <ref type="bibr" target="#b1">[2]</ref>, and its aggregator types GS-mean, GS-meanpool, GS-maxpool which are well-known GNN-based methods.  <ref type="table">Table 5</ref>: Silhouette scores of the t-SNE 2D projections of the learned node representations, based on different hyperparameter α, β, K values. Bold font indicates the two best scores for each dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Hardware and software</head><p>We implemented GIC using the Deep Graph Library <ref type="bibr" target="#b42">[43]</ref> and PyTorch <ref type="bibr" target="#b48">[49]</ref> (the code will be made publicly available after the paper is accepted). We also implemented GIC by modifying DGI's original implementation 1 , which we use in some experiments, e.g., link prediction and clustering. All experiments were performed on a Nvidia Geforce RTX-2070 GPU on a i5-8400 CPU and 32GB RAM machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Additional experiments</head><p>The additional experiments are part of the Ablation Study section. In <ref type="table">Table 5</ref>, we present silhouette scores (SIL) for different datasets, based on different hyperparameter α, β, K values. In <ref type="figure" target="#fig_4">Figure 5</ref>, we also plot the learned node representations for PubMed dataset, after t-SNE 2D projection is applied.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>2D t-SNE projections</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Node representations (after t-SNE 2D projection is applied) and corresponding silhouette scores (SIL) (the higher the better) of leveraging a single graph summary (DGI, top) versus 7 additional cluster summaries (GIC, bottom). GIC is less sensitive to the value of the embedding dimensions F compared to DGI.withÃ ∈ R N ×N andX ∈ R N ×F , for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>GIC's framework. (a) A fake input is created based on the real one. (b) Embeddings are computed for both inputs with a GNN-encoder. (c) The graph and cluster summaries are computed. (d) The goal is to discriminate between real and fake samples based on the computed summaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>α = 0.5, β = 100 t-SNE plots for CORA dataset and the corresponding silhouette scores (SIL) with α ∈ {0, 0.5}, β ∈ {10, 100}, K ∈ {#classes, 32, 128}. The corresponding SIL score for DGI (α = 1) is 0.212.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>α = 0.5, β = 100 t-SNE plots for PubMed dataset and the corresponding silhouette scores (SIL) with α ∈ {0, 0.5}, β ∈ {10, 100}, K ∈ {#classes, 32, 128}. The corresponding SIL score for DGI (α = 1) is 0.077.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Node classification accuracy (in %) of various datasets and for two different train/val sets: balanced and imbalanced. The datasets are randomly split in each run.</figDesc><table><row><cell></cell><cell></cell><cell>Unsupervised</cell><cell cols="2">Semi-supervised</cell></row><row><cell></cell><cell>GIC</cell><cell>DGI</cell><cell>Best Method</cell><cell>Worst Method</cell></row><row><cell>Train/Val. Sets</cell><cell cols="3">Imbalanced Balanced Imbalanced Balanced</cell><cell>Balanced</cell></row><row><cell>CORA</cell><cell>81.7 ±1.5</cell><cell>80.7 ±1.1 80.2 ±1.8</cell><cell>80.0 ±1.3 81.8 ±1.3 (GAT)</cell><cell>76.6 ±1.9 (GS-maxpool)</cell></row><row><cell>CiteSeer</cell><cell>71.9 ±1.4</cell><cell>70.8 ±2.0 71.5 ±1.3</cell><cell>70.5 ±1.2 71.9 ±1.9 (GCN)</cell><cell>67.5 ±2.3 (GS-maxpool)</cell></row><row><cell>PubMed</cell><cell>77.3 ±1.9</cell><cell>77.4 ±1.9 76.2 ±2.0</cell><cell>76.8 ±2.3 78.7 ±2.3 (GAT)</cell><cell>76.1 ±2.3 (GS-maxpool)</cell></row><row><cell>CoauthorCS</cell><cell>89.4 ±0.4</cell><cell>89.3 ±0.7 89.0 ±0.4</cell><cell cols="2">88.7 ±0.8 91.3 ±2.3 (GS-mean) 85.0 ±1.1 (GS-maxpool)</cell></row><row><cell>CoauthorPhysics</cell><cell>93.1 ±0.7</cell><cell>92.4 ±0.9 92.7 ±0.8</cell><cell cols="2">91.8 ±1.0 93.0 ±0.8 (GS-mean) 90.3 ±1.2 (GS-maxpool)</cell></row><row><cell cols="2">AmazonComputers 81.5 ±1.0</cell><cell>79.5 ±1.4 79.0 ±1.7</cell><cell>77.9 ±1.8 83.5 ±2.2 (MoNet)</cell><cell>78.0±19.0 (GAT)</cell></row><row><cell>AmazonPhoto</cell><cell>90.4 ±1.0</cell><cell>89.0 ±1.6 88.2 ±1.7</cell><cell cols="2">86.8 ±1.7 91.4 ±1.3 (GS-mean) 85.7±20.3 (GAT)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">CORA</cell><cell cols="2">CiteSeer</cell><cell cols="2">PubMed</cell></row><row><cell></cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell></row><row><cell cols="2">Spectral Clustering [45] 84.6 ± 0.01</cell><cell>88.5 ± 0.00</cell><cell>80.5 ± 0.01</cell><cell>85.0 ± 0.01</cell><cell>84.2 ± 0.02</cell><cell>87.8 ± 0.01</cell></row><row><cell>DeepWalk [1]</cell><cell>83.1 ± 0.01</cell><cell>85.0 ± 0.00</cell><cell>80.5 ± 0.02</cell><cell>83.6 ± 0.01</cell><cell>84.4 ± 0.00</cell><cell>84.1 ± 0.00</cell></row><row><cell>VGAE [3]</cell><cell>91.4 ± 0.01</cell><cell>92.6 ± 0.01</cell><cell>90.8 ± 0.02</cell><cell>92.0 ± 0.02</cell><cell>96.4 ± 0.00</cell><cell>96.5 ± 0.00</cell></row><row><cell>ARGVA [41]</cell><cell cols="6">92.4 ± 0.004 93.2 ± 0.003 92.4 ± 0.003 93.0 ± 0.003 96.8 ± 0.001 97.1 ± 0.001</cell></row><row><cell>DGI [4]</cell><cell>89.8 ± 0.8</cell><cell>89.7 ± 1.0</cell><cell>95.5 ± 1.0</cell><cell>95.7 ± 1.0</cell><cell>91.2 ± 0.6</cell><cell>92.2 ± 0.5</cell></row><row><cell>GIC</cell><cell>93.5 ± 0.6</cell><cell>93.3 ± 0.7</cell><cell>97.0 ± 0.5</cell><cell>96.8 ± 0.5</cell><cell>93.7 ± 0.3</cell><cell>93.5 ± 0.3</cell></row></table><note>Link prediction scores: Area Under Curve (AUC) score and Average Precision (AP) score (in %).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Clustering results with respect to the true labels.</figDesc><table><row><cell></cell><cell></cell><cell>CORA</cell><cell></cell><cell></cell><cell>CiteSeer</cell><cell></cell><cell></cell><cell>PubMed</cell><cell></cell></row><row><cell></cell><cell>Acc</cell><cell>NMI</cell><cell>ARI</cell><cell>Acc</cell><cell>NMI</cell><cell>ARI</cell><cell>Acc</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>K-means</cell><cell>49.2</cell><cell>31.1</cell><cell>23.0</cell><cell>54.0</cell><cell>30.5</cell><cell>27.9</cell><cell>39.8</cell><cell>0.1</cell><cell>0.2</cell></row><row><cell cols="2">DeepWalk [1] 48,4</cell><cell>32.7</cell><cell>24.3</cell><cell>33.7</cell><cell>8.8</cell><cell>9.2</cell><cell>68.4</cell><cell>27.9</cell><cell>29.9</cell></row><row><cell>DNGR [16]</cell><cell>41.9</cell><cell>31.8</cell><cell>14.2</cell><cell>32.6</cell><cell>18.0</cell><cell>4.4</cell><cell>45.8</cell><cell>15.5</cell><cell>5.4</cell></row><row><cell>TADW [46]</cell><cell>56.0</cell><cell>44.1</cell><cell>33.2</cell><cell>45.5</cell><cell>29.1</cell><cell>22.8</cell><cell>35.4</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>VGAE [3]</cell><cell>60.9</cell><cell>43.6</cell><cell>34.7</cell><cell>40.8</cell><cell>17.6</cell><cell>12.4</cell><cell>67.2</cell><cell>27.7</cell><cell>27.9</cell></row><row><cell cols="2">ARGVA [41] 71.1</cell><cell>52.6</cell><cell>49.5</cell><cell>58.1</cell><cell>33.8</cell><cell>30.1</cell><cell>69.0</cell><cell>30.5</cell><cell>30.6</cell></row><row><cell>DGI [4]</cell><cell cols="9">59.0 (71.3) (56.4) (51.1) (68.8) (44.4) (45.0) (53.3) (18.1) (16.6) 38.6 33.6 57.9 30.9 27.9 49.9 15.1 14.5</cell></row><row><cell>GIC</cell><cell>72.5</cell><cell>53.7</cell><cell>50.8</cell><cell>69.6</cell><cell>45.3</cell><cell>46.5</cell><cell>67.3</cell><cell>31.9</cell><cell>29.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Datasets statistics; whole graph and largest connected component (LCC) of the graph as reported in<ref type="bibr" target="#b32">[33]</ref>.Label rate is the fraction of nodes in the training set (train size equals to 20×#classes) for node classification tasks.</figDesc><table><row><cell></cell><cell cols="3">Classes Features Nodes</cell><cell cols="5">Edges Label rate Nodes LCC Edges LCC Label rate LCC</cell></row><row><cell>CORA</cell><cell>7</cell><cell>1,433</cell><cell>2,708</cell><cell>6,632</cell><cell>0.0517</cell><cell>2,485</cell><cell>5,069</cell><cell>0.0563</cell></row><row><cell>CiteSeer</cell><cell>6</cell><cell>3,703</cell><cell>3,327</cell><cell>4,614</cell><cell>0.0324</cell><cell>2,110</cell><cell>3,668</cell><cell>0.0569</cell></row><row><cell>PubMed</cell><cell>3</cell><cell cols="2">500 19,717</cell><cell>44,324</cell><cell>0.0030</cell><cell>19,717</cell><cell>44,324</cell><cell>0.0030</cell></row><row><cell>CoauthorCS</cell><cell>15</cell><cell cols="2">6,805 18,333</cell><cell>81,894</cell><cell>0.0164</cell><cell>18,333</cell><cell>81,894</cell><cell>0.0164</cell></row><row><cell>CoauthorPhysics</cell><cell>5</cell><cell cols="3">8,415 34,493 247,962</cell><cell>0.0029</cell><cell>34,493</cell><cell>247,962</cell><cell>0.0029</cell></row><row><cell>AmazonComputer</cell><cell>10</cell><cell cols="3">767 13,752 287,209</cell><cell>0.0145</cell><cell>13,381</cell><cell>245,778</cell><cell>0.0149</cell></row><row><cell>AmazonPhoto</cell><cell>8</cell><cell>745</cell><cell cols="2">7,650 143,663</cell><cell>0.0209</cell><cell>7,487</cell><cell>119,043</cell><cell>0.0214</cell></row><row><cell cols="2">8 Supplementary Material</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8.1 Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">DGI implementations are found in https://github.com/dmlc/dgl/tree/master/examples/pytorch/dgi and https: //github.com/PetarV-/DGI.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End to end learning and optimization on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Wilder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ewing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bistra</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Tambe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural and Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international on conference on information and knowledge management</title>
		<meeting>the 24th ACM international on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Harp: Hierarchical representation learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geniepath: Graph neural networks with adaptive receptive paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4424" to="4431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Citeseer: an automatic citation indexing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERNATIONAL CONFERENCE ON DIGITAL LIBRARIES</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umd</forename><surname>Edu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Yee Whye Teh and Mike Titterington</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
		<respStmt>
			<orgName>Chia Laguna Resort</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The use of the area under the roc curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A relationship between the average precision and the area under the roc curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanhua</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on The Theory of Information Retrieval, ICTIR &apos;15</title>
		<meeting>the 2015 International Conference on The Theory of Information Retrieval, ICTIR &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="349" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning graph embedding with adversarial training methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Fu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Deep graph library: Towards efficient and scalable deep learning on graphs. ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Leveraging social media networks for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="478" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
