<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentiable Model Compression via Pseudo Quantization Noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Défossez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
						</author>
						<title level="a" type="main">Differentiable Model Compression via Pseudo Quantization Noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https://github.com/ facebookresearch/diffq</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to add independent pseudo quantization noise to model parameters during training to approximate the effect of a quantization operator. This method, DIFFQ, is differentiable both with respect to the unquantized parameters, and the number of bits used. Given a single hyper-parameter expressing the desired balance between the quantized model size and accuracy, DIFFQ can optimize the number of bits used per individual weight or groups of weights, in a single training. We experimentally verify that our method outperforms state-of-the-art quantization techniques on several benchmarks and architectures for image classification, language modeling, and audio source separation. For instance, on the Wikitext-103 language modeling benchmark, DIFFQ compresses a 16 layers transformer model by a factor of 8, equivalent to 4 bits precision, while losing only 0.5 points of perplexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>An important factor in the adoption of a deep learning model for real-world applications is how easily it can be pushed to remote devices. It has been observed that larger models usually lead to better performance, for instance with larger ResNets <ref type="bibr" target="#b18">(He et al., 2016)</ref> achieving higher accuracies than smaller ones. In response, the community has worked toward smaller, and more efficient models <ref type="bibr" target="#b42">(Tan &amp; Le, 2019</ref>). Yet an EfficientNet-B3 is still almost 50MB, a considerable amount if the model is to be included in online applications, or should be updated with limited network capabilities. For other applications, such as language modeling <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> or source separation <ref type="bibr" target="#b10">(Défossez et al., 2019)</ref>, the typical model size is closer to 1GB, which rules out any kind of mobile usage.</p><p>The simplest method to reduce model size consists is de- † Equal contribution 1 Facebook AI Research. Correspondence to: Alexandre Défossez &lt;defossez@fb.com&gt;.  Uncomp.</p><p>DiffQ (Ours) QAT Uncompressed <ref type="figure">Figure 1</ref>. ImageNet results using EfficientNet-B3 model. We plot the model size vs. model accuracy using different penalty levels. We additionally, present the uncompressed models (uncomp.) and Quantization Aware Training (QAT) using 4 and 8 bits.</p><p>creasing the number of bits used to encode individual weights. For instance, using 16 bits floating point numbers halves the model size, while retaining a sufficient approximation of the set of real numbers, R, to train with first-order optimization methods <ref type="bibr" target="#b32">(Micikevicius et al., 2018)</ref>. When considering lower precision, for instance, 8 or 4 bits, the set of possible values is no longer a good approximation of R, hence preventing the use of first-order optimization methods. Specifically, uniform quantization requires using the round function, which has zero gradients wherever it is differentiable.</p><p>Quantization can be done as a post-processing step to regular training. However, errors accumulate in a multiplicative fashion across layers, leading to a possibly uncontrolled decrease in the model accuracy. <ref type="bibr" target="#b25">Krishnamoorthi (2018)</ref> propose to use a gradient Straight-Through-Estimator (STE) <ref type="bibr" target="#b3">(Bengio et al., 2013)</ref> in order to provide a non-zero gradient to the original weights. This allows the model to adapt to quantization during training and reduces the final degradation of performance. However, <ref type="bibr" target="#b16">Fan et al. (2020)</ref> noticed instability and bias in the learned weights, as STE is not the true gradient to the function.</p><p>The nature of quantization noise has been extensively studied as part of Analog-to-Digital Converters (ADC). In particular, a useful assumption to facilitate the design of post-arXiv:2104.09987v1 [stat.ML] 20 Apr 2021 processing filters for ADC is the independence of the input value and the "Pseudo Quantization Noise" (PQN), as formalized by <ref type="bibr" target="#b50">Widrow et al. (1996)</ref>. In this work, we show that it also applies to deep learning model quantization, and provides a simple framework in which the output and the quantized model size are both differentiable, without any use of STE. This allows to optimally set the number of bits used per individual weight (or group of weights) to a tradeoff between size and accuracy, in a single training and at almost no extra cost. Even when the number of bits to use is fixed, we show that unlike STE, using independent pseudo quantization noise does not introduce bias in the gradient and achieves higher performance.</p><p>Our Contribution: (i) With DIFFQ, we propose to use pseudo quantization noise to approximate quantization at train time, as a differentiable alternative to STE, both with respect to the unquantized weights and number of bits used.</p><p>(ii) We provide a differentiable model size estimate, so that given a single penalty level λ, DIFFQ optimizes the number of bits per weight or group of weights to achieve a given trade-off between model size and accuracy.</p><p>(iii) We provide extensive experimental validation using various models (ConvNets and Transformers) and domains (image classification, language modeling, audio source separation). We demonstrate the efficiency of DIFFQ both in providing small footprint models with comparable performance to the uncompressed ones, together with easy and stable optimization, using only one sensitive hyper-parameter.</p><p>The paper is organized as follows: in Section 2 we present the relevant literature on the topic of quantization, while in Section 3 we provide a background on traditional quantization principles and discuss the limitations of STE based methods. In Section 4, we introduce our PQN based DIFFQ framework and training procedure. In Section 5, we empirically verify that the DIFFQ matches or surpasses the state-of-the-art on standard benchmarks in vision, natural language processing, and audio processing, along with a finer analysis of its behavior. Finally, we discuss open questions and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early network quantization methods focused on lowbitwidth networks such as BinaryNet <ref type="bibr" target="#b8">(Courbariaux et al., 2015;</ref>, XOR-Nets <ref type="bibr" target="#b38">(Rastegari et al., 2016)</ref>, or Ternary networks <ref type="bibr" target="#b27">(Li et al., 2016;</ref><ref type="bibr" target="#b52">Wu et al., 2018)</ref>. Although these methods produce highly quantized models, their performance is not on par with uncompressed ones. To improve accuracies, higher bitwidth quantization methods were studied <ref type="bibr" target="#b23">(Jung et al., 2019;</ref><ref type="bibr" target="#b55">Zhang et al., 2018a;</ref><ref type="bibr" target="#b33">Mishra et al., 2017)</ref>. These methods followed the STE approach <ref type="bibr" target="#b3">(Bengio et al., 2013)</ref>. STE allows the gradients to be backpropagated through the quantizers and, thus, the network weights can be adapted with gradient descent <ref type="bibr" target="#b9">(Courbariaux et al., 2016)</ref>.</p><p>Variational approaches can be used to make the categorical distribution over quantized weights differentiable. <ref type="bibr" target="#b30">Louizos et al. (2019)</ref> uses a Gumbel-softmax <ref type="bibr" target="#b22">(Jang et al., 2017)</ref> but requires 2 hyper-parameters and has no bitwidth tuning. DIFFQ requires a single hyper-parameter while also supporting automatic bitwidth tuning. <ref type="bibr" target="#b39">Shayer et al. (2018)</ref> relies on a Central Limit Theorem application, however this prevents weights from converging to a deterministic value, which would break the assumptions of the CLT. With DIFFQ, weights are free to converge to any optimal value. Finally <ref type="bibr" target="#b46">Ullrich et al. (2017)</ref> uses a gaussian mixture model trained on top of the weights, adding significant complexity both in terms of code, and computation. In contrast, DIFFQ adds only a differentiable term to the loss, optimized along the rest of the model in a end-to-end fashion.</p><p>An alternative is to use a smoothed version of the quantization operator either with a trained meta-network <ref type="bibr" target="#b6">(Chen et al., 2019)</ref>, however as the smoothed operator converges to the true one, gradients will eventually be zero almost everywhere. <ref type="bibr" target="#b17">Gong et al. (2019)</ref> use a meta-network to provide gradients despite quantization. However, their implementation for training the meta-network still relies on STE.</p><p>Additive noise injection has been studied by <ref type="bibr" target="#b1">Baskin et al. (2018a)</ref>, although only during the first few epochs, after which STE based approximation is used. This work was extented to non uniform quantization <ref type="bibr" target="#b2">(Baskin et al., 2018b)</ref>. In contrast, DIFFQ uses only noise injection, and as demonstrated in Section 5.4, achieves a better accuracy for an equivalent compression level than both methods. Non uniform quantization was also studied by <ref type="bibr" target="#b36">Polino et al. (2018)</ref>, but without differentiability with respect to the weights, with worse performance than DIFFQ (see Section 5.4).</p><p>An important contribution from DIFFQ is the automatic tuning of the bitwidth. <ref type="bibr" target="#b49">Wang et al. (2019)</ref>; <ref type="bibr" target="#b13">Elthakeb et al. (2019)</ref> suggested learning a bitwidth assignment policy using reinforcement learning methods. In contrast, our method select bitwidth along training, using only first order optimization. <ref type="bibr" target="#b21">Jain et al. (2019)</ref>; <ref type="bibr" target="#b14">Esser et al. (2020)</ref> proposed learning the quantizer step-size or dynamic-range using STE, but do not allow to select the bitdwidth. <ref type="bibr" target="#b45">Uhlich et al. (2020)</ref> proposed a re-parametrization that allows to select the bitwidth for each layer through first order optimization, while also relying on STE. The re-parametrization is more complex that the additive noise used in DIFFQ, and will suffer from the biased gradient of STE. Results presented in Section 5.4 show that DIFFQ achieves similar or better trade-offs between model size and accuracy. Besides, in the present work we also explore setting a bitwidth for individual groups of weights within each layer, rather than layer-wise.</p><p>The limitations of STE based methods for quantization were first noticed by <ref type="bibr" target="#b28">Liu &amp; Mattina (2019)</ref>. They recommend using during training a linear combination of the unquantized and quantized weight, with the gradient flowing only through the unquantized contribution. In a similar spirit, <ref type="bibr" target="#b16">Fan et al. (2020)</ref> sample for each layer and iteration whether to use the quantized or unquantized weight. Both methods reduce the bias from STE, but also remove some of the quantization noise during training. In contrast method allows to keep a full pseudo quantization noise without the STE bias.</p><p>A last line of related work is Product Quantization <ref type="bibr" target="#b40">(Stock et al., 2019)</ref>, where codewords are being learned to quantize blocks of weights rather than single weights. This method achieves a higher compression level than per-weight quantization but also requires carefully choosing the size of the codebooks for each layer. In contrast, our method requires only choosing a single hyper-parameter to balance between model size and accuracy. Besides, as noted by <ref type="bibr" target="#b16">Fan et al. (2020)</ref>, per-weight quantization and PQ can be combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>Let us consider a weight vector w ∈ R d , where d ∈ N, typically the weights of convolution or linear layer. Each entry of the vector is typically coded over 32 bits with floating-point precision. We aim to reduce the number of possible states to 2 B , where B 32 is the number of bits of precision. First, we assume w i ∈ [0, 1] for all 1 ≤ i ≤ d. In practice, one would first normalize w aŝ</p><formula xml:id="formula_0">w = w − min(w) max(w) − min(w) ,</formula><p>and provide the tuple (min(w), max(w)) separately as a 32 bits IEEE float. Given that for typical deep learning models d 1, storing this range has a negligible cost. For readability, we describe the method for scalar values, however, this can be easily extended to vectors w ∈ R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Uniform quantization</head><p>The simplest quantization methods consist of taking 2 B points evenly spaced in the range [0, 1] and round each entry of w to the nearest point. One then needs to store for each entry w i , the index Q(w i , B) ∈ [0, 2 B − 1] of its nearest point, which can be efficiently coded over B bits.</p><p>Formally, the index of the nearest quantized point is defined by the index function I as</p><formula xml:id="formula_1">I(x, B) = round x · (2 B − 1) , ∀x ∈ [0, 1]. (1)</formula><p>The quantized value is then decoded with the value function V defined as</p><formula xml:id="formula_2">V(q, B) = q 2 B − 1 , ∀q ∈ [2 B − 1].<label>(2)</label></formula><p>Overall, the quantization process is defined as</p><formula xml:id="formula_3">Q(x, B) = V(I(x, B), B), ∀x ∈ [0, 1], B ∈ N * . (3)</formula><p>While the intuitive definition of quantization is for an integer number of bits, we can extend the previous definitions to a real-valued number of bits B ∈ R * + . Note that variants of this scheme exist, for instance, symmetric uniform quantization, which enforces that 0 is always exactly represented <ref type="bibr" target="#b25">(Krishnamoorthi, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization of the quantized weights</head><p>The weight vector w is typically obtained through the process of training a predictor function parameterized by w, denote as f w , to minimize a loss function L,</p><formula xml:id="formula_4">min w∈R d L(f w ),<label>(4)</label></formula><p>where L(f w ) is the empirical risk over a given dataset. The process of quantizing a vector w over B bits introduces a quantization noise N(w, B) = Q(w, B) − w, which is unaware of the training objective L. Even if w is close to the optimum, Q(w, B) might deteriorate arbitrarily the performance of the predictor.</p><p>Thus, given a fixed budget of bits B, one would ideally like to minimize the empirical risk when considering the quantization process,</p><formula xml:id="formula_5">min w∈R d L(f Q(w,B) ),<label>(5)</label></formula><p>where f Q(w,B) is the predictor function using the quantized model parameters.</p><p>Unfortunately, the gradients of Q(w, B) are zero over its definition domain because of the rounding operation, and as a result, it cannot be optimized using first-order optimization methods such as SGD or Adam <ref type="bibr" target="#b24">(Kingma &amp; Ba, 2015)</ref>. One possible solution is to replace the Jacobian of Q(·, B) with the identity matrix during the backward phase, as suggested in the STE method <ref type="bibr" target="#b3">(Bengio et al., 2013)</ref>. The STE method was applied to quantization as the Quantization Aware Training (QAT) technique by <ref type="bibr" target="#b25">Krishnamoorthi (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The instability and bias in STE</head><p>As described by <ref type="bibr" target="#b16">Fan et al. (2020)</ref>, following the STE approach can cause instability during training and bias in the models' gradients and weights. As a result optimization will fail to converge to the optimal value even on simple cases. To demonstrate that, consider the following 1D least-mean-square problem, where B ∈ N * , the optimal weight w * ∈ [0, 1] such that Q(w * , B) = w * , and Q(w * , B) ∈ (0, 1). Given a random variable X ∈ R with σ 2 = E X 2 such that 0 &lt; σ 2 &lt; ∞, we would like to minimize the following using STE based QAT:</p><formula xml:id="formula_6">min w∈[0,1] L(w) := E 1 2 (XQ(w, B) − Xw * ) 2 .<label>(6)</label></formula><p>We immediately have that the optimum is achieved for Q(w, B) = Q(w * , B). Let us try to optimize (6) using SGD with STE starting from w 0 = w * , with w n the sequence of iterates. We call w − and w + the quantized values just under and above w * , and we assume without loss of generality that Q(w * , B) = w + . The expected gradient with STE at iteration n is given by</p><formula xml:id="formula_7">G n = σ 2 (Q(w n , B) − w * ).<label>(7)</label></formula><p>In particular, G 0 = σ 2 (w + − w * ) &gt; 0, and G n will stay positive until Q(w n , B) = w − . At this point, we will have G n &lt; 0, and will stay so until Q(w n , B) = w + . Thus, we observe that using STE, Q(w n , B) will oscillate between w − and w + , while the optimal value is w + . The pattern of oscillation will depend on the learning rate and relative position of w * within the segment [w − , w + ]. Taking a smaller step size will reduce the amplitude of the oscillations of w n , but not of Q(w n , B), which is what interests us. Indeed, w n oscillations are centered at the boundary (w + + w − )/2. We provide one example of those oscillations on <ref type="figure" target="#fig_2">Figure 2</ref> with w * = 0.11, B = 4, X = 1 a.s. and a step size of 0.5.</p><p>Extrapolating to a model with millions of parameters, at any point in time, a significant fraction of the weights could be quantized to a suboptimal value due to the oscillations implied by the STE method. In the following section, we introduce DIFFQ, a method based on independent additive pseudo quantization noise, that does not suffer from such a bias, while approximating well enough quantization noise to perform efficient quantization aware training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Pseudo quantization noise. A classical assumption in digital signal processing when working with quantized signals is that the quantization noise is approximated by independents uniform variables over [−∆/2, ∆/2] with ∆ = 1 2 B −1 the quantization step. This approximation was studied in depth by <ref type="bibr" target="#b50">(Widrow et al., 1996)</ref> as Pseudo Quantization Noise (PQN). Following this assumption, we define the pseudo quantization functionQ for all x ∈ R and B ∈ R + * as If we look back at the example from Section 3.3, using now (8) instead of STE, the expected gradients for SGD become</p><formula xml:id="formula_8">Q(x, B) = x + ∆ 2 · U[−1, 1],<label>(8)</label></formula><formula xml:id="formula_9">G n = E x · w n + ∆ 2 · U[−1, 1] x − w * x = σ 2 (w n − w * ).</formula><p>which cancels out for w n = w * , so that at convergence we indeed have Q(w n , B) = Q(w * , B), i.e. the gradient estimate is unbiased and converges to the right solution.</p><p>Mixed precision. We used a single-precision B for all the entries of the weight vector w. One can instead use different values for different entries. Formally, the entries in w are grouped by considering w ∈ R g×d/g with g the group size and d/g the number of groups. We can then extend the definition of Q(w, B) given by <ref type="formula" target="#formula_14">(3)</ref> and <ref type="formula" target="#formula_8">(8)</ref> to use a number of bits b s for the group s, with b ∈ R * + d/g .</p><p>Training objective. Given w ∈ R g×d/g with g groups of d/g entries, and a number of bits b ∈ N g * , we define the model size, expressed in MegaBytes (1MB = 8 · 2 20 bits)</p><formula xml:id="formula_10">M(b) = g 2 23 d/g s=1 b s .<label>(9)</label></formula><p>A typical objective of quantization is to achieve the best possible performance within a given model size budget or to achieve the smallest model size that reaches a given performance, i.e. we want to minimize with b ∈ N d/g * , and w ∈ R g×d/g either, We can relax b to be real valued, and replace Q by our differentiable pseudo quantization functionQ. Then, following the exact penalty method <ref type="bibr" target="#b4">(Bertsekas (1997)</ref>, Section 4.2, <ref type="bibr" target="#b5">Bertsekas (2014)</ref>, Chapter 4), there is λ(m) &gt; 0 (or λ(l) for the right hand side problem), such that the left hand size problem is equivalent to</p><formula xml:id="formula_11">min w,b L(f Q(w,b) ), s.t. M(b) ≤ m. or min w,b M(b), s.t. L(f Q(w,b) ) ≤ l.<label>(10)</label></formula><formula xml:id="formula_12">min w,b L(fQ (w,b) ) + λ(m)M(b),<label>(11)</label></formula><p>which is fully differentiable with respect to w and b and can be optimized with first order optimization methods.</p><p>Parametrization. In practice, the number of bits used for each group b ∈ R g * + is obtained from a logit parameter l ∈ R g , so that we have</p><formula xml:id="formula_13">b = b min + σ(l)(b max − b min ),<label>(12)</label></formula><p>with σ is the sigmoid function, and b min and b max the minimal and maximal number of bits to use. The trainable parameter l is initialized so that we have b = b init .</p><p>Evaluation and noise distribution. At evaluation time, we round the value b obtained from (11) asb = round(b) and quantize w as Q(w,b). Thus, the amount of quantization noise at evaluation time can be larger than the amount of noise injected at train time. We observed that using a noise distribution with larger support, such as Gaussian noise, makes the model more robust to this operation.</p><p>True model size. The mode size given by <ref type="formula" target="#formula_10">(9)</ref> is used at train time but does not account for part of the true model size.</p><p>At evaluation time, we represent each layer weights by the indexes given by I(w,b). For each layer in the network, we also store two 32 bits float numbers (coding the minimum and maximum scale). Finally, the actual value ofb must be coded, as it is no longer a fixed constant. For each layer, we compute the maximum value of C s = log 2 (1 +b s − b min ) over all groups s ∈ {1, . . . , d/g}. We encode once the value max(C) as an 8-bit integer, and for each group, we then encode b s − b min over max(C) bits. The true size for one layer, expressed in MegaBytes, is thus given bỹ <ref type="bibr">13)</ref> </p><formula xml:id="formula_14">M(b) = 1 2 23   2 · 32 + 8 + d g max(C) + g d/g s=1 b s  <label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We present experimental results for audio source separation, image classification, and language modeling. We show that DIFFQ can systematically provide a model with comparable performance to the uncompressed one while producing a model with a smaller footprint than the baseline methods (STE based). We provide a finer analysis of different aspects of DIFFQ hyper-parameters and their impact on quantized models in Section 5.5. Both experimental code, and a generic framework usable with any architecture in just a few lines, are available on our Github 1 . All hyperparameters for optimization and model definition for all tasks are detailed in the Supplementary Material, Section A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIFFQ hyper-parameters</head><p>For all experiments, we use b min = 2, b max = 15, b init = 8 and Gaussian noise. We observed on most models that taking b min &lt; 2 is unstable, with the notable exception of Resnet-20. We use a separate Adam optimizer <ref type="bibr" target="#b24">(Kingma &amp; Ba, 2015)</ref> for the logit parameters controlling the number of bits used, with a default momentum β 1 = 0.9 and decay β 2 = 0.999. We use the default learning rate α = 1e−3 for all task, except language modeling where we use α = 1e−2. The remaining hyper-parameters are λ, the amount of penalty applied to the model size, and g, the group size. When g is not mentioned, it is set to the default value g = 8, which we found to be the best trade-off between the model freedom and the overhead from storing the number of bits used for each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Music Source Separation</head><p>We use the Demucs architecture by <ref type="bibr" target="#b10">Défossez et al. (2019)</ref> with 64 initial hidden channels. The baseline is enhanced to account for recent improvement from speech source separation <ref type="bibr" target="#b11">(Défossez et al., 2020)</ref> and pitch/tempo shift augmentation <ref type="bibr" target="#b7">(Cohen-Hadria et al., 2019)</ref>. The model is trained on the standard MusDB benchmark <ref type="bibr" target="#b37">(Rafii et al., 2017)</ref> for 180 epochs, and evaluated with the Signal-To-Distortion Ratio (SDR) metric <ref type="bibr" target="#b48">(Vincent et al., 2006)</ref>. The unquantized model is 1GB. We compare DIFFQ with QAT training with either 5 or 4 bits, with the results presented in <ref type="table" target="#tab_1">Table 1</ref>. With 5 bits, QAT is able to replicate almost the same performance as the uncompressed model. However, trying to further compress the model to 4 bits precision leads to a sharp decrease of the SDR, losing 0.3 points, for a 130MB model. In contrast, DIFFQ achieves a model size of 120MB, with only a drop of 0.03 point of SDR compared to the uncompressed baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Language Modeling</head><p>We trained a 16 layers transformer <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> based language model on the Wikitext-103 text corpus (Merity et al., 2016), following , and using the Fairseq framework <ref type="bibr" target="#b34">(Ott et al., 2019)</ref>. Results are pre-  <ref type="table" target="#tab_2">Table 2</ref>. We compare to the Quant-Noise method by <ref type="bibr" target="#b16">Fan et al. (2020)</ref>, but use a reduced layer-drop  of 0.1 instead of 0.2. This both improves the baseline, as well as the performance of DIFFQ models. In order to further improve the performance of DIFFQ with layer-drop, we explicitly set the gradient for the number of bits parameters to zero for all layers that have been dropped. In order to test the compatibility of DIFFQ with efficient int8 kernels, we further quantize the activations to 8 bits using PyTorch native support <ref type="bibr" target="#b35">(Paszke et al., 2019)</ref>.</p><p>While QAT breaks down when trying to get to 4 bits precision (perplexity of 29.9), using DIFFQ allows to achieve an even lower model size (113MB vs. 118 MB for QAT 4 bits) with a perplexity closer to the uncompressed one (18.6, vs. 18.1 uncompressed). Even more interesting, with a lower penalty of 1, DIFFQ produces a model smaller than QAT 8 bits, and with a perplexity better than the baseline. While Quant-Noise significantly improves over QAT, it produces models that are both larger, and with worse perplexity than DIFFQ. Note that we did not use noise injection to emulate activation quantization, and it seems weight level noise injection was sufficient to make the model robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Image Classification</head><p>Next, we evaluated three image classification benchmarks: ImageNet <ref type="bibr" target="#b12">(Deng et al., 2009</ref>), CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b26">(Krizhevsky et al., 2009</ref>). For CIFAR-10 and CIFAR-100 results are reported for MobileNet-v1 <ref type="bibr" target="#b19">(Howard et al., 2017)</ref>, ResNet-18 <ref type="bibr" target="#b18">(He et al., 2016)</ref>, and Wide-ResNet with 28x10, depth and width levels respectively <ref type="bibr" target="#b54">(Zagoruyko &amp; Komodakis, 2016</ref>   CIFAR10 &amp; CIFAR-100. Results for CIFAR-100 are depicted in <ref type="figure" target="#fig_3">Figure 3</ref>. We compare DIFFQ to QAT using 2, 3, and 4 bits quantization. Performance of the uncompressed model is additionally presented as an upper-bound. To understand the effect of the penalty level λ on both model size and accuracy, we train models with DIFFQ using different penalty levels. Notice, as we decrease λ, model size increases together with model accuracy until it reaches a plateau in performance. Exact results are presented in <ref type="table">Table B</ref>.1, in the Supplementary Material, along with the results on CIFAR-10, and a detailed analysis is in Section B.1.</p><p>Results suggest DIFFQ models reach comparable performance to the QAT models while producing models with a smaller footprint. When considering QAT with 2 bits quantization, DIFFQ archives large accuracy improvements while producing a model with equivalent size.</p><p>ImageNet. Results for ImageNet using DeiT-B model are presented in <ref type="table" target="#tab_4">Table 3</ref>. We compared DIFFQ to QAT when training with 4 and 8 bits. Both QAT with 8 bits and DIFFQ We evaluate the performance of DIFFQ on the memoryefficient architecture EfficientNet-B3 model on <ref type="figure">Figure 1</ref>. Those results are also presented in <ref type="table">Table B</ref>.4 in the Supplementary Material. Both QAT 8 bits and DIFFQ achieves similar performance for equivalent model sizes, with a small drop compared to the uncompressed baseline. However, when considering QAT 4 bits, DIFFQ produces a smaller model with a significantly better accuracy level. In fact, for QAT 4, we noticed considerable instability close to the end of the training, see Section B.2 in the Supplementary Material for a more detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison to related work</head><p>On <ref type="table" target="#tab_5">Table 4</ref>, we compare to some of the related work presented in Section 2. Compared with the NICE <ref type="bibr" target="#b1">(Baskin et al., 2018a)</ref> and UNIQ <ref type="bibr" target="#b2">(Baskin et al., 2018b)</ref> methods, which also rely on additive noise, DIFFQ achieves significantly better accuracy for the same model size. We then compare to the differentiable quantization method by <ref type="bibr" target="#b36">(Polino et al., 2018)</ref>, which only optimizes the non uniform quantization points, not the pre-quantization weights. Following their practice, we report numbers after Huffman coding. We achieve a model almost half as small, with a gap of 25% in accuracy, which highlight that optimizing pre-quantization 0 1 2 3 4 5 6 7 8 9 10 Layer group 10 4 10 5 10 6 10 7 10 8</p><p>Size in bits (log scale) fp32 2 bits 3 bits 4 bits 5 bits 6 bits 7 bits 8 bits 9 bits 10 bits 11 bits 12 bits bits bits 0 1 2 3 4 5 6 7 8 9 10 Layer group 10 6 10 7 10 8 10 9</p><p>Size in bits (log scale) fp32 3 bits 4 bits 5 bits 6 bits 7 bits 8 bits 9 bits 10 bits 11 bits bits bits <ref type="figure">Figure 4</ref>. Layer groups wise size of the baseline EfficientNet-B3 (above) and DeiT (below) models (floating point 32 bits) and the quantized model with DIFFQ (λ=5e−3 for EfficientNet-B3, λ=1e−2 for DeiT) with cumulative bits per weights. Layers are grouped in increasing depth (0 is close to the input), "bits bits" shows the capacity needed to encode the weights' bitwidth.</p><p>weights is more important than tuning a non uniform quantization grid. Meta-Quant <ref type="bibr" target="#b6">(Chen et al., 2019)</ref> achieves smaller model size than DIFFQ, with 1 bit per weight, a regime where the PQN assumption breaks down, at the price of losing nearly 10% of accuracy. Finally, compared with the D.Q. method by <ref type="bibr" target="#b45">Uhlich et al. (2020)</ref>, we achieve comparable results on ImageNet with ResNet-18, and both smaller and more accurate models on CIFAR-10 with ResNet-20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Analysis</head><p>In the following, we perform a finer analysis of the behavior of DIFFQ. First we look at the distribution of the number of bits used for different layers for various architectures. We then perform an ablation study, looking at the performance of DIFFQ without learning the number of bits, and the impact of the group size. Finally, we compare the difference in entropy of the learned models with QAT and DIFFQ. <ref type="figure">Figure 4</ref> presents the weight bitwidth as-  <ref type="bibr" target="#b44">(Touvron et al., 2020)</ref> models trained on ImageNet. The capacity distribution over depth for Con-vNets EfficientNet-B3) and Transformers (DeiT) are different (fp32 shows uncompressed capacity). Notice, that the quantization trends are different too: for the ConvNet, smaller bitwidths are used for deeper layers of the model while large bitwidth is more common in the first layers (except for the last linear layer which seems to need some precision). For the Transformer, this effect of varying quantization by layer is similar but less pronounced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bits Histogram</head><p>Ablation Recall, in Section 3.3 we demonstrated the instability of following STE for optimization. In <ref type="table" target="#tab_6">Table 5</ref> we compare QAT to DIFFQ for a ResNet-18 trained on CIFAR-100, where we consider a fixed number of bits for all model parameters. DIFFQ outperforms QAT, where this is especially noticeable while using 2 bits quantization, in which training is less stable for QAT. The same occurs for other architectures (MobileNet, Wide-ResNet) and datasets (CIFAR-10) as shown in <ref type="table">Table B</ref>.3, in the Supplementary.</p><p>Next, we evaluated the affect of the group-size, g, on model size and accuracy, by optimizing DIFFQ models using g ∈ {1, 4, 8, ∞}. When g = ∞, we use a single group for the entire layer. Results for ResNet-18 using CIFAR-100 are summarized in <ref type="figure">Figure 5</ref>. Interestingly, we observed that increasing g, yields in a smaller model size on the expanse of a minor decrease in performance. However, when setting g = ∞ model performance (model size and accuracy) is comparable to g = 8 for this task.</p><p>DIFFQ maximizes entropy usage The model size given by (13) is obtained with a traditional encoding of the quantized model. However, more efficient coding techniques exist when the entropy of the data is low, such as Huffman coding <ref type="bibr" target="#b20">(Huffman, 1952)</ref>. Using the ZLib library, we obtain an estimate of the Huffman compressed model size after quantization. For instance, for the language model described in Uncomp. g = 1 g = 4 g = 8 g = <ref type="figure">Figure 5</ref>. DIFFQ results with various groups sizes (g ∈ {1, 4, 8, ∞}). g = ∞ refers to a single group for the entire layer. For reference, we report the accuracy of the uncompressed model (42.8 MB). Models are Resnet-18 trained on CIFAR-100.</p><p>weight is significantly lower than the maximal one for 8 bits integers. However, the DIFFQ model naive size is 113MB, and after compression by ZLib, gets to 122MB. This is a sign that the underlying entropy is close to its maximal value, with ZLib adding only overhead for no actual gain.</p><p>In <ref type="formula" target="#formula_12">(11)</ref>, we only penalize the naive number of bits used, while asking for the best possible accuracy. In that case, the model maximally use the entropy capabilities for a given number of bits. An interesting line of research would be to replace the model size (9) to account for the actual entropy of the data. We leave that for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We presented DIFFQ, a novel and simple differentiable method for model quantization via pseudo quantization noise addition to models' parameters. Given a single hyperparameter that quantifies the desired trade-off between model size and accuracy, DIFFQ can optimize the number of bits used for each trainable parameter or group of parameters during model training.</p><p>We conduct expensive experimental evaluations on various domains using different model architectures. Results suggest that DIFFQ is superior to the baseline methods on several benchmarks from various domains. On ImageNet, Wikitext-103, and MusDB, we achieve a model size equivalent or smaller to a 4 bits quantized model, while retaining the same performance as the unquantized baseline.</p><p>For future work, we consider adapting the model size penalty to account for Huffman encoding, which could allow to further reduce the model size when it is gzipped. Another line of work would be using PQN to improve activation quantization. While 8-bits activations are well supported, this could open up the way to a 4-bits kernel. deviation 1. We use the default split between train and valid as obtained from the torchvision package.</p><p>ImageNet. We train an EfficientNet as implemented by <ref type="bibr" target="#b51">(Wightman, 2019)</ref>, as well as a DeiT vision transformer <ref type="bibr" target="#b44">(Touvron et al., 2020)</ref> on the ImageNet dataset <ref type="bibr">(Deng et al., 2009) 7</ref> . We use the original dataset split between train and valid. The images go through a random resize crop to 300px, a random horizontal flip, and pixel RGB values are normalized to have zero mean and unit variance.</p><p>ImageNet -EfficientNet. We trained for 100 epochs, using RMSProp <ref type="bibr" target="#b43">(Tieleman &amp; Hinton, 2012)</ref> as implemented in the timm package 8 with a learning rate of 0.0016, a weight decay of 1e − 5 and a momentum of 0.9. The learning rate is decayed by a factor of 0.9875 with every epoch. As a warmup, the learning rate is linearly scaled from 0 to 0.0016 over the first 3 epochs. Following <ref type="bibr" target="#b51">(Wightman, 2019)</ref>, we evaluate with an exponential moving average of the weights of the model, with a decay of 0.99985. We use the random erase augmentation from <ref type="bibr" target="#b51">(Wightman, 2019)</ref>, as well as cutmix <ref type="bibr" target="#b53">(Yun et al., 2019)</ref>, with a probability of 0.2 and parameter to the beta distribution of 0.2. All the models are trained on 8 GPUs. For DIFFQ, we used the penalties λ in {5e−4, 1e−3, 5e−3, 1e−2, 5e−2, 0.1, 0.5} and the default group size g = 8.</p><p>ImageNet -DeiT. We use the official DeiT implementation by <ref type="bibr">Touvron et al. (2020) 9</ref> , with the default training parameters, but without exponential moving averaging of the weights. More precisely, we trained for 300 epochs over 16 GPUs, with a batch size per GPU of 64, AdamW <ref type="bibr" target="#b29">(Loshchilov &amp; Hutter, 2019)</ref>, a weight decay of 0.05, learning rate of 5e−4, cosine learning rate scheduler, a learning rate warmup from 1e−6 over 5 epochs and label smoothing <ref type="bibr" target="#b41">(Szegedy et al., 2016)</ref>. As data augmentation, we used color-jitter, random erase, and either cutmix or mixup <ref type="bibr" target="#b56">(Zhang et al., 2018b)</ref>.</p><p>For DIFFQ, we tested the penalty λ in {1e−3, 1e−2, 0.1, 0.5, 1, 5}, and group size g in {1, 4, 8}. We use a minimum number of bits of 3, instead of 2, as this led to better stability. We use Adam <ref type="bibr" target="#b24">(Kingma &amp; Ba, 2015)</ref> to optimize the bits parameters, with a learning rate of 5e−4. We report on <ref type="table">Table B</ref>.1 the results on the CIFAR10/100 datasets, which are shown for CIFAR100 in <ref type="figure" target="#fig_3">Figure 3</ref> in the main paper. Results are presented using MobileNet-v1, ResNet-18, and WideResNet. For CIFAR100 the presented results used for creating <ref type="figure" target="#fig_3">Figure 3</ref> in the main paper. As we cannot show all the DIFFQ runs, we selected for each model and dataset two versions: v1 is the smallest model that has an accuracy comparable to the baseline (accuracy is greater than 1 − 1/100 times the baseline accuracy), while v2 is the model with the highest accuracy that is comparable in size with the QAT 2 bits model (size must be smaller than 1 + 1/100 times the baseline size, except for MobileNet, for which we had to allow a 4% relative increase in size. The penalty and group size selected with this procedure is displayed on <ref type="table">Table B</ref>.2. Looking first at v1 models, we achieve on all tasks and datasets a model that is competitive with the baseline (sometimes even better), with a model size that is smaller than a QAT 4 bits model (for instance more than 2MB saved on a ResNet-18 trained on CIFAR-10 compared to QAT 4 bits, for the same accuracy). Now for v2, first note that as the minimum number of bits used by DIFFQ is exactly 2, it is not possible here to make a model smaller than QAT 2 bits. However, even with as little as 0.01 MB extra, DIFFQ can get up to 30% increase in accuracy compared to QAT 2 bits (for a Wide ResNet). On all architectue and datasets, the gain from DIFFQ over QAT 2 bits is at least 10% accuracy. This confirms in practice the bias of STE-based methods when the number of bits is reduced, a bias that we already demonstrated in theory in Section 3.3. In particular, it is interesting that the largest improvement provided by DIFFQ is for the Wide ResNet model, which should be the easiest to quantize. But having the largest number of weights, it also likely the one that is the most sensitive to the oscillations of QAT quantized weights described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Supplementary results</head><p>Next, in <ref type="table">Table B</ref>.3 we compare QAT against DIFFQ for model quantization using a fixed number of bits. These results are complementary to <ref type="table" target="#tab_6">Table 5</ref> in the main paper where we report results for CIFAR-100 using ResNet-18 model only. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. EfficientNet-b3 on ImageNet</head><p>On <ref type="table">Table B</ref>.4 we report the results for training EfficientNet-b3 <ref type="bibr" target="#b42">(Tan &amp; Le, 2019)</ref> on the ImageNet dataset, matching the results reported on <ref type="figure">Figure 1</ref>.</p><p>As previously, we selected two versions of DIFFQ, one matching the size of QAT 8bits, and one smallest than QAT 4 bits. At 8 bits, DIFFQ achieves the same accuracy as the uncompressed baseline, for a slightly smaller model than QAT 8bits. As we lower the number of bits, we again see a clear advantage for DIFFQ, with both a smaller model (5.7MB against 6.1MB) than QAT 4bits, and significantly higher accuracy (76.8% vs. 57.3%).</p><p>The lower accuracy for QAT4 on ImageNet led us to take a closer look at the model performance. <ref type="figure">Figure 6</ref> depicts the model accuracy as a function of the number of epochs for both QAT4 and DIFFQ. Notice, similarly to the toy example presented in <ref type="table">Table B</ref>.4. Image classification results for the ImageNet benchmark. Results are presented for DIFFQ and QAT using 4 and 8 bits using the EfficientNet-b3 model <ref type="bibr" target="#b42">(Tan &amp; Le, 2019)</ref>. We report Top-1 Accuracy (Acc.) together with Model Size (M.S. Section 3.3 training with QAT4 creates instability in the model optimization (especially near model convergence), which leads to significant differences in performance across adjacent epochs. When considering DIFFQ, model optimization is stable and no such differences are observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Activation Quantization for Language Modeling</head><p>In <ref type="table">Table B</ref>.5 we report language modeling results for a 16-layers Transformer models while applying activation quantization. Unlike the results in <ref type="table" target="#tab_2">Table 2</ref> where we used per-channel activation quantization, here we report results with a histogram quantizer. Additionally when considering histogram quantizer, results suggest DIFFQ is superior to both QAT and QN when considering both model size and model performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Preprint version.Copyright 2021 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Using STE and SGD to optimize the 1D least-meansquare problem given by (6) (with B = 4 and X = 1 a.s.). Q(wn, B) oscillates between the quantized value just above (w+) and just under (w−) the unquantized ground truth w * , while wn oscillates around the boundary (w+ + w−)/2. Diminishing the step size will reduce the oscillations of wn, but will not reduce the scale of oscillations of the quantized value Q(wn, B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Model accuracy and size on CIFAR-100 using Mo-bileNet, ResNet-18, and WideResNet (WRN) models for various penalty levels using DIFFQ, QAT 4 and 8 models, and the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This pseudo quantization function is differentiable with respect to x and B. Unlike QAT, this differentiability does not require an STE. It also provides a meaningful gradient with respect to the number of bits used B (extended to be real-valued).</figDesc><table><row><cell>with U[−1, 1] an independent sample from the uniform dis-</cell></row><row><cell>tribution over [−1, 1].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Music source separation results for the Demucs model<ref type="bibr" target="#b10">(Défossez et al., 2019)</ref>. Results are presented for DIFFQ and QAT using 4 and 5 bits. We report Signal-to-Distortion Ration (SDR) together with Model Size (M.S.).</figDesc><table><row><cell></cell><cell cols="2">SDR ↑ M. S. (MB) ↓</cell></row><row><cell>UNCOMPRESSED</cell><cell>6.31</cell><cell>1014</cell></row><row><cell>QAT 4BITS</cell><cell>5.99</cell><cell>130</cell></row><row><cell>QAT 5BITS</cell><cell>6.27</cell><cell>162</cell></row><row><cell>DIFFQ (λ=3e−4)</cell><cell>6.28</cell><cell>120</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Language modeling results for a 16 layer Transformer trained on Wikitext-103. We also test combining weight and activation quantization. We compared DIFFQ to QAT and Quant-Noise (QN) method proposed by<ref type="bibr" target="#b16">Fan et al. (2020)</ref> (models with † were trained with a layer-drop of 0.2.).</figDesc><table><row><cell>Activations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Image classification results for the ImageNet benchmark. Results are presented for DIFFQ and QAT using 4 and 8 bits using the DeiT model<ref type="bibr" target="#b44">(Touvron et al., 2020)</ref>. We report Top-1 Accuracy (Acc.) together with Model Size (M.S.).</figDesc><table><row><cell></cell><cell cols="2">TOP-1 ACC. (%) ↑ M.S. (MB) ↓</cell></row><row><cell>UNCOMPRESSED</cell><cell>81.8</cell><cell>371.4</cell></row><row><cell>QAT 4BITS</cell><cell>79.2</cell><cell>41.7</cell></row><row><cell>QAT 8BITS</cell><cell>81.6</cell><cell>82.9</cell></row><row><cell>DIFFQ (λ=1e−2)</cell><cell>82.0</cell><cell>45.7</cell></row><row><cell>DIFFQ (λ=0.1)</cell><cell>81.5</cell><cell>33.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison of DIFFQ against baslines presented in Section 2. Each table section is for a specific dataset and model. We report accuracy (Acc.) and Model Size (M.S.). Sizes marked with † are reported after Huffman coding, following<ref type="bibr" target="#b36">Polino et al. (2018)</ref>.</figDesc><table><row><cell>METHOD</cell><cell>ACC. ↑</cell><cell>M.S. ↓</cell></row><row><cell cols="2">CIFAR10 -ResNet-18</cell><cell></cell></row><row><cell>DIFFQ (ours)</cell><cell>93.9</cell><cell>2.7 MB</cell></row><row><cell>NICE (Baskin et al., 2018a)</cell><cell>92.7</cell><cell>2.7 MB</cell></row><row><cell>UNIQ (Baskin et al., 2018b)</cell><cell>89.1</cell><cell>2.7 MB</cell></row><row><cell cols="2">CIFAR100 -Wide-ResNet</cell><cell></cell></row><row><cell>DIFFQ (ours)</cell><cell cols="2">75.6 4.7 MB  †</cell></row><row><cell>Diff. Quant. (Polino et al., 2018)</cell><cell cols="2">49.3 7.9 MB  †</cell></row><row><cell cols="2">ImageNet -ResNet-18</cell><cell></cell></row><row><cell>DIFFQ (ours)</cell><cell>69.4</cell><cell>5.3 MB</cell></row><row><cell>DQ (Uhlich et al., 2020)</cell><cell>70.1</cell><cell>5.4 MB</cell></row><row><cell>Meta-Quant (Chen et al., 2019)</cell><cell>60.3</cell><cell>1.3 MB</cell></row><row><cell cols="2">CIFAR-10 -ResNet-20</cell><cell></cell></row><row><cell>DIFFQ (ours)</cell><cell>91.7</cell><cell>69 KB</cell></row><row><cell>DQ (Uhlich et al., 2020)</cell><cell>91.4</cell><cell>70 KB</cell></row><row><cell cols="3">reach comparable performance to the uncompressed model,</cell></row><row><cell cols="3">while DIFFQ yields a model almost half of the size as QAT,</cell></row><row><cell cols="3">however still bigger than QAT with 4 bits. When we increase</cell></row><row><cell cols="3">λ, we get a smaller model-size than QAT with 4 bits but</cell></row><row><cell>with better accuracy levels.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>A comparison between QAT and DIFFQ while we consider a fixed number of bits for all model parameters, specifically using 2, 3, and 4 bits. We report Accuracy (Acc.) and Model Size (M.S.) for a ResNet-18 model trained on CIFAR-100.</figDesc><table><row><cell></cell><cell cols="2">ACC. (%) ↑ M. S. (MB) ↓</cell></row><row><cell>UNCOMPRESSED</cell><cell>77.9</cell><cell>42.81</cell></row><row><cell>QAT 2BITS</cell><cell>58.7</cell><cell>2.72</cell></row><row><cell>QAT 3BITS</cell><cell>73.7</cell><cell>4.05</cell></row><row><cell>QAT 4BITS</cell><cell>77.3</cell><cell>5.39</cell></row><row><cell>DIFFQ 2BITS</cell><cell>66.6</cell><cell>2.72</cell></row><row><cell>DIFFQ 3BITS</cell><cell>76.7</cell><cell>4.05</cell></row><row><cell>DIFFQ 4BITS</cell><cell>77.5</cell><cell>5.39</cell></row><row><cell cols="3">signment over layer groups for the EfficientNet-B3 (Tan &amp;</cell></row><row><cell>Le, 2019) and DeiT</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>, the QAT 8 model gets further compressed from</cell></row><row><cell>236MB to 150MB, showing that the entropy of its quantized</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table B.1. Detailed results of QAT and DIFFQ on the CIFAR-10/100 datasets. For each architecture and dataset, we provide the performance of the baseline, QAT models with 2 to 4 bits, and two DIFFQ runs: v1. is the smallest model that is within a small range of the baseline performance, v2. is the best model of comparable size with QAT 2 bits, selected from the pool of candidates described in Section A.3. For Wide-ResNet, we report a single variant of DIFFQ, as it is both the smallest and the one with the best accuracy.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MOBILENET</cell><cell></cell><cell>RESNET-18</cell><cell cols="2">WIDERESNET</cell></row><row><cell></cell><cell></cell><cell cols="6">ACC. (%) ↑ M. S. (MB) ↓ ACC. (%) ↑ M. S. (MB) ↓ ACC. (%) ↑ M. S. (MB) ↓</cell></row><row><cell></cell><cell>UNCOMPRESSED</cell><cell>90.9</cell><cell>12.3</cell><cell>95.3</cell><cell>42.7</cell><cell>95.3</cell><cell>139.2</cell></row><row><cell>CIFAR-10</cell><cell>QAT 2BITS QAT 3BITS QAT 4BITS</cell><cell>78.1 88.2 90.1</cell><cell>0.88 1.26 1.64</cell><cell>87.2 94.0 95.0</cell><cell>2.70 4.03 5.36</cell><cell>70.8 94.3 94.4</cell><cell>8.81 13.16 17.50</cell></row><row><cell></cell><cell>DIFFQ V1</cell><cell>90.3</cell><cell>0.94</cell><cell>94.9</cell><cell>3.17</cell><cell>94.1</cell><cell>8.81</cell></row><row><cell></cell><cell>DIFFQ V2</cell><cell>87.9</cell><cell>0.91</cell><cell>93.9</cell><cell>2.71</cell><cell>94.1</cell><cell>8.81</cell></row><row><cell>CIFAR-100</cell><cell>UNCOMPRESSED QAT 2BITS QAT 3BITS QAT 4BITS DIFFQ V1</cell><cell>68.1 10.9 59.7 66.9 68.5</cell><cell>12.6 0.91 1.29 1.69 1.10</cell><cell>77.9 58.7 73.7 77.3 77.6</cell><cell>42.8 2.72 4.05 5.39 4.82</cell><cell>76.2 46.5 75.0 75.5 75.3</cell><cell>139.4 8.83 13.18 17.53 8.83</cell></row><row><cell></cell><cell>DIFFQ V2</cell><cell>64.6</cell><cell>0.94</cell><cell>71.7</cell><cell>2.72</cell><cell>75.6</cell><cell>8.84</cell></row><row><cell cols="2">B.1. CIFAR-10/100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table B.2. Penalty λ and group size g for the v1 and v2 DIFFQ models reported on Table B.1</figDesc><table><row><cell></cell><cell cols="6">MOBILENET RESNET-18 WIDERESNET</cell></row><row><cell></cell><cell>λ</cell><cell>g</cell><cell>λ</cell><cell>g</cell><cell>λ</cell><cell>g</cell></row><row><cell>CIFAR-10</cell><cell>DIFFQ V1 1 DIFFQ V2 5</cell><cell>16 8</cell><cell>0.1 5</cell><cell>8 4</cell><cell>5 5</cell><cell>16 16</cell></row><row><cell>CIFAR-100</cell><cell>DIFFQ V1 1 DIFFQ V2 5</cell><cell>16 16</cell><cell>0.05 5</cell><cell>4 8</cell><cell>5 1</cell><cell>16 16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table B.3. A comparison between QAT and DIFFQ while we consider a fixed number of bits for all model parameters, specifically using 2, 3, and 4 bits. Results are reported for CIFAR-10 and CIFAR-100 using MobileNet-v1, ResNet-18. and WideResNet. We report Accuracy(Acc.) and Model Size (M.S.). ↑ M. S. (MB) ↓ ACC. (%) ↑ M. S. (MB) ↓ ACC. (%) ↑ M. S. (MB) ↓</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MOBILENET</cell><cell></cell><cell>RESNET-18</cell><cell cols="2">WIDERESNET</cell></row><row><cell cols="3">UNCOMPRESSED QAT 2BITS QAT 3BITS QAT 4BITS DIFFQ 2BITS ACC. (%) CIFAR-10 90.9 78.1 88.2 90.1 84.1 DIFFQ 3BITS 89.7</cell><cell>12.3 0.88 1.26 1.64 0.88 1.26</cell><cell>95.3 87.2 94.0 95.0 92.3 94.4</cell><cell>42.8 2.70 4.03 5.36 2.70 4.03</cell><cell>95.3 70.8 94.3 94.4 94.4 94.4</cell><cell>139.4 8.81 13.16 17.50 8.81 13.16</cell></row><row><cell></cell><cell>DIFFQ 4BITS</cell><cell>90.4</cell><cell>1.64</cell><cell>95.1</cell><cell>5.36</cell><cell>94.6</cell><cell>17.50</cell></row><row><cell></cell><cell>UNCOMPRESSED</cell><cell>68.1</cell><cell>12.6</cell><cell>77.9</cell><cell>42.8</cell><cell>76.2</cell><cell>139.4</cell></row><row><cell>CIFAR-100</cell><cell>QAT 2BITS QAT 3BITS QAT 4BITS DIFFQ 2BITS DIFFQ 3BITS</cell><cell>10.9 59.7 66.9 17.2 60.1</cell><cell>0.91 1.29 1.69 0.91 1.29</cell><cell>58.7 73.7 77.3 66.6 76.7</cell><cell>2.72 4.05 5.39 2.72 4.05</cell><cell>46.5 75.0 75.5 72.8 76.9</cell><cell>8.82 13.18 17.53 8.82 13.18</cell></row><row><cell></cell><cell>DIFFQ 4BITS</cell><cell>66.8</cell><cell>1.69</cell><cell>77.5</cell><cell>5.39</cell><cell>76.9</cell><cell>17.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>). TOP-1 ACC. (%) ↑ M.S. (MB) ↓ Model accuracy vs. epochs for ImageNet using EfficientNet-b3. Resutls are presented for both QAT4 and DIFFQ.</figDesc><table><row><cell cols="3">UNCOMPRESSED</cell><cell></cell><cell>81.6</cell><cell></cell><cell></cell><cell>46.7</cell></row><row><cell cols="2">QAT 4BITS</cell><cell></cell><cell></cell><cell>57.3</cell><cell></cell><cell></cell><cell>6.3</cell></row><row><cell cols="2">QAT 8BITS</cell><cell></cell><cell></cell><cell>81.3</cell><cell></cell><cell></cell><cell>12.0</cell></row><row><cell cols="3">DIFFQ (λ=0.5)</cell><cell></cell><cell>75.4</cell><cell></cell><cell></cell><cell>5.7</cell></row><row><cell cols="3">DIFFQ (λ=5e−2)</cell><cell></cell><cell>80.8</cell><cell></cell><cell></cell><cell>9.92</cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>50 55 60 65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40 45</cell><cell>QAT4 DiffQ</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250 Epochs 300</cell><cell>350</cell><cell>400</cell><cell>450</cell></row><row><cell>Figure 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table B.5. Language modeling results for a 16 layer Transformer trained on Wikitext-103. We also test combining weight and activation quantization using a histogram quantizer. We compared DIFFQ to QAT and Quant-Noise (QN) method proposed by<ref type="bibr" target="#b16">Fan et al. (2020)</ref> (models with † were trained with layer drop.).</figDesc><table><row><cell>WEIGHTS</cell><cell cols="3">ACTIVATION PPL ↓ M. S. (MB) ↓</cell></row><row><cell cols="2">UNCOMPRESSED (OURS) -</cell><cell>18.1</cell><cell>942</cell></row><row><cell>QAT 8BITS</cell><cell>-</cell><cell>18.2</cell><cell>236</cell></row><row><cell>QAT 4BITS</cell><cell>-</cell><cell>28.8</cell><cell>118</cell></row><row><cell>DIFFQ (λ=1, g=16)</cell><cell>-</cell><cell>18.0</cell><cell>182</cell></row><row><cell>DIFFQ (λ=10, g=16)</cell><cell>-</cell><cell>18.5</cell><cell>113</cell></row><row><cell>8 BITS</cell><cell>8 BITS</cell><cell>19.5</cell><cell>236</cell></row><row><cell>QAT 8BITS</cell><cell>8 BITS</cell><cell>26.0</cell><cell>236</cell></row><row><cell>QAT 4BITS</cell><cell>8 BITS</cell><cell>34.6</cell><cell>118</cell></row><row><cell>DIFFQ (λ=1, g=16)</cell><cell>8 BITS</cell><cell>19.1</cell><cell>182</cell></row><row><cell>DIFFQ (λ=10, g=16)</cell><cell>8 BITS</cell><cell>19.2</cell><cell>113</cell></row><row><cell>UNCOMPRESSED  †</cell><cell>-</cell><cell>18.3</cell><cell>942</cell></row><row><cell>QN 8 BITS †</cell><cell>QN 8 BITS</cell><cell>18.7</cell><cell>236</cell></row><row><cell>QN 4 BITS †</cell><cell>QN 8 BITS</cell><cell>20.5</cell><cell>118</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/diffq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://sigsep.github.io/datasets/musdb.html 3 https://www.surina.net/soundtouch/soundstretch.html 4 https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/ 5 https://github.com/pytorch/fairseq/tree/master/examples/quant noise 6 https://github.com/pytorch/vision</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">http://www.image-net.org/ 8 https://github.com/rwightman/pytorch-image-models 9 https://github.com/facebookresearch/deit</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for Differentiable Model Compression via Pseudo Quantization Noise</head><p>We provide in Section A all the details on the exact hyper-parameters, models, and datasets used for the results in Section 5 of the main paper. Then, we provide supplementary results in Section B, in particular tables for the scatter plots given on <ref type="figure">Figures 1 and 3</ref>. Finally, we present the code provided along with this Supplementary Material, in the code folder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed experimental setup</head><p>All experiments are conducted using NVIDIA V100 GPUs with either 16GB or 32GB RAM, depending on the applications (with language modeling requiring larger GPUs.). For all models trained with QAT or DIFFQ, we do not quantize tensors with a size under 0.01 MB (0.1 MB for the DeiT model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Music Source Separation</head><p>We train a Demucs source separation model <ref type="bibr" target="#b10">(Défossez et al., 2019)</ref> with a depth of 6 and 64 initial hidden channels, on the MusDB dataset <ref type="bibr" target="#b37">(Rafii et al., 2017)</ref> 2 . Following <ref type="bibr" target="#b11">Défossez et al. (2020)</ref>, we upsample the input audio by a factor of 2 before feeding it to the model and downsample the output by a factor of 2 before computing the loss. Following Cohen-Hadria et al. <ref type="formula">(2019)</ref>, we also use pitch-shift/tempo stretch data augmentation using the soundstretch package 3 . A batch goes through this transformation with a probability of 20%, the tempo is changed by a uniform fractional amount between -12% and +12%. The pitch is shifted by at most 2 semitones. . Those data augmentation strongly improved the baseline. We train for 180 epochs, with a batch size of 64 and Adam <ref type="bibr" target="#b24">(Kingma &amp; Ba, 2015)</ref> with a learning rate of 3e−4. All other training details are exactly as in <ref type="bibr" target="#b10">(Défossez et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Language Modeling</head><p>We trained a 16 layers transformer <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> based language model on the Wikitext-103 text corpus <ref type="bibr" target="#b31">(Merity et al., 2016</ref>) 4 , following , and using the Fairseq framework <ref type="bibr" target="#b34">(Ott et al., 2019)</ref>. We used the hyper-parameters and the script provided by <ref type="bibr" target="#b16">(Fan et al., 2020)</ref> in the Fairseq repository 5 , however, and unlike what they mention in their paper, this script does not include layer drop . For DIFFQ, we tried the penalty levels λ in {1, 5, 10}, with group size 8, as well as λ = 10 and g = 16.</p><p>Tied weights and DIFFQ. The model we trained was configured so that the word embedding in the first layer and the weight of the adaptive softmax are bound to the same value. It is important to detect such bounded parameters with DIFFQ, as otherwise, a different number of bits could be used for what is in fact, the very same tensor. Not only do we use a single bits logit parameter when a parameter tensor is reused multiple times, but for each forward, we make sure that the pseudo quantization noise is sampled only once and reused appropriately. Failure to do so led to a significant worsening of the performance at validation time.</p><p>A.3. Image classification CIFAR10/100. On the CIFAR10/100 datasets, we train 3 different models: MobileNet-v1 <ref type="bibr" target="#b19">(Howard et al., 2017)</ref>, <ref type="bibr">ResNet-18 (He et al., 2016)</ref>, and a Wide-ResNet with 28x10 depth and width levels respectively <ref type="bibr" target="#b54">(Zagoruyko &amp; Komodakis, 2016)</ref>. All experiments are conducted on a single GPU with a batch size of 128, SGD with a learning rate of 0.1, momentum of 0.9, weight decay of 5e−4. The learning rate is decayed by a factor of 0.2 every 60 iterations. To generate <ref type="figure">Figure 3</ref>, we evaluated DIFFQ for λ in {0.01, 0.05, 0.1, 0.5, 1, 5} and the group size g in {4, 8, 16}.</p><p>The dataset has been obtained from the torchvision package 6 . The input images are augmented with a random crop of size 32 with padding of 4, and a random horizontal flip. The RGB pixel values are normalized to mean 0 and standard</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nice</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00162</idno>
		<title level="m">Noise injection and clamping estimation for neural network quantization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Uniq: Uniform noise injection for non-uniform quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10969</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonlinear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="334" to="334" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Constrained optimization and Lagrange multiplier methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to quantize by learning to penetrate non-differentiable quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaquant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving singing voice separation using deep u-net and wave-unet with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen-Hadria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binaryconnect</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00363</idno>
		<title level="m">Training deep neural networks with binary weights during propagations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<title level="m">Training deep neural networks with weights and activations constrained to +1 or -1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Défossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real time speech enhancement in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Défossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Releq: An automatic reinforcement learning approach for deep quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elthakeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pilligundla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS ML for Systems workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learned step size quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Training with quantization noise for extreme model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno>ICLR 2021</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Differentiable soft quantization: Bridging fullprecision and low-bit neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobilenets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A method for the construction of minimumredundancy codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
		<idno type="DOI">10.1109/JRPROC.1952.273898</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1952" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Trained quantization thresholds for accurate and efficient fixedpoint inference of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Dick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to quantize deep networks by optimizing quantization intervals with task loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4350" to="4359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">A whitepaper. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04711</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Ternary weight networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning low-precision neural networks without straight-through estimator (ste)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01061</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relaxed quantization for discretized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wrpn: Wide reduced-precision networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nurvitadhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Model compression via distillation and quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistarh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The musdb18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning discrete weights using the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">And the bit goes down: Revisiting the quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Machine Learning</title>
		<meeting>of the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mixed precision dnns: All you need is a good parametrization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cardinaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Soft weight-sharing for neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Neural Information Processing Systems</title>
		<meeting>of Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Haq: Hardware-aware automated quantization with mixed precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8612" to="8620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Statistical theory of quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on instrumentation and measurement</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="361" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Training and inference with integers in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations</title>
		<meeting>of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
