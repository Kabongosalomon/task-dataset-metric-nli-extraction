<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Framework for Relation Extraction with Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University (THUAI)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing National Research Center for Information Science &amp; Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University (THUAI)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing National Research Center for Information Science &amp; Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiexi</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Physics</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University (THUAI)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing National Research Center for Information Science &amp; Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University (THUAI)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing National Research Center for Information Science &amp; Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Framework for Relation Extraction with Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing methods determine relation types only after all the entities have been recognized, thus the interaction between relation types and entity mentions is not fully modeled. This paper presents a novel paradigm to deal with relation extraction by regarding the related entities as the arguments of a relation. We apply a hierarchical reinforcement learning (HRL) framework in this paradigm to enhance the interaction between entity mentions and relation types. The whole extraction process is decomposed into a hierarchy of two-level RL policies for relation detection and entity extraction respectively, so that it is more feasible and natural to deal with overlapping relations. Our model was evaluated on public datasets collected via distant supervision, and results show that it gains better performance than existing methods and is more powerful for extracting overlapping relations 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Extracting entities, relations, or events from unstructured texts is crucial for building large-scale, reusable knowledge which can facilitate many other tasks <ref type="bibr" target="#b9">(Mintz et al. 2009;</ref><ref type="bibr" target="#b10">Nadeau and Sekine 2007)</ref>, including knowledge base construction <ref type="bibr" target="#b0">(Dong et al. 2014;</ref><ref type="bibr" target="#b8">Luan et al. 2018)</ref>, question answering <ref type="bibr" target="#b1">(Fader, Zettlemoyer, and Etzioni 2014)</ref>, and biomedical text mining .</p><p>The task of relation extraction is to identify relations (e s , r, e t ) 2 , a triple consisting of a relation type r, a source entity e s and a target entity e t . In this paper, we propose a novel joint extraction paradigm in the framework of hierarchical reinforcement learning <ref type="bibr" target="#b13">(Sutton, Precup, and Singh 1999)</ref>, where we first detect a relation and then extract the corresponding entities as the argument of a relation.</p><p>Our model detects relation indicators by a high-level reinforcement learning (RL) process and identifies the participating entities for the relation by a low-level RL process. As shown in <ref type="figure">Figure 1</ref>, the extraction process makes sequential scans from the beginning to the end of a sentence (I). The high-level process is to detect a relation indicator at some particular position. If a certain relation is identified, a lowlevel sequential process is triggered to identify the corresponding entities for that relation (II). When the low-level subtask for entity extraction is completed (III), the highlevel RL process continues its scan to search for the next relation (IV) in the sentence.</p><p>This paradigm has strengths in dealing with two issues existing in prior studies. First, most traditional models <ref type="bibr" target="#b2">(Gormley, Yu, and Dredze 2015;</ref><ref type="bibr" target="#b3">Hoffmann et al. 2011;</ref><ref type="bibr" target="#b9">Miwa and Bansal 2016)</ref> determine a relation type only after all the entities have been recognized, whereas the interaction between the two tasks is not fully captured. In some sense, these methods are aligning a relation to entity pairs, and therefore, they may introduce additional noise since a sentence containing an entity pair may not truly mention the relation <ref type="bibr" target="#b17">(Zhang et al. 2013)</ref>, or may describe multiple relations <ref type="bibr" target="#b14">(Takamatsu, Sato, and Nakagawa 2012)</ref>.</p><p>Second, there still lacks the elegance of the joint extraction method to deal with one-to-many problems (overlapping relations): one entity may participate in multiple relations in the same sentence (see Steve Blichick in <ref type="figure">Figure 1</ref>), or even the same entity pair within a sentence is associated with different relations. To our best knowledge, CopyR <ref type="bibr" target="#b16">(Zeng et al. 2018</ref>) is the only method that discussed this issue, which views relation extraction as a triple generation process. However, this method, as our experiments reveal, strongly relies on the training data, and cannot extract multi-word entity mentions.</p><p>In our paradigm, the first issue is handled by treating entities as the arguments of a relation. The dependency between entity mentions and relation types is formulated through designing the state representations and rewards in the highlevel and low-level RL processes. The interaction is well captured since the main task (high-level RL process for relation detection) passes messages when launching a subtask (low-level RL process for entity extraction), and the lowlevel rewards, signifying how well a subtask is completed, are passed back to the main task. In this manner, the interaction between relation types and entity mentions can be better modeled.</p><p>The second issue is addressed by our hierarchical structure. By decomposing relation extraction into a high-level <ref type="figure">Figure 1</ref>: An example sentence which has two overlapping relations (Steve Belichick, parent-children, Bill Belichick), (Steve Belichick, place-of-death, Annapolis). The solid arrow indicates the high-level relation detection process, and the dashed arrow for low-level entity extraction. The dotted arrow marks a transition between the two processes. This example shows how overlapping relations are extracted (Steve Blichick is included in both triples). task for relation detection and a low-level task for entity extraction, multiple relations in a sentence can be handled separately and sequentially. As shown in <ref type="figure">Figure 1</ref>, the first relation is extracted when the main task detects the first relation type (parent-children), and the second relation is subsequently extracted when the second relation type (place-of-death) is triggered, even though the two relations share the same entity (Steve Blichick). Experiments demonstrate the proposed paradigm achieves strong performance over the baselines in extracting overlapping relations.</p><p>In summary, our contributions are in two folds: • We design a novel end-to-end hierarchical paradigm to jointly identify entity mentions and relation types, which decomposes the task into a high-level task for relation detection and a low-level task for entity extraction. • By incorporating reinforcement learning into this paradigm, the proposed method outperforms baselines in modeling the interactions between the two tasks, and extracting overlapping relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Traditional pipelined approaches treat entity extraction and relation classification as two separate tasks <ref type="bibr" target="#b9">(Mintz et al. 2009;</ref><ref type="bibr" target="#b2">Gormley, Yu, and Dredze 2015;</ref><ref type="bibr" target="#b14">Tang et al. 2015)</ref>. They first extract the token spans in the text to detect entity mentions, and then discover the relational structures between entity mentions. Although it is flexible to build pipelined methods, these methods suffer from error propagation since downstream modules are largely affected by the errors introduced by upstream modules.</p><p>To address this problem, a variety of joint learning methods was proposed. <ref type="bibr" target="#b5">Kate and Mooney (2010)</ref> proposed a card-pyramid graph structure for joint extraction, and Hoffmann et al. (2011) developed graph-based multi-instance learning algorithms. However, the two methods both applied a greedy search strategy to reduce the exploration space aggressively, which limits the performance. Other studies employed a structured learning approach <ref type="bibr" target="#b7">(Li and Ji 2014;</ref><ref type="bibr" target="#b9">Miwa and Sasaki 2014)</ref>. All these models depend on heavy feature engineering, which requires much manual efforts and domain expertise.</p><p>On the other hand, <ref type="bibr" target="#b0">Björne et al. (2011)</ref> proposed to first extract relation triggers, which refer to a phrase that explicitly expresses the occurrence of a relation in a sentence, and then determine their arguments to reduce the task complexity. Open IE systems ReVerb <ref type="bibr" target="#b0">(Fader, Soderland, and Etzioni 2011)</ref> identifies relational phrases using lexical constraints, which also follows a "relation"-first, "argument"-second approach. But there are many cases where no relation trigger appears in a sentence so that such relations cannot be captured in these methods.</p><p>Neural models for joint relation extraction are investigated in recent studies <ref type="bibr" target="#b6">(Katiyar and Cardie 2016;</ref><ref type="bibr" target="#b17">Zhang, Zhang, and Fu 2017)</ref>. Miwa and Bansal (2016) proposed a neural model that shares parameters for entity extraction and relation classification, but the two tasks are separately handled, and the final decision is obtained via exhaustively enumerating the combinations between detected entity mentions and relation types. Unlike aforementioned methods that all the entities are recognized first, <ref type="bibr" target="#b18">Zheng et al. (2017)</ref> used a tagging scheme which applies a Cartesian product of the relation type tags and the entity mention tags, and thus each word is assigned a unique tag that encodes entity mentions and relation types simultaneously. However, it is unable to deal with overlapping relations in a sentence: if an entity is the argument of multiple relations, the tag for the entity should not be unique. The recent study <ref type="bibr" target="#b16">(Zeng et al. 2018)</ref> is closely related to ours that aims to handle overlapping relations. It employs multiple decoders based on sequenceto-sequence (Seq2Seq) learning where a decoder copies an entity word from the source sentence and each triple in a sentence is generated by different decoders, but such a method strongly relies on the annotation of training data and it cannot extract an entity that has multiple words.</p><p>Reinforcement learning has been witnessed in information extraction very recently. RL was employed to acquire and incorporate external evidence in event extraction (Narasimhan, Yala, and Barzilay 2016). <ref type="bibr" target="#b2">Feng et al. (2018)</ref> used RL to train an instance selector to denoise training data obtained via distant supervision for relation classification. Improvement was reported in distant supervision relation type extraction by exploring RL to redistribute false posi-tives into the negative examples (Qin, Xu, and Wang 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Extraction Framework</head><p>Overview First of all, we define relation indicator as follows: Definition 1. Relation indicator is the position in a sentence when sufficient information has been mentioned to identify a semantic relation. Different from relation trigger (i.e., explicit relation mention), relation indicators can be verbs (e.g. die of), nouns (e.g. his father), or even prepositions (e.g. from/by), other symbols such as comma and period (As shown in <ref type="figure">Figure 1</ref>, the relation type place-of-death can be signified till the comma position).</p><p>Relation indicator is crucial for our model to complete the extraction task, because the entire extraction task is decomposed into relation indicator detection and entity mention extraction. The entire extraction process works as follows. An agent predicts a relation type at a particular position when it scans a sentence sequentially. Note that this process of relation detection needs no annotation of entities, thus different from relation classification which is to identify the relations between pairs of entities. When there is no sufficient evidence to indicate a semantic relation at a time step, the agent may choose NR, a special relation type that indicates no relation. Otherwise a relation indicator is triggered, the agent launches a subtask for entity extraction to identify the arguments of the relation, the two entities. When the entity mentions are identified, the subtask is completed and the agent continues to scan the rest of the sentence for other relations.</p><p>Such a process can be naturally formulated as a semi-Markov decision process <ref type="bibr" target="#b13">(Sutton, Precup, and Singh 1999)</ref>: 1) a high-level RL process that detects a relation indicator in a sentence; 2) a low-level RL process that identifies the associated entities for the corresponding relation. By decomposing the task into a hierarchy of two RL processes, the model is advantageous at dealing with sentences which have multiple relation types for the same entity pair, or one-tomany entities in which an entity is the argument of multiple relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Detection with High-level RL</head><p>The high-level RL policy µ aims to detect the relations in a sentence S = w 1 w 2 · · · w L , which can be regarded as a conventional RL policy over options. An option refers to a highlevel action, and a low-level RL process will be launched once an option is executed by the agent.</p><formula xml:id="formula_0">Option: The option o t is selected from O = {NR} ∪ R</formula><p>where NR indicates no relation, and R is the relation type set. When a low-level RL process enters a terminal state, the control of the agent will be taken over to the high-level RL process to execute the next options. State: The state s h t ∈ S of the high level RL process at time step t, is represented by: 1) the current hidden state h t , 2) the relation type vector v r t (the embedding of the latest option o t that o t = NR, a learnable parameter), and 3) the state from the last time step s t−1 3 , formally represented by</p><formula xml:id="formula_1">s h t = f h (W h s [h t ; v r t ; s t−1 ]),<label>(1)</label></formula><p>where f h (·) is a non-linear function implemented by MLP.</p><p>To obtain the hidden state h t , we introduce a sequence Bi-LSTM over the current input word embedding w t :</p><formula xml:id="formula_2">− → h t = −−−−→ LST M ( −−→ h t−1 , w t ), ← − h t = ←−−−− LST M ( ←−− h t+1 , w t ), h t = [ − → h t ; ← − h t ].</formula><p>(2)</p><p>Policy: The stochastic policy for relation detection µ : S → O which specifies a probability distribution over options:</p><formula xml:id="formula_3">o t ∼ µ(o t |s h t ) = sof tmax(W µ s h t ).<label>(3)</label></formula><p>Reward: Then, the environment provides intermediate reward r h t to estimate the future return when executing o t . The reward is computed as below:</p><formula xml:id="formula_4">r h t = −1, if o t not in S 0, if o t = NR 1, if o t in S.<label>(4)</label></formula><p>If o t = NR at certain time step, the agent transfers to a new high-level inter-option state at the next time step. Otherwise the low-level policy will execute the entity extraction process. The inter-option state will not transfer until the subtask <ref type="figure">Figure 4</ref>: The entity annotation scheme for the example sentence in <ref type="figure">Figure 1</ref> when the agent predicts a relation type parent-children between Steve Belichick and Bill Belichick. In this example, New England Patriots and Annapolis are not-concerned entities with respect to relation type parent-children.</p><p>over current option o t is done, which may take multiple time steps. Such a semi-Markov process continues until the last option about the last word w L of S is sampled. Finally, a final reward r h f in is obtained to measure the sentence-level extraction performance that µ detects:</p><formula xml:id="formula_5">r h f in = F β (S) = (1 + β 2 )P rec · Rec β 2 P rec + Rec ,<label>(5)</label></formula><p>where F β is the weighted harmonic mean of precision and recall in terms of the relations in S. P rec/Rec indicates precision/recall respectively, computed over one sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Extraction with Low-level RL</head><p>Once the high-level policy has predicted a non-NR relation type, the low-level policy π will extract the participating entities for the corresponding relation. The low-level policy over actions (primitive actions) is formulated very similarly as the high-level policy over options. To make the predicted relation type accessible in the low-level process, the option o t from the high level RL is taken as additional input throughout the low-level extraction process. Action: The action at each time step is to assign an entity tag to the current word. The action space, i.e., entity tag space A = ({S, T, O} × {B, I}) ∪ {N}, where S represents the participating source entity, T for the target one, O for the entities that are not associated with the predicted relation type o t , and N for for non-entity words. Note that, the same entity mention may be assigned with different S/T/O tags depending on different relation types concerned at the moment. In this way, the model can deal with overlapping relations. In addition, we use the B/I symbols to represent the beginning word and the inside of an entity, respectively. Refer to <ref type="figure">Figure 4</ref> for an example. State: Similar to the policy for relation detection, the lowlevel intra-option state s l t is represented by 1) the hidden state h t of current word embedding w t , 2) the entity tag vector v e t which is a learnable embedding of a t−1 , 3) the state from previous time step s t−1 , and 4) the context vector c t using the relational state representation assigned to the latest option s h t in Eq. (1), as follows:</p><formula xml:id="formula_6">c t = g(W l h s h t ) s l t = f l (W l s [h t ; v e t ; s t−1 ; c t ]),<label>(6)</label></formula><p>where h t is the hidden state obtained from the Bi-LSTM module in Eq.</p><p>(2), and f l (·), g(·) are non-linear functions implemented by MLP. Note that s t−1 may be a state either from the high-level RL process or the low-level one. Policy: The stochastic policy for entity extraction π : S → A outputs an action distribution given intra-option state s l t and the high-level option o t that launches the current subtask.</p><formula xml:id="formula_7">a t ∼ π(a t |s l t ; o t ) = sof tmax(W π [o t ]s l t ),<label>(7)</label></formula><p>where W π is an array of |R| matrices. Reward 4 : Given the relation type o t , the entity tag for each word can be easily obtained by sampling actions from the policy. Therefore, an immediate reward r l t is provided when the action a t is sampled by simply measuring the prediction error over gold-standard annotation:</p><formula xml:id="formula_8">r l t = λ(y t ) · sgn(a l t = y t (o t )),<label>(8)</label></formula><p>where sgn(·) is the sign function, and y(o t ) is the goldstandard entity tag conditioned on the predicted relation type o t . Here λ(y) is a bias weight for down-weighing non-entity tag, defined as follows:</p><formula xml:id="formula_9">λ(y) = 1, if y = N α, if y = N.<label>(9)</label></formula><p>The smaller α leads to less reward on words that are not entities. In this manner, the model avoids to learn a trivial policy that predicts all words as N (non-entity words). When all the actions are sampled, an additional final reward r l f in is computed. If all the entity tags are predicted correctly, then the agent receives +1 reward, otherwise -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Policy Learning</head><p>To optimize the high-level policy, we aim to maximize the expected cumulative rewards from the main task at each time step t as the agent samples trajectories following the highlevel policy µ, which can be computed as follows:</p><formula xml:id="formula_10">J(θ µ,t ) = E s h ,o,r h ∼µ(o|s h ) [ T k=t γ k−t r h k ],<label>(10)</label></formula><p>where µ is parameterized by θ µ , γ is a discount factor in RL, and the whole sampling process µ takes T time steps before it terminates. Similarly, we learn the low-level policy by maximizing the expected cumulative intra-option rewards from the subtask over option o t when the agent samples along low-level policy π(·|o t ) at time step t:</p><formula xml:id="formula_11">J(θ π,t ; o t ) =E s l ,a,r l ∼π(a|s l ;o t ) [ T k=t γ k−t r l k ],<label>(11)</label></formula><p>if the subtask ends at time step T . By decomposing the cumulative rewards into a Bellman equation, we acquire:</p><formula xml:id="formula_12">R µ (s h t , o t ) =E[ N −1 j=0 γ j r h t+j + γ N R µ (s h t+N , o t+N )|s h t , o t ], R π (s l t , a t ; o t ) =E[r l t + γR π (s l t+1 , a t+1 ; o t )|s l t , a t ],<label>(12)</label></formula><p>where N is the number of time steps that a subtask continues when the entity extraction policy runs upon option o t , so the agent's next option is o t+N . In particular, if o t = NR, then N = 1. Then, we use policy gradient methods <ref type="bibr" target="#b12">(Sutton et al. 2000)</ref> with the REINFORCE algorithm <ref type="bibr" target="#b15">(Williams 1992)</ref> to optimize both high-level and low-level policies. With the likelihood ratio trick, the gradient for the high-level policy yields:</p><formula xml:id="formula_13">∇ θµ J(θ µ,t ) =E s h ,o,r h ∼µ(o|s h ) [R µ (s h t , o t ) ∇ θµ log µ(o|s h t )],<label>(13)</label></formula><p>and the gradient for the low-level policy yields:</p><formula xml:id="formula_14">∇ θπ J(θ π,t ; o t ) =E s l ,a,r l ∼π(a|s l ;o t ) [R π (s l t , a t ; o t ) ∇ θπ log π(a|s l t ; o t )].<label>(14)</label></formula><p>The entire training process is described at Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Setting</head><p>Datasets We evaluated our model on the New York Times corpus which is developed by distant supervision and contains noisy relations. The corpus has two versions: 1) The original version generated by aligning the raw data with Freebase relations <ref type="bibr" target="#b12">(Riedel, Yao, and McCallum 2010)</ref>; 2) A smaller version of which the test set was manually annotated <ref type="bibr" target="#b3">(Hoffmann et al. 2011)</ref>. We name the original version as NYT10, and the smaller version as NYT11. We split some of the training data from NYT11 to construct NYT11-plus, which will be described later.</p><p>We filtered the datasets by removing 1) the relations in the training set whose relation type does not exist in the test set; 2) the sentences that contain no relations at all. Such a preprocess is also in line with the settings in the literature (for instance, Tagging). All the baselines are evaluated in this setting for fair comparison. The statistics of the two filtered datasets are presented in <ref type="table">Table 1</ref> For each dataset, we randomly chose 0.5% data from the training set for validation.</p><p>Parameter Settings All hyper-parameters are tuned on the validation set. The dimension of all vectors in Eq. (1), (2) and (6) is 300. The word vectors are initialized using Glove vectors <ref type="bibr" target="#b10">(Pennington, Socher, and Manning 2014)</ref> and are updated during training. Both relation type vectors and entity tag vectors are initialized randomly. The learning rate is 4e − 5, the mini-batch size is 16, α = 0.1 in Eq. (9), β = 0.9 in Eq. (5), and the discount factor γ = 0.95.</p><p>Evaluation Metrics We adopted standard micro-F 1 to evaluate the performance. We compared whether the extracted entity mentions can be exactly matched with those in a relation. A triplet is regarded as correct if the relation type and the two corresponding entities are all correct.</p><p>Baselines We chose two types of baselines: one is pipelined methods (FCM), and the other is joint learning methods which include feature-based methods (MultiR and CoType) and neural methods <ref type="bibr">(SPTree, Tagging and CopyR)</ref>. We used open source codes and conducted the experiments by ourselves. FCM (Gormley, Yu, and Dredze 2015): a compositional model that combines lexicalized linguistic contexts and word embeddings to learn representations for the substructures of a sentence in relation extraction 5 .</p><p>MultiR <ref type="bibr" target="#b3">(Hoffmann et al. 2011</ref>): a typical distant supervision method performing sentence-level and corpus-level extraction, which uses multi-instance weighting to deal with noisy labels in training data. CoType <ref type="bibr" target="#b11">(Ren et al. 2017</ref>): a domain-independent framework by jointly embedding entity mentions, relation mentions, text features, and type labels into representations, which formulates extraction as a global embedding problem. SPTree (Miwa and Bansal 2016): an end-to-end relation extraction model that represents both word sequence and dependency tree structures using bidirectional sequential and tree-structured LSTM-RNNs.</p><p>Tagging <ref type="bibr" target="#b18">(Zheng et al. 2017)</ref>: an approach that treats joint extraction as a sequential labeling problem using a tagging schema where each tag encodes entity mentions and relation types at the same time.</p><p>CopyR <ref type="bibr" target="#b16">(Zeng et al. 2018</ref>): a Seq2Seq learning framework with a copy mechanism for joint extraction, where multiple decoders are applied to generate triples to handle overlapping relations.  The results on relation extraction are presented in <ref type="table" target="#tab_3">Table  2</ref>. Noticeably, there is a significant gap between the performance on noisy data (NYT10) and that on clean data (NYT11) as all the models are trained on noisy data. It can be seen that our method (HRL) outperforms the baselines on the two datasets. Significant improvements can be observed on NYT10, which indicates that our method is more robust to noisy data. Results on NYT11 show that neural models (SPTree, Tagging and CopyR) are more effective than pipelined (FCM) or feature-based (MultiR and CoType) methods. CopyR is introduced to extract overlapping relations, but it yields poor performance on the NYT11 test set where there is almost no overlapping relation in a sentence (370 relations among 369 sentences). Whereas our model is still comparable to SPTree and performs remarkably better than other baselines. Note that SPTree utilizes more linguistic resources (e.g., POS tags, chunks, syntactic parsing trees). This implies that our model is also robust to the data distribution of relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overlapping Relation Extraction</head><p>We prepared another two test sets to verify the effectiveness of our model on extracting overlapping relations. Note that overlapping relations can be classified into two types.</p><p>• Type I: two triples share only one entity within a sentence • Type II: two triples share two entities (both head and tail entities) within a sentence</p><p>The first set, NYT11-plus, is annotated manually and consists of 149 sentences split from the original NYT11 training data. The set contains 210/97 overlapping relations for type I/II respectively. The second set, NYT10-sub, is a subset of the test set of NYT10, and has 715 sentences, but without manual annotation. This set contains 90/2,082 overlapping relations for type I/II respectively. To summarize, most of the overlapping relations in NYT11-plus is of type I; while most in NYT10-sub is of type II.  <ref type="bibr">.272 .315 .292 .466 .229 .307 Tagging .256 .237 .246 .292 .220 .250 CopyR .392 .263 .315 .329 .224 .264 HRL .815 .475 .600 .441 .321 .372</ref>  Results on NYT10-sub show that the baselines are very weak to extract overlapping relations of type II on the noisy data, which is consistent with our statement that existing joint extraction approaches cannot deal with overlapping relations effectively in nature. By contrast, our method did not deteriorate too much in performance comparing to that in <ref type="table" target="#tab_3">Table 2</ref>, and even obtained a larger gain on precision.</p><p>Results on NYT11-plus demonstrate that our method had a substantial F 1 improvement over all the baselines in extracting overlapping relations of type I on the clean data, indicating that our method can extract overlapping relations more accurately. SPTree had a high precision but low recall since it simply matches one relation type to an entity pair, suffering from ignoring the case of overlapping relations. Tagging had low performance in extracting overlapping relations because it assigns a unique tag to an entity even if that entity participates in overlapping relations. Though CopyR claimed that it can extract overlapping relations of both types, it fails to extract the relations from clean data effectively as it strongly relies on the annotation of the noisy training data.</p><p>To conclude, we can see that extracting overlapping relations is more challenging by comparing results in <ref type="table" target="#tab_3">Table  2</ref> and those in <ref type="table" target="#tab_4">Table 3</ref>, and our model is better in extracting two types of overlapping relations no matter the data is noisy or clean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction between the Two Policies</head><p>To justify the effectiveness of integrating entities into a relation and how the interactions are built between the two policies, we investigated the performance on relation detection (classification). In this setting, a prediction is treated as correct as long as the relation type is correctly predicted. The prediction is derived from the high-level policy.   The results in <ref type="table" target="#tab_8">Table 5</ref> demonstrate that our method performs better in relation detection on both datasets. The improvements on NYT11-plus are more remarkable as our paradigm is more powerful to extract multiple relations from a sentence. The results indicate that our extraction paradigm which regards entities as arguments of a relation can better capture the relational information in the text. When removing the low-level entity extraction policy from our model (HRL-Ent), the performance has changed slightly on NYT11 because each sentence almost contains only one relation in this test set (370 relations among 369 sentences). In this case, the interaction between the two policies has almost no influence on relation detection. However, dramatic drops are observed on NYT11-plus where we have 327 relations from 149 sentences, implying that our method (HRL) captures the dependency across multiple extraction tasks and the high-level policy benefits from such interactions. Therefore, our hierarchical extraction framework indeed enhances the interaction between relation detection and entity extraction. <ref type="table" target="#tab_7">Table 4</ref> presents some extraction examples by our model to demonstrate the ability to extract overlapping relations. The first sentence shows the case that an entity pair has multiple relations (type II). Two relations (Rupert Murdoch, person-company, News Corporation) and (News Corporation, company-founder, Rupert Murdoch) share the same entity pair but have different relation types. The model first detects the relation type person-company at "Murdoch", and then detects the other relation type company-founder at the comma position, just next to the word "Murdoch". This shows that relation detection is triggered when sufficient evidence has been gathered at a particular position. And the model can classify the same entities into either source or target entities (for instance, Rupert Murdoch is a source entity for person-company whereas a target entity for company-founder), demonstrating the advantage of our hierarchical framework which can assign dynamic tags to words conditioned on different relation types. In addition, Rupert Murdoch has a relation with Australia, where the two entities locate far from each other. Though this is more difficult to detect, our model can still extract the relation correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>The second sentence gives another example where an entity is involved in multiple relations (type I). In this sentence, (Steven A. Ballmer, person-company, Microsoft) and (Bill Gates, person-company, Microsoft) share the same relation type and target entity, but have different source entities. When the agent scans to the word "Microsoft", the model detects the first relation. The agent then detects the second relation when it scans to the word "Gates". This further demonstrates the benefit of our hierarchical framework which has strengths in extracting overlapping relations by firstly detecting relation and then finding the entity arguments. In addition, our model predicts another relation (Bill Gates, founder-of, Microsoft), which is wrong for this sentence because there is no explicit mention of the relation. This may result from the noise produced by distant supervision, where there are many noisy sentences that are aligned to that relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, we present a hierarchical extraction paradigm which approaches relation extraction via hierarchical reinforcement learning. The paradigm treats entities as the arguments of a relation, and decomposes the relation extraction task into a hierarchy of two subtasks: high-level relation indicator detection and low-level entity mention extraction. The high-level policy for relation detection identifies multiple relations in a sentence, and the low-level policy for entity extraction launches a subtask to further extract the related entities for each relation. Thanks to the nature of this hierarchical approach, it is good at modeling the interactions between the two subtasks, and particularly excels at extracting overlapping relations. Experiments demonstrate that our approach outperforms state-of-the-art baselines.</p><p>As future work, this hierarchical extraction framework can be generalized to many other pairwise or triple-wise extraction tasks such as aspect-opinion mining or ontology induction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of a hierarchical agent in relation extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of a two-level hierarchical policy structure. Left panel shows the high-level policy for relation detection, and right panel shows the low-level policy for entity extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Main results on relation extraction.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows the performance of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on extracting overlapping relations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The lawsuit contended that the chairman of the[ [ News Corporation ] Et-Company ] Es-Founder , [ [ [ Rupert Murdoch ] Es-Company ] Et-Founder ] Es-Nationality ,l promised certain rights to shareholders , including the vote on the poison pill , in return for their approval of the company 's plan to reincorporate in the United States from [ Australia ] Et-Nationality . Both [ Steven A. Ballmer ] Es-Company , [ [ [ Microsoft ] Et-Company ] Et-Company ] Es-Founder 's chief executive , and [ [ Bill Gates ] Es-Company ] Et-Founder ,l the chairman , have been involved in that debate inside the company , according to that person .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Extraction examples by our model. The words in a bracket represents an entity extracted by the model. Es stands for source entity and Et for target entity. A predicted relation indicator is marked in background color (e.g. "Murdoch" in the first instance). The entities which form a triple are bracketed in the same color.Ent .676 .676 .676 .577 .321 .413  HRL  .654 .654 .654 .626 .456 .527    </figDesc><table><row><cell>Model</cell><cell>NYT11 Prec Rec</cell><cell>F 1</cell><cell>NYT11-plus Prec Rec F 1</cell></row><row><cell>FCM</cell><cell cols="3">.502 .479 .490 .447 .327 .378</cell></row><row><cell>MultiR</cell><cell cols="3">.465 .439 .451 .423 .336 .375</cell></row><row><cell>CoType</cell><cell cols="3">.558 .558 .558 .491 .413 .449</cell></row><row><cell>SPTree</cell><cell cols="3">.650 .614 .631 .700 .343 .460</cell></row><row><cell>CopyR</cell><cell cols="3">.480 .714 .574 .626 .426 .507</cell></row><row><cell>HRL-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison on relation detection.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">where st−1 = s h t−1 if the agent sampled a high-level option at last time step t − 1, and st−1 = s l t−1 if the agent sampled a low-level action.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We only discuss the situation where o t is included in S, i.e. the subtask option o t is correctly predicted. Otherwise, all the lowlevel rewards are set to 0, which can be seen that the agent has done nothing with the low-level policy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">As FCM cannot detect entity mentions alone, we used the NER results and related features obtained from another baseline CoType.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was jointly supported by the National Science Foundation of China (Grant No.61876096/61332007), and the National Key R&amp;D Program of China (Grant No. 2018YFC0830200). We would like to thank Prof. Xiaoyan Zhu for her generous support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extracting contextualized complex biological events with rich graphbased feature sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Björne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zettlemoyer</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved relation extraction with feature-rich compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
		<idno>Gu et al. 2016</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Community challenges in biomedical text mining over 10 years: success, failure and the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="144" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint entity and relation extraction using card-pyramid parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
	<note>and Mooney</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Investigating lstms for joint extraction of opinion entities and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="919" to="929" />
		</imprint>
	</monogr>
	<note>Katiyar and Cardie</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>and Ji</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving information extraction by acquiring external evidence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yala</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barzilay ; Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Pennington, Socher, and Manning</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2137" to="2147" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cotype: Joint extraction of typed entities and relations with knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccallum ; Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Precup</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Singh ; Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reducing wrong labels in distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sato</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<idno>Tang et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards accurate distant supervision for relational facts extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
