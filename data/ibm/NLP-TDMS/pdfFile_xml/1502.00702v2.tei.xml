<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HOPE ( Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><forename type="middle">)</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<addrLine>4700 Keele Street</addrLine>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HOPE ( Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel model for high-dimensional data, called the Hybrid Orthogonal Projection and Estimation (HOPE) model, which combines a linear orthogonal projection and a finite mixture model under a unified generative modeling framework. The HOPE model itself can be learned unsupervised from unlabelled data based on the maximum likelihood estimation as well as discriminatively from labelled data. More interestingly, we have shown the proposed HOPE models are closely related to neural networks (NNs) in a sense that each hidden layer can be reformulated as a HOPE model. As a result, the HOPE framework can be used as a novel tool to probe why and how NNs work, more importantly, to learn NNs in either supervised or unsupervised ways. In this work, we have investigated the HOPE framework to learn NNs for several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results have shown that the HOPE framework yields significant performance gains over the current state-of-the-art methods in various types of NN learning problems, including unsupervised feature learning, supervised or semi-supervised learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning systems normally consist of several distinct steps in design, namely feature extraction and data modeling. In feature extraction, some engineering tricks are used to preprocess raw data to extract useful and representative features for the underlying data sets. As a result, this stage is sometimes called feature engineering. For high-dimensional data, the feature extraction stage needs to distill "good" features that are representative enough to discriminate different data samples but also it has to perform effective dimensionality reduction to generate less correlated features that can be easily modeled in a lower dimensional space. In data modeling, an appropriate model is selected to model data in the lower-dimensional feature space. There are a wide range of models available for this purpose, such as k-Nearest-Neighbours (kNN) methods, decision trees, linear discriminant models, neural networks, statistical models from the exponential family, or mixtures of the exponential family distributions, and so on. Subsequently, all unknown model parameters are estimated from available training samples based on certain learning criterion, such as maximum likelihood estimation (MLE) or discriminative learning.</p><p>In many traditional machine learning methods, feature extraction and data modeling are normally conducted independently in two loosely-coupled stages, where feature extraction parameters and model parameters are separately optimized based on rather different criteria. Particularly, feature extraction is normally regarded as a pre-processing stage, where feature extraction parameters are estimated under some quite loose conditions, such as the assumption that data is normally distributed in a high-dimensional space as implied in linear discriminant analysis (LDA) and principal component analysis (PCA). On the other hand, neural networks (NNs) favor an end-to-end learning process, which is normally considered as one exception to the above paradigm. In practice, it has been widely observed that NNs are capable of dealing with almost any type of raw data directly without any explicit feature engineering. In the recent resurgence of NNs in "deep learning", more and more empirical results have demonstrated that deep neural networks (DNNs) can effectively de-correlate high-dimensional raw data and automatically learn useful features from large training sets, without being disturbed by "the curse of dimensionality". However, it still remains as an open question why NNs can handle it and what mechanism is used by NNs to de-correlate high-dimensional raw data to learn good feature representations for many real-world complicated tasks.</p><p>In this paper, we first propose a novel data modeling framework for high-dimensional data, namely Hybrid Orthogonal Projection and Estimation (HOPE). The key argument for the HOPE framework is that feature extraction and data modeling should not be decoupled into two separate stages in learning and a good feature extraction module can not be learned based on some over-simplified and unrealistic modeling assumptions. The feature extraction and data modeling must be jointly learned and optimized by considering the complex nature of data distributions. This is particularly important in coping with high-dimensional data arising from most real-world applications, such as image, video and speech signals. In the HOPE framework, we propose to model high-dimensional data by combining a relatively simple feature extraction model, namely a linear orthogonal projection, with a powerful statistical model for data modeling, namely a finite mixture model of the exponential family distributions, under a unified generative modeling framework. In this paper, we consider two possible choices for the mixture models, namely Gaussian mixture models (GMMs) and mixtures of the von Mises-Fisher (movMFs) distributions. First of all, an orthogonal linear projection is used in feature extraction to ensure that the highly-correlated high-dimensional raw data is first projected onto a lower-dimensional latent feature space, where all feature dimensions are largely de-correlated. This will give us huge advantages to model data in this feature space rather than the original data space. Secondly, in the HOPE framework, we propose to use a powerful model to represent data in the lower-dimensional feature space, rather than using any over-simplified models for computational convenience. This is very important since any real-world data tend to follow a rather complex distribution, which can always be approximated by a finite mixture model up to any arbitrary precision. Thirdly, the most important argument in HOPE is that both the orthogonal projection and the mixture model must be learned jointly according to a single unified criterion. In this paper, we first study how to learn HOPE in an unsupervised manner based on the conventional maximum likelihood (ML) criterion and also explain that the HOPE models can also be learned in a supervised way based on any discriminative learning criterion.</p><p>Another important finding in this work is that the proposed HOPE models are closely related to neural networks (NNs) currently widely used in deep learning. As we will show, any single hidden layer in the most popular rectified linear (ReLU) NNs can always be reformulated as a HOPE model consisting of a linear orthogonal projection and a mixture of von Mises-Fisher distributions (movMFs). This formulation helps to explain how NNs actually deal with highdimensional data and why NNs can de-correlate almost any types of high-dimensional data to generate good feature representations. More importantly, this formulation may open up new possibilities to learn NNs more effectively. For example, both supervised and unsupervised learning algorithms for the HOPE models can be easily applied to learning NNs. By imposing an explicit orthogonal constraint on the feature extraction layer, we will show that the HOPE methods are very effective in learning NNs for both supervised and unsupervised learning. In unsupervised learning, we have shown that the maximum likelihood (ML) based HOPE learning algorithms can serve as a very effective unsupervised learning method to learn NNs from unlabelled data. Our experimental results have shown that the ML-based HOPE learning algorithm can learn good feature representations in an unsupervised way without using any data labels. These unsupervised learned features may be fed to some simple post-stage classifiers, such as linear SVM, to yield comparable performance as deep NNs supervised learned end-toend with data labels. Our proposed unsupervised learning algorithms significantly outperform the previous methods based on the Restricted Boltzmann Machine (RBM) <ref type="bibr" target="#b11">[12]</ref> and the autoencoder variants <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>. Moreover, in supervised learning, relying on the HOPE models, we have managed to learn some shallow NNs from scratch, which perform comparably with the state-ofthe-art deep neural networks (DNNs), as opposed to learn shallow NNs to mimic a pre-trained deep neural network as in <ref type="bibr" target="#b1">[2]</ref>. Finally, the HOPE models can also be used to train deep neural networks and it normally provides significant performance gain over the standard NN learning methods. These results have suggested that the orthogonal constraint in HOPE may serve as a good model regularization in learning of NNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Dimensionality reduction in feature space is a well-studied topic in machine learning. Among many, PCA is the most popular technique in this category. PCA is defined as the orthogonal projection of the high-dimensional data onto a lower dimensional linear space, known as the principal subspace, such that the variance of the projected data is maximized <ref type="bibr" target="#b5">[6]</ref>. The nice property of PCA is that it can be formulated as an eigenvector problem of the data covariance matrix, where a simple closed-form solution exists. Moreover, in the probabilistic PCA <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>, PCA can be expressed as the maximum likelihood solution to a probabilistic latent variable model. In this case, if the projected data are assumed to follow a zero-mean unit-covariance Gaussian distribution in the principal subspace, the probabilistic PCA can also be solved by an exact closed-form solution related to the eigenvectors of the data covariance matrix. The major limitation of PCA is that it is constrained to learn a linear subspace. Many approaches have been proposed to perform nonlinear dimension reduction to learn possible nonlinear manifolds embedded within a high dimensional data space. One way to model the nonlinear structure is through a combination of linear models, so that we make a piece-wise linear approximation to the manifold. This can be obtained by a clustering technique to partition the data set into local groups with standard PCA applied to each group, such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref>. In <ref type="bibr" target="#b27">[28]</ref>, the highdimensional raw data is assumed to follow a mixture model, where each component may perform its own maximum likelihood PCA in a local region. However, in these methods, it may be quite challenging to perform effective clustering or estimate good mixture models for high-dimensional data due to the strong correlation in various data dimensions. Alternatively, in <ref type="bibr" target="#b12">[13]</ref>, a flexible nonlinear method is proposed to reduce feature dimension based on a deep auto-associative neural network.</p><p>Similar to PCA, the Fisher's linear discriminant analysis (LDA) can also be viewed as a linear dimensionality reduction technique. However, PCA is unsupervised in the sense that PCA depends only on the data while Fisher's LDA is supervised since it uses both data and class-label information. The high-dimensional data are linearly projected to a subspace where various classes are best distinguished as measured by the Fisher criterion. In <ref type="bibr" target="#b18">[19]</ref>, the so-called heteroscedastic discriminant analysis (HDA) is proposed to extend LDA to deal with highdimensional data with heteroscedastic covariance, where a linear projection can be learned from data and class labels based on the maximum likelihood criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hybrid Orthogonal Projection and Estimation (HOPE)</head><p>Consider a standard PCA setting, where each data sample is represented as a high-dimensional vector x with dimensionality D. Our goal is to learn a linear projection, represented as a matrix U, to map each data sample onto a space having dimensionality M &lt; D, which is called the latent feature space hereafter in this paper. Our proposed HOPE model is essentially a generative model in nature but it may also be viewed as a generalization to extend the probabilistic PCA in <ref type="bibr" target="#b28">[29]</ref> to consider a complex data distribution that has to be modeled by a finite mixture model in the latent feature space. This setting is very different from <ref type="bibr" target="#b27">[28]</ref>, where the original data x is modeled by mixture models in the original higher D-dimensional raw data space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HOPE: combining generalized PCA with generative model</head><p>Assume we have a full-size D × D orthogonal matrixÛ, satisfyingÛ TÛ =ÛÛ T = I, each data sample x in the original D-dimensional data space can be decomposed based on all orthogonal row vectors ofÛ, denoted as u i with i = 1, · · · , D, as follows:</p><formula xml:id="formula_0">x = D i=1 (x · u i ) u i .<label>(1)</label></formula><p>As shown in PCA, each high-dimensional data x can normally be represented fairly precisely in a lower-dimensional principal subspace and the contributions from the remaining dimensions may be viewed as random residual noises that have sufficiently small variances. Therefore, we have</p><formula xml:id="formula_1">x = (x · u 1 ) u 1 + · · · + (x · u M ) u M signal componentx + (x · u M +1 ) u M +1 + · · · + (x · u D ) u D noise componentx<label>(2)</label></formula><p>Here we are interested in learning an M × D projection matrix, denoted as U, to extract the signal componentx. First of all, if M (M &lt; D) is selected properly, the projection may serve as an effective feature extraction for signals as well as a mechanism to eliminate unwanted noises from the higher dimensional raw data. This may make the subsequent learning process more robust and less prone to overfitting. Secondly, all M row vectors u i with i = 1, · · · , M are learned to represent signals well in a lower M -dimension space. Furthermore, since all u i are orthogonal, it implies the latent features are largely de-correlated.This will significantly simplify the following learning problem as well.</p><p>In this case, each D-dimension data sample, x, is linearly projected onto an M -dimension vector z as z = Ux, where U is an orthogonal matrix, satisfying UU T = I. Meanwhile, we denote the projection of the unwanted noise componentx as n, and n can be similarly computed as n = Vx, where V is another orthogonal matrix corresponding to all noise dimensions, satisfying VV T = I. Moreover, V is orthogonal to U, i.e. VU T = 0. In overall, we may represent the above projection as follows:</p><formula xml:id="formula_2">z; n = U; V x =Ûx<label>(3)</label></formula><p>whereÛ is the above-mentioned D × D orthogonal projection matrix. Moreover, it is straightforward to show that the signal componentx and the residual noisē x in the original data space can be easily computed from the above projections as follows:</p><formula xml:id="formula_3">x = U T z = U T Ux (4) x = x −x = (I − U T U)x<label>(5)</label></formula><p>In the following, we consider how to learn the projection matrix U to represent D-dimensional data well in a lower M -dimension feature space. If this projection is learned properly, we may assume the above signal projection, z, and the residual noise projection, n, are independent in the latent feature space. Therefore, we may derive the probability distribution of the original data as follows:</p><formula xml:id="formula_4">p(x) = |Û −1 | · p(z) · p(n)<label>(6)</label></formula><p>whereÛ −1 denotes the Jacobian matrix to linearly map data from the projected space back to the original data space. IfÛ is orthonormal, the above Jacobian term equals to one. In this work, we follow <ref type="bibr" target="#b28">[29]</ref> to assume the residual noise projection n follows an isotropic covariance Gaussian distribution in the (D-M)-dimensional space, i.e. p(n) ∼ N (n | 0, σ 2 I), 1 where σ 2 is a variance parameter to be learned from data. As for the signal projection z, we adopt a quite different approach, as described below in detail. In all previous works, the signal projection z is assumed to follow a simple distribution in the M -dimension space. For example, z is assumed to follow a zero-mean unit-covariance Gaussian distribution in probabilistic PCA <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>. The advantage of this assumption is that an exact closed-form solution may be derived to calculate the projection matrix U using the spectral methods based on the data covariance matrix.</p><p>However, in most real-world applications, z still locates in a very high-dimensional space even after the linear projection, it does not make sense to assume z follows a simple unimodal distribution. As widely reported in the literature, it is empirically observed that real-world data normally do not follow a unimodal distribution in a high-dimensional space and they usually appear only in multiple concentrated regions in the high-dimensional space. More realistically, it is better to assume that z follows a finite mixture model in the M -dimension feature space because a finite mixture model may theoretically approximate any arbitrary statistical distribution as long as a sufficiently large number of mixture components are used. For simplicity, we may assume z follows a finite mixture of some exponential family distributions:</p><formula xml:id="formula_5">p(z) = K k=1 π k · f k (z|θ k )<label>(7)</label></formula><p>where π k denotes mixture weights with K k=1 π k = 1, and f k (z|θ k ) stands for a unimodal distribution from the exponential family with model parameters θ k . We use Θ to denote all model parameters in the mixture model, i.e., Θ = {θ k , π k | k = 1, · · · , K}. In practice, f k (z|θ k ) is chosen from the exponential family based on the nature of data. In this paper, we consider two possible choices for high-dimensional continuous data, namely the multivariate Gaussian distributions and the von Mises-Fisher (vMF) distributions. The learning algorithm can be easily extended to other models in the exponential family.</p><p>For example, if we choose the multivariate Gaussian distribution, then z follows a Gaussian mixture model (GMM) as follows:</p><formula xml:id="formula_6">p(z) = K k=1 π k · f k (z|θ k ) = K k=1 π k · N (z | µ k , Σ k )<label>(8)</label></formula><p>where N (z | µ k , Σ k ) denotes a multivariate Gaussian distribution with the mean vector µ k and the covariance matrix Σ k . Since the projection matrix U is orthogonal, all dimensions in z are highly de-correlated. Therefore, it is reasonable to assume each Gaussian component has a diagonal covariance matrix Σ k . This may significantly simplify the model estimation of GMMs. Alternatively, we may select a less popular model, i.e., the von Mises-Fisher (vMF) distribution. <ref type="bibr" target="#b1">2</ref> The vMF distribution may be viewed as a generalized normal distribution defined on a high-dimensional spherical surface. In this case, z follows a mixture of the von Mises-Fisher (movMF) distributions as follows:</p><formula xml:id="formula_7">p(z) = K k=1 π k · f k (z|θ k ) = K k=1 π k · C M (|µ k |) · e z·µ k<label>(9)</label></formula><p>where z is located on the surface of an M-dimensional sphere, i.e., |z| = 1, µ k denotes all model parameters of the k-th vMF component and it is an M -dimensional vector in this case, and C M (κ) is the probability normalization term of the k-th vMF component defined as:</p><formula xml:id="formula_8">C M (κ) = κ M/2−1 (2π) M/2 I M/2−1 (κ)<label>(10)</label></formula><p>where I v (·) denotes the modified Bessel function of the first kind at order v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Unsupervised Learning of HOPE Models</head><p>Obviously, the HOPE model is essentially a generative model that combines feature extraction and data modelling together, and thus its model parameters, including both the projection matrix and the mixture model, can be estimated based on the maximum likelihood (ML) criterion, just like normal generative models as well as the probabilistic PCA in <ref type="bibr" target="#b28">[29]</ref>. However, since z follows a mixture distribution, no closed-form solution is available to derive either the projection matrix or the mixture model. In this case, some iterative optimization algorithms, such as stochastic gradient descent (SGD) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, may be used to jointly estimate both the projection matrix U and the underlying mixture model altogether to maximize a joint likelihood function.</p><p>In this section, we assume the projection matrices, not only U but also the wholeÛ, are all orthonormal. As a result, the Jacobian term in eq.(6) disappears since it equals to one. Refer to Appendix A for the case where U is not orthonormal. Given a training set is available as X = {x n | n = 1, · · · , N }, assume that all x n are normalized to be of unit length as |x n | = 1, the joint log-likelihood function related to all HOPE parameters, including the projection matrix U, the mixture model Θ = {θ k |k = 1, · · · , K} and residual noise variance σ, can be expressed as follows:</p><formula xml:id="formula_9">L(U, Θ, σ | X) = N n=1 ln Pr(x n ) = N n=1 ln Pr(z n ) + ln Pr(n n ) = N n=1 ln K k=1 π k · f k (Ux n |θ k ) L 1 (U,Θ) + N n=1 ln N n n | 0, σ 2 I L 2 (U,σ)<label>(11)</label></formula><p>The HOPE parameters, including U, Θ and σ, can all be estimated by maximizing the above likelihood function as:</p><formula xml:id="formula_10">{U * , Θ * , σ * } = arg max U,Θ,σ L(U, Θ, σ | X)<label>(12)</label></formula><p>subject to the orthogonal constraint:</p><formula xml:id="formula_11">UU T = I.<label>(13)</label></formula><p>There are many methods to enforce the above orthogonal constraint in the optimization. For example, we may periodically apply the Gram-Schmidt process in linear algebra to orthogonalize U during the optimization process. In this work, for computational efficiency, we follow <ref type="bibr" target="#b3">[4]</ref> to cast the orthogonal constraint condition in eq.(13) as a penalty term in the objective function to convert the above constrained optimization problem into an unconstrained one as follows:</p><formula xml:id="formula_12">{U * , Θ * , σ * } = arg max U,Θ,σ L(U, Θ, σ | X) − β · D(U)<label>(14)</label></formula><p>where β (β &gt; 0) is a control parameter to balance the contribution of the penalty term, and the penalty term D(U) is a differentiable function as:</p><formula xml:id="formula_13">D(U) = M i=1 M j=i+1 |u i · u j | |u i | · |u j |<label>(15)</label></formula><p>with u i denoting the i-th row vector of the projection matrix U, and u i · u j representing the inner product of u i and u j . The norms of all row vectors of U need to be normalized to one after each update.</p><p>In this work, we propose to use the stochastic gradient descent (SGD) method to optimize the objective function in eq. <ref type="bibr" target="#b13">(14)</ref>. In this case, given any training data or a mini-batch of them, we calculate the gradients of the objective function with respect to the projection matrix, U, and the parameters of the mixture model, Θ, and then update them iteratively until the objective function converges. The gradients of L 1 (U, Θ) depends on the mixture model to be used. In the following, we first consider how to compute the derivatives of D(U) and L 2 (U, σ), which are general for all HOPE models. After that, as two examples, we will show how to calculate the derivatives of L 1 (U, Θ) for GMMs and movMFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dealing with the penalty term D(U)</head><p>Following <ref type="bibr" target="#b3">[4]</ref>, the gradients of the penalty term D(U) with respect to each row vector, u i (i = 1, · · · , M ), can be easily derived as follows:</p><formula xml:id="formula_14">∂D(U) ∂u i = M j=1 g ij · u j u i · u j − u i u i · u i<label>(16)</label></formula><p>where g ij denotes the absolute cosine value of the angle between two row vectors, u i and u j , computed as follows:</p><formula xml:id="formula_15">g ij = |u i · u j | |u i | · |u j | .<label>(17)</label></formula><p>The above derivatives can be equivalently represented as the following matrix form:</p><formula xml:id="formula_16">∂D(U) ∂U = (D − B)U<label>(18)</label></formula><p>where D is an M × M matrix, with its elements computed as d ij = sign(u i ·u j )</p><formula xml:id="formula_17">|u i |·|u j | (1 ≤ i, j ≤ M ), and B is an M × M diagonal matrix, with its diagonal elements computed as b ii = j=1 g ij u i ·u i (1 ≤ i ≤ M ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dealing with the noise model term L 2</head><p>The log-likelihood function related to the noise model, L 2 (U, σ), can be expressed as:</p><formula xml:id="formula_18">L 2 (U, σ) = − N (D − M ) 2 ln(σ 2 ) − 1 2σ 2 N n=1 n T n n n .<label>(19)</label></formula><p>And we have:</p><formula xml:id="formula_19">n T n n n = (x n − U T z n ) T (x n − U T z n ) = (x n − U T Ux n ) T (x n − U T Ux n ) = x T n x n − 2x T n U T Ux n + x T n U T UU T Ux n<label>(20)</label></formula><p>Therefore, L 2 (U, σ) can be expressed as:</p><formula xml:id="formula_20">L 2 (U, σ) = − N (D − M ) 2 ln(σ 2 ) − 1 2σ 2 N n=1 x T n x n − 2x T n U T Ux n + x T n U T UU T Ux n<label>(21)</label></formula><p>The gradient with respect to U 3 can be derived as follows:</p><formula xml:id="formula_21">∂L 2 (U, σ) ∂U = 1 σ 2 N n=1 2Ux n x T n − UU T Ux n x T n − Ux n x T n U T U = 1 σ 2 N n=1 U x n (x n ) T +x n (x n ) T .<label>(22)</label></formula><p>For the noise variance σ 2 , we can easily derive the following closed-form update formula by vanishing its derivative to zero:</p><formula xml:id="formula_22">σ 2 = 1 N (D − M ) N n=1 n T n n n<label>(23)</label></formula><p>As long as the learned noise variance σ 2 is small enough, maximizing the above term L 2 will force all signal dimensions into the projection matrix U and only the residual noises will be modelled by L 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Computing L 1 for GMMs</head><p>In this section, we consider how to compute the partial derivatives of L 1 (U, Θ) for GMMs. Assume each mini-batch X consists of a small subset of randomly selected training samples, X = {x n | n = 1, · · · , N }, the log likelihood function of HOPE models with GMMs can be represented as follows:</p><formula xml:id="formula_23">L 1 (U, Θ) = N n=1 ln K k=1 π k · N (Ux n | µ k , Σ k )<label>(24)</label></formula><p>The partial derivative of L 1 (U, Θ) w.r.t the mean vector, µ k , of the k-th Gaussian component can be calculated as follows:</p><formula xml:id="formula_24">∂L 1 (U, Θ) ∂µ k = N n=1 γ k (z n ) · Σ −1 k (z n − µ k )<label>(25)</label></formula><p>where z n = Ux n , and γ k (z n ) denotes the so-called occupancy statistics of the k-th Gaussian component, computed as</p><formula xml:id="formula_25">γ k (z n ) = π k N (zn|µ k ,Σ k ) K j=1 π j N (zn|µ j ,Σ j )</formula><p>.</p><p>The partial derivative of L 1 (U, Θ) w.r.t π k can be simply derived as follows:</p><formula xml:id="formula_26">∂L 1 (U, Θ) ∂π k = N n=1 γ k (z n ) π k<label>(26)</label></formula><p>The partial derivative of L 1 (U, Θ) w.r.t the Σ k is computed as follows:</p><formula xml:id="formula_27">∂L 1 (U, Θ) ∂Σ k = − 1 2 N n=1 γ k (z n ) Σ −1 k − Σ −1 k (z n − µ k )(z n − µ k ) T Σ −1 k .<label>(27)</label></formula><p>When we use the above gradients to update Gaussian covariance matrices in SGD, we have to impose additional constraints to ensure all covariance matrices are positive semidefinite. However, if we adopt diagonal covariance matrices for all Gaussian components, these constraints can be implemented in a fairly simple way. Finally, the partial derivative of L 1 (U, Θ) w.r.t the projection matrix U is computed as:</p><formula xml:id="formula_28">∂L 1 (U, Θ) ∂U = N n=1 K k=1 γ k (z n ) · Σ −1 k (µ k − z n )x T n .<label>(28)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computing L 1 for movMFs</head><p>Similarly, we derive all partial derivatives of L 1 (U, Θ) for a mixture of vMFs (movMFs). In this case, given a mini-batch of training samples, X = {x n | n = 1, · · · , N }, the log-likelihood function of the HOPE model with movMFs can be expressed as follows:</p><formula xml:id="formula_29">L 1 (U, Θ) = N n=1 ln K k=1 π k · C M (|µ k |) · e zn·µ k<label>(29)</label></formula><p>where each z n must be normalized to be of unit length 4 as required by the vMF distribution as:z</p><formula xml:id="formula_30">n = Ux n , z n =z n |z n | .<label>(30)</label></formula><p>Similar to the HOPE models with GMMs, we first define an occupancy statistic for k-th vMF component as:</p><formula xml:id="formula_31">γ k (z n ) = π k · C M (|µ k |) · e zn·µ k K j=1 π j · C M (|µ j |) · e zn·µ j .<label>(31)</label></formula><p>In a similar way, we can derive the partial derivatives of L 1 (U, Θ) with respect to π k , µ k and U as follows:</p><formula xml:id="formula_32">∂L 1 (U, Θ) ∂π k = N n=1 γ k (z n ) π k<label>(32)</label></formula><p>Algorithm 1 SGD-based Maximum Likelihood Learning Algorithm for HOPE randomly initialize u i (i = 1, · · · , M ), π k and θ k (k = 1, · · · , K)</p><formula xml:id="formula_33">for epoch = 1 to T do for minibatch X in training set do U ← U + · ∂L 1 (U,Θ) ∂U + ∂L 2 (U,σ) ∂U − β · ∂D(U) ∂U θ k ← θ k + · ∂L 1 (U,Θ) ∂θ k (∀k) π k ← π k + · ∂L 1 (U,Θ) ∂π k (∀k) σ 2 ← 1 N (D−M ) N n=1 n T n n n π k ← π k j π j (∀k) and u i ← u i |u i | (∀i) end for end for ∂L 1 (U, Θ) ∂µ k = N n=1 γ k (z n ) · z n − µ k |µ k | · I M/2 (|µ k |) I M/2−1 (|µ k |) (33) ∂L 1 (U, Θ) ∂U = N n=1 K k=1 γ k (z n ) |z n | · (I − z n z T n )µ k x T n<label>(34)</label></formula><p>Refer to Appendix B for all details on how to derive the above derivatives for the movMF distributions. Moreover, when movMFs are used, we need some special treatments to compute the Bessel functions in vMFs, i.e, I v (·), as shown in eqs. <ref type="bibr" target="#b30">(31)</ref> and (33). In this work, we adopt the numerical method in <ref type="bibr" target="#b0">[1]</ref> to approximate the Bessel functions, refer to the Appendix C for the numerical details on this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The SGD-based Learning Algorithm</head><p>Because all mixture weights, π k (k = 1, · · · K), and all row vectors, u i (i = 1, · · · , M ) of the projection matrix satisfy the constraints: K k π k = 1 and |u j | = 1 (∀j). During the SGD learning process, π k and u i must be normalized after each update as follows:</p><formula xml:id="formula_34">π k ← π k j π j (35) u i ← u i |u i | .<label>(36)</label></formula><p>Finally, we summarize the SGD algorithm to learn the HOPE models based on the maximum likelihood (ML) criterion in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning Neural Networks as HOPE</head><p>As described above, the HOPE model may be used as a novel model for high-dimensional data. The HOPE model itself can be efficiently learned unsupervised from unlabelled data based on the above-mentioned maximum likelihood criterion. Moreover, if data labels are available, a variety of discriminative training methods, such as those in <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, may be used to learn the HOPE model supervised based on some other discriminative learning criteria.</p><p>More interestingly, as we will elucidate here, there exists strong relationship between the HOPE models and neural networks (NNs). First of all, we will show that the HOPE models may be used as a new tool to probe the mechanism why NNs work so well in practice. Under the new HOPE framework, we may explain why NNs can almost universally excel on a variety of data types and how NNs can handle various types of highly-correlated high-dimensional data, which may be quite challenging to many other machine learning models. Secondly, more importantly, the HOPE framework provides us with some new approaches to learn NNs: (i) Unsupervised learning: the maximum likelihood estimation of HOPE may be directly applied to learn NNs from unlabelled data; (ii) Supervised learning: the HOPE framework can be incorporated into the normal supervised learning of NNs by explicitly imposing some orthogonal constraints in learning. This may improve the learning of NNs and yield better and more compact models. A HOPE model normally consists of two stages: i) a linear orthogonal projection from the raw data space to the latent feature space; ii) a generative model defined as a finite mixture model in the latent feature. As a result, we may depict every HOPE model as a two-layer network: a linear projection layer and a nonlinear model layer, as shown in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>. The first layer represents the linear orthogonal projection from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Linking HOPE to Neural Networks</head><formula xml:id="formula_35">x (x ∈ R D ) to z (z ∈ R M ): z = Ux.</formula><p>The second layer represents the underlying finite mixture model in the feature space. Taking movMFs as an example, each node in the model layer represents the log-likelihood contribution from one mixture component as follows:</p><formula xml:id="formula_36">φ k = ln (π k · f k (z|θ k )) = ln π k + ln C M (|µ k |) + z · µ k .<label>(37)</label></formula><p>Given an input x (assuming x is projected to z in the latent feature space), if we know all φ k (1 ≤ k ≤ K) in the model layer, we can easily compute the log-likelihood value of x from the HOPE model in eq.(9) as follows:</p><formula xml:id="formula_37">ln p(z) = ln K k=1 exp(φ k ).<label>(38)</label></formula><p>Moreover, all φ k (1 ≤ k ≤ K) may be used as a set of distance measurements to locate the input projection, z, in the latent space as trilateration, shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Therefore, all φ k (1 ≤ k ≤ K) may be viewed as a set of features to distinctly represent the original input x. Furthermore, we may use a preset threshold ε to prune the raw measurements, φ k (1 ≤ k ≤ K), as follows:</p><formula xml:id="formula_38">η k = max(0, φ k − ε)<label>(39)</label></formula><p>to eliminate those small log likelihood values from some faraway mixture components. Pruning small log-likelihood values as above may result in several benefits. Firstly, this pruning operation does not affect the total likelihood from the mixture model because it is always dominated by only a few top components. Therefore, we have:</p><formula xml:id="formula_39">ln p(z) ≈ ε + ln K k=1 exp(η k ).</formula><p>Secondly, this pruning step does not affect the trilateration problem in <ref type="figure" target="#fig_1">Figure 2</ref>. This is similar to the Global Positioning System (GPS) where the weak signals from faraway satellites are not used for localization. More importantly, the above pruning operation may improve robustness of the features since these small log-likelihood values may become very noisy. Note that the above pruning operation is similar to the rectified linear nonlinearity in regular ReLU neural networks. Here we give a more intuitive explanation for the popular ReLU operation under the HOPE framework. In this way, as shown in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>, all rectified log likelihood values in the model layer, i.e., η k (1 ≤ k ≤ K), may be viewed as a sensory map to measure each input, x, in the latent feature space using all mixture components as the probers. Each pixel in the map, i.e., a pruned measurement η k , roughly tells the distance between the centroid of a mixture component and the input projection z in the latent feature space. Under some condition (e.g., K M ), the input projection can be precisely located based on these pruned η k values as a trilateration problem in the M -dimensional feature space, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Therefore, this sensory map may be viewed as a feature representation learned to represent the input x, which may be fed to a softmax classifier to form a normal shallow neural network, or to another HOPE model to form a deep neural network.</p><p>Moreover, since the projection layer is linear, it can be mathematically combined with the upper model layer to generate a single layer structure, as shown in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>. If movMFs are used in HOPE, it is equivalent to a hidden layer in normal rectified linear (ReLU) neural networks. <ref type="bibr" target="#b4">5</ref> And the weight matrix in the merged layer can be simply derived from the HOPE model parameters, U and Θ. It is simple to show that the weight vectors for each hidden node k (1 ≤ k ≤ K) in <ref type="figure" target="#fig_0">Figure 1</ref>  Even though the linear projection layer may be merged with the model layer after all model parameters are learned, however, it may be beneficial to keep them separate during the model learning process. In this way, the model capacity may be controlled by two distinct control parameters: i) M can be selected properly to filter out noise components as in eq.(2) to prevent overfitting in learning; ii) K may be chosen independently to ensure the model is complex enough to model very big data sets for more difficult tasks. Moreover, we may enforce the orthogonal constraint, i.e., UU T = I, during the model learning to ensure that all dimensions of z are mostly de-correlated in the latent feature space, which may significantly simplify the density estimation in the feature space using a finite mixture model.</p><p>The formulation in <ref type="figure" target="#fig_0">Figure 1</ref> (a) helps to explain the underlying mechanism how neural networks work. Under the HOPE framework, it becomes clear that each hidden layer in neural networks may actually perform two different tasks implicitly, namely feature extraction and data modeling. This may shed some light on why neural nets can directly deal with various types of highly-correlated high-dimensional data <ref type="bibr" target="#b22">[23]</ref> without any explicit dimension reduction and feature de-correlation steps.</p><p>Based on the above discussion, a HOPE model is mathematically equivalent to a hidden layer in neural networks. For example, each movMF HOPE model can be collapsed into a single weight matrix, same as a hidden layer of ReLU NNs. On the contrary, any weight matrix in ReLU NNs may be decomposed as a product of two matrices as in <ref type="figure" target="#fig_0">Figure 1</ref> (a) as long as M is not less than the rank of the weight matrix. In practice, we may deliberately choose a smaller value for M to regularize the models. Under this formulation, it is clear that neural networks may be trained under the HOPE framework. There are several advantages to learn neural networks under the HOPE framework. First of all, the modelling capacity of neural networks may be explicitly controlled by selecting proper values for M and K, each of which is chosen for a different purpose. Secondly, we can easily apply the maximum likelihood estimation of HOPE models in section 4 to unsupervised or semi-supervised learning to learn NNs from unlabelled data. Thirdly, the useful orthogonal constraints may be incorporated into the normal back-propagation process to learn better NN models in supervised learning as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Unsupervised Learning of Neural Networks as HOPE</head><p>The maximum likelihood estimation method for HOPE in section 4 can be used to learn neural networks layer by layer in an unsupervised learning mode. All HOPE model parameters in <ref type="figure" target="#fig_0">Figure 1</ref> (a) are first estimated based on the maximum likelihood criterion as in section 4. Next, the two layers in the HOPE model are merged to form a regular NN hidden layer as in <ref type="figure" target="#fig_0">Figure 1</ref> (b). In this case, class labels are not required to learn all network weights and neural networks can be learned from unlabelled data under a theoretically solid framework. This is similar to the Hebbian style learning <ref type="bibr" target="#b24">[25]</ref> but it has a well-founded and converging objective function in learning. As described above, the rectified log-likelihood values from the HOPE model, i.e., η k (1 ≤ k ≤ K), may be viewed as a sensory map using all mixture components as the probers in the latent feature space, which may serve as a good feature representation for the original data. At the end, a small amount of labelled data may be used to learn a simple classifier, either a softmax layer or a linear support vector machine (SVM), on the top of the HOPE layers, which takes the sensory map as input for final classification or prediction.</p><p>In unsupervised learning, the learned orthogonal projection matrix U may be viewed as a generalized PCA, which performs dimension reduction by considering the complex distribution modeled by a finite mixture model in the latent feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Supervised Learning of Neural Networks as HOPE</head><p>The HOPE framework can also be applied to the supervised learning of neural networks when data labels are available. Let us take ReLU neural networks as example, each hidden layer in a ReLU neural network, as shown in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>, may be viewed as a HOPE model and thus it can be decomposed as a combination of a projection layer and a model layer, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (a). In this case, M needs to be chosen properly to avoid overfitting. In other words, each hidden layer in ReLU NNs is represented as two layers during learning, namely a linear projection layer and a nonlinear model layer. If data labels are available, as in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, instead of using the maximum likelihood criterion, we may use other discriminative training criteria <ref type="bibr" target="#b14">[15]</ref> to form the objective function to learn all network parameters. Let us take the popular minimum cross-entropy error criterion as an example, given a training set of the input data and the class labels, i.e., X = {x t , l t | 1 ≤ t ≤ T }, we may use the HOPE outputs, i.e., all η k in eq.(39) to form the cross-entropy objective function as follows:</p><formula xml:id="formula_40">F CE (U, µ k , b k | X) = − T t=1 log exp η lt (x t ) K k=1 exp η k (x t )<label>(40)</label></formula><p>Obviously, the standard back-propagation algorithm may be used to optimize the above objective function to learn all decomposed HOPE model parameters end-to-end, including U, {µ k , b k | 1 ≤ k ≤ K} from all HOPE layers. The only difference is that the orthogonal constraints, as in eq.(13), must be imposed for all projection layers during training, where the derivatives in eq.(18) must be incorporated in the standard back-propagation process to update each projection matrix U to ensure it is orthonormal. Note that in the supervised learning based on a discriminative training criterion, the unit-length normalization of the data projections in eq.(30) can be relaxed for computational simplicity since this normalization is only important for computing the likelihood for pure probabilistic models. After the learning, each pair of projection and model layers can be merged into a single hidden layer. After merging, the resultant network remains the exactly same network structure as normal ReLU neural networks. This learning method is related to the well-known low-rank matrix factorization method used for training deep NNs in speech recognition <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>. However, since we impose the orthogonal constraints for all projection matrices during the training process, it may lead to more compact models and/or better performance.</p><p>In supervised learning, the learned orthogonal projection matrix U may be viewed as a generalized LDA or HDA <ref type="bibr" target="#b18">[19]</ref>, which optimizes the data projection to maximize (or minimize) the underlying discriminative learning criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">HOPE for Deep Learning</head><p>As above, the HOPE framework can be used to learn rather strong shallow NN models. However, this does not hinder HOPE from building deeper models for deep learning. As shown in <ref type="figure" target="#fig_3">Figure  3</ref>, we may have two different structures to learn very deep neural networks under the HOPE framework. In <ref type="figure" target="#fig_3">Figure 3</ref> (a), one HOPE model is used as the first layer primarily for feature extraction and a deep neural network is concatenated on top of it as a powerful classifier to form a deep structure. The deep model in <ref type="figure" target="#fig_3">Figure 3</ref> (a) may be learned in either supervised or semiunsupervised mode. In semi-unsupervised learning, the HOPE model is learned based on the maximum likelihood estimation and the upper deep NN is learned supervised. Alternatively, if we have enough labelled data, we may jointly learn both HOPE and DNN in a supervised mode. In <ref type="figure" target="#fig_3">Figure 3</ref> (b), we may even stack multiple HOPE models to form another deep model structure. In this case, each HOPE model generates a sensory feature map in each HOPE layer. Just like all pixels in a normal image, all thresholded log-likelihood values in the sensory feature map are also highly correlated, especially for those mixture components locating relatively close in the feature space. Thus, it makes sense to add another HOPE model on top of it to de-correlate features and perform data modeling at a finer granularity. The deep HOPE model structures in <ref type="figure" target="#fig_3">Figure 3</ref> (b) can also be learned in a either supervised or unsupervised mode. In unsupervised learning, these HOPE layers are learned layer-wise using the maximum likelihood estimation. In supervised learning, all HOPE layers are learned in back-propagation with orthonormal constraints being imposed to all projection layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we will investigate the proposed HOPE framework in learning neural networks for several standard image and speech recognition tasks under several different learning conditions: i) unsupervised feature learning; ii) supervised learning; iii) semi-supervised learning. The examined tasks include the image recognition tasks using the MNIST data set, and the speech recognition task using the TIMIT data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">MNIST: Image Recognition</head><p>The MNIST data set <ref type="bibr" target="#b19">[20]</ref> consists of 28 × 28 pixel greyscale images of handwritten digits 0-9, with 60,000 training and 10,000 test examples. In our experiments, we first evaluate the performance of unsupervised feature learning using the HOPE model with movMFs. Secondly, we investigate the performance of supervised learning of DNNs under the HOPE framework, and further study the effect of the orthogonal constraint in the HOPE framework. Finally, we consider a semi-supervised learning scenario with the HOPE models, where all training samples (without labels) are used to learn feature representation unsupervised and then a portion of training data (along with labels) is used to learn post-stage classification models supervised. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Unsupervised Feature Learning on MNIST</head><p>In this experiment, we first randomly extract many small patches from the original unlabelled training images on MNIST. Each patch is of 6-by-6 in dimension, represented as a vector in R D , with D = 36. In this work, we randomly extract 400,000 patches in total from the MNIST training set for unsupervised feature learning. Moreover, every patch is normalized by subtracting the mean and being divided by the standard deviation of its elements. In the unsupervised feature learning, we follow the same experimental setting in <ref type="bibr" target="#b8">[9]</ref>, where an unsupervised learning algorithm is used to learn a "black box" feature extractor to map each input vector in R D to another K-dimension feature vector. In this work, we have examined several different unsupervised learning algorithms for feature learning: (i) kmeans clustering; (ii) spherical kmeans (spkmeans) clustering; (iii) mixture of vMF (movMF), (iv) PCA based dimension reduction plus movMF (PCA-movMF); and (v) the HOPE model with movMFs (HOPE-movMF). As for kmeans and spkmeans, the only difference is that different distance measures are used in clustering: kmeans uses the Euclidean distance while spkmeans uses the cosine distance. As for the movMF model, we can use the expectation maximization (EM) algorithm for estimation, as described in <ref type="bibr" target="#b2">[3]</ref>. In the following, we briefly summarize the experimental details for these feature extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">kmeans:</head><p>We first apply the k-means clustering method to learn K centroids µ k from all extracted patch input vectors. For each learned centroid µ k , we use a soft threshold function to compute each feature as: f k (x) = max(0, |x − µ k | − ε), where ε is a pre-set threshold. In this way, we may generate a K-dimension feature vector for each patch input vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">spkmeans:</head><p>As for the spk-means clustering, we need to normalize all input patch vectors to be of unit length before clustering them into K different centroids based on the cosine distance measure. Given each learned centroid µ k , we can compute each feature as f k (x) = max(0, x T µ k − ε).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">movMF:</head><p>We also need to normalize all input patch vectors to be of unit length. We use the EM algorithm to learn all model parameters µ k . For each learned centroid µ k of the movMF model, we compute one feature as f k (x) = max 0, ln(π k ) + ln(C N (|µ k |)) + x T µ k − ε .</p><p>4. PCA-movMF: Comparing to movMF, the only difference is that we first use PCA for dimension reduction, reducing all input patch vectors from R D to R M . Then, we use the same method to estimate an movMF model for the reduced D-dimension feature vectors. In this experiment, we set M = 20 to reserve 99.5% of the total sum of all eigenvalues in PCA. For each learned vMF model µ k , we compute one feature as f k (x) = max 0, ln(π k ) + ln(C N (|µ k |)) + z T µ k − ε .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">HOPE-movMF:</head><p>We use the maximum likelihood estimation method described in section 4 to learn a HOPE model with movMFs. For each learned vMF component, we can compute one feature as f k (x) = max(0, ln(π k ) + ln(C M (|µ k |)) +z · µ k − ε). Similar to PCA-movMF, we also set M = 20 here.</p><p>Furthermore, since HOPE-movMF is learned using SGD, we need to tune some hyper-parameters for HOPE, such as learning rate, mini-batch size, β and σ 2 . In this work, the learning rate is set to 0.002, minibatch size is set to 100, we set β = 1.0, and the noise variance is manually set to σ 2 = 0.1 for convenience. After learning the above models, they are used as feature extractors. We use the same method as described in <ref type="bibr" target="#b8">[9]</ref> to generate a feature representation for each MNIST image, where each feature extractor is convolving over an MNIST image to obtain the feature representations for all local patches in the image. Next, we split the image into four equally-sized quadrants and the feature vectors in each quadrant are all summed up to pool as one feature vector. In this way, we can get a 4K-dimensional feature vector for each MNIST image, where K is the number of all learned features for each patch. Finally, we use these pooled 4K-dimension feature vectors for all training images, along with the labels, to estimate a simple linear SVM as a post-stage classifier for image classification. The experimental results are shown in <ref type="table" target="#tab_0">Table 1</ref>. We can see that spkmeans and movMF can achieve much better performance than kmeans. The PCA-based dimension reduction leads to further performance gain. Finally, the jointly trained HOME model with movMFs yields the best performance, e.g., 0.64% in classification error rate when K = 1200. This is a very strong performance for unsupervised feature learning on MNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Supervised Learning of Neural Networks as HOPE on MNIST</head><p>In this experiment, we use the MNIST data set to examine the supervised learning of rectified linear (ReLU) neural networks under the HOPE framework, as discussed in section 5.3.</p><p>Here we follow the normalized initialization in <ref type="bibr" target="#b9">[10]</ref> to randomly initialize all NN weights, without using pre-training. We adopt a small modification to the method in <ref type="bibr" target="#b9">[10]</ref> by adding a factor to control the dynamic range of initial weights as follows:</p><formula xml:id="formula_41">W i ∼ − γ · √ 6 √ n i + n i+1 , γ · √ 6 √ n i + n i+1<label>(41)</label></formula><p>where n i denotes the number of units in the i-th layer. For the ReLU units, due to the unbounded behavior of the activation function, activations of ReLU units might grow unbounded. To handle this numerical problem, we shrink the dynamic range of initial weights by using a small factor γ (γ = 0.5), which is equivalent to scaling the activations. We use SGD to train ReLU neural networks using the following learning schedule:</p><formula xml:id="formula_42">t = 0 · α t (42) m t = t T m f + (1 − t T )m i t &lt; T m f t ≥ T (43)</formula><p>where t and m t denote the learning rate and momentum for the t-th epoch, and we set all control parameters as m i = 0.5, m f = 0.99, α = 0.998. We totally run T = 50 epochs for learning without dropout and run T = 500 epochs for learning with dropout. Moreover, the weight decay is used here and it is set to 0.00001. Furthermore, for the HOPE model, the control parameter for the orthogonal constraint, β, is set to 0.01 in all experiments. In this work, we do not use any data augmentation method. Under the HOPE framework, we decompose each ReLU hidden layer into two layers as in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>. In this experiment, we first examine the supervised learning of NNs with or without imposing the orthogonal constraints to all projection layers. Firstly, we investigate the performance of a neural network containing only a single hidden layer, decomposed into a pair of a linear projection layer and a nonlinear model layer. Here we evaluate neural networks with a different number of hidden units (K) and a varying size of the projection layer (M ). From the experimental results shown in <ref type="table">Table.</ref> 2, we can see that the HOPE-trained NN can achieve much better performance than the baseline NN, especially when smaller values are used for M . This supports that the projection layer may eliminate residual noises in data to avoid over-fitting when M is properly set. However, after we relax the orthogonal constraint in the HOPE model, as shown in <ref type="table" target="#tab_1">Table 2</ref>, the performance of the models using only linear projection layers gets much worse than those of the HOPE models as well as that of the baseline NN. These results verify that orthogonal projection layers are critical in the HOPE models. Furthermore, in <ref type="figure">Figure.</ref> 4, we have plotted the learning curves of the total sum of all correlation coefficients among all row vectors in the learned projection matrix U, i.e., i =j |u i ·u j | |u i ||u j | . We can see that all projection vectors tend to get strongly correlated (especially when M is large) in the linear projection matrix as the learning proceeds. On the other hand, the orthogonal constraint can effectively de-correlate all the projection vectors. Moreover, we show all correlation coefficients, i.e., |u i ·u j | |u i ||u j | , of the linear projection matrix and the HOPE orthogonal projection matrix as two images in <ref type="figure" target="#fig_5">Figure 5</ref>, which clearly shows that the linear projection matrix has many strongly correlated dimensions and the HOPE projection matrix is (as expected) orthogonal .</p><p>As the MNIST training set is very small, we further use the dropout technique in <ref type="bibr" target="#b13">[14]</ref> to improve the model learning on the MNIST task. In this experiment, the visible dropout probability is set to 20% and the hidden layer dropout probability is set to 30%. In <ref type="table" target="#tab_2">Table 3</ref>, we compare a 1-hidden-layer shallow NN with two HOPE models (M =200,400). The results show that the HOPE framework can significantly improve supervised learning of NNs. Under the HOPE framework, we can train very simple shallow neural networks from scratch, which can yield comparable performance as deep models. For example, on the MNIST task, as shown in <ref type="table" target="#tab_2">Table 3</ref>, we may achieve 0.85% in classification error rate using a shallow neural network (with only one hidden layer of 2000 nodes) trained under the HOPE framework. Furthermore, we consider to build deeper models (two-hidden-layer NNs) under the HOPE framework. Using    the two different structures in <ref type="figure" target="#fig_3">Figure 3</ref>, we can further improve the classification error rate to 0.81%, as shown in <ref type="table">Table.</ref> 4. To the best of our knowledge, this is one of the best results reported on MNIST without using CNNs and data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Semi-supervised Learning on MNIST</head><p>In this experiment, we combine the unsupervised feature learning with supervised model learning and examine the classification performance when only limited labelled data is available. Here we also list the results using convolutional deep belief networks (CDBN) in <ref type="bibr" target="#b20">[21]</ref> as a baseline system. In our experiments, we use the raw pixel features and unsupervised learned (USL) features in section 6.1.1. As example, we choose the unsupervised learned features from the HOPE-movMF model (K = 800) in <ref type="table" target="#tab_0">Table 1</ref> 7 . Next, we concatenate the features to a post-stage classifier, which is supervised trained using only a portion of the training data, ranging from 1000 to 60000 (all). We consider many different types of classifiers here, including linear SVM, regular DNNs and HOPE-trained DNNs. Note that all classifiers are trained separately from the feature learning. All results are summarized in <ref type="table" target="#tab_4">Table 5</ref>. It shows that we can achieve the best performance when we combine the HOPE-trained USL features with HOPE-trained poststage classifiers. The gains are quite dramatic no matter how much labelled data is used. For example, when only 5000 labelled samples are used, our method can achieve 0.90% in error rate, which significantly outperforms all other methods including CDBN in <ref type="bibr" target="#b20">[21]</ref>. At last, as we use all training data for the HOPE model, we can achieve 0.40% in error rate. To the best of our knowledge, this is one of the best results reported on MNIST without using data augmentation. Furthermore, our best system uses a quite simple model structure, consisting of a HOPE-trained feature extraction layer of 800 nodes and a HOPE-trained NN of two hidden layers (1200 node in each layer), which is much smaller and simpler than those top-performing systems on MNIST. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">TIMIT: Speech Recognition</head><p>In this experiment, we examine the supervised learning of shallow and deep neural networks under the HOPE framework for a standard speech recognition task using the TIMIT data set. The HOPE-based supervised learning method is compared with the regular back-propagation training method. We use the minimum cross-entropy learning criterion here. The TIMIT speech corpus consists of a training set of 462 speakers, a separate development set of 50 speakers for cross-validation, and a core test set of 24 speakers. All results are reported on the 24-speaker core test set. The speech waveform data is analyzed using a 25-ms Hamming window with a 10-ms fixed frame rate. The speech feature vector is generated by a Fouriertransform-based filter-banks that include 40 coefficients distributed on the Mel scale and energy, along with their first and second temporal derivatives. This leads to a 123-dimension feature vector per speech frame. We further concatenate 15 consecutive frames within a long context window of (7+1+7) to feed to the models, as 1845-dimension input vectors <ref type="bibr" target="#b31">[32]</ref>. All speech data are normalized by subtracting the mean of the training set and being divided by the standard deviation of the training set on each dimension so that all input vectors have zero mean and unit variance. We use 183 target class labels (i.e., 3 states for each of the 61 phones) for the DNN training. After decoding, these 61 phone classes are mapped to a set of 39 classes for the final scoring as in <ref type="bibr" target="#b21">[22]</ref>. In our experiments, a bi-gram language model at phone level, estimated from all transcripts in the training set, is used for speech recognition.</p><p>We first train ReLUs based shallow and deep neural networks as our baseline systems. The networks are trained using the back-propagation algorithm, with a mini-batch size of 100. The initial learn rate is set to 0.004 and it is kept unchanged if the error rate on the development set is still decreasing. Afterwards, the learning rate is halved after every epoch, and the whole training procedure is stopped when the error reduction on the development set is less than 0.1% in two consecutive iterations. In our experiments, we also use momentum and weight decay, which are set to 0.9 and 0.0001, respectively. When we use the mini-batch SGD to train neural  <ref type="bibr" target="#b31">[32]</ref> and another recent work <ref type="bibr" target="#b1">[2]</ref>, using deep neural networks on TIMIT. From the results, we can see that the HOPE-trained NNs can consistently outperform the regular NNs by an about 0.8% absolute reduction in phone recognition error rates. Moreover, the HOPE-trained neural networks are much smaller than their counterpart DNNs in number of model parameters if the HOPE layers are not merged. After merging, they have the exactly the same model structure as their counterpart NNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Final Remarks</head><p>In this paper, we have proposed a novel model, called hybrid orthogonal projection and estimation (HOPE), for high-dimensional data. The HOPE model combines feature extraction and data modeling under a unified generative modeling framework so that both feature extractor and data model can be jointly learned either supervised or unsupervised. More interestingly, we have shown that the HOPE models are closely related to neural networks in a way that each hidden layer in NNs can be reformulated as a HOPE model. Therefore, the proposed HOPE related learning algorithms can be easily applied to perform either supervised or unsupervised learning for neural networks. We have evaluated the proposed HOPE models in learning NNs on several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results have strongly supported that the HOPE models can provide a very effective unsupervised learning method for NNs. Meanwhile, the supervised learning of NNs can also be conducted under the HOPE framework, which normally yields better performance and more compact models.</p><p>We are currently investigating the HOPE model to learn convolution neural networks (CNNs) for more challenging image recognition tasks, such as CIFAR and ImageNet. At the same time, we are also examining the HOPE-based unsupervised learning for various natural language processing (NLP) tasks. These results will be reported in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Learning HOPE whenÛ is not orthonormal</head><p>In some tasks, in additional to dimension reduction, we may want to use the projection matrix U to perform signal whitening to ensure the signal projection z has roughly the same variance along all dimensions. This has been shown to be quite important for many image recognition and speech recognition tasks. In this case, we may still want to impose the orthogonal constraints among all row vectors of U, i.e., u i · u j = 0 (i, j = 1, · · · , M, i = j), but these row vectors may not be of unit length, i.e., |u i | ≥ 1 (i = 1, · · · , M ). Moreover, it is better not to whiten the residual noises in the remaining D − M dimensions to amplify them unnecessarily. Therefore, we still enforce the unit-length constraints for the matrix V, i.e., |u i | = 1 (i = M + 1, · · · , D). Because U is not orthonormal anymore, when we compute the likelihood function of the original data in eq.(11) for HOPE, we have to include the Jacobian term as follows:</p><p>L(U, Θ, σ | X) = </p><p>Because UU T = I here, we have x n = U T (UU T ) −1 z n + V T n n (45) and then we have</p><p>x T n x n = n T n V + z T n (UU T ) −1 U V T n n + U T (UU T ) −1 z n = n T n n n + z T n (UU T ) −1 z n = n T n n n + x T n U T (UU T ) −1 Ux n (46) Therefore, we may derive the residual noise energy as:</p><formula xml:id="formula_44">n T n n n = x T n x n − x T n U T (UU T ) −1 Ux n = x T n I − U T (UU T ) −1 U x n<label>(47)</label></formula><p>In this case, L 2 (U, σ) can be expressed as:</p><formula xml:id="formula_45">L 2 (U, σ) = − N 2 ln(σ 2 ) − 1 2σ 2 N n=1 x T n x n − x T n U T (UU T ) −1 Ux n<label>(48)</label></formula><p>Therefore, its gradient with respect to U, can be derived as follows:</p><formula xml:id="formula_46">∂L 2 (U, σ) ∂U = 1 σ 2 N n=1 (UU T ) −1 Ux n x T n − (UU T ) −1 Ux n x T n U T (UU T ) −1 U = 1 σ 2 N n=1 (UU T ) −1 Ux n x T n I − U T (UU T ) −1 U<label>(49)</label></formula><p>Next, we consider the Jacobian term, J (U), which can be computed as follows:</p><formula xml:id="formula_47">J (U) = N · ln |Û −1 | = −N M i=1 ln |u i |<label>(50)</label></formula><p>Because of ∂J (U) ∂u i = − N ·u i |u i | 2 (i = 1, · · · , M ), it is easy to show that its derivative with respect to U can be derived as:</p><formula xml:id="formula_48">∂J (U) ∂U = −N · UU T −1 U.<label>(51)</label></formula><p>Similarly, the HOPE parameters, i.e., U, Θ and σ, can be estimated by maximizing the above likelihood function as follows:</p><p>{U * , Θ * , σ * } = arg max U,Θ,σ L(U, Θ, σ | X)</p><p>subject to an orthogonal constraint:</p><formula xml:id="formula_50">UU T = Φ,<label>(53)</label></formula><p>where Φ is a diagonal matrix. The above constraint can also be implemented as an penalty term similar to eq.(15). However, the norm of each row vector is relaxed as follows:</p><p>|u i | ≥ 1 (i = 1, · · · , M ) (54)</p><p>The log-likelihood function related to the signal model, L 1 (U, Θ), and signal variance, σ 2 , are calculated in the same way as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Derivatives of movMFs</head><p>The partial derivatives of the objective function in eq.(29) w.r.t all µ k can be computed as follows:</p><formula xml:id="formula_51">∂L 1 (U, Θ) ∂µ k = N n=1</formula><p>π k · C M (|µ k |) ∂κ k µ k · e z T n ·µ k + C M (|µ k |) · e z T n ·µ k · z n K j=1 π j · C M (|µ j |) · e z·µ j (55)</p><p>where we have ∂ |µ k | ∂µ k = µ k |µ k | .</p><p>As to C M (κ), for brevity, let us denote s = M 2 − 1, and ξ = (2π) s+1 . Thus, we may derive</p><formula xml:id="formula_53">C M (κ) = −C M (κ) · I s+1 (κ) I s (κ)<label>(58)</label></formula><p>Substituting eq. (56) and eq. (58) into eq. (55), we obtain the partial derivative of the objective function in eq. (29) w.r.t µ k as follows:</p><formula xml:id="formula_54">∂L 1 (U, Θ) ∂µ k = N n=1 π k · C M (|µ k |) · e zn·µ k z n − µ k |µ k | · I M/2 (|µ k |) I M/2−1 (|µ k |) K j=1 π j · C M (|µ j |) · e zn·µ j = N n=1 γ(z nk ) · z n − µ k |µ k | · I M/2 (|µ k |) I M/2−1 (|µ k |)<label>(59)</label></formula><p>where γ(z nk ) = π k ·C M (|µ k |)·e zn·µ k K j=1 π j ·C M (|µ j |)·e zn·µ j is the occupancy statistics of k-th component of z n . Next, let us consider the partial derivative of the objective function in eq.(29) w.r.t U. Based on the chain rule, we have ∂L 1 (U, Θ) ∂U = ∂L 1 (U, Θ) ∂ z n · ∂ z n ∂U = ∂z T n ∂z n · ∂L 1 (U, Θ) ∂z n · ∂ z n ∂U (60) Furthermore, we may derive </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of a HOPE model as a layered network structure in (a). It may be equivalently reformulated as a hidden layer in neural nets shown in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the HOPE features as trilateration in the latent feature space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b) may be derived as w k = U T µ k and its bias is computed as b k = ln π k + ln C M (|µ k |) − ε.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Two structures to learn deep networks with HOPE: (a) Stacking a DNN on top of one HOPE layer; (b) Stacking multiple HOPE layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The learning curves of the total sum of correlation coefficients of the linear projection and orthogonal HOPE projection matrices, respectively. left: M =200, K=1k,5k,10k,50k; right: M =400, K=1k,5k,10k,50k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The correlation coefficients of the linear projection matrix (left) and the orthogonal HOPE projection matrix (right) are shown as two images. Here M = 400 and K = 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>ln |Û − 1 |</head><label>1</label><figDesc>+ ln Pr(z n ) + ln Pr(n n ) = N · ln |Û −1 | f k (Ux n |θ k ) L 1 (U,Θ) + N n=1 ln N n n | 0, σ 2 I L 2 (U,σ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>C</head><label></label><figDesc>M (κ) = s · κ s−1 ξI s (κ) − κ s · I s (κ)where I v (·) denotes the Bessel function of the first kind at the order v. Because we haveκI s+1 (κ) = κI s (κ) − sI s (κ) ⇒ s κ − I s (κ) I s (κ) = − I s+1 (κ) I s (κ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>∂L 1 (π</head><label>1</label><figDesc>k · C M (|µ k |) · e zn·µ k K j=1 π j · C M (|µ j |) · e zn·µ j · µ k = eq.(61) and eq. (62) into eq. (60), we can obtain∂L 1 (U, Θ) ∂U = N n=1 K k=1 γ(z nk ) |z n | (I − z n z T n )µ k x T n(63)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification error rates (in %) on the MNIST test set using supervised learned linear SVMs as classifiers and unsupervised learned features (4K-dimension) from different models.</figDesc><table><row><cell>model / K=</cell><cell>400 800 1200 1600</cell></row><row><cell>kmeans</cell><cell>1.41 1.31 1.16 1.13</cell></row><row><cell>spkmeans</cell><cell>1.09 0.90 0.86 0.81</cell></row><row><cell>movMF</cell><cell>0.89 0.82 0.81 0.84</cell></row><row><cell cols="2">PCA-movMF 0.87 0.75 0.73 0.74</cell></row><row><cell cols="2">HOPE-movMF 0.76 0.71 0.64 0.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification error rates (in %) on the MNIST test set using a shallow NN with one hidden layer of different hidden units. Neither dropout nor data augmentation is used here for quick turnaround. Two numbers in brackets,[M, K], indicate a HOPE model with the orthogonal constraint in the projection, and two number in parentheses, (M, K), indicate the same model structure without imposing the orthogonal constraint in the projection.</figDesc><table><row><cell>Net Architecture / K=</cell><cell>1k</cell><cell>2k</cell><cell>5k</cell><cell>10k 50k</cell></row><row><cell>Baseline: 784-K-10</cell><cell cols="4">1.49 1.35 1.28 1.30 1.32</cell></row><row><cell cols="5">HOPE1: 784-[100-K]-10 1.18 1.20 1.17 1.18 1.19</cell></row><row><cell cols="5">HOPE2: 784-[200-K]-10 1.21 1.20 1.17 1.19 1.18</cell></row><row><cell cols="5">HOPE3: 784-[400-K]-10 1.19 1.23 1.25 1.25 1.25</cell></row><row><cell cols="5">Linear1: 784-(100-K)-10 1.45 1.49 1.43 1.45 1.48</cell></row><row><cell cols="5">Linear2: 784-(200-K)-10 1.52 1.50 1.54 1.55 1.54</cell></row><row><cell cols="5">Linear3: 784-(400-K)-10 1.53 1.52 1.49 1.52 1.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of classification error rates (in %) on the MNIST test set between a shallow NN and two HOPE-trained NNs with the same model structure. Dropout is used in this experiment.</figDesc><table><row><cell>Net Architecture / K=</cell><cell>1k</cell><cell>2k</cell><cell>5k</cell></row><row><cell cols="4">NN baseline: 784-K-10 1.05 1.01 1.01</cell></row><row><cell cols="4">HOPE1: 784-[200-K]-10 0.99 0.85 0.89</cell></row><row><cell cols="4">HOPE2: 784-[400-K]-10 0.86 0.86 0.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">Comparison of classification error rates (in %) on the MNIST test set between a 2-</cell></row><row><cell cols="4">hidden-layer DNN and two HOPE-trained DNNs with similar model structures, with or without</cell></row><row><cell>using dropout.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>Net Architecture</cell><cell cols="2">without dropout with dropout</cell></row><row><cell>DNN baseline</cell><cell>784-1200-1200-10</cell><cell>1.25</cell><cell>0.92</cell></row><row><cell>HOPE + NN</cell><cell>784-[400-1200]-1200-10</cell><cell>0.99</cell><cell>0.82</cell></row><row><cell>HOPE*2</cell><cell>784-[400-1200]-[400-1200]-10</cell><cell>0.97</cell><cell>0.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Classification error rates (in %) on the MNIST test set using raw pixel features or unsupervised learned features, along with different post-stage classifiers trained separately by limited labeled data. Here USL denotes the unsupervised learned features from HOPE</figDesc><table><row><cell>-movMF</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Supervised learning of neural networks on TIMIT with and without using the HOPE framework. The two numbers in a bracket, [M, K], indicate a HOPE model with M-dimension features and K mixture components. FACC: frame classification accuracy from neural networks. PER: phone error rate in speech recognition.networks under the HOPE framework, the control parameter for the orthogonal constraints, i.e. β, is set to be 0.01.In our experiments, we compare the standard NNs with the HOPE-trained NNs for two network architectures, one shallow network with one hidden layer of 10240 hidden nodes and one deep network with 3 hidden layers of 2048 nodes. The performance comparison between them is shown inTable.6. Our results are comparable with</figDesc><table><row><cell>model</cell><cell>Net Architecture</cell><cell cols="2">FACC (%) PER (%)</cell></row><row><cell>NN</cell><cell>1845-10240-183</cell><cell>61.45</cell><cell>23.85</cell></row><row><cell>HOPE-NN</cell><cell>1845-[256-10240]-183</cell><cell>62.11</cell><cell>23.04</cell></row><row><cell>DNN</cell><cell>1845-3*2048-183</cell><cell>63.13</cell><cell>22.37</cell></row><row><cell cols="2">HOPE-DNN 1845-[512-2048]-2*2048-183</cell><cell>63.55</cell><cell>21.59</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Without losing generality, we may simply normalize the training data to ensure that the residual noises have zero mean.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The main reason why we are interested in the von Mises-Fisher (vMF) distributions is that the choice of the vMF model can strictly link our HOPE model to regular neural networks in deep learning. We will elucidate this later in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We may use the constraint UU T = I to significantly simplify the above derivation. However, that leads to a gradient computation strongly relying on the orthogonal constraint. Since we use SGD to iteratively optimize all model parameters, including U. We can not ensure UU T = I strictly holds anytime in the SGD process. Therefore, the simplified gradient usually yields poor convergence performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In practice, we usually normalize all original data, xn, to be of unit length: |xn| = 1, prior to the HOPE model. In this case, as long as M is properly chosen (e.g., to be large enough), the projection matrix U is always learned to extract from xn as much energy as possible. Therefore, this normalization step may be skipped because the norm of the projected zn is always very close to one even without normalization in this stage, i.e., |zn| = |zn| ≈ 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">On the other hand, if GMMs are used in HOPE, it can be similarly shown that it is equivalent to a hidden layer in Radial basis function (RBF) networks<ref type="bibr" target="#b23">[24]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Matlab codes are available at https://wiki.eecs.yorku.ca/lab/MLL/projects:hope:start for readers to reproduce all MNIST results reported in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">For the HOPE-movMF model with K = 800, there are 115 empty clusters. Thus, the unsupervised learned features are of 2740 in dimension.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Numerical Methods for I v (·)</head><p>In the learning algorithm for movMFs, we may need to compute the Bessel functions, I v (·), in several places. First of all, we need to compute the normalization term C M (κ) when calculating the likelihood function of a vMF distribution as in eq.(9). Secondly, we need to calculate the rations of the modified Bessel functions, A d (κ) = I M/2 (κ) I M/2−1 (κ) , in eq.(59). As we know, the modified Bessel functions of the first kind take the following form:</p><p>From eq. (64), we can see that when κ d, I d (κ) overflows quite rapidly. Meanwhile, when κ = o(d) and d → ∞, I d (κ) underflows quite rapidly. In this work, we use the approximation strategy in eq. (9.7.7) on page 378 of <ref type="bibr" target="#b0">[1]</ref> as follows:</p><p>where we have t = 1</p><p>with the functions u k (t) taking the following forms:</p><p>Refer to page 366 of <ref type="bibr" target="#b0">[1]</ref> for other higher orders u k (t). Usually, the sum of the term [1+ ∞ k=1 u k (t)</p><p>d k ] in eq.(65) is very small and it is safe to eliminate it from evaluation in most cases. Then, after substituting eq.(66) and eq.(67) into eq.(65), the logarithm of the approximated modified Bessel function is finally computed as follows:</p><p>In this work, the approximation in eq.(68) is used to compute all Bessel functions in the learning algorithms for movMFs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Handbook of mathematical functions with formulas, graphs, and mathematical tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abramowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Stegun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Washington : U.S. Govt. Print</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 27</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Clustering on the unit hypersphere using von mises-fisher distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1345" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incoherent training of deep neural networks to de-correlate bottleneck features for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>of IEEE International Conference on Acoustics, Speech, and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 19</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Lectures on Machine Learning</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="146" to="168" />
		</imprint>
	</monogr>
	<note>Bousquet and U. von Luxburg)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The tradeoffs of large scale learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization for Machine Learning</title>
		<editor>S. Sra, S. Nowozin and S. J. Wright</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1964" />
			<biblScope unit="page" from="351" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modelling the manifolds of images of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Revow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">15271554</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>preprint arXiv 1207.0580</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative training for automatic speech recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer and Speech, Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="608" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parameter estimation of statistical models using convex optimization: An advanced method of discriminative training for speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="115" to="127" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative learning of generative models: large margin multinomial mixture models for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dimension reduction by local principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1493" to="1516" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Heteroscedastic discriminant analysis and reduced rank HMMs for improved speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Andreou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="297" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speaker-independent phone recognition using hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1641" to="1648" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Investigations of deep neural networks for large vocabulary continuous speech recognition: Why DNN surpasses GMMs in acoustic modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Symposium on Chinese Spoken Language Processing</title>
		<meeting>of International Symposium on Chinese Spoken Language essing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal approximation using radial-basis-function networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Neural Networks and Brain Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Rolls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Treves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EM algorithms for PCA and SPCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="626" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Low-rank matrix factorization for deep neural network training with high-dimensional output targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustic, Speech, Signal Processing</title>
		<meeting>IEEE International Conference on Acoustic, Speech, Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICASSP 2013</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mixtures of probabilistic principal component analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="482" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic principle component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning (ICML)</title>
		<meeting>the 25th international conference on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Restructuring of deep neural network acoustic models with singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2013</title>
		<meeting>Interspeech 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast adaptation of deep neural network based on discriminant codes for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1713" to="1725" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
