<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image Denoising</term>
					<term>Convolutional Neural Net- works</term>
					<term>Residual Learning</term>
					<term>Batch Normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise (AWGN) at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks such as Gaussian denoising, single image super-resolution and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Image denoising is a classical yet still active topic in low level vision since it is an indispensable step in many practical applications. The goal of image denoising is to recover a clean image x from a noisy observation y which follows an image degradation model y = x + v. One common assumption is that v is additive white Gaussian noise (AWGN) with standard deviation σ. From a Bayesian viewpoint, when the likelihood is known, the image prior modeling will play a central role in image denoising. Over the past few decades, various models have been exploited for modeling image priors, including nonlocal self-similarity (NSS) models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, sparse models <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, gradient models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and Markov random field (MRF) models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. In particular, the NSS models are popular in state-of-the-art methods such as BM3D <ref type="bibr" target="#b1">[2]</ref>, LSSC <ref type="bibr" target="#b3">[4]</ref>, NCSR <ref type="bibr" target="#b5">[6]</ref> and WNNM <ref type="bibr" target="#b12">[13]</ref>.</p><p>Despite their high denoising quality, most of the image prior-based methods typically suffer from two major drawbacks. First, those methods generally involve a complex optimization problem in the testing stage, making the denoising process time-consuming <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Thus, most of the priorbased methods can hardly achieve high performance without sacrificing computational efficiency. Second, the models in general are non-convex and involve several manually chosen parameters, providing some leeway to boost denoising performance.</p><p>To overcome the limitations of prior-based approaches, several discriminative learning methods have been recently developed to learn image prior models in the context of truncated inference procedure. The resulting models are able to get rid of the iterative optimization procedure in the test phase. Schmidt and Roth <ref type="bibr" target="#b13">[14]</ref> proposed a cascade of shrinkage fields (CSF) method that unifies the random field-based model and the unrolled half-quadratic optimization algorithm into a single learning framework. Chen et al. <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> proposed a trainable nonlinear reaction diffusion (TNRD) model which learns a modified fields of experts <ref type="bibr" target="#b11">[12]</ref> image prior by unfolding a fixed number of gradient descent inference steps. Some of the other related work can be found in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Although CSF and TNRD have shown promising results toward bridging the gap between computational efficiency and denoising quality, their performance are inherently restricted to the specified forms of prior. To be specific, the priors adopted in CSF and TNRD are based on the analysis model, which is limited in capturing the full characteristics of image structures. In addition, the parameters are learned by stage-wise greedy training plus joint fine-tuning among all stages, and many handcrafted parameters are involved. Another nonnegligible drawback is that they train a specific model for a certain noise level, and are limited in blind image denoising.</p><p>In this paper, instead of learning a discriminative model with an explicit image prior, we treat image denoising as a plain discriminative learning problem, i.e., separating the noise from a noisy image by feed-forward convolutional neural networks (CNN). The reasons of using CNN are three-fold. First, CNN with very deep architecture <ref type="bibr" target="#b18">[19]</ref> is effective in increasing the capacity and flexibility for exploiting image characteristics. Second, considerable advances have been achieved on regularization and learning methods for training CNN, including Rectifier Linear Unit (ReLU) <ref type="bibr" target="#b19">[20]</ref>, batch normalization <ref type="bibr" target="#b20">[21]</ref> and residual learning <ref type="bibr" target="#b21">[22]</ref>. These methods can be adopted in CNN to speed up the training process and improve the denoising performance. Third, CNN is well-suited for parallel computation on modern powerful GPU, which can be exploited to improve the run time performance.</p><p>We refer to the proposed denoising convolutional neural network as DnCNN. Rather than directly outputing the denoised imagex, the proposed DnCNN is designed to predict the residual imagev, i.e., the difference between the noisy observation and the latent clean image. In other words, the proposed DnCNN implicitly removes the latent clean image with the operations in the hidden layers. The batch normalization technique is further introduced to stabilize and enhance the training performance of DnCNN. It turns out that residual learning and batch normalization can benefit from each other, and their integration is effective in speeding up the training and boosting the denoising performance.</p><p>While this paper aims to design a more effective Gaussian denoiser, we observe that when v is the difference between the ground truth high resolution image and the bicubic upsampling of the low resolution image, the image degradation model for Guassian denoising can be converted to a single image super-resolution (SISR) problem; analogously, the JPEG image deblocking problem can be modeled by the same image degradation model by taking v as the difference between the original image and the compressed image. In this sense, SISR and JPEG image deblocking can be treated as two special cases of a "general" image denoising problem, though in SISR and JPEG deblocking the noise vs are much different from AWGN. It is natural to ask whether is it possible to train a CNN model to handle such general image denoising problem? By analyzing the connection between DnCNN and TNRD <ref type="bibr" target="#b15">[16]</ref>, we propose to extend DnCNN for handling several general image denoising tasks, including Gaussian denoising, SISR and JPEG image deblocking.</p><p>Extensive experiments show that, our DnCNN trained with a certain noise level can yield better Gaussian denoising results than state-of-the-art methods such as BM3D <ref type="bibr" target="#b1">[2]</ref>, WNNM <ref type="bibr" target="#b12">[13]</ref> and TNRD <ref type="bibr" target="#b15">[16]</ref>. For Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising), DnCNN with a single model can still outperform BM3D <ref type="bibr" target="#b1">[2]</ref> and TNRD <ref type="bibr" target="#b15">[16]</ref> trained for a specific noise level. The DnCNN can also obtain promising results when extended to several general image denoising tasks. Moreover, we show the effectiveness of training only a single DnCNN model for three general image denoising tasks, i.e., blind Gaussian denoising, SISR with multiple upscaling factors, and JPEG deblocking with different quality factors.</p><p>The contributions of this work are summarized as follows: 1) We propose an end-to-end trainable deep CNN for Gaussian denoising. In contrast to the existing deep neural network-based methods which directly estimate the latent clean image, the network adopts the residual learning strategy to remove the latent clean image from noisy observation. 2) We find that residual learning and batch normalization can greatly benefit the CNN learning as they can not only speed up the training but also boost the denoising performance. For Gaussian denoising with a certain noise level, DnCNN outperforms state-of-the-art meth-ods in terms of both quantitative metrics and visual quality. 3) Our DnCNN can be easily extended to handle general image denoising tasks. We can train a single DnCNN model for blind Gaussian denoising, and achieve better performance than the competing methods trained for a specific noise level. Moreover, it is promising to solve three general image denoising tasks, i.e., blind Gaussian denoising, SISR, and JPEG deblocking, with only a single DnCNN model.</p><p>The remainder of the paper is organized as follows. Section II provides a brief survey of related work. Section III first presents the proposed DnCNN model, and then extends it to general image denoising. In Section IV, extensive experiments are conducted to evaluate DnCNNs. Finally, several concluding remarks are given in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Neural Networks for Image Denoising</head><p>There have been several attempts to handle the denoising problem by deep neural networks. In <ref type="bibr" target="#b22">[23]</ref>, Jain and Seung proposed to use convolutional neural networks (CNNs) for image denoising and claimed that CNNs have similar or even better representation power than the MRF model. In <ref type="bibr" target="#b23">[24]</ref>, the multi-layer perceptron (MLP) was successfully applied for image denoising. In <ref type="bibr" target="#b24">[25]</ref>, stacked sparse denoising auto-encoders method was adopted to handle Gaussian noise removal and achieved comparable results to K-SVD <ref type="bibr" target="#b4">[5]</ref>. In <ref type="bibr" target="#b15">[16]</ref>, a trainable nonlinear reaction diffusion (TNRD) model was proposed and it can be expressed as a feed-forward deep network by unfolding a fixed number of gradient descent inference steps. Among the above deep neural networks based methods, MLP and TNRD can achieve promising performance and are able to compete with BM3D. However, for MLP <ref type="bibr" target="#b23">[24]</ref> and TNRD <ref type="bibr" target="#b15">[16]</ref>, a specific model is trained for a certain noise level. To the best of our knowledge, it remains uninvestigated to develop CNN for general image denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Residual Learning and Batch Normalization</head><p>Recently, driven by the easy access to large-scale dataset and the advances in deep learning methods, the convolutional neural networks have shown great success in handling various vision tasks. The representative achievements in training CNN models include Rectified Linear Unit (ReLU) <ref type="bibr" target="#b19">[20]</ref>, tradeoff between depth and width <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>, parameter initialization <ref type="bibr" target="#b26">[27]</ref>, gradient-based optimization algorithms <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, batch normalization <ref type="bibr" target="#b20">[21]</ref> and residual learning <ref type="bibr" target="#b21">[22]</ref>. Other factors, such as the efficient training implementation on modern powerful GPUs, also contribute to the success of CNN. For Gaussian denoising, it is easy to generate sufficient training data from a set of high quality images. This work focuses on the design and learning of CNN for image denoising. In the following, we briefly review two methods related to our DnCNN, i.e., residual learning and batch normalization.</p><p>1) Residual Learning: Residual learning <ref type="bibr" target="#b21">[22]</ref> of CNN was originally proposed to solve the performance degradation problem, i.e., even the training accuracy begins to degrade along with the increasing of network depth. By assuming that the residual mapping is much easier to be learned than the original unreferenced mapping, residual network explicitly learns a residual mapping for a few stacked layers. With such a residual learning strategy, extremely deep CNN can be easily trained and improved accuracy has been achieved for image classification and object detection <ref type="bibr" target="#b21">[22]</ref>.</p><p>The proposed DnCNN model also adopts the residual learning formulation. Unlike the residual network <ref type="bibr" target="#b21">[22]</ref> that uses many residual units (i.e., identity shortcuts), our DnCNN employs a single residual unit to predict the residual image. We further explain the rationale of residual learning formulation by analyzing its connection with TNRD <ref type="bibr" target="#b15">[16]</ref>, and extend it to solve several general image denoising tasks. It should be noted that, prior to the residual network <ref type="bibr" target="#b21">[22]</ref>, the strategy of predicting the residual image has already been adopted in some low-level vision problems such as single image superresolution <ref type="bibr" target="#b30">[31]</ref> and color image demosaicking <ref type="bibr" target="#b31">[32]</ref>. However, to the best of our knowledge, there is no work which directly predicts the residual image for denoising.</p><p>2) Batch Normalization: Mini-batch stochastic gradient descent (SGD) has been widely used in training CNN models. Despite the simplicity and effectiveness of mini-batch SGD, its training efficiency is largely reduced by internal covariate shift <ref type="bibr" target="#b20">[21]</ref>, i.e., changes in the distributions of internal nonlinearity inputs during training. Batch normalization <ref type="bibr" target="#b20">[21]</ref> is proposed to alleviate the internal covariate shift by incorporating a normalization step and a scale and shift step before the nonlinearity in each layer. For batch normalization, only two parameters per activation are added, and they can be updated with back-propagation. Batch normalization enjoys several merits, such as fast training, better performance, and low sensitivity to initialization. For further details on batch normalization, please refer to <ref type="bibr" target="#b20">[21]</ref>.</p><p>By far, no work has been done on studying batch normalization for CNN-based image denoising. We empirically find that, the integration of residual learning and batch normalization can result in fast and stable training and better denoising performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED DENOISING CNN MODEL</head><p>In this section, we present the proposed denoising CNN model, i.e., DnCNN, and extend it for handling several general image denoising tasks. Generally, training a deep CNN model for a specific task generally involves two steps: (i) network architecture design and (ii) model learning from training data. For network architecture design, we modify the VGG network <ref type="bibr" target="#b18">[19]</ref> to make it suitable for image denoising, and set the depth of the network based on the effective patch sizes used in state-of-the-art denoising methods. For model learning, we adopt the residual learning formulation, and incorporate it with batch normalization for fast training and improved denoising performance. Finally, we discuss the connection between DnCNN and TNRD <ref type="bibr" target="#b15">[16]</ref>, and extend DnCNN for several general image denoising tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Depth</head><p>Following the principle in <ref type="bibr" target="#b18">[19]</ref>, we set the size of convolutional filters to be 3 × 3 but remove all pooling layers. Therefore, the receptive field of DnCNN with depth of d should be (2d+1)×(2d+1). Increasing receptive field size can make use of the context information in larger image region. For better tradeoff between performance and efficiency, one important issue in architecture design is to set a proper depth for DnCNN.</p><p>It has been pointed out that the receptive field size of denoising neural networks correlates with the effective patch size of denoising methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Moreover, high noise level usually requires larger effective patch size to capture more context information for restoration <ref type="bibr" target="#b33">[34]</ref>. Thus, by fixing the noise level σ = 25, we analyze the effective patch size of several leading denoising methods to guide the depth design of our DnCNN. In BM3D <ref type="bibr" target="#b1">[2]</ref>, the non-local similar patches are adaptively searched in a local widow of size 25 × 25 for two times, and thus the final effective patch size is 49×49. Similar to BM3D, WNNM <ref type="bibr" target="#b12">[13]</ref> uses a larger searching window and performs non-local searching iteratively, resulting in a quite large effective patch size (361 × 361). MLP <ref type="bibr" target="#b23">[24]</ref> first uses a patch of size 39 × 39 to generate the predicted patch, and then adopts a filter of size 9 × 9 to average the output patches, thus its effective patch size is 47×47. The CSF <ref type="bibr" target="#b13">[14]</ref> and TNRD <ref type="bibr" target="#b15">[16]</ref> with five stages involves a total of ten convolutional layers with filter size of 7 × 7, and their effective patch size is 61 × 61. <ref type="table" target="#tab_0">Table I</ref> summarizes the effective patch sizes adopted in different methods with noise level σ = 25. It can be seen that the effective patch size used in EPLL <ref type="bibr" target="#b32">[33]</ref> is the smallest, i.e., 36×36. It is interesting to verify whether DnCNN with the receptive field size similar to EPLL can compete against the leading denoising methods. Thus, for Gaussian denoising with a certain noise level, we set the receptive field size of DnCNN to 35 × 35 with the corresponding depth of 17. For other general image denoising tasks, we adopt a larger receptive field and set the depth to be 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>The input of our DnCNN is a noisy observation y = x + v. Discriminative denoising models such as MLP <ref type="bibr" target="#b23">[24]</ref> and CSF <ref type="bibr" target="#b13">[14]</ref> aim to learn a mapping function F(y) = x to predict the latent clean image. For DnCNN, we adopt the residual learning formulation to train a residual mapping R(y) ≈ v, and then we have x = y − R(y). Formally, the averaged mean squared error between the desired residual images and estimated ones from noisy input</p><formula xml:id="formula_0">(Θ) = 1 2N N i=1 R(y i ; Θ) − (y i − x i ) 2 F<label>(1)</label></formula><p>can be adopted as the loss function to learn the trainable</p><formula xml:id="formula_1">parameters Θ in DnCNN. Here {(y i , x i )} N i=1</formula><p>represents N noisy-clean training image (patch) pairs. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the architecture of the proposed DnCNN for learning R(y). In the following, we explain the architecture of DnCNN and the strategy for reducing boundary artifacts.  To sum up, our DnCNN model has two main features: the residual learning formulation is adopted to learn R(y), and batch normalization is incorporated to speed up training as well as boost the denoising performance. By incorporating convolution with ReLU, DnCNN can gradually separate image structure from the noisy observation through the hidden layers. Such a mechanism is similar to the iterative noise removal strategy adopted in methods such as EPLL and WNNM, but our DnCNN is trained in an end-to-end fashion. Later we will give more discussions on the rationale of combining residual learning and batch normalization.</p><p>2) Reducing Boundary Artifacts: In many low level vision applications, it usually requires that the output image size should keep the same as the input one. This may lead to the boundary artifacts. In MLP <ref type="bibr" target="#b23">[24]</ref>, boundary of the noisy input image is symmetrically padded in the preprocessing stage, whereas the same padding strategy is carried out before every stage in CSF <ref type="bibr" target="#b13">[14]</ref> and TNRD <ref type="bibr" target="#b15">[16]</ref>. Different from the above methods, we directly pad zeros before convolution to make sure that each feature map of the middle layers has the same size as the input image. We find that the simple zero padding strategy does not result in any boundary artifacts. This good property is probably attributed to the powerful ability of the DnCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Integration of Residual Learning and Batch Normalization for Image Denoising</head><p>The network shown in <ref type="figure" target="#fig_0">Fig. 1</ref> can be used to train either the original mapping F(y) to predict x or the residual mapping R(y) to predict v. According to <ref type="bibr" target="#b21">[22]</ref>, when the original mapping is more like an identity mapping, the residual mapping will be much easier to be optimized. Note that the noisy observation y is much more like the latent clean image x than the residual image v (especially when the noise level is low). Thus, F(y) would be more close to an identity mapping than R(y), and the residual learning formulation is more suitable for image denoising. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the average PSNR values obtained using these two learning formulations with/without batch normalization under the same setting on gradient-based optimization algorithms and network architecture. Note that two gradient-based optimization algorithms are adopted: one is the stochastic gradient descent algorithm with momentum (i.e., SGD) and the other one is the Adam algorithm <ref type="bibr" target="#b29">[30]</ref>. Firstly, we can observe that the residual learning formulation can result in faster and more stable convergence than the original mapping learning. In the meanwhile, without batch normalization, simple residual learning with conventional SGD cannot compete with the state-of-the-art denoising methods such as TNRD (28.92dB). We consider that the insufficient performance should be attributed to the internal covariate shift <ref type="bibr" target="#b20">[21]</ref> caused by the changes in network parameters during training. Accordingly, batch normalization is adopted to address it. Secondly, we observe that, with batch normalization, learning residual mapping (the red line) converges faster and exhibits better denoising performance than learning original mapping (the blue line). In particular, both the SGD and Adam optimization algorithms can enable the network with residual learning and batch normalization to have the best results. In other words, it is the integration of residual learning formulation and batch normalization rather than the optimization algorithms (SGD or Adam) that leads to the best denoising performance.</p><p>Actually, one can notice that in Gaussian denoising the residual image and batch normalization are both associated with the Gaussian distribution. It is very likely that residual learning and batch normalization can benefit from each other for Gaussian denoising <ref type="bibr" target="#b0">1</ref> . This point can be further validated by the following analyses.  • On the one hand, residual learning benefits from batch normalization. This is straightforward because batch normalization offers some merits for CNNs, such as alleviating internal covariate shift problem. From <ref type="figure" target="#fig_1">Fig. 2</ref>, one can see that even though residual learning without batch normalization (the green line) has a fast convergence, it is inferior to residual learning with batch normalization (the red line). • On the other hand, batch normalization benefits from residual learning. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, without residual learning, batch normalization even has certain adverse effect to the convergence (the blue line). With residual learning, batch normalization can be utilized to speedup the training as well as boost the performance (the red line). Note that each mini-bath is a small set (e.g., 128) of images. Without residual learning, the input intensity and the convolutional feature are correlated with their neighbored ones, and the distribution of the layer inputs also rely on the content of the images in each training mini-batch. With residual learning, DnCNN implicitly removes the latent clean image with the operations in the hidden layers. This makes that the inputs of each layer are Gaussian-like distributed, less correlated, and less related with image content. Thus, residual learning can also help batch normalization in reducing internal covariate shift.</p><p>To sum up, the integration of residual learning and batch normalization can not only speed up and stabilize the training process but also boost the denoising performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Connection with TNRD</head><p>Our DnCNN can also be explained as the generalization of one-stage TNRD <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Typically, TNRD aims to train a discriminative solution for the following problem</p><formula xml:id="formula_2">min x Ψ(y − x) + λ K k=1 N p=1 ρ k ((f k * x) p ),<label>(2)</label></formula><p>from an abundant set of degraded-clean training image pairs. Here N denotes the image size, λ is the regularization parameter, f k * x stands for the convolution of the image x with the k-th filter kernel f k , and ρ k (·) represents the k-th penalty function which is adjustable in the TNRD model. For Gaussian denoising, we set Ψ(z) = 1 2 z 2 . The diffusion iteration of the first stage can be interpreted as performing one gradient descent inference step at starting point y, which is given by</p><formula xml:id="formula_3">x 1 = y − αλ K k=1 (f k * φ k (f k * y)) − α ∂Ψ(z) ∂z z=0 ,<label>(3)</label></formula><p>wheref k is the adjoint filter of f k (i.e.,f k is obtained by rotating 180 degrees the filter f k ), α corresponds to the stepsize and ρ k (·) = φ k (·). For Gaussian denoising, we have ∂Ψ(z) ∂z z=0 = 0, and Eqn. (3) is equivalent to the following expression</p><formula xml:id="formula_4">v 1 = y − x 1 = αλ K k=1 (f k * φ k (f k * y)),<label>(4)</label></formula><p>where v 1 is the estimated residual of x with respect to y.</p><p>Since the influence function φ k (·) can be regarded as pointwise nonlinearity applied to convolution feature maps, Eqn. (4) actually is a two-layer feed-forward CNN. As can be seen from <ref type="figure" target="#fig_0">Fig. 1</ref>, the proposed CNN architecture further generalizes onestage TNRD from three aspects: (i) replacing the influence function with ReLU to ease CNN training; (ii) increasing the CNN depth to improve the capacity in modeling image characteristics; (iii) incorporating with batch normalization to boost the performance. The connection with one-stage TNRD provides insights in explaining the use of residual learning for CNN-based image restoration. Most of the parameters in Eqn. (4) are derived from the analysis prior term of Eqn. <ref type="bibr" target="#b1">(2)</ref>. In this sense, most of the parameters in DnCNN are representing the image priors.</p><p>It is interesting to point out that, even the noise is not Gaussian distributed (or the noise level of Gaussian is unknown), we still can utilize Eqn. (3) to obtain v 1 if we have</p><formula xml:id="formula_5">∂Ψ(z) ∂z z=0 = 0.<label>(5)</label></formula><p>Note that Eqn. (5) holds for many types of noise distributions, e.g., generalized Gaussian distribution. It is natural to assume that it also holds for the noise caused by SISR and JPEG compression. It is possible to train a single CNN model for several general image denoising tasks, such as Gaussian denoising with unknown noise level, SISR with multiple upscaling factors, and JPEG deblocking with different quality factors. Besides, Eqn. (4) can also be interpreted as the operations to remove the latent clean image x from the degraded observation y to estimate the residual image v. For these tasks, even the noise distribution is complex, it can be expected that our DnCNN would also perform robustly in predicting residual image by gradually removing the latent clean image in the hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Extension to General Image Denoising</head><p>The existing discriminative Gaussian denoising methods, such as MLP, CSF and TNRD, all train a specific model for a fixed noise level <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref>. When applied to Gaussian denoising with unknown noise, one common way is to first estimate the noise level, and then use the model trained with the corresponding noise level. This makes the denoising results affected by the accuracy of noise estimation. In addition, those methods cannot be applied to the cases with non-Gaussian noise distribution, e.g., SISR and JPEG deblocking.</p><p>Our analyses in Section III-D have shown the potential of DnCNN in general image denoising. To demonstrate it, we first extend our DnCNN for Gaussian denoising with unknown noise level. In the training stage, we use the noisy images from a wide range of noise levels (e.g., σ ∈ [0, 55]) to train a single DnCNN model. Given a test image whose noise level belongs to the noise level range, the learned single DnCNN model can be utilized to denoise it without estimating its noise level.</p><p>We further extend our DnCNN by learning a single model for several general image denoising tasks. We consider three specific tasks, i.e., blind Gaussian denoising, SISR, and JPEG deblocking. In the training stage, we utilize the images with AWGN from a wide range of noise levels, down-sampled images with multiple upscaling factors, and JPEG images with different quality factors to train a single DnCNN model. Experimental results show that the learned single DnCNN model is able to yield excellent results for any of the three general image denoising tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>A. Experimental setting 1) Training and Testing Data: For Gaussian denoising with either known or unknown noise level, we follow <ref type="bibr" target="#b15">[16]</ref> to use 400 images of size 180 × 180 for training. We found that using a larger training dataset can only bring negligible improvements. To train DnCNN for Gaussian denoising with known noise level, we consider three noise levels, i.e., σ = 15, 25 and 50. We set the patch size as 40 × 40, and crop 128 × 1, 600 patches to train the model. We refer to our DnCNN model for Gaussian denoising with known specific noise level as DnCNN-S.</p><p>To train a single DnCNN model for blind Gaussian denoising, we set the range of the noise levels as σ ∈ [0, 55], and the patch size as 50 × 50. 128 × 3, 000 patches are cropped to train the model. We refer to our single DnCNN model for blind Gaussian denoising task as DnCNN-B.</p><p>For the test images, we use two different test datasets for thorough evaluation, one is a test dataset containing 68 natural images from Berkeley segmentation dataset (BSD68) <ref type="bibr" target="#b11">[12]</ref> and the other one contains 12 images as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Note that all those images are widely used for the evaluation of Gaussian denoising methods and they are not included in the training dataset.</p><p>In addition to gray image denoising, we also train the blind color image denoising model referred to as CDnCNN-B. We use color version of the BSD68 dataset for testing and the remaining 432 color images from Berkeley segmentation dataset are adopted as the training images. The noise levels are also set into the range of [0, 55] and 128 × 3, 000 patches of size 50×50 are cropped to train the model.</p><p>To learn a single model for the three general image denoising tasks, as in <ref type="bibr" target="#b34">[35]</ref>, we use a dataset which consists of 91 images from <ref type="bibr" target="#b35">[36]</ref> and 200 training images from the Berkeley segmentation dataset. The noisy image is generated by adding Gaussian noise with a certain noise level from the range of [0, 55]. The SISR input is generated by first bicubic downsampling and then bicubic upsampling the highresolution image with downscaling factors 2, 3 and 4. The JPEG deblocking input is generated by compressing the image with a quality factor ranging from 5 to 99 using the MATLAB JPEG encoder. All these images are treated as the inputs to a single DnCNN model. Totally, we generate 128×8,000 image patch (the size is 50 × 50) pairs for training. Rotation/flip based operations on the patch pairs are used during mini-batch learning. The parameters are initialized with DnCNN-B. We refer to our single DnCNN model for these three general image denoising tasks as DnCNN-3. To test DnCNN-3, we adopt different test set for each task, and the detailed description will be given in Section IV-E.</p><p>2) Parameter Setting and Network Training: In order to capture enough spatial information for denoising, we set the network depth to 17 for DnCNN-S and 20 for DnCNN-B and DnCNN-3. The loss function in Eqn. (1) is adopted to learn the residual mapping R(y) for predicting the residual v. We initialize the weights by the method in <ref type="bibr" target="#b26">[27]</ref> and use SGD with weight decay of 0.0001, a momentum of 0.9 and a mini-batch size of 128. We train 50 epochs for our DnCNN models. The learning rate was decayed exponentially from 1e − 1 to 1e − 4 for the 50 epochs.</p><p>We use the MatConvNet package <ref type="bibr" target="#b36">[37]</ref> to train the proposed DnCNN models. Unless otherwise specified, all the experiments are carried out in the Matlab (R2015b) environment running on a PC with Intel(R) Core(TM) i7-5820K CPU  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Compared Methods</head><p>We compare the proposed DnCNN method with several state-of-the-art denoising methods, including two non-local similarity based methods (i.e., BM3D <ref type="bibr" target="#b1">[2]</ref> and WNNM <ref type="bibr" target="#b12">[13]</ref>), one generative method (i.e., EPLL <ref type="bibr" target="#b32">[33]</ref>), three discriminative training based methods (i.e., MLP <ref type="bibr" target="#b23">[24]</ref>, CSF <ref type="bibr" target="#b13">[14]</ref> and TNRD <ref type="bibr" target="#b15">[16]</ref>). Note that CSF and TNRD are highly efficient by GPU implementation while offering good image quality. The implementation codes are downloaded from the authors' websites and the default parameter settings are used in our experiments. The testing code of our DnCNN models can be downloaded at https://github.com/cszn/DnCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative and Qualitative Evaluation</head><p>The average PSNR results of different methods on the BSD68 dataset are shown in <ref type="table" target="#tab_0">Table II</ref>. As one can see, both DnCNN-S and DnCNN-B can achieve the best PSNR results than the competing methods. Compared to the benchmark BM3D, the methods MLP and TNRD have a notable PSNR gain of about 0.35dB. According to <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b37">[38]</ref>, few methods can outperform BM3D by more than 0.3dB on average. In contrast, our DnCNN-S model outperforms BM3D by 0.6dB on all the three noise levels. Particularly, even with a single model without known noise level, our DnCNN-B can still outperform the competing methods which is trained for the known specific noise level. It should be noted that both DnCNN-S and DnCNN-B outperform BM3D by about 0.6dB when σ = 50, which is very close to the estimated PSNR bound over BM3D (0.7dB) in <ref type="bibr" target="#b37">[38]</ref>. <ref type="table" target="#tab_0">Table III</ref> lists the PSNR results of different methods on the 12 test images in <ref type="figure" target="#fig_2">Fig. 3</ref>. The best PSNR result for each image with each noise level is highlighted in bold. It can be seen that the proposed DnCNN-S yields the highest PSNR on most of the images. Specifically, DnCNN-S outperforms the competing methods by 0.2dB to 0.6dB on most of the images and fails to achieve the best results on only two images "House" and "Barbara", which are dominated by repetitive structures. This result is consistent with the findings in <ref type="bibr" target="#b38">[39]</ref>: non-local means based methods are usually better on images with regular and repetitive structures whereas discriminative training based methods generally produce better results on images with irregular textures. Actually, this is intuitively reasonable because images with regular and repetitive structures meet well with the non-local similarity prior; conversely, images with irregular textures would weaken the advantages of such specific prior, thus leading to poor results.</p><p>Figs. 4-5 illustrate the visual results of different methods. It can be seen that BM3D, WNNM, EPLL and MLP tend to produce over-smooth edges and textures. While preserving sharp edges and fine details, TNRD is likely to generate artifacts in the smooth region. In contrast, DnCNN-S and DnCNN-B can not only recover sharp edges and fine details but also yield visually pleasant results in the smooth region.</p><p>For color image denoising, the visual comparisons between CDnCNN-B and the benchmark CBM3D are shown in Figs. 6-7. One can see that CBM3D generates false color artifacts in some regions whereas CDnCNN-B can recover images with more natural color. In addition, CDnCNN-B can generate images with more details and sharper edges than CBM3D.    denoising within a wide range of noise levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Run Time</head><p>In addition to visual quality, another important aspect for an image restoration method is the testing speed. <ref type="table" target="#tab_0">Table IV</ref> shows the run times of different methods for denoising images of sizes 256 × 256, 512 × 512 and 1024 × 1024 with noise level 25. Since CSF, TNRD and our DnCNN methods are well-suited for parallel computation on GPU, we also give the corresponding run times on GPU. We use the Nvidia cuDNN-v5 deep learning library to accelerate the GPU computation of the proposed DnCNN. As in <ref type="bibr" target="#b15">[16]</ref>, we do not count the memory transfer time between CPU and GPU. It can be seen that the proposed DnCNN can have a relatively high speed on CPU and it is faster than two discriminative models, MLP and CSF. Though it is slower than BM3D and TNRD, by taking the image quality improvement into consideration, our DnCNN is still very competitive in CPU implementation. For the GPU time, the proposed DnCNN achieves very appealing computational efficiency, e.g., it can denoise an image of size 512 × 512 in 60ms with unknown noise level, which is a distinct advantage over TNRD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments on Learning a Single Model for Three General Image Denoising Tasks</head><p>In order to further show the capacity of the proposed DnCNN model, a single DnCNN-3 model is trained for three general image denoising tasks, including blind Gaussian denoising, SISR and JPEG image deblocking. To the best of our knowledge, none of the existing methods have been reported for handling these three tasks with only a single model. Therefore, for each task, we compare DnCNN-3 with the specific state-of-the-art methods. In the following, we describe the compared methods and the test dataset for each task:</p><p>• For Gaussian denoising, we use the state-of-the-art BM3D and TNRD for comparison. The BSD68 dataset    are used for testing the performance. For BM3D and TNRD, we assume that the noise level is known. • For SISR, we consider two state-of-the-art methods, i.e., TNRD and VDSR <ref type="bibr" target="#b34">[35]</ref>. TNRD trained a specific model for each upscalling factor while VDSR <ref type="bibr" target="#b34">[35]</ref> trained a single model for all the three upscaling factors (i.e., 2, 3 and 4). We adopt the four testing datasets (i.e., Set5 and Set14, BSD100 and Urban100 <ref type="bibr" target="#b39">[40]</ref>) used in <ref type="bibr" target="#b34">[35]</ref>. • For JPEG image deblocking, our DnCNN-3 is compared with two state-of-the-art methods, i.e., AR-CNN <ref type="bibr" target="#b40">[41]</ref> and TNRD <ref type="bibr" target="#b15">[16]</ref>. The AR-CNN method trained four specific models for the JPEG quality factors 10, 20, 30 and 40, respectively. For TNRD, three models for JPEG quality factors 10, 20 and 30 are trained. As in <ref type="bibr" target="#b40">[41]</ref>, we adopt the Classic5 and LIVE1 as test datasets. <ref type="table" target="#tab_5">Table V</ref> lists the average PSNR and SSIM results of different methods for different general image denoising tasks. As one can see, even we train a single DnCNN-3 model for the three different tasks, it still outperforms the nonblind TNRD and BM3D for Gaussian denoising. For SISR, it surpasses TNRD by a large margin and is on par with VDSR. For JPEG image deblocking, DnCNN-3 outperforms AR-CNN by about 0.3dB in PSNR and has about 0.1dB PSNR gain over TNRD on all the quality factors. <ref type="figure" target="#fig_0">Fig. 9 and Fig. 10</ref> show the visual comparisons of different methods for SISR. It can be seen that both DnCNN-3 and VDSR can produce sharp edges and fine details whereas TNRD tend to generate blurred edges and distorted lines. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the JPEG deblocking results of different methods. As one can see, our DnCNN-3 can recover the straight line whereas AR-CNN and TNRD are prone to generate distorted lines. <ref type="figure" target="#fig_0">Fig. 12</ref> gives an additional example to show the capacity of the proposed model. We can see that DnCNN-3 can produce visually pleasant output result even the input image is corrupted by several distortions with different levels in different regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, a deep convolutional neural network was proposed for image denoising, where residual learning is adopted to separating noise from noisy observation. The batch normalization and residual learning are integrated to speed up the training process as well as boost the denoising performance. Unlike traditional discriminative models which train specific models for certain noise levels, our single DnCNN model has the capacity to handle the blind Gaussian denoising with unknown noise level. Moreover, we showed the feasibility to train a single DnCNN model to handle three general image denoising tasks, including Gaussian denoising with unknown noise level, single image super-resolution with multiple upscaling factors, and JPEG image deblocking with different quality factors. Extensive experimental results demonstrated that the proposed method not only produces favorable image denoising performance quantitatively and qualitatively but also has promising run time by GPU implementation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of the proposed DnCNN network. 1) Deep Architecture: Given the DnCNN with depth D, there are three types of layers, shown in Fig. 1 with three different colors. (i) Conv+ReLU: for the first layer, 64 filters of size 3 × 3 × c are used to generate 64 feature maps, and rectified linear units (ReLU, max(0, ·)) are then utilized for nonlinearity. Here c represents the number of image channels, i.e., c = 1 for gray image and c = 3 for color image. (ii) Conv+BN+ReLU: for layers 2 ∼ (D − 1), 64 filters of size 3 × 3 × 64 are used, and batch normalization [21] is added between convolution and ReLU. (iii) Conv: for the last layer, c filters of size 3 × 3 × 64 are used to reconstruct the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The Gaussian denoising results of four specific models under two gradient-based optimization algorithms, i.e., (a) SGD, (b) Adam, with respect to epochs. The four specific models are in different combinations of residual learning (RL) and batch normalization (BN) and are trained with noise level 25. The results are evaluated on 68 natural images from Berkeley segmentation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The 12 widely used testing images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Average PSNR improvement over BM3D/CBM3D with respect to different noise levels by our DnCNN-B/CDnCNN-B model. The results are evaluated on the gray/color BSD68 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8</head><label>8</label><figDesc>shows the average PSNR improvement over BM3D/CBM3D with respect to different noise levels by DnCNN-B/CDnCNN-B model. It can be seen that our DnCNN-B/CDnCNN-B models consistently outperform BM3D/CBM3D by a large margin on a wide range of noise levels. This experimental result demonstrates the feasibility of training a single DnCNN-B model for handling blind Gaussian</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Noisy / 14.76dB (b) BM3D / 26.21dB (c) WNNM / 26.51dB (d) EPLL / 26.36dB (e) MLP / 26.54dB (f) TNRD / 26.59dB (g) DnCNN-S / 26.90dB (h) DnCNN-B / 26.92dB Denoising results of one image from BSD68 with noise level 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Denoising results of the image "parrot" with noise level 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Color image denoising results of one image from the DSD68 dataset with noise level 35. (a) Ground-truth (b) Noisy / 15.07dB (c) CBM3D / 26.97dB (d) CDnCNN-B / 27.87dB Color image denoising results of one image from the DSD68 dataset with noise level 45.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .Fig. 10 .Fig. 11 .Fig. 12 .</head><label>9101112</label><figDesc>Single image super-resolution results of "butterfly" from Set5 dataset with upscaling factor 3. (a) Ground-truth (b) TNRD / 32.00dB (c) VDSR / 32.58dB (d) DnCNN-3 / 32.73dB Single image super-resolution results of one image from Urban100 dataset with upscaling factor 4. (a) JPEG / 28.10dB (b) AR-CNN / 28.85dB (c) TNRD / 29.54dB (d) DnCNN-3 / 29.70dB JPEG image deblocking results of "Carnivaldolls" from LIVE1 dataset with quality factor 10. (a) Input Image (b) Output Residual Image (c) Restored Image An example to show the capacity of our proposed model for three different tasks. The input image is composed by noisy images with noise level 15 (upper left) and 25 (lower left), bicubically interpolated low-resolution images with upscaling factor 2 (upper middle) and 3 (lower middle), JPEG images with quality factor 10 (upper right) and 30 (lower right). Note that the white lines in the input image are just used for distinguishing the six regions, and the residual image is normalized into the range of [0, 1] for visualization. Even the input image is corrupted with different distortions in different regions, the restored image looks natural and does not have obvious artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>EFFECTIVE PATCH SIZES OF DIFFERENT METHODS WITH NOISE LEVEL σ = 25.</figDesc><table><row><cell>Methods</cell><cell>BM3D [2]</cell><cell>WNNM [13]</cell><cell>EPLL [33]</cell><cell>MLP [24]</cell><cell>CSF [14]</cell><cell>TNRD [16]</cell></row><row><cell>Effective Patch Size</cell><cell>49 × 49</cell><cell>361 × 361</cell><cell>36 × 36</cell><cell>47 × 47</cell><cell>61 × 61</cell><cell>61 × 61</cell></row><row><cell>EŽŝƐǇ /ŵĂŐĞ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ZĞƐŝĚƵĂů /ŵĂŐĞ</cell></row><row><cell>ŽŶ| н ZĞ&gt;h</cell><cell>ŽŶ| н E н ZĞ&gt;h</cell><cell></cell><cell>ŽŶ| н E н ZĞ&gt;h</cell><cell>ŽŶ| н E н ZĞ&gt;h</cell><cell></cell><cell>ŽŶ|</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II THE</head><label>II</label><figDesc>AVERAGE PSNR(DB) RESULTS OF DIFFERENT METHODS ON THE BSD68 DATASET. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.</figDesc><table><row><cell>Methods</cell><cell>BM3D</cell><cell>WNNM</cell><cell>EPLL</cell><cell>MLP</cell><cell>CSF</cell><cell>TNRD</cell><cell>DnCNN-S</cell><cell>DnCNN-B</cell></row><row><cell>σ = 15</cell><cell>31.07</cell><cell>31.37</cell><cell>31.21</cell><cell>-</cell><cell>31.24</cell><cell>31.42</cell><cell>31.73</cell><cell>31.61</cell></row><row><cell>σ = 25</cell><cell>28.57</cell><cell>28.83</cell><cell>28.68</cell><cell>28.96</cell><cell>28.74</cell><cell>28.92</cell><cell>29.23</cell><cell>29.16</cell></row><row><cell>σ = 50</cell><cell>25.62</cell><cell>25.87</cell><cell>25.67</cell><cell>26.03</cell><cell>-</cell><cell>25.97</cell><cell>26.23</cell><cell>26.23</cell></row><row><cell cols="5">3.30GHz and an Nvidia Titan X GPU. It takes about 6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">hours, one day and three days to train DnCNN-S, DnCNN-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">B/CDnCNN-B and DnCNN-3 on GPU, respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III THE</head><label>III</label><figDesc>PSNR(DB) RESULTS OF DIFFERENT METHODS ON 12 WIDELY USED TESTING IMAGES.</figDesc><table><row><cell cols="2">(a) Ground-truth</cell><cell></cell><cell cols="3">(b) Noisy / 17.25dB</cell><cell></cell><cell cols="3">(c) CBM3D / 25.93dB</cell><cell></cell><cell cols="3">(d) CDnCNN-B / 26.58dB</cell></row><row><cell>Images</cell><cell>C.man</cell><cell>House</cell><cell>Peppers</cell><cell>Starfish</cell><cell>Monar.</cell><cell>Airpl.</cell><cell>Parrot</cell><cell>Lena</cell><cell>Barbara</cell><cell>Boat</cell><cell>Man</cell><cell>Couple</cell><cell>Average</cell></row><row><cell>Noise Level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>σ = 15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM3D [2]</cell><cell>31.91</cell><cell>34.93</cell><cell>32.69</cell><cell>31.14</cell><cell>31.85</cell><cell>31.07</cell><cell>31.37</cell><cell>34.26</cell><cell>33.10</cell><cell>32.13</cell><cell>31.92</cell><cell>32.10</cell><cell>32.372</cell></row><row><cell>WNNM [13]</cell><cell>32.17</cell><cell>35.13</cell><cell>32.99</cell><cell>31.82</cell><cell>32.71</cell><cell>31.39</cell><cell>31.62</cell><cell>34.27</cell><cell>33.60</cell><cell>32.27</cell><cell>32.11</cell><cell>32.17</cell><cell>32.696</cell></row><row><cell>EPLL [33]</cell><cell>31.85</cell><cell>34.17</cell><cell>32.64</cell><cell>31.13</cell><cell>32.10</cell><cell>31.19</cell><cell>31.42</cell><cell>33.92</cell><cell>31.38</cell><cell>31.93</cell><cell>32.00</cell><cell>31.93</cell><cell>32.138</cell></row><row><cell>CSF [14]</cell><cell>31.95</cell><cell>34.39</cell><cell>32.85</cell><cell>31.55</cell><cell>32.33</cell><cell>31.33</cell><cell>31.37</cell><cell>34.06</cell><cell>31.92</cell><cell>32.01</cell><cell>32.08</cell><cell>31.98</cell><cell>32.318</cell></row><row><cell>TNRD [16]</cell><cell>32.19</cell><cell>34.53</cell><cell>33.04</cell><cell>31.75</cell><cell>32.56</cell><cell>31.46</cell><cell>31.63</cell><cell>34.24</cell><cell>32.13</cell><cell>32.14</cell><cell>32.23</cell><cell>32.11</cell><cell>32.502</cell></row><row><cell>DnCNN-S</cell><cell>32.61</cell><cell>34.97</cell><cell>33.30</cell><cell>32.20</cell><cell>33.09</cell><cell>31.70</cell><cell>31.83</cell><cell>34.62</cell><cell>32.64</cell><cell>32.42</cell><cell>32.46</cell><cell>32.47</cell><cell>32.859</cell></row><row><cell>DnCNN-B</cell><cell>32.10</cell><cell>34.93</cell><cell>33.15</cell><cell>32.02</cell><cell>32.94</cell><cell>31.56</cell><cell>31.63</cell><cell>34.56</cell><cell>32.09</cell><cell>32.35</cell><cell>32.41</cell><cell>32.41</cell><cell>32.680</cell></row><row><cell>Noise Level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>σ = 25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM3D [2]</cell><cell>29.45</cell><cell>32.85</cell><cell>30.16</cell><cell>28.56</cell><cell>29.25</cell><cell>28.42</cell><cell>28.93</cell><cell>32.07</cell><cell>30.71</cell><cell>29.90</cell><cell>29.61</cell><cell>29.71</cell><cell>29.969</cell></row><row><cell>WNNM [13]</cell><cell>29.64</cell><cell>33.22</cell><cell>30.42</cell><cell>29.03</cell><cell>29.84</cell><cell>28.69</cell><cell>29.15</cell><cell>32.24</cell><cell>31.24</cell><cell>30.03</cell><cell>29.76</cell><cell>29.82</cell><cell>30.257</cell></row><row><cell>EPLL [33]</cell><cell>29.26</cell><cell>32.17</cell><cell>30.17</cell><cell>28.51</cell><cell>29.39</cell><cell>28.61</cell><cell>28.95</cell><cell>31.73</cell><cell>28.61</cell><cell>29.74</cell><cell>29.66</cell><cell>29.53</cell><cell>29.692</cell></row><row><cell>MLP [24]</cell><cell>29.61</cell><cell>32.56</cell><cell>30.30</cell><cell>28.82</cell><cell>29.61</cell><cell>28.82</cell><cell>29.25</cell><cell>32.25</cell><cell>29.54</cell><cell>29.97</cell><cell>29.88</cell><cell>29.73</cell><cell>30.027</cell></row><row><cell>CSF [14]</cell><cell>29.48</cell><cell>32.39</cell><cell>30.32</cell><cell>28.80</cell><cell>29.62</cell><cell>28.72</cell><cell>28.90</cell><cell>31.79</cell><cell>29.03</cell><cell>29.76</cell><cell>29.71</cell><cell>29.53</cell><cell>29.837</cell></row><row><cell>TNRD [16]</cell><cell>29.72</cell><cell>32.53</cell><cell>30.57</cell><cell>29.02</cell><cell>29.85</cell><cell>28.88</cell><cell>29.18</cell><cell>32.00</cell><cell>29.41</cell><cell>29.91</cell><cell>29.87</cell><cell>29.71</cell><cell>30.055</cell></row><row><cell>DnCNN-S</cell><cell>30.18</cell><cell>33.06</cell><cell>30.87</cell><cell>29.41</cell><cell>30.28</cell><cell>29.13</cell><cell>29.43</cell><cell>32.44</cell><cell>30.00</cell><cell>30.21</cell><cell>30.10</cell><cell>30.12</cell><cell>30.436</cell></row><row><cell>DnCNN-B</cell><cell>29.94</cell><cell>33.05</cell><cell>30.84</cell><cell>29.34</cell><cell>30.25</cell><cell>29.09</cell><cell>29.35</cell><cell>32.42</cell><cell>29.69</cell><cell>30.20</cell><cell>30.09</cell><cell>30.10</cell><cell>30.362</cell></row><row><cell>Noise Level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>σ = 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM3D [2]</cell><cell>26.13</cell><cell>29.69</cell><cell>26.68</cell><cell>25.04</cell><cell>25.82</cell><cell>25.10</cell><cell>25.90</cell><cell>29.05</cell><cell>27.22</cell><cell>26.78</cell><cell>26.81</cell><cell>26.46</cell><cell>26.722</cell></row><row><cell>WNNM [13]</cell><cell>26.45</cell><cell>30.33</cell><cell>26.95</cell><cell>25.44</cell><cell>26.32</cell><cell>25.42</cell><cell>26.14</cell><cell>29.25</cell><cell>27.79</cell><cell>26.97</cell><cell>26.94</cell><cell>26.64</cell><cell>27.052</cell></row><row><cell>EPLL [33]</cell><cell>26.10</cell><cell>29.12</cell><cell>26.80</cell><cell>25.12</cell><cell>25.94</cell><cell>25.31</cell><cell>25.95</cell><cell>28.68</cell><cell>24.83</cell><cell>26.74</cell><cell>26.79</cell><cell>26.30</cell><cell>26.471</cell></row><row><cell>MLP [24]</cell><cell>26.37</cell><cell>29.64</cell><cell>26.68</cell><cell>25.43</cell><cell>26.26</cell><cell>25.56</cell><cell>26.12</cell><cell>29.32</cell><cell>25.24</cell><cell>27.03</cell><cell>27.06</cell><cell>26.67</cell><cell>26.783</cell></row><row><cell>TNRD [16]</cell><cell>26.62</cell><cell>29.48</cell><cell>27.10</cell><cell>25.42</cell><cell>26.31</cell><cell>25.59</cell><cell>26.16</cell><cell>28.93</cell><cell>25.70</cell><cell>26.94</cell><cell>26.98</cell><cell>26.50</cell><cell>26.812</cell></row><row><cell>DnCNN-S</cell><cell>27.03</cell><cell>30.00</cell><cell>27.32</cell><cell>25.70</cell><cell>26.78</cell><cell>25.87</cell><cell>26.48</cell><cell>29.39</cell><cell>26.22</cell><cell>27.20</cell><cell>27.24</cell><cell>26.90</cell><cell>27.178</cell></row><row><cell>DnCNN-B</cell><cell>27.03</cell><cell>30.02</cell><cell>27.39</cell><cell>25.72</cell><cell>26.83</cell><cell>25.89</cell><cell>26.48</cell><cell>29.38</cell><cell>26.38</cell><cell>27.23</cell><cell>27.23</cell><cell>26.91</cell><cell>27.206</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV RUN</head><label>IV</label><figDesc>TIME (IN SECONDS) OF DIFFERENT METHODS ON IMAGES OF SIZE 256 × 256, 512 × 512 AND 1024 × 1024 WITH NOISE LEVEL 25. FOR CSF, TNRD AND OUR PROPOSED DNCNN, WE GIVE THE RUN TIMES ON CPU (LEFT) AND GPU (RIGHT). IT IS ALSO WORTH NOTING THAT SINCE THE RUN TIME ON GPU VARIES GREATLY WITH RESPECT TO GPU AND GPU-ACCELERATED LIBRARY, IT IS HARD TO MAKE A FAIR COMPARISON BETWEEN CSF, TNRD AND OUR PROPOSED DNCNN. THEREFORE, WE JUST COPY THE RUN TIMES OF CSF AND TNRD ON GPU FROM THE ORIGINAL PAPERS.</figDesc><table><row><cell>Methods</cell><cell>BM3D</cell><cell>WNNM</cell><cell>EPLL</cell><cell>MLP</cell><cell>CSF</cell><cell>TNRD</cell><cell>DnCNN-S</cell><cell>DnCNN-B</cell></row><row><cell>256×256</cell><cell>0.65</cell><cell>203.1</cell><cell>25.4</cell><cell>1.42</cell><cell>2.11 / -</cell><cell>0.45 / 0.010</cell><cell>0.74 / 0.014</cell><cell>0.90 / 0.016</cell></row><row><cell>512×512</cell><cell>2.85</cell><cell>773.2</cell><cell>45.5</cell><cell>5.51</cell><cell>5.67 / 0.92</cell><cell>1.33 / 0.032</cell><cell>3.41 / 0.051</cell><cell>4.11 / 0.060</cell></row><row><cell>1024×1024</cell><cell>11.89</cell><cell>2536.4</cell><cell>422.1</cell><cell>19.4</cell><cell>40.8 / 1.72</cell><cell>4.61 / 0.116</cell><cell>12.1 / 0.200</cell><cell>14.1 / 0.235</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V AVERAGE</head><label>V</label><figDesc>PSNR(DB)/SSIM RESULTS OF DIFFERENT METHODS FOR GAUSSIAN DENOISING WITH NOISE LEVEL 15, 25 AND 50 ON BSD68 DATASET, SINGLE IMAGE SUPER-RESOLUTION WITH UPSCALING FACTORS 2, 3 AND 4 ON SET5, SET14, BSD100 AND URBAN100 DATASETS, JPEG IMAGE DEBLOCKING WITH QUALITY FACTORS 10, 20, 30 AND 40 ON CLASSIC5 AND LIVE1 DATASETS. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Gaussian Denoising</cell><cell></cell></row><row><cell>Dataset</cell><cell>Noise Level</cell><cell>BM3D PSNR / SSIM</cell><cell>TNRD PSNR / SSIM</cell><cell>DnCNN-3 PSNR / SSIM</cell></row><row><cell></cell><cell>15</cell><cell>31.08 / 0.8722</cell><cell>31.42 / 0.8826</cell><cell>31.46 / 0.8826</cell></row><row><cell>BSD68</cell><cell>25</cell><cell>28.57 / 0.8017</cell><cell>28.92 / 0.8157</cell><cell>29.02 / 0.8190</cell></row><row><cell></cell><cell>50</cell><cell>25.62 / 0.6869</cell><cell>25.97 / 0.7029</cell><cell>26.10 / 0.7076</cell></row><row><cell></cell><cell cols="3">Single Image Super-Resolution</cell><cell></cell></row><row><cell>Dataset</cell><cell>Upscaling Factor</cell><cell>TNRD PSNR / SSIM</cell><cell>VDSR PSNR / SSIM</cell><cell>DnCNN-3 PSNR / SSIM</cell></row><row><cell></cell><cell>2</cell><cell>36.86 / 0.9556</cell><cell>37.56 / 0.9591</cell><cell>37.58 / 0.9590</cell></row><row><cell>Set5</cell><cell>3</cell><cell>33.18 / 0.9152</cell><cell>33.67 / 0.9220</cell><cell>33.75 / 0.9222</cell></row><row><cell></cell><cell>4</cell><cell>30.85 / 0.8732</cell><cell>31.35 / 0.8845</cell><cell>31.40 / 0.8845</cell></row><row><cell></cell><cell>2</cell><cell>32.51 / 0.9069</cell><cell>33.02 / 0.9128</cell><cell>33.03 / 0.9128</cell></row><row><cell>Set14</cell><cell>3</cell><cell>29.43 / 0.8232</cell><cell>29.77 / 0.8318</cell><cell>29.81 / 0.8321</cell></row><row><cell></cell><cell>4</cell><cell>27.66 / 0.7563</cell><cell>27.99 / 0.7659</cell><cell>28.04 / 0.7672</cell></row><row><cell></cell><cell>2</cell><cell>31.40 / 0.8878</cell><cell>31.89 / 0.8961</cell><cell>31.90 / 0.8961</cell></row><row><cell>BSD100</cell><cell>3</cell><cell>28.50 / 0.7881</cell><cell>28.82 / 0.7980</cell><cell>28.85 / 0.7981</cell></row><row><cell></cell><cell>4</cell><cell>27.00 / 0.7140</cell><cell>27.28 / 0.7256</cell><cell>27.29 / 0.7253</cell></row><row><cell></cell><cell>2</cell><cell>29.70 / 0.8994</cell><cell>30.76 / 0.9143</cell><cell>30.74 / 0.9139</cell></row><row><cell>Urban100</cell><cell>3</cell><cell>26.42 / 0.8076</cell><cell>27.13 / 0.8283</cell><cell>27.15 / 0.8276</cell></row><row><cell></cell><cell>4</cell><cell>24.61 / 0.7291</cell><cell>25.17 / 0.7528</cell><cell>25.20 / 0.7521</cell></row><row><cell></cell><cell></cell><cell cols="2">JPEG Image Deblocking</cell><cell></cell></row><row><cell>Dataset</cell><cell>Quality Factor</cell><cell>AR-CNN PSNR / SSIM</cell><cell>TNRD PSNR / SSIM</cell><cell>DnCNN-3 PSNR / SSIM</cell></row><row><cell></cell><cell>10</cell><cell>29.03 / 0.7929</cell><cell>29.28 / 0.7992</cell><cell>29.40 / 0.8026</cell></row><row><cell>Classic5</cell><cell>20 30</cell><cell>31.15 / 0.8517 32.51 / 0.8806</cell><cell>31.47 / 0.8576 32.78 / 0.8837</cell><cell>31.63 / 0.8610 32.91 / 0.8861</cell></row><row><cell></cell><cell>40</cell><cell>33.34 / 0.8953</cell><cell>-</cell><cell>33.77 / 0.9003</cell></row><row><cell></cell><cell>10</cell><cell>28.96 / 0.8076</cell><cell>29.15 / 0.8111</cell><cell>29.19 / 0.8123</cell></row><row><cell>LIVE1</cell><cell>20 30</cell><cell>31.29 / 0.8733 32.67 / 0.9043</cell><cell>31.46 / 0.8769 32.84 / 0.9059</cell><cell>31.59 / 0.8802 32.98 / 0.9090</cell></row><row><cell></cell><cell>40</cell><cell>33.63 / 0.9198</cell><cell>-</cell><cell>33.96 / 0.9247</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It should be pointed out that this does not mean that our DnCNN can not handle other general denoising tasks well.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonlocal image and movie denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="139" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An iterative regularization method for total variation-based image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="460" to="489" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What makes a good model of natural images?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient belief propagation with learned higher-order Markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="269" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Markov random field modeling in image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fields of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="229" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5261" to="5269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative non-blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="604" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascades of regression tree fields for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="677" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Residual interpolation for color image demosaicking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Monno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2304" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Natural image denoising: Optimality and inherent bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2833" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia Conference</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Patch complexity, finite pixel correlations and optimal denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="73" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning how to combine internal and external denoising methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
