<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmented GRAPH-BERT for Graph Instance Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Segmented GRAPH-BERT for Graph Instance Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In graph instance representation learning, both the diverse graph instance sizes and the graph node orderless property have been the major obstacles that render existing representation learning models fail to work. In this paper, we will examine the effectiveness of GRAPH-BERT on graph instance representation learning, which was designed for node representation learning tasks originally. To adapt GRAPH-BERT to the new problem settings, we re-design it with a segmented architecture instead, which is also named as SEG-BERT (Segmented GRAPH-BERT) for reference simplicity in this paper. SEG-BERT involves no node-order-variant inputs or functional components anymore, and it can handle the graph node orderless property naturally. What's more, SEG-BERT has a segmented architecture and introduces three different strategies to unify the graph instance sizes, i.e., full-input, padding/pruning and segment shifting, respectively. SEG-BERT is pre-trainable in an unsupervised manner, which can be further transferred to new tasks directly or with necessary fine-tuning. We have tested the effectiveness of SEG-BERT with experiments on seven graph instance benchmark datasets, and SEG-BERT can out-perform the comparison methods on six out of them with significant performance advantages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Different from the previous graph neural network <ref type="bibr">(GNN)</ref> works <ref type="bibr" target="#b34">(Zhang et al., 2020;</ref><ref type="bibr" target="#b28">Veličković et al., 2018;</ref><ref type="bibr" target="#b12">Kipf &amp; Welling, 2016)</ref>, which mainly focus on the node embeddings in large-sized graph data, we will study the representation learning of the whole graph instances in this paper. Representative examples of graph instance data studied in research include the human brain graph, molecular graph, and real-estate community graph, whose nodes (usually only in tens or hundreds) represent the brain regions, atoms and POIs, respectively. Graph instance representation learning has been demonstrated to be an extremely difficult task. Besides the diverse input graph instance sizes, attributes and extensive connections, the inherent nodeorderless property  brings about more challenges on the model design.</p><p>Existing graph representation learning approaches are mainly based on the convolutional operator <ref type="bibr" target="#b14">(Krizhevsky et al., 2012)</ref> or the approximated graph convolutional operator <ref type="bibr" target="#b7">(Hammond et al., 2011;</ref><ref type="bibr" target="#b5">Defferrard et al., 2016)</ref> for pattern extraction and information aggregation. By adapting CNN <ref type="bibr" target="#b14">(Krizhevsky et al., 2012)</ref> or GCN <ref type="bibr" target="#b12">(Kipf &amp; Welling, 2016)</ref> to graph instance learning settings, <ref type="bibr" target="#b21">(Niepert et al., 2016;</ref><ref type="bibr" target="#b29">Verma &amp; Zhang, 2018;</ref><ref type="bibr" target="#b35">Zhang et al., 2018;</ref><ref type="bibr" target="#b36">Zhang &amp; Chen, 2019;</ref><ref type="bibr" target="#b2">Chen et al., 2019)</ref> introduce different strategies to handle the node orderless properties. However, due to the inherent learning problems with graph convolutional operator, such models have also been criticized for serious performance degradation on deep architectures . Deep sub-graph pattern learning  is another emerging new-trend on graph instance representation learning. Different from these aforementioned methods,  introduces the IsoNN (Isomorphic Neural Net) to learn the sub-graph patterns automatically for graph instance representation learning, which requires the identical-sized graph instance input and cannot handle node attributes.</p><p>In this paper, we introduce a new graph neural network, i.e., SEG-BERT (Segmented GRAPH-BERT), for graph instance representation learning based on the recent GRAPH-BERT model. Originally, GRAPH-BERT is introduced for node representation learning <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>, whose learning results have been demonstrated to be effective for various node classification/clustering tasks already. Meanwhile, to adapt GRAPH-BERT to the new problem settings, we re-design it in several major aspects: (a) SEG-BERT involves no node-order-variant inputs or functional components. SEG-BERT excludes the nodes' relative positional embeddings from the initial inputs; whereas both the selfattention <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>  LG] 9 Feb 2020 graph instance input to fit the model configuration. SEG-BERT has a segmented architecture and introduces three strategies to unify the graph instance sizes, i.e., full-input, padding/pruning and segment shifting, which help feed SEG-BERT with the whole input graph instances (or graph segments) for representation learning. (c) SEG-BERT reintroduces graph links as initial inputs. Considering that some graph instances may have no node attributes, SEG-BERT re-introduces the graph links (in an artificial fixed node order) back as one part of the initial inputs for graph instance representation learning. (d) SEG-BERT uses the graph residual terms of the whole graph instance. Since we focus on learning the representations of the whole graph instances, the graph residual terms involved in SEG-BERT will be defined for all the nodes in the graph (or segments) instead of merely for the target nodes <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>. SEG-BERT is pre-trainable in an unsupervised manner, and the pre-trained model can be further transferred to new tasks directly or with necessary fine-tuning. To be more specific, in this paper, we will explore the pre-training and fine-tuning of SEG-BERT for several graph instance studies, e.g., node attribute reconstruction, graph structure recovery, and graph instance classification. Such explorations will help construct the functional model pipelines for graph instance learning, and avoid the unnecessary redundant learning efforts and resource consumptions.</p><p>We summarize our contributions of this paper as follows:</p><p>• Graph Representation Learning Unification: We examine the effectiveness of GRAPH-BERT in graph instance representation learning task in this paper. The success of this paper will help unify the currently disconnected representation learning tasks on nodes and graph instances, as discussed in <ref type="bibr" target="#b32">(Zhang, 2019)</ref>, with a shared methodology, which will even allow the future model transfer across graph datasets with totally different properties, e.g., from social networks to brain graphs.</p><p>• Node-Orderless Graph Instances: We introduce a new graph neural network model, i.e., SEG-BERT, for graph instance representation learning by re-designing GRAPH-BERT. SEG-BERT only relies on the attention learning mechanisms and the node-order-invariant inputs and functional components in SEG-BERT allow it to handle the graph instance node orderless properties very well.</p><p>• Segmented Architecture: We design SEG-BERT in a segmented architecture, which has a reasonable-sized input portal. To unify the diverse sizes of input graph instances to fit the input portals, SEG-BERT introduces three strategies, i.e., full-input, padding/pruning and segment shifting, which can work well for different learning scenarios, respectively.</p><p>• Pre-Train &amp; Transfer &amp; Fine-Tune: We study the unsupervised pre-training of SEG-BERT on graph instance studies, and explore to transfer such pre-trained models to the down-stream application tasks directly or with necessary fine-tuning. In this paper, we will pre-train SEG-BERT with unsupervised node attribute reconstruction and graph structure recovery tasks, and further fine-tune SEG-BERT on supervised graph classification as the down-stream tasks.</p><p>The remaining parts of this paper are organized as follows.</p><p>We will introduce the related work in Section 2. Detailed information about the SEG-BERT model will be introduced in Section 4, whereas the pre-training and fine-tuning of SEG-BERT will be introduced in Section 5 in detail. The effectiveness of SEG-BERT will be tested in Section 6. Finally, we will conclude this paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several interesting research topics are related to this paper, which include graph neural networks, graph representation learning and BERT.</p><p>GNNs and Graph Representation Learning: Different from the node representation learning <ref type="bibr" target="#b12">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b28">Veličković et al., 2018)</ref>, GNNs proposed for the graph representation learning aim at learning the representation for the entire graph instead <ref type="bibr" target="#b20">(Narayanan et al., 2017)</ref>.</p><p>To handle the graph node permutation invariant challenge, solutions based various techniques, e.g., attention <ref type="bibr" target="#b18">Meltzer et al., 2019)</ref>, pooling <ref type="bibr" target="#b23">Ranjan et al., 2019;</ref><ref type="bibr" target="#b10">Jiang et al., 2018)</ref>, capsule net , Weisfeiler-Lehman kernel <ref type="bibr" target="#b13">(Kriege et al., 2016)</ref> and sub-graph pattern learning and matching , have been proposed.</p><p>For instance,  studies the graph classification task with a capsule network;  defines a dual attention mechanism for improving graph convolutional network on graph representation learning; and  introduces an end-to-end learning model for graph representation learning based on an attention pooling mechanism. On the other hand, <ref type="bibr" target="#b23">(Ranjan et al., 2019)</ref> focuses on studying the sparse and differentiable pooling method to be adopted with graph convolutional network for graph representation learning; <ref type="bibr" target="#b13">(Kriege et al., 2016)</ref> examines the optimal assignments of kernels for graph classification, which can out-perform the Weisfeiler-Lehman kernel on benchmark datasets; and <ref type="bibr" target="#b10">(Jiang et al., 2018)</ref> proposes to introduce the Gaussian mixture model into the graph neural network for representation learning.</p><p>As an emerging new-trend,  explores the deep sub-graph pattern learning and proposes to learn interpretable graph representations by involving subgraph matching into a graph neural network.</p><p>BERT: TRANSFORMER <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> and BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> based models have almost dominated NLP and related research areas in recent years due to their great representation learning power. Prior to that, the mainstream sequence transduction models in NLP are mostly based on complex recurrent <ref type="bibr" target="#b8">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b4">Chung et al., 2014)</ref> or convolutional neural networks <ref type="bibr" target="#b11">(Kim, 2014)</ref>. However, as introduced in <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>, the inherently sequential nature precludes parallelization within training examples. To address such a problem, a brand new representation learning model solely based on attention mechanisms, i.e., the TRANSFORMER, is introduced in <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>, which dispense with recurrence and convolutions entirely. Based on TRANS-FORMER, <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> further introduces BERT for deep language understanding, which obtains new state-ofthe-art results on eleven natural language processing tasks. By extending TRANSFORMER and BERT, many new BERT based models, e.g., T5 <ref type="bibr" target="#b22">(Raffel et al., 2019)</ref>, ERNIE <ref type="bibr" target="#b26">(Sun et al., 2019)</ref> and RoBERTa , can even outperform the human beings on almost all NLP benchmark datasets. Some extension trials of BERT on new areas have also been observed. In <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>, the authors explore to extend BERT for graph representation learning, which discard the graph links and learns node representations merely based on the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>In this section, we will first introduce the notations used in this paper. After that, we will provide the definitions of several important terminologies, and then introduce the formal statement of the studied problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations</head><p>In the sequel of this paper, we will use the lower case letters (e.g., x) to represent scalars, lower case bold letters (e.g., x) to denote column vectors, bold-face upper case letters (e.g., X) to denote matrices, and upper case calligraphic letters (e.g., X ) to denote sets or high-order tensors. Given a matrix X, we denote X(i, :) and X(:, j) as its i th row and j th column, respectively. The (i th , j th ) entry of matrix X can be denoted as either X(i, j). We use X and x to represent the transpose of matrix X and vector x. For vector x, we represent its L p -norm as</p><formula xml:id="formula_0">x p = ( i |x(i)| p ) 1 p . The Frobenius-norm of matrix X is represented as X F = ( i,j |X(i, j)| 2 ) 1 2 .</formula><p>The element-wise product of vectors x and y of the same di-mension is represented as x ⊗ y, whose concatenation is represented as x y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Terminology Definitions</head><p>Here, we will provide the definitions of several important terminologies used in this paper, which include graph instance and graph instance set.</p><p>DEFINITION 1 (Graph Instance): Formally, a graph instance studied in this paper can be denoted as G = (V, E, w, x), where V and E denote the sets of nodes and links in the graph, respectively. Mapping w : E → R projects links in the graph to their corresponding weight. For unweighted graphs, we will have w(e i,j ) = 1, ∀e i,j ∈ E and w(e i,j ) = 0, ∀e i,j ∈ V × V \ E. For the nodes in the graph instance, they may also be associated with certain attributes, which can be represented by mapping x : V → X (here, X = R dx denotes the attribute vector space and d x is the space dimension).</p><p>Based on the above definition, given a node v i in graph instance G, we can represent its connection weights with all the other nodes in the graph as</p><formula xml:id="formula_1">w i = [w(e i,j )] vj ∈V ∈ R |V|×1 . Meanwhile, the raw attribute vector representa- tion of v i can also be simplified as x i = x(v i ).</formula><p>The size of graph instance G can be denoted as the number of involved nodes, i.e., |V|. For the graph instances studied in this paper, they can be in different sizes actually, which together can be represented as a graph instance set.</p><p>DEFINITION 2 (Graph Instance Set): For each graph instance studied in this paper, it can be attached with some pre-defined class labels. Formally, we can represent n labeled graph instances studied in this paper as set</p><formula xml:id="formula_2">G = {(G i , y i )} n i=1 , where y i ∈ Y denotes the label vector of G i (here, Y = R dy is the class label vector space and d y is the space dimension).</formula><p>For representation simplicity, in reference to the graph instance set (without labels), we can also denote it as G, which will be used in the following problem statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Problem Formulation</head><p>Based on the notations and terminologies defined above, we can provide the problem statement as follows.</p><p>Problem Statement: Formally, given the labeled graph instance set G, we aim at learning a mapping f : G → R d h to project the graph instances to their corresponding latent representations (d h denotes the hidden representation space dimension). What's more, we cast extra requirements on the mapping f in this paper: (a) representations learned by f should be invariant to node orders, (b) f can accept graph instances in various sizes, as well as diverse cate- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The SEG-BERT Model</head><p>In this section, we will provide the detailed information about the SEG-BERT model for graph instance representation learning. As illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, the SEG-BERT model has several key components: (1) graph instance serialization to reshape the various-sized input graph instances into node list, where the node orders will not affect the learning results;</p><p>(2) initial embedding extraction to define the initial input feature vectors for all nodes in the graph instance;</p><p>(3) input size unification to fit the input portal size of graph-transformer; (4) graph-transformer to learn the nodes' representations in the graph instance (or segments) with several layers; (5) representation fusion to integrate the learned representations of all nodes in the graph instance; and (6) functional component to compute and output the learning results. These components will all be introduced in this section in detail, whereas the learning detail of SEG-BERT will be discussed in the follow-up Section 5 instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Graph Serialization and Initial Embeddings</head><p>Formally, given a graph instance G ∈ G from the graph set, we can denote its structure as G = (V, E, w, x), involving node set V and link set E, respectively. To simplify the notations, we will not indicate the graph instance index subscript in this section. Both the initial inputs and the functional components in SEG-BERT are node-orderinvariant, i.e., the nodes' learned representations are not dependent on the node orders. Therefore, regardless of the nodes' orders, we can serialize node set V into a sequence</p><formula xml:id="formula_3">[v 1 , v 2 , · · · , v |V| ].</formula><p>For the same graph instance, if it is fed to train/tune SEG-BERT for multiple times, the node order in the list can change arbitrarily without affecting the learning representations.</p><p>For each node in the list, e.g., v i , we can represent its raw features as a vector x i = x(v i ) ∈ R dx×1 , which can cover various types of information, e.g., node tags, attributes, textual descriptions and even images. Via certain embedding mappings, we can denote the embedded feature representation of v i 's raw features as</p><formula xml:id="formula_4">e (x) i = Embed (x i ) ∈ R d h ×1 .<label>(1)</label></formula><p>Depending on the input features, different approaches can be utilized to define the Embed(·) function, e.g., CNN for image features, LSTM for textual features, positional embedding for tags and MLP for real-number features. In the case when the graph instance nodes have no raw attributes on nodes, a dummy zero vector will be used to fill in the embedding vector e (x) i entries by default.</p><p>To handle the graph instances without node attributes, in this paper, we will also extend the original GRAPH-BERT model by defining the node adjacency neighborhood embeddings. Meanwhile, to ensure such an embedding is node-order invariant, we will cast an artificially fixed node order on this embedding vector (which is fixed forever for the graph instance). For simplicity, we will just follow the node subscript index as the node orders in this paper. Formally, for each node v i , following the fixed artificial node order, we can denote its adjacency neighbors as a vector</p><formula xml:id="formula_5">w i = [w(i, j)] vj ∈V ∈ R |V|×1 .</formula><p>Via several fully connected (FC) layers based mappings, we can represent the embedded representation of v i 's adjacency neighborhood information as</p><formula xml:id="formula_6">e (w) i = FC-Embed (w i ) ∈ R d h ×1 .<label>(2)</label></formula><p>The degrees of nodes can illustrate their basic proper-ties <ref type="bibr" target="#b3">(Chung et al., 2003;</ref><ref type="bibr" target="#b1">Bondy, 1976)</ref>, and according to <ref type="bibr" target="#b16">(Lovász, 1996)</ref> for the Markov chain or random walk on graphs, their final stationary distribution will be proportional to the nodes' degrees. Node degree is also a nodeorder invariant actually. Formally, we can represent the degree of v i in the graph instance as D(v i ) ∈ N, and its embedding can be represented as</p><formula xml:id="formula_7">e (d) i = Position-Embed (D(v i )) = sin D(v i ) 10000 2l d h , cos D(v j ) 10000 2l+1 d h d h 2 l=0 , (3) where e (d) i ∈ R d h ×1</formula><p>and the vector index l will iterate through the vector to compute the entry values based on the sin(·) and cos(·) functions.</p><p>In addition to the node raw feature embedding, node adjacency neighborhood embedding and node degree embedding, we will also include the nodes' Weisfeiler-Lehman role embedding vector in this paper, which effectively denotes the nodes' global roles in the input graph. Nodes' Weisfeiler-Lehman code is node-order-invariant, which denotes a positional property of the nodes actually. Formally, given a node v i in the input graph instance, we can denote its pre-computed WL code as WL(v i ) ∈ N, whose corresponding embeddings can be denoted as</p><formula xml:id="formula_8">e (r) i = Position-Embed (WL(v i )) ∈ R d h ×1 .</formula><p>(4) SEG-BERT doesn't include the relative positional embedding and relative hop distance embedding used in <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>, as there exist no target node for the graph instances studied in this paper. Based on the above descriptions, we can represent the initially computed input embedding vectors of node v i in graph G as</p><formula xml:id="formula_9">h (0) i = sum e (x) i , e (w) i , e (d) i , e (r) i ∈ R d h ×1 . (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Graph Instance Size Unification Strategies</head><p>Different from <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>, where the sampled subgraphs all have the identical size, the graph instance input usually have different number of nodes instead. To handle such a problem, we design SEG-BERT with an instance size unification component in this paper. Formally, we can denote the input portal size of SEG-BERT used in this paper as k, i.e., it can take the initial input embedding vectors of k nodes at a time. Depending on the input graph instance sizes, SEG-BERT will define the parameter k and handle the graph instances with different strategies:</p><p>• Full-Input Strategy: The input portal size k of SEG-BERT is defined as the largest graph instance size in the dataset, and dummy node padding will be used to expand all graph instances to k nodes (zero padding for the connections, raw attributes and other tags).</p><p>• Padding/Pruning Strategy: The input portal size k of SEG-BERT is assigned with a value slightly above the graph instance average size. For the graph instance input with less than k nodes, dummy node padding (zero padding) is used to expand the graph to k nodes; whereas for larger input graph instances, a k-node sub-graph (e.g., the first k nodes) will be extracted from them and the remaining nodes will be pruned.</p><p>• Segment Shifting Strategy: A fixed input portal size k will be pre-specified, which can be a very small number. For the graph instance input with node set V, the nodes will be divided into |V| k segments, and dummy node padding will be used for the last segment if necessary. SEG-BERT will shift along the segments to learn all the nodes representations in the graph instances.</p><p>Generally, the full-input strategy will use all the input graph nodes for representation learning, but for a small-graph set with a few number of extremely large graph instance(s), a very large k will be used in SEG-BERT, which may introduce unnecessary high time costs. The padding/pruning strategy balances the parameter k among all the graph instances and can learn effective representations in an efficient way, but it may have information loss for the pruned parts of some graph instances. Meanwhile, the segment shifting strategy can balance between the full-input strategy and padding/pruning strategy, which fuses the graph instance global information for representation learning with a small model input portal. More experimental tests of such different graph instance size unification strategies will also be explored with experiments on real-world benchmark datasets to be introduced in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Segmented Graph-Transformer</head><p>To learn the graph instance representations, we will introduce the segmented graph-transformer in this part to update such nodes' representations iteratively with D layers. Formally, based on the above descriptions, we can denote the current segment as s j = [v j,1 , v j,2 , · · · , v j,k ], whose initial input embeddings can be denoted as H <ref type="formula">(5)</ref>. Depending on the different unification strategies adopted, the segment can denote all the graph nodes, pruned/padded node subset or the node segments, respectively. For the segmented graph-transformer, it will update the segment representations iteratively for each layer ∀l ∈ {1, · · · , D} according to the following (a) Train Acc (IMDB) (b) Test Acc (IMDB) (c) Train Acc (Proteins) (d) Test Acc (Proteins) <ref type="figure">Figure 2</ref>: Learning records of SEG-BERT on the IMDB-Multi social-graph dataset and the Proteins bio-graph dataset. For the graph data with discrete structures, most of the comparison models studied in the experiments will overfit the training data easily and an early stop is usually necessary. The x axis: iteration, and the y axis: training/testing loss.</p><formula xml:id="formula_10">(0) j = [h (0) j,1 , h (0) j,2 , · · · , h (0) j,k ] ∈ R k×d h and h (0) j,i ∈ R d h ×1 is de- fined in Equation</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>equation:</head><formula xml:id="formula_11">H (l) j = G-Transformer(H (l−1) j ), = Transformer(H (l−1) j ) + G-Res(H (l−1) j , X j ),<label>(6)</label></formula><p>where Transformer(·) and G-Res(·) denote the transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> and graph residual terms , respectively. Here, we need to add some remarks on the graph residual terms used in the model. Different from <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>, which integrates the target node residual terms to all the nodes in the batch, (1) for the graph instances without any node attributes, the nodes adjacency neighborhood vectors will be used to compute the residual terms, and (2) we compute the graph residual term with the whole graph instance instead in this paper (not merely for the target node).</p><p>Such a process will iterate through all the segments of the input graph instance nodes, and the finally learned node representations can be denoted as</p><formula xml:id="formula_12">H (D) = [h (D) 1 , h (D) 2 , · · · , h (D)</formula><p>|V| ] ∈ R |V|×d h . In this paper, for presentation simplicity, we just assume all the hidden layers are of the same dimension d h . Considering that we focus on learning the representations of the entire graph instance in this paper, SEG-BERT will integrate such node representations together to define the fused graph instance representation vector as follows:</p><formula xml:id="formula_13">z = Fusion H (D) = 1 |V| |V| i=1 h (D) 1 .<label>(7)</label></formula><p>Both vector z and matrix H (D) will be outputted to the down-stream application tasks for the model training/tuning and graph instance representation learning, which will be introduced in the follow-up section in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SEG-BERT Learning</head><p>In this part, we will focus on the pre-training and finetuning of SEG-BERT with several concrete graph instance learning tasks. To be more specific, we will introduce two unsupervised pre-training tasks to learn SEG-BERT based on node raw attribute reconstruction and graph structure recovery, which ensure the learned node and graph representations can capture both the raw attribute and structure information in the graph instances. After that, we will introduce one supervised tasks to fine-tune SEG-BERT for the graph instance classification, which will cast extra refinements on the learned node and graph representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Unsupervised Pre-Training</head><p>The pre-training tasks used in this paper will enable SEG-BERT to effectively capture both the node raw attributes and graph instance structures in the learned nodes and graph representation vectors. In the case where the graph instances have no node raw attributes, only graph structure recovery will be used for the pre-training. Formally, based on the learned representations H (D) of all the nodes in the graph instance, e.g., G, with several fully connected layers, we can project such latent representations to their raw features. Furthermore, by comparing the reconstructed feature matrix against the ground-truth raw features and minimizing their differences, we will be able to pre-train the SEG-BERT model with all the graph instances. Furthermore, based on the nodes learned representations, we can also compute the closeness (e.g., cosine similarity of the representation vectors) of the node pairs in the graph. Furthermore, compared against the ground-truth graph connection weight matrix, we can define the introduced loss term for graph structure recovery of all the graph instances to pre-train the SEG-BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Transfer and Fine-Tuning</head><p>When applying the pre-trained SEG-BERT and the learned representations in new application tasks, e.g., graph instance classification, necessary fine-tuning can be needed. Formally, we can denote the batch of labeled graph instance as T = {(G i , y i )} i , where y i denotes the label vector of graph instance G i . Based on the fused representation vector z i learned for graph instance G i ∈ G, we can further project it to the label vector with several fully connected layers together with the softmax normalization function, <ref type="table">Table 1</ref>: Experimental results of different comparison methods. For the results not reported in the previous works, we mark the corresponding entries with '−' in the table. The entries are the accuracy scores (mean±std) achieved by the baseline methods with the 10 folds. For SEG-BERT(padding/pruning, none) and SEG-BERT(padding/pruning, raw), they denote SEG-BERT with the padding/pruning strategy and different graph residual terms (raw vs none). At the last row on SEG-BERT*, we show the best results obtained by SEG-BERT with all these three unification strategies. Methods Accuracy (mean±std) WL <ref type="bibr" target="#b25">(Shervashidze et al., 2011)</ref> 73.40±4.63 49.33±4.75 79.02±1.77 82.05±0.36 74.68±0.49 59.90±4.30 82.19±0.18 GK <ref type="bibr" target="#b24">(Shervashidze et al., 2009)</ref> 65.87±0.98 43.89±0.38 72.84±0.28 81.58±2.11 71.67±0.55 57.26±1.41 62.28±0.29 DGK <ref type="bibr" target="#b31">(Yanardag &amp; Vishwanathan, 2015)</ref> 66.96±0.56 44.55±0.52 73.09±0.25 87.44±2.72 75.68±0.54 60.08±2.55 80.31±0.46 AWE <ref type="bibr" target="#b9">(Ivanov &amp; Burnaev, 2018)</ref> 74.45±5.83 51.54±3.61 73.93±1.94 87.87±9.76 − − − PSCN <ref type="bibr" target="#b21">(Niepert et al., 2016)</ref> 71.00±2.29 45.23±2.84 72.60±2.15 88.95±4.37 75.00±2.51 62.29±5.68 76.34±1.68 DAGCN  − − − 87.22±6.10 76.33±4.3 62.88±9.61 81.68±1.69 SPI-GCN <ref type="bibr" target="#b0">(Atamna et al., 2019)</ref> 60   70.03±0.86 47.83±0.85 73.76±0.49 85.83±1.66 75.54±0.94 58.59±2.47 74.44±0.47 GCAPS-CNN <ref type="bibr" target="#b29">(Verma &amp; Zhang, 2018)</ref>  which can be denoted aŝ</p><formula xml:id="formula_14">y i = softmax (FC (z i )) ∈ R dy×1 .<label>(8)</label></formula><p>Meanwhile, based on the known ground-truth label vector of graph instances in the training set, we can define the introduced loss term for the graph instance based on the corss-entropy term as follows:</p><formula xml:id="formula_15">gc = (Gi,yi)∈T dy j=1 −y i (j) logŷ i (j).<label>(9)</label></formula><p>By optimizing the above loss term, we will be able to refine SEG-BERT and the learned graph instance representations based on the application tasks specifically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>To test the effectiveness of SEG-BERT, extensive experiments on real-world graph instance benchmark datasets will be done in this section. What's more, we will also compare SEG-BERT with both the classic and state-of-theart graph instance representation learning baseline methods to demonstrate its advantages.</p><p>Reproducibility: Both the datasets and source code used can be accessed via the github page 1 . Detailed information 1 https://github.com/jwzhanggy/SEG-BERT about the server used to run the model can be found at the footnote 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Dataset and Experimental Settings</head><p>Dataset Descriptions: The graph instance datasets used in this experiments include IMDB-Binary, IMDB-Multi and COLLAB, as well as MUTAG, PROTEINS, PTC and NCI1, which are all the benchmark datasets as used in <ref type="bibr" target="#b31">(Yanardag &amp; Vishwanathan, 2015)</ref> and all the follow-up graph classification papers <ref type="bibr" target="#b36">(Zhang &amp; Chen, 2019;</ref><ref type="bibr" target="#b29">Verma &amp; Zhang, 2018;</ref><ref type="bibr" target="#b30">Xu et al., 2018)</ref>. Among them, IMDB-Binary, IMDB-Multi and COLLAB are the social graph datasets; whereas the remaining ones are the bio-graph datasets. Basic statistical information about the datasets is also available at the top of <ref type="table">Table 1</ref>.</p><p>Comparison Baselines: The comparison baseline methods used in this paper include (1) conventional graph kernel based methods, e.g., Weisfeiler-Lehman subtree kernel (WL) <ref type="bibr" target="#b25">(Shervashidze et al., 2011)</ref> and graphlet count kernel (GK) <ref type="bibr" target="#b24">(Shervashidze et al., 2009</ref>); (2) existing deep learning based methods, e.g., Deep Graph Kernel (DGK) <ref type="bibr" target="#b31">(Yanardag &amp; Vishwanathan, 2015)</ref> and AWE <ref type="bibr" target="#b9">(Ivanov &amp; Burnaev, 2018)</ref>, PATCHY-SAN (PSCN) <ref type="bibr" target="#b21">(Niepert et al., 2016)</ref>;  , Simple Permutation-Invariant Graph Convolutional Network (SPI-GCN) <ref type="bibr" target="#b0">(Atamna et al., 2019)</ref>, Graph Capsule CNN (GCAPS-CNN) <ref type="bibr" target="#b29">(Verma &amp; Zhang, 2018)</ref> and Deep Graph CNN (DGCNN) , and Capsule Graph Neural Network (CapsGNN) <ref type="bibr" target="#b36">(Zhang &amp; Chen, 2019)</ref>. Evaluation Metric: The learning performance of these methods will be evaluated by Accuracy as the metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings:</head><p>In the experiments, we will first compare SEG-BERT with the padding/pruning strategy for input size unification against the baseline methods, which can help test if a sub-structures in graph instances can capture the characteristics of the whole graph instance or not. The other two strategies will be discussed at the end of this section in detail. For the input portal size k, it is assigned with a value slightly larger than the graph instance average sizes of each dataset. All the graph instances in the datasets will be partitioned into train, validate, testing sets according to the ratio 8:1:1 with the 10-fold cross validation, where the validation set is used for parameter tuning and selection. Meanwhile, for fair comparisons (existing works use 9 : 1 train/test partition without validation set), such 10% validation set will also be used as training data as well to further fine-tune SEG-BERT after the parameter selection.</p><p>Default Model Parameter Settings: If not clearly specified, the results reported in this paper are based on the following parameter settings of SEG-BERT: input portal size: k = 25 (MUTAG), k = 50 (IMDB-Binary, IMDB-Multi, NCI1, PTC) and k = 100 (COLLAB, PROTEINS); hidden size: 32; attention head number: 2; hidden layer number: D = 2; learning rate: 0.0005 (PTC) and 0.0001 (others); weight decay: 5e −4 ; intermediate size: 32; hidden dropout rate: 0.5; attention dropout rate: 0.3; graph residual term: raw/none; training epoch: 500 (early stop if necessary to avoid over-fitting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Graph Instance Classification Results</head><p>Model Train Records: As illustrated in <ref type="figure">Figure 2</ref>, we show the learning records (training/testing accuracy) of SEG-BERT on both the IMDB-Multi social graph and the Protein bio-graph datasets. According to the plots, graph instance classification is very different from classification task on other data types, as the model can get over-fitting easily. Similar phenomena have been observed for most comparison methods on the other datasets as well. Even though the default training epoch is 500 mentioned above, an early stop of training SEG-BERT is usually necessary. In this paper, we decide the early-stop learning epoch number based on the partitioned validation set.</p><p>Main Results: The main learning results of of all the comparison methods are provided in <ref type="table">Table 1</ref>. The SEG-BERT method shown here adopts the padding/pruning strategy to handle the graph data input. The residual terms adopted are indicated by the SEG-BERT method name in the parentheses. According to the evaluation results, SEG-BERT can greatly improve the learning performance on most of the benchmark datasets. For instance, the accuracy score of SEG-BERT (none) on IMDB-Binary is 75.40, which is much higher than many of the state-of-the-art baseline methods, e.g., DGCNN, GCAPS-CNN and CapsGNN. Similarly learning performance advantages have also been observed on the other datasets, except NCI1.</p><p>For the raw residual term used in SEG-BERT, for some of the datasets as shown in <ref type="table">Table 1</ref>, e.g., MUTAG, it can improve the learning performance; whereas for the remaining ones, its effectiveness is not very significant. The main reason can be for most of the graph instances studied here they don't have node attributes and the discrete nodes adjacency neighborhood embeddings can be very sparse, which renders the computed residual terms to be less effective for performance improvement.</p><p>Graph Size Unification Strategy Analysis: The analyses of the different graph size unification strategies is provided in <ref type="table" target="#tab_3">Table 2</ref>. For SEG-BERT with the full-input strategy on COLLAB, PROTEIN and NCI1 (with more instances and large max graph sizes), the training time costs are too high (&gt; 3 days to run the 10 folds). So, the results on these three datasets are not shown here. As illustrated in <ref type="table" target="#tab_3">Table 2</ref>, the performance of full-input is better than padding/pruning, but it also consumes the highest time cost. The padding/pruning strategy has the lowest time costs but its performance is slightly lower than fullinput and segment shifting. Meanwhile, the learning performance of segment shifting balances between full-input and padding/pruning, which can even out-perform full-input as it introduce far less dummy paddings in the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have introduce a new graph neural network model, namely SEG-BERT, for graph instance representation learning. With several significant modifications, the re-designed SEG-BERT has no node-order-variant inputs or function components, which can handle the node orderless property very well. SEG-BERT has an extendable architecture. For large-sized graph instance input, SEG-BERT will divide the nodes into segments, whereas all the nodes in the graph will be used for the representation learning of each segment. We have tested the effectiveness of SEG-BERT on several concrete application tasks, and the experimental results demonstrate that SEG-BERT can outperform the state-of-the-art graph instance representation learning models effectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and the representation fusion component in SEG-BERT can both handle the graph node orderless property naturally. (b) SEG-BERT unifies arXiv:2002.03283v1 [cs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>An Illustration of the SEG-BERT Model for Graph Instance Representation Learning. gories of information inputs, (c) f can be effectively pretrained with the unsupervised learning tasks, and (d) representations learned by f can also be transferred to the downstream application tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>BERT(padding/pruning, none) 75.40±2.29 52.27±1.55 78.42±1.29 89.24±7.78 77.09±4.15 68.86±4.17 70.15±1.84 SEG-BERT(padding/pruning, raw) 74.70±3.74 50.60±3.03 74.90±1.78 89.80±6.71 76.28±2.91 64.84±6.77 68.10±2.55 SEG-BERT* 77.20±3.09 53.40±2.12 78.42±1.29 90.85±6.58 77.09±4.15 68.86±4.17 70.15±1.84</figDesc><table><row><cell></cell><cell>71.69±3.40 48.50±4.10 77.71±2.51</cell><cell>−</cell><cell cols="2">76.40±4.17 66.01±5.91 82.72±2.38</cell></row><row><cell>CapsGNN (Zhang &amp; Chen, 2019)</cell><cell cols="3">73.10±4.83 50.27±2.65 79.62±0.91 86.67±6.88 76.28±3.63</cell><cell>−</cell><cell>78.35±1.55</cell></row><row><cell>SEG-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results of different graph instance size unification strategies in SEG-BERT. No residual terms are used here, and the default epoch number is 500. The time denotes the average time cost for mode training in the 10 folds. For SEG-BERT with the segment shifting strategy, the default parameter k is set as 20. 90±1.76 136 2808.70 53.33±2.53 89 2397.42 90.85±6.58 28 55.73 68.01±4.23 109 700.98 Padding/Pruning 75.40±2.29 50 1312.42 52.27±1.55 50 1654.66 89.24±7.78 25 55.19 68.86±4.17 50 223.47 Segment Shifting 77.20±3.09 20 1525.97 53.40±2.12 20 1730.08 90.29±7.74 20 88.40 66.54±4.18 20 295.83 and (3) state-of-the-art deep learning methods, e.g., Dual Attention Graph Convolutional Network (DAGCN)</figDesc><table><row><cell>Strategies</cell><cell>IMDB-B Accuracy k Time(s)</cell><cell>IMDB-M Accuracy k Time(s)</cell><cell>MUTAG Accuracy k Time(s) Accuracy</cell><cell>PTC k Time(s)</cell></row><row><cell>Full-Input</cell><cell>76.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">IFM Lab, Department of Computation, Florida State University, Tallahassee, FL, USA.. Correspondence to: Jiawei Zhang &lt;jiawei@ifmlab.org&gt;. Work in progress. Preprint Version.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">GPU Server: ASUS X99-E WS motherboard, Intel Core i7 CPU 6850K@3.6GHz (6 cores, 40 PCIe lanes), 3 Nvidia GeForce GTX 1080 Ti GPU (11 GB buffer each), 128 GB DDR4 memory and 128 GB SSD swap.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Simple Permutation-Invariant Graph Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atamna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sokolovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Crivello</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Spi-Gcn</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-02093451" />
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
	<note>working paper or preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph Theory With Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bondy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<publisher>Elsevier Science Ltd., GBR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DAGCN: dual attention graph convolutional networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.02278" />
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectra of random graphs with given expected degrees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vu</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0937490100</idno>
		<ptr target="https://www.pnas.org/content/100/11/6313" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6313" to="6318" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.3555" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3555</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.acha.2010.04.005</idno>
		<ptr target="http://dx.doi.org/10.1016/j.acha.2010.04.005" />
		<imprint>
			<date type="published" when="2011-03" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Applied and Computational Harmonic Analysis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Anonymous walk embeddings. CoRR, abs/1805.11921</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1805.11921" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gaussian-induced convolution for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno>abs/1811.04393</idno>
		<ptr target="http://arxiv.org/abs/1811.04393" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno>doi: 10. 3115/v1/D14-1181</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1181" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<title level="m">A robustly optimized BERT pretraining approach. CoRR, abs</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Random walks on graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Combinatorics</title>
		<editor>Miklós, D., Sós, V. T., and Szőnyi, T.</editor>
		<meeting><address><addrLine>Budapest</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="353" to="398" />
		</imprint>
	</monogr>
	<note>Paul Erdős is Eighty</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Capsule neural networks for graph classification using explicit tensorial graph representations. CoRR, abs/1902.08399</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D G</forename><surname>Mallea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Bentley</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1902.08399" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A permutation invariant graph neural network for graph classification. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D G</forename><surname>Mallea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinet</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.03046" />
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Isomorphic neural network for graph representation learning and classification. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isonn</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.09495" />
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1707.05005" />
		<title level="m">Learning distributed representations of graphs. CoRR, abs/1707.05005</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<idno>abs/1605.05273</idno>
		<ptr target="http://arxiv.org/abs/1605.05273" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07979</idno>
		<title level="m">Adaptive structure aware pooling for learning hierarchical graph representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v5/shervashidze09a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics</title>
		<editor>van Dyk, D. and Welling, M.</editor>
		<meeting>the Twelth International Conference on Artificial Intelligence and Statistics<address><addrLine>Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="16" to="18" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weisfeilerlehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ERNIE 2.0: A continual pretraining framework for language understanding. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.12412" />
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Attention is all you need. CoRR, abs/1706.03762</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08090</idno>
		<title level="m">Graph capsule convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>abs/1810.00826</idno>
		<ptr target="http://arxiv.org/abs/1810" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Association for Computing Machinery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2783417</idno>
		<ptr target="https://doi.org/10.1145/2783258.2783417" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
	<note>Deep graph kernels</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graph Neural Networks for Small Graph and Giant Network Representation Learning: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00187</idno>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gresnet: Graph residual network for reviving deep gnns from suspended animation. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An end-toend deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Capsule graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Byl8BnRcYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
