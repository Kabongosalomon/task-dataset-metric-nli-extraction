<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-02">2 Dec 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>An</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshayu</forename><surname>Girase</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-02">2 Dec 2020</date>
						</imprint>
					</monogr>
					<note>From Goals, Waypoints &amp; Paths To Long Term Human Trajectory Forecasting</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We tackle the problem of long term human trajectory forecasting. Given the past motion of an agent (blue) on a scene over the last five seconds, we aim to predict the multimodal future motion upto the next minute . To achieve this, we propose to factorizing overall multimodality into its epistemic and aleatoric factors. The epistemic factor is modeled with an estimated distribution over the long term goals while the aleatoric factor is modeled as a distribution over the intermediate waypoints and trajectory . This is repeated for multiple goals and waypoints in parallel for scene-compliant multimodal human trajectory forecasting . Each color indicates predicted trajectories for different sampled goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Human trajectory forecasting is an inherently multimodal problem. Uncertainty in future trajectories stems from two sources: (a) sources that are known to the agent but unknown to the model, such as long term goals and (b) sources that are unknown to both the agent &amp; the model, such as intent of other agents &amp; irreducible randomness in decisions. We propose to factorize this uncertainty into its epistemic &amp; aleatoric sources. We model the epistemic uncertainty through multimodality in long term goals and the aleatoric uncertainty through multimodality in waypoints * indicates equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sequence prediction is a fundamental problem in several engineering disciplines such as signal processing, pattern recognition, control engineering and in virtually any domain concerned with temporal measurements. From the seminal work of A. A. Markov <ref type="bibr" target="#b28">[29]</ref> on predicting the next syllable in the poem Eugene Onegin with Markov chains, to modern day autoregressive descendants like GPT-3 <ref type="bibr" target="#b5">[6]</ref>, next element prediction in a sequence has a long standing history. Time series forecasting is a key instantiation of the sequence prediction problem in the setting where the sequence is formed by elements sampled in time. Several classic techniques such as Autoregressive Moving Average Models (ARMA) <ref type="bibr" target="#b42">[43]</ref> have been incorporated in deep learning architectures <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b17">18]</ref> in modern day state-of-theart time series forecasting methods <ref type="bibr" target="#b36">[37]</ref>. However, humans are not inanimate Newtonian entities, slave to predetermined physical laws &amp; forces. Predicting the future motion of a billiard ball smoothly rolling on a pool table under friction and physical constraints is a prob-lem of different nature from forecasting human motion and positions. Humans are goal conditioned agents that, unlike the ball, exert their will through actions to achieve a desired outcome <ref type="bibr" target="#b39">[40]</ref>. Anticipating human motion is of fundamental importance to dynamic agents such as other humans, autonomous robots <ref type="bibr" target="#b2">[3]</ref> and self-driving vehicles <ref type="bibr" target="#b38">[39]</ref>. Human motion is inherently goal directed and is put in place by the agent to bring about a desired effect.</p><p>Nevertheless, even conditioned on the agent's past motion and overarching long term goals, is the future trajectory deterministic? Consider yourself standing at a crossing on a busy street, waiting for the pedestrian light to turn green. While you have every intention of crossing the street, the exact future trajectory remains stochastic as you might swerve to avoid other pedestrians, speed up your pace if the light is about to turn red, or pause abruptly if an unruly cyclist dashes by. Hence, even conditioned on the past observed motion and scene semantics, future human motion is inherently stochastic <ref type="bibr" target="#b15">[16]</ref> owing to both epistemic uncertainty caused by latent decision variables like long term goals and aleatoric variability <ref type="bibr" target="#b10">[11]</ref> stemming from random decision variables such as environmental factors. This dichotomy is even sharper in long term forecasting, since due to the increased uncertainty in future, the aleatoric randomness influences the trajectory much more strongly in long rather than short temporal horizons.</p><p>This motivates a factorized multimodal approach for human dynamics modeling where both factors of stochasticity are modeled hierarchically rather than lumped jointly. We hypothesize that the long term latent goals of the agent represent the epistemic uncertainty with motion prediction. This is motivated by the observation that while the agent has a goal in mind while planning and executing their trajectory, this is unknown to the prediction system. In physical terms, this is akin to the question of where the agent wants to go. Similarly, the aleatoric uncertainty is expressed in the stochasticity of the path leading to the goal, which encompasses factors like agent's handedness, environment variables such as other agents, partial scene information available to the agent and most importantly, the unconscious randomness in human decisions <ref type="bibr" target="#b18">[19]</ref>. In physical terms, this is akin to the question of how the agent reaches the goal.</p><p>Hence, we propose to model the epistemic uncertainty first and then the aleatoric stochasticity conditioned on the obtained estimate. Concretely, with the RGB scene and the past motion history we first estimate an explicit probability distribution over the agent's final positions at the end of the trajectory, i.e. the agent's long term goals. This represents the epistemic uncertainty in the prediction system. We also estimate distributions over a few chosen future waypoint positions which along with the sampled goal points are used to obtain explicit probability maps over all the remaining trajectory positions. This represents the aleatoric uncertainty in the prediction system. Together the samples from the epistemic goal and the aleatoric waypoint &amp; trajectory distribution form the predicted future trajectory.</p><p>In summary, our contribution is threefold. First, we propose a novel long term prediction setting that extends upto a minute in the future which is about an order of magnitude longer than previous literature. We also benchmark performance of previous state-of-the-art short horizon prediction models on this setting along with simple baselines. Second, we propose Y-net, a scene-compliant long term trajectory prediction network that explicitly models both the goal and path multimodalities by making effective use of the scene semantics. Third, we show that the factorized multimodality modeling enables Y-net to improve the state-of-the-art both on the proposed long term settings and the well-studied short term prediction settings. We benchmark Y-net's performance on the Stanford Drone <ref type="bibr" target="#b30">[31]</ref> and the ETH <ref type="bibr" target="#b29">[30]</ref>/UCY <ref type="bibr" target="#b22">[23]</ref> benchmark in the short term setting, where it outperforms previous approaches by significant margins of 26.9% &amp; 5.6% respectively, on ADE and by 34.0% and 51.9% respectively, on FDE metric. Further, we also study Y-net's performance in the proposed long term prediction setting on the Stanford Drone &amp; the Intersection Drone Dataset <ref type="bibr" target="#b4">[5]</ref> where it substantially improves the performance of state-of-the-art short term methods by over 50.6% and 35.0% respectively, on ADE and 77.1% and 55.9% respectively, on FDE metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Several recent studies have investigated human trajectory prediction in different settings. Broadly, these approaches can be grouped on the basis of the proposed formulation for multimodality in forecasting, inputs signals available to the prediction model and the nature and form of prediction results furnished by the model. Several diverse input signals such as agent's past motion history <ref type="bibr" target="#b16">[17]</ref>, human pose <ref type="bibr" target="#b26">[27]</ref>, RGB scene image <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>, scene semantic cues <ref type="bibr" target="#b7">[8]</ref>, location <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4]</ref> &amp; gaze of other pedestrian <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">46]</ref> in the scene, moving vehicles such as cars <ref type="bibr" target="#b35">[36]</ref> and also latent inferred signals such as agent's goals <ref type="bibr" target="#b27">[28]</ref> have been used. The form of prediction results produced are also diverse with multimodality <ref type="bibr" target="#b25">[26]</ref> and scene-compliant forecasting being central to the prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unimodal Forecasting</head><p>Early trajectory forecasting work focused on unimodal predictions of the future. Social Forces <ref type="bibr" target="#b16">[17]</ref> proposes modeling interactions as attractive and repulsive forces and future trajectory as a deterministic path evolving under these forces. Social LSTM <ref type="bibr" target="#b0">[1]</ref> focuses on other agents in the scene and models their effects through a novel pooling module. <ref type="bibr" target="#b44">[46]</ref> tackles motion forecasting in ego-centric views and develops a system that exploits subtle cues like body pose and gaze along with camera wearer's ego-motion for other agent's future location prediction. <ref type="bibr" target="#b41">[42]</ref> proposes to use attention to model current agent's interaction with other agent's. <ref type="bibr" target="#b26">[27]</ref> predicts trajectory as the 'global' branch for pose prediction and proposes to condition downstream tasks such as pose prediction on predicted unimodal trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multimodality through Generative Modeling</head><p>A line of work aims to model the stochasticity inherent in future prediction through a latent variable with a defined prior distribution through approaches such as conditional variational auto-encoders <ref type="bibr" target="#b19">[20]</ref>. Lee et al. <ref type="bibr" target="#b21">[22]</ref> propose DE-SIRE, an inverse reinforcement learning based approach that uses multimodality in sampling of a latent variable that is ranked and optimized with a refinement module. CF-VAE <ref type="bibr" target="#b3">[4]</ref> uses normalizing flows with VAEs for modeling structure in sequences such as trajectories. <ref type="bibr" target="#b26">[27]</ref> introduces the use of a CVAE for capturing multimodality in the final position of the pedestrians conditioned on the past motion history. Trajectron++ <ref type="bibr" target="#b35">[36]</ref> represents agent's trajectories in a graph structured recurrent network for scene complaint trajectroy forecasting, taking into account the interaction with a diverse set of agents. CGNS <ref type="bibr" target="#b23">[24]</ref> uses variational divergence minimization procedure in multimodal latent space to learn feasible regions for future trajectories.</p><p>A different line of work includes Social GAN <ref type="bibr" target="#b13">[14]</ref> which uses adversarial losses <ref type="bibr" target="#b12">[13]</ref> for incorporating multimodality in predictions. SoPhie <ref type="bibr" target="#b34">[35]</ref> further incorporates attention modules to model agent's interactions with the environment and other agents.</p><p>While such generative approaches do produce diverse trajectories, overall coverage of critical modes cannot be guaranteed and little control is afforded over the properties of predicted trajectories such as direction, number of samples etc. In contrast, our method, Y-net, estimates explicit probability maps which allow easily incorporating spatial constraints for a downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multimodality through spatial probability estimates</head><p>Another line of work obtains multimodality via estimated probability maps. Activity Forecasting from Kitani et al. <ref type="bibr" target="#b20">[21]</ref> proposes to use a hidden Markov Decision process for modeling the future paths. However, in contrast to our work the future predictions in <ref type="bibr" target="#b20">[21]</ref> are conditioned on the activity label such as 'approach car', 'depart car' etc. More recently, some works have used a grid based scene representation for estimating probabilities for future time steps <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b9">10]</ref>. Relatedly, some prior works such as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b7">8]</ref> propose a goal-conditioned trajectory forecasting method. However, no prior works have proposed factorized modeling of epistemic uncertainty or goals &amp; aleatoric uncertainty or paths as Y-net uses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The problem of multimodal trajectory prediction can be formulated formally as follows. Given a RGB scene image I and past positions of a pedestrian in the scene I denoted by {u n } np n=1 for the past t p = n p /FPS seconds sampled at the frame rate FPS, the model aims to predict the position of the pedestrian for the next t f seconds in the future, denoted by {u i n } np+n f n=np where t f = n f /FPS. Since the future is stochastic, multiple predictions for the future trajectories are produced. In this work, we factorize the overall stochasticity into two modes. First are the modes relating to epistemic uncertainty i.e. multimodality in the final destination of the agent for which the module produces K e predictions. Second are the modes relating to the aleatoric uncertainty i.e. multimodality in the path taken to the destination stemming from uncontrolled randomness given the goal, for which the module produces K a separate predictions for each given destination. In the short temporal horizon limit, since the overall path length is small, the options for paths to a given goal are limited and similar to each other. Hence, this results in setting K a = 1 and so the total paths predicted (K in prior works) is the same as K e . However, for longer temporal horizons, there are several paths to the same goal and hence K a &gt; 1. Next, we describe in detail the working of our model, Y-net and its three sub-networks U e , U g and U t (Section 3.1) followed by details of the non-parametric sampling process (Section 3.2) and loss functions (Section 3.3) used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Y-net Sub-Networks</head><p>To effectively use the scene information in the semantic space with the trajectory information expressed in coordinates, alignment needs to be created between the different signals. Some prior works <ref type="bibr" target="#b34">[35]</ref> achieve this by encoding the two-dimensional RGB image I as a one-dimensional hidden state vector extracted from some pretrained network. While this provides the network with scene information, any meaningful spatial signal gets highly conflated when flattened into a vector and pixel alignment is destroyed. This is highlighted in <ref type="bibr" target="#b27">[28]</ref> which establish previous state-ofthe-art without any RGB information underlining the misuse of image information in prior works. In this work, we adopt a trajectory on scene heatmap representation that solved the alignment issue by representing the trajectory spatially in the same two-dimensional space as image I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Trajectory on Scene Heatmap Representation</head><p>The RGB image I is first processed with a semantic segmentation network such as U-net <ref type="bibr" target="#b32">[33]</ref> that produces segmentation map S of I comprising of N c classes determined according to the affordance provided by the surface to an agent for actions such as walking, standing, running etc. In <ref type="figure">Figure 2</ref>: Model Architecture: Y-net comprises of three sub-networks U e , U g &amp; U t modeled after the Unet architecture <ref type="bibr" target="#b32">[33]</ref> (Section 3.1). Y-net adopts a factorized approach to multimodality, expressing the stochasticity in goals &amp; waypoints through estimated distributions furnished by U g . And multimodality in paths is achieved through estimated probability distributions obtained by U t conditioned on samples from U g for predicting diverse multimodal scene-compliant futures. a parallel branch, the past motion history {u n } np n=1 of agent p is converted to a trajectory heatmap H of spatial sizes of I and n p channels corresponding to the past t p seconds sampled at the frame rate. Mathematically,</p><formula xml:id="formula_0">H(n, i, j) = 2 (i, j) − u n max (x,y)∈I (x, y) − u n</formula><p>The heatmap trajectory representation is then concatenated with the semantic map S along the channel dimension producing the trajectory on scene heatmap tensor H S a H × W × (N c + n p ) dimensional input tensor which is passed to the encoder network U e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Trajectory on Scene Heatmap Encoder</head><p>The tensor H S is processed with the encoder U e designed as a U-net encoder <ref type="bibr" target="#b32">[33]</ref> as shown in <ref type="figure">Figure 2</ref>. The encoder U e consists of a total of N Ue blocks, where the spatial dimensions are reduced from H × W to H U × W U halving after every block using max pooling and channel depth is increased sequentially from N c + n p to C Ue doubling after a certain number of blocks. The final spatially compact and deep representation H Ue along with the N Ue − 1 intermediate feature tensors of varying spatial resolution are passed onto the goal decoder U g and the trajectory decoder U t as discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Goal &amp; Waypoint Heatmap Decoder</head><p>The feature maps of U e at various spatial resolutions are passed onto the goal &amp; waypoint heatmap decoder U g which is modeled after the expansion arm in the U-net architecture <ref type="bibr" target="#b32">[33]</ref>. After a center block consisting of two convolutional layers which takes the most compact feature map H Ue from U e , each block of the expansion arm starts by expanding the previous feature map, spatially doubling the resolution in every block using bilinear up-sampling and convolution (together forming Deconvolution <ref type="bibr" target="#b32">[33]</ref>).</p><p>Further, after every Deconvolution, the corresponding intermediate representations from U e are merged and the features are passed through two convolution layers. Merging intermediate high resolution feature maps from U e is necessary since just using H Ue would severely limit the final resolution of the goal heatmap, thus missing fine spatial details that are preserved in the intermediate feature maps.</p><p>Deconvolution, feature merging and convolution form a U-net block. U g consists of N Ug blocks, followed by a per-pixel sigmoid that produces N w + 1 estimated spatial heatmaps. This includes the probability distribution of the final goal of the agent i.e. the non-parametric distribu-tionP(u np+n f ) and N w intermediate waypoint probability heatmaps at frame steps w i ∈ {n p , n p + 1, . . . , n p + n f } ∀ i = 1, . . . , N w represented as non-paramateric dis-tributionsP(u wi ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Trajectory Heatmap Decoder</head><p>The estimated goal and waypoint distributions are sampled as described in Section 3.2 and the obtained sampleû np+n f for the goal and {û wi } N w i=1 for the intermediate waypoints are converted to a heatmap representation as described in Section 3.1. The obtained conditioning tensor H Ug is spatially downsampled to match the corresponding block's size. Those are passed along with the past motion and scene representation H Ue and the N Ue − 1 intermediate high resolution tensors to the trajectory decoder network U t . U t is modeled after the expansion arm of a U-net as well and proceeds in a similar fashion as U g , as described in Section 3.1.3 with a total of N Ut expansion blocks. The final trajectory distribution is obtained with a channel independent per-pixel sigmoid that produces for each future timestep a separate heatmaps of spatial size H × W corresponding to the position of the agent in the scene over the next n f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Non-parametric Distribution Sampling</head><p>Given a distribution P of the position of the agent in a future frame represented non-parametrically as a matrix of probabilities X, we aim to sample a two-dimensional point as our estimate for the position of the agent at that time step. This is difficult to achieve reliably in practice since the estimated distribution P is noisy. Hence, taking a naive argmax is not robust. Instead we propose to use the softargmax operation <ref type="bibr" target="#b11">[12]</ref> to approximate the most likely position in a robust &amp; stable fashion. Mathematically,</p><formula xml:id="formula_1">softargmax(X) =   i i j e Xij i,j e Xij , j j i e Xij i,j e Xij  </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Test-Time Sampling Trick (TTST)</head><p>In situations where multiple samples are required, such as during testing, sampling from P can be carried in a straightforward fashion by considering X as a categorical distribution with given probabilities X(i, j) for position (i, j). However, this approach doesn't take into account the number of samples required from P all of which jointly will represent the quality of the estimated P. For example, if only a few samples are required, sampling indiscriminately spatially is sub-optimal since samples are likely to be wasted if drawn from low probability regions or sampled from adjacent regions.</p><p>We propose a 'Test-Time Sampling Trick' (TTST) that is cognizant of the number of goal samples, K e , needed for evaluation. During testing, we propose to first sample a large number of points (10,000 in our experiments) from the estimated distribution P in a K e -agnostic manner.</p><p>To eliminate outliers, we suppress samples from pixels (i, j) with probability X ij below a threshold thr rel . It is set adaptively for each probability matrix X separately to a fraction of the highest occurring value in that matrix: thr rel = max(X) * 0.01.</p><p>Further, to control the tradeoff between diversity and precision, we use the hyper parameter temperature T . Before the pixel-wise sigmoid operation, we divide the predicted logit probability map through T . Lower temperature values results in probability maps X with low entropy, i.e. the probability mass is concentrated in a smaller number of pixels and samples are increasingly drawn from more likely regions. Higher temperature values increase the diversity of samples. For the short term setting, we use T = 1.0, while for our proposed long term setting, we increase the diversity by setting T = 1.8.</p><p>Then, we propose to run the fast clustering algorithm Kmeans on these sampled points with the number of clusters set to K e − 1. The cluster centers obtained from the K-means algorithm, along with the softargmax sampled point, form the final set of K samples to be used for evaluation. Note that while in spirit this is similar to the 'truncation trick' proposed in PECNet <ref type="bibr" target="#b27">[28]</ref>, the 'truncation trick' is K-agnostic and requires a well suited σ T to be chosen experimentally beforehand for a given K. Further, their 'truncation trick' operates in the latent variable space with no direct control over the final generated samples since in <ref type="bibr" target="#b27">[28]</ref> multi-modality is introduced through implicit approaches like Variational Auto-encoders. Alleviating these limitations, TTST provides direct control on the sampled points, is cognizant of the number of samples needed K and hence, does not require any K-specific tuning because of the design choice of using explicit probability heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Conditioned Waypoint sampling</head><p>Goal and waypoints are dependent to each other. <ref type="figure" target="#fig_0">Figure 3</ref> shows an example, where the road forks into three different paths. If the sampled goal lies on the bottom path, the agent most likely won't pass through a waypoint on the upper or middle path, and will prefer a waypoint on the bottom road.</p><p>Following this intuition, we introduce a hierarchical prior to condition the waypoint sampling on the already sampled goal and waypoints.</p><p>We first sample the goal. By fixing the goal the possible locations of the next waypoint is constrained. We assume that the waypoint lies on a straight line segment between the sampled goal and the past trajectory at wi−np n f of the line segment's length, e.g. with N w = 1 waypoint, it would lie in the middle of the segment. This assumption is too hard and can lead to unrealistic paths not complying with the environment constraints. To relax this assumption, we use a multivariate Gaussian prior with mean at the assumed location. The variance is chosen adaptively by considering the distance between the agent's last observed position and the sampled goal as σ ⊥ = ||u np+n f − u np ||/α where α is a scaling hyper parameter. We set α = 6 in our experiments. Intuitively, the greater the distance between the current position and the sampled goal, the more uncertain is the waypoint position. σ ⊥ is the variance perpendicular to the line segment, and we set the variance parallel to the line segment to σ = β * σ ⊥ with β = 0.5 in our experiments. This constrains the possible waypoint position more in the direction of travel and leaves more room for uncertainty in the perpendicular direction.</p><p>We multiply the described multivariate Gaussian prior pixel-wise to the predicted waypoint distribution. Fusing prior and predicted distribution leads to scene-compliant waypoints in the correct direction. As seen in the example, it suppresses probability mass on the upper and middle road. From the resulting distribution we use softargmax for the first waypoint and sample the remaining K a −1 waypoints randomly to get two-dimensional points from the distribution.</p><p>If there is more than one waypoint, i.e. N w &gt; 1, we repeat the above process for the next waypoint at w i and condition it to the previously sampled waypoint at w i+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>Since we use the trajectory on scene representation (Section 3.1) we impose losses directly on the estimated distri-butionP rather than on the samples. The ground truth future is represented as a Gaussian heatmap P centered at the observed points with a predetermined variance σ H . All three networks, U e , U g and U t are trained end to end jointly using a weighted combination of binary cross entropy losses on the predicted goal, waypoint and trajectory distributions.</p><formula xml:id="formula_2">L goal = BCE(P(u np+n f ),P(u np+n f )) L waypoint = N w i=1 BCE(P(u wi ),P(u wi )) L trajectory = np+n f i=np BCE(P(u i ),P(u i )) L = L goal + λ 1 L waypoint + λ 2 L trajectory</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use a total of three datasets to study Y-net's performance -the Stanford Drone Dataset (SDD) <ref type="bibr" target="#b31">[32]</ref> (for both short &amp; long term), the Intersection Drone Dataset (InD) <ref type="bibr" target="#b4">[5]</ref> (for long term only) and the ETH [30] / UCY <ref type="bibr" target="#b22">[23]</ref> forecasting benchmark (for short term only).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stanford Drone Dataset:</head><p>We benchmark our proposed model on the popular Stanford drone dataset <ref type="bibr" target="#b31">[32]</ref> where several recently proposed methods have improved state-of-the-art performance significantly in the past few years <ref type="bibr">[45]</ref>. The dataset is comprised of more than 11, 000 unique pedestrians across 20 top-down scenes captured on the Stanford university campus in bird's eye view using a flying drone. It has over 40, 000 agent-scene interactions and has enjoyed very popular usage in trajectory prediction literature in the short temporal horizon setting. For short term prediction, we follow the well-established preprocessed data and splits from the TrajNet benchmark <ref type="bibr" target="#b33">[34]</ref> setup used by <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref>, sampling at FPS = 2.5 yielding an input sequence of length n p = 8 (3.2 seconds) and output of length n f = 12 (4.8 seconds).</p><p>In our proposed long term setting, we split the raw data of Stanford Drone Dataset (SDD) in the same fashion as proposed in TrajNet benchmark <ref type="bibr" target="#b33">[34]</ref> evaluating on the same scenes, all of which are not seen during training. The raw data is recorded in FPS = 30 and we first downsample the data to our proposed FPS = 1, thus yielding a n p = 5 for t f = 5 seconds in the past and predicting upto one minute into the future. We use the middle point of the raw bounding boxes to get the same coordinate representation as the preprocessed short term setting. The data contains various types of agent beyond pedestrians (bicyclists, skateboarders, cars, buses, and golf carts), we filter out all non-pedestrians and short trajectories below n p + n f out. As the raw data is noisy and contains temporal discontinuities, we split the trajectories at those discontinuities. We use a sliding window approach without overlap to split up long trajectories, resulting in our final dataset. After those steps, the dataset contains 1502 trajectories for n p = 5 and n f = 30. Further, we label the scenes with semantic segmentation maps consisting of the following N c = 5 "stuff" classes <ref type="bibr" target="#b6">[7]</ref> depending on the affordability of class to pedestrians: pavement, terrain, structure, tree and road.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intersection Drone Dataset:</head><p>We propose to use the Intersection drone dataset <ref type="bibr" target="#b4">[5]</ref> for benchmarking trajectory forecasting in long horizon settings. The dataset comprises over 10 hours of measurements over 4 distinct intersection in an urban environment. We use similar steps for the Intersection Drone Dataset as for SDD. To evaluate Y-net and the baselines performance on unseen scenes during training, we only use location ID 4 during testing. The raw video and detection are in FPS = 25, and again, we downsample the data to FPS = 1. We then filter out non-pedestrians and short trajectories and use a sliding window approach without overlap to split long trajectories. Since the data lies in world coordinates, we convert it into pixel coordinates by scaling with the provided scale factors from the authors. After the preprocessing steps, inD contains 1,396 long term trajectories with n p = 5 and n f = 30. To evaluate performance on unseen environments, we are using location ID 4 only during testing time. The scene is labeled with the same N c = 5 classes as in SDD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETH &amp; UCY datasets:</head><p>The ETH/UCY benchmarks have been widely used for benchmarking trajectory forecasting models in the short horizon setting in the recent years <ref type="bibr" target="#b43">[44]</ref>. Forecasting performance has improved by over ∼ 64% on average, within the last two years itself <ref type="bibr" target="#b13">[14]</ref>.</p><p>It comprises of five different scenes all of which report position in world coordinates (in meters). We follow the leave one out validation strategy as outlines in prior works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref>. We use the preprocessed data from <ref type="bibr" target="#b13">[14]</ref> 1 , also used by state-of-the-art methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>. Similar to SDD, the frames are sampled at FPS = 2.5 predicting n f = 12 frames, t f = 4.8 seconds into the future given last n p = 8 frames comprising of t p = 3.2 seconds of the past.</p><p>Our model represents trajectories as heatmaps and hence needs the coordinates in pixel space. The ETH/UCY data lie in world coordinates. To project the data from meter into pixel space we use the provided homography matrices from the original dataset for the ETH <ref type="bibr" target="#b29">[30]</ref> scenes ETH and HOTEL and create our own homography matrices for the UCY <ref type="bibr" target="#b22">[23]</ref> scenes UNIV, ZARA1 and ZARA2. To enable fair comparisons, we convert our predictions back to world coordinates using the inverse homography matrices and calculate our errors with the untouched raw data in world coordinates, to avoid any errors from our projection.</p><p>For all ETH and &amp; UCY datasets, since the classes of affordances furnished by the surfaces present is small, we use N c = 2, identifying each pixel as either belonging to class 'road' or 'not road'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Segmentation Model Implementation Details</head><p>To incorporate constraints and interactions of the agents with the scene, we pretrain a semantic segmentation model to efficiently use the sparse scene image data. Stanford Drone Dataset contains 60 scene images in total, while inD only contains images from four recording locations. We use the U-net model <ref type="bibr" target="#b32">[33]</ref> with ResNet101 <ref type="bibr" target="#b14">[15]</ref> backbone. The ResNet101 encoder's weights are pretrained on ImageNet, while the weights for the U-net decoder and segmentation head are randomly initialized. The images are downsampled by a factor of four (SDD) and three (inD), padded to be divisible by 32 as required for U-net and cropped to 256×256. The data is augmented spatially by rotation, flipping, scaling and perspective transformation and we introduce Gaussian noise, blurring as well as color, brightness and contrast shifts. The semantic maps are manually labeled into the N c = 5 classes mentioned above, as well as a dummy class for padding and black areas in the inD dataset. We only use the corresponding images from the trajectory train scenes for training to evaluate the performance of Ynet on unseen environments for both SDD and inD.  The SDD segmentation model is trained using ADAM optimizer to reduce the Dice Loss <ref type="bibr" target="#b37">[38]</ref> with an initial learning rate of 1 × 10 −4 and batch size of 4. The learning rate is decreased to 1 × 10 −5 after 1500 epochs. Further we freeze the ResNet101 backbone for the first 200 epochs.</p><p>As inD only contains images from four locations, we use the pretrained SDD model and freeze the encoder for the first 1000 epochs to avoid catastrophic forgetting. All other hyper parameters are the same as for training SDD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Y-net Implementation Details</head><p>We train the entire network end to end with ADAM optimizer with a learning rate of 1 × 10 −4 and batch size of 8. We scale the overall loss by a factor of 1000. Since the scene images I have different heights and widths in all datasets, we ensure that each batch only contains image and trajectories from the same scene. Y-net does not use fully-connected layers and therefore can handle images of different sizes, without cropping or padding to the same shape. The RGB scene image I and trajectory heatmaps H are downsampled by 4 for SDD, 3 for inD and 1.5 for ETH/UCY to save memory and padded to be divisible by 32. For fair comparisons with previous methods we upsample the predicted trajectories back to its original size and compare with the ground-truth data in original scale. All scene images and trajectories are augmented by spatial flipping and rotation in 90°steps, increasing the number of training data by a factor of eight. The encoder blocks in U e have output channel dimensions <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">64,</ref><ref type="bibr">64,</ref><ref type="bibr">64]</ref> and both U g and U t start with two convolutional layers of output channels 128, followed by blocks of output channel dimensions [64, 64, 64, 32, 32]. We use λ 1 = λ 2 = 1 to weight the binary cross entropy loss. The ground-truth trajectory heatmaps has a variance of σ H = 4 pixels.</p><p>During training U g predicts the goal and waypoint distribution for all n p time steps as an auxiliary task. This helps to let the sub-network learn the dynamics of pedestrian trajectories better. During inference, we only use the goal and  <ref type="table">Table 3</ref>: Long term trajectory forecasting Results: We benchmark performance on our proposed long horizon forecasting setting predicting t f = 30 seconds into the future given t p = 5 seconds past motion history. All reported error are in pixels (lower is better) for K e = 20 with additional results for varying K a with a fixed K e .</p><p>shapes of U t blocks. By using the ground-truth, U t learns to predict trajectories leading towards the goal, while passing the waypoints. During inference, we use the (TTST) sampled goals and waypoints predicted by U g . On ETH/UCY, we further experiment with deformable convolutional layers as proposed in <ref type="bibr" target="#b8">[9]</ref>.</p><p>We will release all our data, both raw &amp; processed, code for reproducing experiments across all the datasets &amp; labeled semantic maps for reproducibility and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Metrics</head><p>We use the established Average Displacement Error (ADE) and Final Displacement Error (FDE) metrics for measuring performance of future predictions. ADE is calculated as the 2 error between the predicted future and the ground truth averaged over the entire trajectory while FDE is the 2 error between the predicted future and ground truth for the final predicted point <ref type="bibr" target="#b1">[2]</ref>. Following prior works <ref type="bibr" target="#b13">[14]</ref>, in the case of multiple future predictions, the final error is reported as the min error over all predicted futures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Baseline models</head><p>We benchmarks against several state-of-the-art methods across both short and long term trajectory forecasting settings which we describe briefly.</p><p>• Social GAN <ref type="bibr" target="#b13">[14]</ref> proposes a Generative Adversarial Network (GAN) to predict multi-modal trajectory autoregressively.</p><p>• SoPhie <ref type="bibr" target="#b34">[35]</ref> also uses a GAN and extends it by attention modules to incorporate other agents and scene context.</p><p>• Conditional Flow VAE (CF-VAE) <ref type="bibr" target="#b3">[4]</ref> proposes a conditional normalizing flow based Variational Auto-Encoder that models future uncertainty without disentangling underlying factors.</p><p>• Conditional Generative Neural System (CGNS) <ref type="bibr" target="#b23">[24]</ref> proposes variational divergence minimization in latent space to learn feasible regions for future trajectories.</p><p>• P2TIRL <ref type="bibr" target="#b9">[10]</ref> proposes a grid based trajectory forecasting method learnt using maximum entropy inverse reinforcement learning.</p><p>• DESIRE <ref type="bibr" target="#b21">[22]</ref> also proposes an inverse reinforcement learning approach for prediction by planning.</p><p>• SimAug <ref type="bibr" target="#b24">[25]</ref> is a recently proposed method that uses additional adversarially generated 3D multi-view data for adapating to novel viewpoints in forecasting and improve the Multiverse model <ref type="bibr" target="#b25">[26]</ref>.</p><p>• PECNet <ref type="bibr" target="#b27">[28]</ref> is the prior state-of-the art method on short-term trajectory prediction on the Stanford Drone Dataset. They propose to use goal-conditioning but does not account for multi-modality in the path to the goal.</p><p>• TNT <ref type="bibr" target="#b45">[47]</ref> closely improves upon PECNet's performance for K = 5 samples on SDD and is the prior state-of-the-art in that setting.  <ref type="table">Table 4</ref>: Ablation results for Conditioned Waypoint Sampling (CWS) and TTST: We benchmark the performance of Y-net with and without our proposed CWS and TTST on our long horizon forecasting setting, predicting t f = 30 seconds into the future given t p = 5 seconds of past motion history. All reported errors are in pixels (lower is better) for N w = 1, K e = 20 and K a = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDD</head><p>works using K = 20 trajectory samples for final evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">ETH &amp; UCY Results</head><p>We also report results on the ETH/UCY benchmark in <ref type="table" target="#tab_1">Table  2</ref>. Similar to SDD, we set K e = 20, K a = 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Long Term Forecasting Results</head><p>To study the effect of epistemic &amp; aleatoric uncertainty, we propose a long term trajectory forecasting setting with a prediction horizon upto 10 times longer than prior works, extending to a minute. To benchmark, we retrain PECNet <ref type="bibr" target="#b27">[28]</ref>, the previous state-of-the-art method from short term forecasting &amp; Social GAN <ref type="bibr" target="#b13">[14]</ref> for each t f in the long horizon setting. We also train a recurrent short term baseline using the PECNet model (R-PECNet) where the model is trained only for t f = 5 seconds and is fed its own predictions recurrently for predicting longer temporal horizons. <ref type="table">Table 3</ref> reports the baseline and our results on the Stanford Drone (SDD) and Intersection Drone Datasets (InD) for a time horizon of t f = 30 seconds in the future given the   We show various heatmaps and visualizations for three different scenes (rows) in SDD testset. The first column shows the past observed trajectory for last t p = 5 seconds in blue. The second column shows the heatmap from U g for t f = 30 seconds in the future (goal multimodality) and some sampled goals from the estimated distribution. The third column shows trajectory heatmaps from U t conditioned on a sampled goal from column three (path multimodality). The last column shows the predicted trajectories, green indicating the ground-truth trajectories &amp; red our multimodal predictions. <ref type="table">Table 4</ref> shows an ablation of the Conditioned Waypoint Sampling CWS. While it doesn't affect the goal sampling, i.e. the FDE, the ADE decreases by 24.3%. <ref type="table">Table 4</ref> shows the effectiveness of our proposed TTST. TTST reduces the error on SDD and inD by 9.1% and 18.5% in ADE, respectively, and 30.4% and 35.0% in FDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Forecasting Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Conditoned Waypoint Sampling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">Test-Time Sampling Trick</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.4">Varying Prediction Horizon</head><p>We also compare Y-net with two flavours of PECNet, one retrained separately for each prediction horizon t f , and another trained only for t f = 5 seconds but evaluated recurrently for long horizons (R-PECNet). We observe that the difference in ADE between Y-net &amp; PECNet grows as prediction horizon increases from 5 to 60 seconds. This shows Y-net's adaptability for long prediction horizons owing to factorized mulitmodality modeling. We also observe that for PECNet, training a separate model for different time horizons is significantly better than using a short temporal horizon model recurrently. This motivates our proposal for studying long term forecasting since short term models behave very poorly when applied out of box recurrently to longer term settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.5">Varying K a</head><p>We also report results with K a = 2 &amp; 5 for studying the improvement in performance from aleatoric multimodality in <ref type="table">Table 3</ref>. We observe a consistent improvement in ADE on both datasets, thus indicating the diversity in predicted paths given the same estimated final goal u np+n f . We also report extensive results for varying the path multimodality K a with a fixed K e for various choice of K e &amp; K a in <ref type="figure" target="#fig_2">Figure 5</ref>. Additionally for baselining, we benchmark against PECNet <ref type="bibr" target="#b26">[27]</ref> evaluated with K e times more samples than the corresponding Y-net model while varying K a . We show consistent ADE improvements for various K e while increasing K a , indicating effective use of multimodality. Further, even with K = K e * 20 samples, i.e. 20 times more samples for each K e , PECNet's performance is significantly worse than Y-net at K e = 20 for all K a highlighting the importance of factorizing goal and path multimodality for diverse &amp; accurate future trajectory modeling. and path multimodality for long term human trajectory prediction (30 seconds horizon). Given the past 5 seconds input history (green), we predict diverse future trajectories (current location in orange, past in red). Best viewed in Adobe Acrobat Reader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.6">Qualitative Results</head><p>We show qualitative results for long term trajectory prediction (t f = 30) on SDD in <ref type="figure" target="#fig_3">Figure 6</ref> and through a GIF temporally in <ref type="figure" target="#fig_4">Figure 7</ref>. We observe that Y-net predicts diverse scene-complaint trajectories, with both future goal and path multimodalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In summary, we present Y-net, a scene-compliant trajectory forecasting network with factorized goal and path multimodalities. Y-net uses the U-net structure <ref type="bibr" target="#b32">[33]</ref> for explicitly modeling probability heatmaps for epistemic and aleatoric uncertainties. Overall, Y-net improves previous state-of-the-art performance by 34.0% on the SDD and by 51.9% on ETH/UCY benchmarks in the short term setting. We also propose a new long term trajectory forecasting setting (prediction horizon upto a minute) for exemplifying the epistemic and aleatoric uncertainty dichotomy. In this setting, we benchmark on the Stanford Drone &amp; Intersection Drone dataset where Y-net exceeds previous state-of-the-art by over 77.1% and 55.9% respectively thereby highlighting the importance of modeling factorized stochasticity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Conditioned Waypoint sampling: The first image shows the scene with the past trajectory in blue. The yellow star indicates the sampled goal. The unconditioned waypoint distribution can be seen in the second image; the distribution is goal-agnostic and therefore probability mass is distributed over all three roads. The third image is the resulting waypoint distribution, by multiplying the multivariate Gaussian prior with the unconditioned prediction. The final image shows the trajectory towards the sampled goal while crossing the waypoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Benchmarking Performance against Time Horizons: On prediction horizons upto a minute, we observe a consistently growing difference in ADE between Ynet and PECNet, highlighting the important of factorized goal &amp; path modeling in long term forecasting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Benchmarking performance against aleatoric uncertainty (Ka): Fixing the goal multimodality (Ke) we vary Ka to observe the effect of path multimodality. Also, we benchmark against PECNet by allowing it 20 times more samples for each Ka for a fair compare against the Ke = 20 Y-net curve. past t p = 5 seconds input. All reported results are with K e = 20 for Y-net conditioned on N w = 1 intermediate waypoint at w i = 20, i.e. midway temporally between the observed inputs and the estimated goal. All reported baseline results are at K = 20 for fair comparisons with our K e = 20, K a = 1 setting. On SDD, we observe that our proposed model outperforms the state-of-the-art short term baseline on the long horizon setting as well, achieving an ADE of<ref type="bibr" target="#b45">47</ref>.94 and FDE 66.72 improving upon PECNet's performance by over 50%. Similarly, Y-net outperforms PECNet on InD improving ADE performance from 20.25 to 14.99 and FDE from 32.95 to 21.13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative Long Term Trajectory Forecasting Results:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>GIF Visualization: Demonstrating the goal, waypoint</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>S-GAN CF-VAE P2TIRL SimAug PECNet* Y-net (Ours) DESIRE TNT* PECNet Y-net (Ours) Short temporal horizon forecasting results on SDD: Our method significantly outperforms previous state-of-theart methods (indicated by *) on the Stanford Drone Dataset<ref type="bibr" target="#b31">[32]</ref> on both the ADE &amp; FDE metrics for both settings of K, where K represents the number of multimodal samples . Reported errors are in pixels with t p = 3.2 sec, t f = 4.8 sec, n p = 8, n f = 12. Lower is better.</figDesc><table><row><cell>K = 20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Short term forecasting results on ETH/UCY benchmark: Our proposed method establishes new state-of-the-art results (previous results denoted by *) on both the ADE &amp; the FDE metrics on the popular ETH-UCY benchmark using standard short-horizon settings (same as SDD) and K = 20. Reported errors are in meters. Lower is better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>shows our proposed model achieving an ADE of 7.85 and FDE of 11.85 at K e = 20 which outperforms the previous state-of-the-art performance of PECNet<ref type="bibr" target="#b27">[28]</ref> by 26.8% on ADE and 34.0% on FDE. Further, at K = 5 it achieves an ADE of 11.49 &amp; FDE of 20.23 outperforming previous state-of-the-art performance of TNT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Here as well, we observe that our proposed model achieves an ADE of 0.18 &amp; FDE of 0.27 improving the previously state-ofthe-art performance from Trajectron++ [36] of 0.19 ADE &amp; 0.41 FDE by about 5.6% ADE and 51.9% FDE.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">&amp; paths. To exemplify this dichotomy, we also propose a novel long term trajectory forecasting setting, with prediction horizons upto a minute, an order of magnitude longer than prior works. Finally, we present Y-net, a scene compliant trajectory forecasting network that exploits the proposed epistemic &amp; aleatoric structure for diverse trajectory predictions across long prediction horizons. Y-net significantly improves previous state-of-the-art performance on both (a) The well studied short prediction horizon settings on the Stanford Drone &amp; ETH/UCY datasets and (b) The proposed long prediction horizon setting on the re-purposed Stanford Drone &amp; Intersection Drone datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/agrimgupta92/sgan</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">N w waypoint distributions as needed. The trajectory sub-network U t is trained using the ground-truth goal and waypoints. Those are represented as trajectory heatmaps as described in Section 3.1.1 and downsampled spatially to fit the corresponding feature map</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">• Trajectron++<ref type="bibr" target="#b35">[36]</ref> proposes a recurrent graph based forecasting model incorporating dynamic constrains such as other moving agents and scene information. This work also hold the prior state-of-the art on ETH/UCY short-term trajectory prediction benchmark.4.5. Short Term Forecasting Results4.5.1 Stanford Drone ResultsTable 1presents results on SDD in the short term setting i.e. t p = 3.2 seconds, t f = 4.8 seconds. We follow the standard split from<ref type="bibr" target="#b33">[34]</ref> and report results with K e = 5 &amp; 20.Since there's limited aleatoric multimodality in short term settings, we use K a = 1 thus being comparable to prior</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Socially-aware large-scale crowd forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2203" to="2210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning motion patterns of persons for mobile service robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maren</forename><surname>Bennewitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292)</title>
		<meeting>2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3601" to="3606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hanselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09008</idno>
		<title level="m">Bernt Schiele, and Christoph-Nikolas Straehle. Conditional flow variational autoencoders for structured sequence prediction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The ind dataset: A drone dataset of naturalistic road user trajectories at german intersections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Moers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Runde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Vater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Eckstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ilya Sutskever</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>and Dario Amodei. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1209" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term human motion prediction with scene context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiket</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00735</idno>
		<title level="m">Trajectory forecasts in unknown environments conditioned on grid-based plans</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Aleatory or epistemic? does it matter? Structural safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Der Kiureghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ove</forename><surname>Ditlevsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">6.2. 2.3 softmax units for multinoulli output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="180" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Generative adversarial networks</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2255" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Helbing</surname></persName>
		</author>
		<title level="m">Stochastische Methoden, nichtlineare Dynamik und quantitative Modelle sozialer Prozesse. Shaker</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Thinking, fast and slow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Macmillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Andrew</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiorgos</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01631</idno>
		<title level="m">Conditional generative neural system for probabilistic trajectory prediction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Simaug: Learning robust representations from 3d simulation for pedestrian trajectory prediction in unseen cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The garden of forking paths: Towards multi-future trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Disentangling human dynamics for pedestrian locomotion forecasting with noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshayu</forename><surname>Girase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02025</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>It is not the journey but the destination: Endpoint conditioned trajectory prediction</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An example of statistical investigation of the text eugene onegin concerning the connection of samples in chains. Lecture at the physical-mathematical faculty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Markov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Academy of Sciences</title>
		<imprint>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="1913-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving data association by joint modeling of pedestrian trajectories and groupings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="452" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory prediction in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<editor>Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Trajnet: Towards a benchmark for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1349" to="1358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03093</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Think globally, act locally: A deep neural network approach to highdimensional time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4837" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Jorge</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Probabilistic robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="57" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding and sharing intentions: The origins of cultural cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malinda</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Call</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Behne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrike</forename><surname>Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="675" to="691" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Natural vision based method for predicting pedestrian behaviour in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Vasishta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Vaufreydaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Spalanzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Social attention: Modeling attention in human crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Vemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Whittle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hypothesis testing in time series analysis</title>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Eth/ucy trajectory prediction benchmark</title>
		<ptr target="https://paperswithcode.com/sota/trajectory-prediction-on-ethucy" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Paper with code</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Future person localization in first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Yonetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08294</idno>
		<title level="m">Target-driven trajectory prediction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
