<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun-Cheng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität</orgName>
								<address>
									<settlement>München 2 Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Wald</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität</orgName>
								<address>
									<settlement>München 2 Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität</orgName>
								<address>
									<settlement>München 2 Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität</orgName>
								<address>
									<settlement>München 2 Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität</orgName>
								<address>
									<settlement>München 2 Google</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SceneGraphFusion le g d) wall floor cabinet chair table door window bookshelf counter other furn. chair table cabinet floor attached on standing on a) b) c) <ref type="figure">Figure 1</ref>. We create a globally consistent 3D scene graph b) by fusing predictions of a graph neural network (GNN) from an incremental geometric segmentation created from an RGB-D sequence a). Our method merges nodes on the same object instance and naturally grows and improves over time when new segments and surfaces are discovered, see c). As a by-product, our method produces accurate panoptic segmentation of large-scale 3D scans. The nodes represent the different object segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Scene graphs are a compact and explicit representation successfully used in a variety of 2D scene understanding tasks. This work proposes a method to incrementally build up semantic scene graphs from a 3D environment given a sequence of RGB-D frames. To this end, we aggregate PointNet features from primitive scene components by means of a graph neural network. We also propose a novel attention mechanism well suited for partial and missing graph data present in such an incremental reconstruction scenario. Although our proposed method is designed to run on submaps of the scene, we show it also transfers to entire 3D scenes. Experiments show that our approach outperforms 3D scene graph prediction methods by a large margin and its accuracy is on par with other 3D semantic and panoptic segmentation methods while running at 35Hz.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. We create a globally consistent 3D scene graph b) by fusing predictions of a graph neural network (GNN) from an incremental geometric segmentation created from an RGB-D sequence a). Our method merges nodes on the same object instance and naturally grows and improves over time when new segments and surfaces are discovered, see c). As a by-product, our method produces accurate panoptic segmentation of large-scale 3D scans. The nodes represent the different object segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Scene graphs are a compact and explicit representation successfully used in a variety of 2D scene understanding tasks. This work proposes a method to incrementally build up semantic scene graphs from a 3D environment given a sequence of RGB-D frames. To this end, we aggregate PointNet features from primitive scene components by means of a graph neural network. We also propose a novel attention mechanism well suited for partial and missing graph data present in such an incremental reconstruction scenario. Although our proposed method is designed to run on submaps of the scene, we show it also transfers to entire 3D scenes. Experiments show that our approach outperforms 3D scene graph prediction methods by a large margin and its accuracy is on par with other 3D semantic and panoptic segmentation methods while running at 35Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>High-level scene understanding is a fundamental task in computer vision required for many applications in fields such as robotics and augmented or mixed reality. Boosted by the availability of inexpensive depth sensors, real-time dense SLAM algorithms <ref type="bibr" target="#b41">[35,</ref><ref type="bibr" target="#b28">22,</ref><ref type="bibr" target="#b43">37,</ref><ref type="bibr" target="#b65">59]</ref> and large scale 3D datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b62">56]</ref>, the research focus has shifted from reconstructing the 3D scene geometry to enhancing the 3D maps with semantic information about scene components. Several methods have deployed a neural network to process a complete 3D scan of a scene <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">10,</ref><ref type="bibr" target="#b50">44,</ref><ref type="bibr" target="#b47">41,</ref><ref type="bibr" target="#b31">25,</ref><ref type="bibr" target="#b25">19,</ref><ref type="bibr" target="#b24">18,</ref><ref type="bibr" target="#b23">17,</ref><ref type="bibr" target="#b8">9]</ref>. However, these all require 3D geometry as prior information and they typically operate in an offline fashion, i.e. without satisfying real-time requirements, which are fundamental for many real-world applications. Real-time scene understanding that incrementally built 3D scans poses important challenges such as handling partial, incomplete, and ambiguous scene geometry where object shapes may change dramatically over time. Learning a robust 3D feature that can cope with this variability is difficult. Furthermore, fusing multiple, potentially contradictory network predictions to ensure consistency in the global map, is also challenging. Recently, in the image domain, semantic scene graphs have been used to derive relationships among scene entities <ref type="bibr" target="#b33">[27,</ref><ref type="bibr" target="#b66">60,</ref><ref type="bibr" target="#b42">36,</ref><ref type="bibr" target="#b68">62,</ref><ref type="bibr" target="#b22">16]</ref>. Scene graphs demonstrated to be a powerful abstract representation for scene understanding. Being compact and explicit, they are beneficial for complex tasks such as image captioning <ref type="bibr" target="#b67">[61,</ref><ref type="bibr" target="#b27">21]</ref>, generation <ref type="bibr" target="#b26">[20]</ref>, manipulation <ref type="bibr" target="#b6">[7]</ref> or visual questioning and answering <ref type="bibr" target="#b58">[52]</ref>. For this reason, recent works have explored scene graph prediction from entire 3D scans in an offline manner <ref type="bibr" target="#b63">[57,</ref><ref type="bibr" target="#b0">1]</ref>. Furthermore, building up semantic graph maps online is a major challenge, requiring not only to efficiently detect semantic instances in the scene but also to robustly estimate predicates between them, while dealing with partial and incomplete 3D geometry.</p><p>In this work, we propose a real-time method to incrementally build, in parallel to 3D mapping, a globally consistent semantic scene graph, as shown in <ref type="figure">Fig. 1</ref>. Our approach relies on a geometric segmentation method <ref type="bibr" target="#b56">[50]</ref> and a novel inductive graph network, which handles missing edges and nodes in partial 3D point clouds. Our scene nodes are geometric segments of primitive shapes. Their 3D features are propagated in a graph network that aggregates features of neighborhood segments. Our method predicts scene semantics and identifies object instances by learning relationships among clusters of over-segmented regions. Towards this end, we propose to learn additional relationships, referred to as same part in an end-to-end manner.</p><p>The main contributions of this work can be summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose the first online 3D scene graph prediction, i.e. incrementally fusing predictions from currently observed sub-maps into a globally consistent semantic graph model. (2) Due to a new relationship type, nodes are merged into 3D instances, resembling panoptic segmentation.(3) We introduce a novel attention method that can handle partial and incomplete 3D data, as well as highly dynamic edges, which is required for incremental scene graph prediction. Our experiments show that we outperform 3D scene graph prediction and achieve on par performance on 3D semantic and instance segmentation benchmarks while running in 35Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic SLAM</head><p>Several 3D scene understanding methods leverage deep learning to perform either semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">10,</ref><ref type="bibr" target="#b50">44,</ref><ref type="bibr" target="#b47">41,</ref><ref type="bibr" target="#b25">19]</ref>, or instance segmentation/object detection <ref type="bibr" target="#b24">[18,</ref><ref type="bibr" target="#b45">39,</ref><ref type="bibr" target="#b31">25,</ref><ref type="bibr" target="#b8">9]</ref> from the complete 3D volume or point cloud of the scene. Conversely, incremental semantic SLAM approaches do not assume a full 3D scan to be available, instead directly operate on the incoming frames of RGB(-D) sequences <ref type="bibr" target="#b37">[31,</ref><ref type="bibr" target="#b59">53,</ref><ref type="bibr" target="#b51">45]</ref>. Such methods simultaneously carry out a 3D reconstruction of the scene, while extracting the corresponding semantics of the currently observed surface. To this end, some incremental methods transfer image predictions from a convolutional neural network (CNN) to 3D, passing the data from the image to the 3D recon-struction <ref type="bibr" target="#b37">[31]</ref>. <ref type="bibr" target="#b55">[49]</ref> propose a monocular approach that constructs the 3D geometry from a depth prediction, rather than a depth image. These incremental approaches often require a sophisticated fusion and/or a regularization method to deal with multiple, potentially contradictory, predictions, and to handle spatial and temporal consistency <ref type="bibr" target="#b37">[31,</ref><ref type="bibr" target="#b39">33,</ref><ref type="bibr" target="#b55">49]</ref>. Other approaches fuse the 2D image and 3D reconstruction <ref type="bibr" target="#b69">[63]</ref>. These semantic SLAM methods such as Seman-ticFusion <ref type="bibr" target="#b37">[31]</ref>, ProgressiveFusion <ref type="bibr">[38]</ref> or FusionAware <ref type="bibr" target="#b69">[63]</ref> are able to reconstruct 3D semantic scene maps in real-time, but are not able to differentiate between individual object instances. Object-level SLAM approaches focus on object instances while sometimes requiring prior knowledge of the scene such as an object database or semantic class annotations <ref type="bibr" target="#b53">[47,</ref><ref type="bibr" target="#b57">51,</ref><ref type="bibr" target="#b36">30,</ref><ref type="bibr" target="#b18">12,</ref><ref type="bibr" target="#b21">15]</ref>.</p><p>Segmentation techniques are often used to reduce data complexity and meet the required runtime on limited resources. Several methods <ref type="bibr" target="#b57">[51,</ref><ref type="bibr" target="#b32">26,</ref><ref type="bibr" target="#b38">32,</ref><ref type="bibr" target="#b64">58,</ref><ref type="bibr" target="#b21">15]</ref> incorporate the efficient incremental segmentation method proposed in <ref type="bibr" target="#b56">[50]</ref> to perform online scene understanding.</p><p>Semantic SLAM methods achieve great performance in computing a semantic or object-level representation but neither focus on semantic scene graphs nor on semantic relationships between object instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Scene Graphs for Images and 3D Data</head><p>Graph Neural Networks (GNNs) have recently emerged as a popular inference tool for many challenging tasks <ref type="bibr" target="#b60">[54,</ref><ref type="bibr" target="#b48">42,</ref><ref type="bibr" target="#b35">29,</ref><ref type="bibr" target="#b61">55,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b54">48]</ref>. In particular, GNNs have been proposed to infer scene graphs from images <ref type="bibr" target="#b68">[62,</ref><ref type="bibr" target="#b48">42]</ref>, where scene entities are the nodes of the graph, e.g. object instances. Scene graph prediction goes beyond instance segmentation by adding relationships between instances. Although scene graphs are adapted from computer graphics, their semantic extension has become an important research area in computer vision. Since the introduction of a large scale 2D scene graph dataset <ref type="bibr" target="#b30">[24]</ref>, several graph prediction methods have been proposed focusing on message passing with recurrent neural networks <ref type="bibr" target="#b66">[60]</ref>, iterative statistical optimization <ref type="bibr" target="#b5">[6]</ref> or methods to handle limited data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. Furthermore, recent datasets with 3D semantic scene graph annotations have been proposed <ref type="bibr" target="#b19">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b63">57]</ref>, alongside with 3D graph estimation methods. <ref type="bibr" target="#b63">[57]</ref> predict semantic scene graphs from a ground truth class-agnostic segmentation of the 3D scene. <ref type="bibr" target="#b19">[13]</ref> use an object detector on a sequence of images to construct 3D quadrics -their object representation of choice. The geometric and visual features are then processed with a recurrent neural network. <ref type="bibr" target="#b0">[1]</ref> use mask predictions and a multi-view regularization technique on sampled images to compute relationships derived from detected object instances. They construct a 3D scene graph of a building that includes object semantics, rooms and cameras, as well as the relationships between these entities. <ref type="bibr" target="#b52">[46]</ref>    <ref type="figure">Figure 2</ref>. Overview of the proposed SceneGraphFusion framework. Our method takes a stream of RGB-D images a) as input to create an incremental geometric segmentation b). Then, the properties of each segment and a neighbor graph between segments are constructed. The properties d) and neighbor graph e) of the segments that have been updated in the current frame c) are used as the inputs to compute node and edge features f) and to predict a 3D scene graph g). Finally, the predictions are h) fused back into a globally consistent 3D graph.   <ref type="figure">Figure 3</ref>. A representation of our efficient graph update strategy. Given a network with a basic encoder (Enc l=0 ) and two message passing layers (GNN l=1 , GNN l=2 ), by storing different layers of features separately. When node A is updated, we can reuse the lower-layer features from other nodes without recomputing them. We visualize the operations needed at each node with color.</p><p>extended this model to include dynamic scene entities, e.g. humans. Nevertheless, all of these methods work offline and expect the reconstructed 3D scene as an input. Similarly to <ref type="bibr" target="#b63">[57]</ref>, we predict graphs of semantic nature, but in contrast to <ref type="bibr" target="#b63">[57]</ref>, our graph prediction does not require any prior scene knowledge and is able to segment instances and their semantic information, as well as their relationships, in real-time, while the scene is being reconstructed. <ref type="figure">Fig. 2</ref> illustrates the pipeline of our SceneGraphFusion framework. Our system consists of two separate cores: a reconstruction and segmentation pipeline adapted from <ref type="bibr" target="#b56">[50]</ref> (Sec. 3.1), and a scene graph prediction network (SPN) (Sec. 4). Our system takes a sequence of RGB-D frames with associated poses as input to reconstruct a segmented map of the scene, while estimating a neighbor graph and properties of each segment. Then, a subset of the neighbor graph and the properties of the segments that have been recently observed are fed into our graph network to predict node and edge semantics. Finally, the predictions are fused into the globally consistent 3D scene graph. To maintain real-time performance of our system, we separate the scene graph prediction process into a different thread. The 3D scene graph is asynchronously predicted and fused from the reconstruction pipeline. Our semantic scene graph G consists of a set of tuples (V, E) with nodes V and edges E. Nodes represent segments with their object categories, and edges represent the semantic relationships (predicates) between nodes, such as standing on and attached to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Incremental 3D Scene Graph Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scene Reconstruction with Property Building</head><p>An incremental and computationally efficient method of estimating instances is required to enable construction of a scene graph in real-time. We use the incremental geometrical segmentation method in <ref type="bibr" target="#b56">[50]</ref> to build a globally consistent segmentation map, and incorporate it with our online property update and neighbor graph building.</p><p>Geometric Segmentation and Reconstruction. Given input RGB-D frames and associated poses, the incremental segmentation algorithm generates a global 3D segmentation map, shown in <ref type="figure">Fig. 2b</ref>, by performing incremental segmentation on top of a dense reconstruction algorithm. The 3D segmentation map consists of a set of segments S = {s 1 , . . . , s n }. Each segment stores a set of 3D points P i where each point has a 3D coordinate a normal and a color. Our map is updated at every new frame, by adding new segments and merging or removing old ones.</p><p>Segment Properties. In addition to segment reconstruction, we compute segment properties (see <ref type="figure">Fig. 2d</ref>) to describe a segment shape, i.e. centroid p i ∈ R 3 , standard deviation of the position of points σ i ∈ R 3 , size of the axis-</p><formula xml:id="formula_0">aligned bounding box b i = (b x , b y , b z ) ∈ R 3 , maximum length l i = max (b x , b y , b z ) ∈ R and bounding box volume ν i = b x ·b y ·b z ∈ R.</formula><p>Reconstructing the segments in this incremental manner allows us to update the properties of each node efficiently. These properties are updated by checking every modification of the points in the segment.</p><p>Neighbor Graph. Additionally, we construct a neighbor graph having nodes as segments and edges as the connection between the segments, as depicted in <ref type="figure">Fig. 2e</ref>. To find the adjacent segments, we compute the distances between all the combinations of the bounding box of the segments. The segment pairs where the distance is less than a certain threshold are added as edges (we use 0.5 meters as a proximity threshold in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prediction with Graph Structure</head><p>Next, we feed the segments properties and the neighbor graph to our graph network to predict the segment label and predicate on each segment and edge, shown in <ref type="figure">Fig. 2f</ref>-g. The detailed description of our graph network architecture can be found in Sec. 4. Since our segment reconstruction process is incremental, only those segments that are currently observed in the input frame are updated. Therefore, we only feed a subset of the segments and the neighbor graph which consists of the segments that have been updated in recent frames, this improving scalability and efficiency. To identify the newly updated segments, we store the segment size and timestamp whenever the segment is fed into the network. If the segment size changes more than 10%, or the segment has not been updated for 60 frames, they are flagged and fed into the network. Segments are continuously observed and outdated segments and their neighbors are extracted and processed with our graph neural network. For the sake of efficiency we store all features computed from our SPN in our neighbor graph. According to the message passing process in GNN <ref type="bibr" target="#b20">[14]</ref>, when a lower-layer feature of a node is updated, only the higherlayer features of this node, its direct neighbors, and their edge features are affected. This allows us to re-use previously computed features, as shown in <ref type="figure">Fig. 3</ref>, and greatly improve prediction efficiency and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Scene Graph Fusion</head><p>Finally, the predicted semantics of nodes and edges in the neighbor graph are fused into a globally consistent semantic scene graph, depicted in <ref type="figure">Fig. 2h</ref>. Due to the incremental nature of our method, as described in Sec. 3.2, the semantics of each segment and edge are predicted multiple times, resulting in potentially contradictory outcomes. To handle this, we apply a running average approach <ref type="bibr" target="#b3">[4]</ref> to fuse the predictions of the same segment or edge. For each segment and edge in our neighbor graph, we store a weight w and a probability µ for each class or predicate prediction. Given a new prediction with probability µ t at time t, we update the previously stored weight w t−1 and probability</p><formula xml:id="formula_1">µ t−1 as µ t = µ t · w t + µ t−1 · w t−1 w t + w t−1 ,<label>(1)</label></formula><formula xml:id="formula_2">w t = min w max , w t + w t−1 ,<label>(2)</label></formula><p>where w max = 100 is the maximum weight value. Importantly, since our framework predicts semantics at segment level, we are able to store and preserve the whole label probability distribution using a much smaller memory footprint compared to point-level methods <ref type="bibr" target="#b37">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Scene Graph Prediction</head><p>The use of segments obtained by the geometric segmentation method requires the design of a robust feature, since the shape of each segment is usually incomplete and relatively simple, and changes overtime during reconstruction. The feature of each segment can be enhanced with neighbor information by using a GNN. However, the number of neighbors of each segment changes over time, posing a serious challenge for the training process.</p><p>Dealing with dynamic nodes and edges in a GNN is known as inductive learning. Existing methods focus mainly on how to spread attention across all the neighbors <ref type="bibr" target="#b60">[54,</ref><ref type="bibr" target="#b61">55]</ref>, or estimate the attention between nodes <ref type="bibr" target="#b2">[3]</ref>. However, in either case, a missing edge still affects all the aggregated messages. To deal with this problem, we propose a novel feature-wise-attention (FAT), that re-weights individual latent features at each target node. By applying a max function on this re-weighted embedding, this strategy yields aggregated features that are less affected by missing neighboring points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Architecture</head><p>The network architecture of our framework is shown in <ref type="figure">Fig. 2f</ref>-g (grey box). Our architecture is inspired from <ref type="bibr" target="#b63">[57]</ref>, with modification of some major components. Given a) a set of segments, b) the properties of each segment, and c) a neighbor graph, our network outputs a semantic scene graph by predicting class and predicate for each segment and edge respectively.</p><p>Node Feature. The point cloud P i of each segment s i is encoded with a PointNet <ref type="bibr" target="#b46">[40]</ref> f p (·) into a latent feature that represents the primitive shape of each segment. We concatenate the spatial invariant properties described in Sec. 3.1, i.e. standard deviation σ i , log of bounding box size b i , length l i , and volume ν i , with f p (P i ) to handle the scale insensitive limitation caused by normalization of the input points on the unit sphere such that</p><formula xml:id="formula_3">v i = [f p (P i ) , σ i , ln (b i ), ln (ν i ) ln (l i )],<label>(3)</label></formula><p>where [·] denotes a concatenation function.</p><p>Edge Feature. The visual features of the edges are computed with the properties of the connected segments. Given an edge between a source node i and a target node j where j = i, the edge visual feature e ij is computed such that</p><formula xml:id="formula_4">r ij = [p i − p j , σ i − σ j , b i − b j , ln li lj , ln νi νj ], (4) e ij = g s (r ij ) ,<label>(5)</label></formula><p>where g s (·) is a multi-layer perception (MLP) projecting the paired segment properties into a latent space.</p><p>GNN Feature. After the initial feature embedding on nodes and edges, we propagate the features using a GNN with 2 message passing layers to enhance the features by enclosing the neighborhood information. Our GNN updates both node and edge features in each message passing layer . In each layer, the node and v i and edge features e ij are updated as follows:</p><formula xml:id="formula_5">v +1 i = g v [v i , max j∈N (i) FAN v i , e ij , v j ] ,<label>(6)</label></formula><formula xml:id="formula_6">e +1 ij = g e [v i , e ij , v j ] ,<label>(7)</label></formula><p>where g v (·) and g e (·) are MLPs, N (i) is the set of neighbors indices of node i, and FAN(·) is the proposed featurewise attention network, which is detailed in Sec. 4.2.</p><p>Class Prediction and Losses. Finally, the node class and the edge predicate are predicted by means of two MLP classifiers. Similarly to <ref type="bibr" target="#b63">[57]</ref>, our network can be trained end-toend with a joint cross entropy loss, for both, object L obj and predicates L pred .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature-wise Attention</head><p>Our feature-wise attention (FAT) module takes as input a query Q of dimensions d q and targets T of dimensions d τ . It estimates a weight distribution of dimensions d τ by using a MLP g a (·) with a softmax operation to normalize and distribute the weight. Then, the attention is calculated by element-wise multiplication of the weight matrix and the target T,</p><formula xml:id="formula_7">FAT(Q, T) = softmax (g a (Q)) T,<label>(8)</label></formula><p>where denotes element-wise multiplication. The use of softmax across the entire target dimension d τ gives us a single weight matrix across all feature dimensions. We employ a multi-head approach as in <ref type="bibr" target="#b60">[54,</ref><ref type="bibr" target="#b54">48]</ref> to allow a more flexible attention distribution. The input feature dimension of Q and T are divided into h heads Q = [q 1 , . . . , q h ] and T = [τ 1 , . . . , τ h ] with q i ∈ R dq/h and τ i ∈ R dτ /h . For each head, the same attention function as in equation <ref type="formula" target="#formula_7">(8)</ref> is applied, then the values from each ground truth instance segmentation same part relationship of geometric segmentation <ref type="figure">Figure 4</ref>. The same part relationship is generated between the segments corresponding to the same object instance.</p><p>head are concatenated back to the dimensions d τ to obtain the multi-head attention:</p><formula xml:id="formula_8">MFAT (Q, T) = FAT(q i , τ i ) h i=1 .<label>(9)</label></formula><p>Unlike scaled dot-product attention <ref type="bibr" target="#b60">[54]</ref>, our approach does not distribute across edges. Instead, it learns to spread the attention across the feature dimensions of each target node. Towards this end, we design a feature-wise attention network (FAN). Given a source node feature v i , an edge feature e ij , and a target node feature v j , we compute the weighted message as</p><formula xml:id="formula_9">FAN (v i , e ij , v j ) = MFAT ([ĝ q (v i ) ,ĝ e (e ij )] ,ĝ τ (v j )) ,<label>(10)</label></formula><p>whereĝ q (·) ,ĝ e (·) ,ĝ τ (·) are single layer perceptrons to map v i , e ij , v j to dimensions dq 2 , dq 2 , d τ respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Data Generation</head><p>We introduce a same part relationship to allow our network to cluster segments from the same object, enabling instance-level object segmentation on the over-segmented map. We generate training and testing data using the estimated segmentation and the ground truth instance annotations. Given a scene segmented by a geometrical segmentation method and its corresponding ground truth provided as object instance annotations, we find the best match between each estimated segment and the ground truth objects via nearest neighbor search. In particular, the best match is obtained by maximizing the area of intersection between the given segment and the ground truth objects. We reject matches where the area of intersection is less than 50% of the segment surface. In addition, we consider a valid match only if its corresponding segment does not cover any other ground truth objects by more than 10% of their area. In the case of multiple segments corresponding to the same object instance, we add the same part relationship between all of them, as shown in <ref type="figure">Fig. 4</ref>. Finally, if ground truth relationships exist on that object instance, they are inherited by all the segments. The experiments were conducted on the complete 3D data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>In Sec. 6.1 we evaluate our scene graph prediction on 3DSSG <ref type="bibr" target="#b63">[57]</ref>. The performance of our method is reported on full scenes given ground truth instances and second with geometric segments. We then show how relationships/graphs help with object prediction. In Sec. 6.2 we focus on the byproduct of our method, panoptic segmentation by reporting segmentation scores on ScanNet <ref type="bibr" target="#b4">[5]</ref>. Finally, in Sec. 6.3 we provide a runtime analysis of our method compared to other incremental semantic segmentation approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Semantic Scene Graph Prediction</head><p>Following the evaluation scheme in <ref type="bibr" target="#b63">[57]</ref>, we separately report relationship, object, and predicate prediction accuracy with a top-n evaluation metric. Following <ref type="bibr" target="#b68">[62]</ref>, the relationship score is the multiplication of the object, subject, and predicate probability. Object and predicate metrics are calculated directly with the respective classification scores.</p><p>Ground Truth Instances. In Tbl. 1 we report the 3D scene graph prediction accuracy independently from the segmentation quality. The evaluation was conducted on the full 3D scene with the class-agnostic ground truth segmentation, as carried out in <ref type="bibr" target="#b63">[57]</ref>. We followed the data split proposed in 3DSSG with 160 object classes and 26 different relationships. Our method outperforms <ref type="bibr" target="#b63">[57]</ref> with a significant margin of +0.45 / +0.21 (R@50 / R@100) for relationship prediction due to small improvements in predicate and object classification. Note that our method can run offline on the pre-computed 3D data -as done here -but is also able to handle partial and incomplete shapes in an incremental online setup which is analyzed in following paragraph.</p><p>Geometric Segments. In Tbl. 2 we compare the performance of incremental 6 -7 and full scene graph prediction 5 based on our geometric segmentation. 6 is slightly worse than 5 but generates predictions on the fly. Our proposed fusion 7 improves the performance further. Tbl. 2 additionally shows that we outperform 3DSSG <ref type="bibr" target="#b63">[57]</ref> with a small margin without any attention method and with a large margin when using our proposed feature-wise attention, FAT <ref type="bibr" target="#b4">5</ref> . FAT 5 also outperforms other attention mechanisms GAT <ref type="bibr" target="#b61">[55]</ref>  scene graph prediction. The input of the methods is either the full 3D scene 1 -6 , processed offline (f ) or a stream of RGB-D images processed incrementally (i), <ref type="bibr">6 , 7</ref> . For these experiments, we first acquired the geometric segmentation <ref type="bibr" target="#b55">[49]</ref> from the RGB-D sequences of 3RScan <ref type="bibr" target="#b62">[56]</ref>. The final training data was generated with the pipeline described in Sec. 5. We trained the networks with 20 NYUv2 <ref type="bibr" target="#b40">[34]</ref> object classes used on the ScanNet <ref type="bibr" target="#b4">[5]</ref> benchmark. Furthermore, only support predicates are used and relationships with too few occurrences are ignored. This leads to 8 predicates, including the same part relationship which we added in the data generation process. More details on the training setup and chosen hyper-parameters used in this experiment can be found in the supplementary material. A qualitative result of our graph prediction is shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, more examples can also be found in the supplementary.</p><p>Predicate Influence on Object Classification. To verify if learning inter-instance relationship improves object classification, we train our network without the predicate loss. Tbl. 3 shows that object classification indeed benefits from joint relationship prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">3D Panoptic/Semantic Segmentation</head><p>To evaluate the quality of the semantic/panoptic segmentation of our method, we trained the network with Scan-Net <ref type="bibr" target="#b4">[5]</ref>. We follow the ScanNet benchmark and evaluate with the IoU metric. Since InSeg <ref type="bibr" target="#b56">[50]</ref> reconstructs and segments the scene with a different reconstruction algorithm and excludes small and unstable geometric segments, some   points might be missing in our 3D map. When evaluating, we address this issue by either a) mapping the points in our reconstruction to the nearest neighbor (NN) of the ScanNet ground truth 3D model or b) ignoring points where no corresponding 3D geometry was reconstructed.</p><p>3D Semantic Segmentation. In Tbl. 5, we compare our method against other incremental semantic segmentation methods, specifically SemanticFusion <ref type="bibr" target="#b37">[31]</ref>, ProgressiveFusion [38] and FusionAware <ref type="bibr" target="#b69">[63]</ref> using the mean average precision (mAP). Our method has the second best mAP while running at 35Hz on a CPU, as detailed in Sec. 6.3 and the supplementary material. Qualitative results of our semantic segmentation are shown in the bottom row of <ref type="figure" target="#fig_4">Fig. 6</ref>.</p><p>3D Panoptic Segmentation. To evaluate panoptic segmentation we use the metrics proposed in <ref type="bibr" target="#b29">[23]</ref>, namely panoptic quality (PQ), segmentation quality (SQ), and recognition quality (RQ). In Tbl. 4 we compare our method against PanopticFusion <ref type="bibr" target="#b39">[33]</ref>. Due to the missing scene geometry on which our approach relies, PanopticFusion outperforms our method with respect to the computed RQ. Nevertheless, SQ and PQ are on par or slightly worse. A comparison of only valid scene regions -by skipping unreconstructed parts -often results in a better performance. We provide an ablation study in Tbl. 6 to validate the effectiveness of the same part relationship and our proposed fusion mechanism. Finally, the qualitative results of our panoptic segmentation are shown in the top row of <ref type="figure" target="#fig_4">Fig. 6</ref>  <ref type="table">Table 6</ref>. Ablation study: Analysis of the effect of our fusion mechanism and the same part relationship on the panoptic segmentation task evaluated on ScanNet <ref type="bibr" target="#b4">[5]</ref>. PQ stands for panoptic, SQ for segmentation, and RQ for recognition quality.</p><p>Robustness against Missing Information. In this experiment, we evaluate the robustness of different attention methods against noisy data in form of missing edges. We train our network without attention, using GAT <ref type="bibr" target="#b61">[55]</ref>, SDPA <ref type="bibr" target="#b60">[54]</ref>, and our proposed FAT. The experimental setup is shared with Tbl. 2. In Tbl. 7 we compare the performance of the different attention methods on the full scene (f ) and all edges (@1.0, left column) and with a random edge drop of 50% (@0.5, right column), see Tbl. 7. The reference metric is the intersection over union (IoU). Our proposed attention mechanism, FAT, consistently outperforms the other approaches in full-edge and drop-edge scenarios. For the sake of space, a more detailed per-class avg. IoU (@1.0) avg. IoU (@0.5) Ours w/o attention 33.5 29.5 Ours SDPA <ref type="bibr" target="#b60">[54]</ref> 33.0 29.7 Ours GAT <ref type="bibr" target="#b61">[55]</ref> 11.5 12.5 Ours FAT 49.3 41.9 evaluation is available in the supplementary, interestingly showing that some classes rely on the messages from neighbors more than others such as e.g. bathtub, shower curtain, and windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Runtime Analysis</head><p>We measured the runtime of our system on the ScanNet sequence scene0645 01. Our machine is equipped with an Intel Core i7-8700 CPU 3.2GHz CPU with 12 threads. Notably, our method only uses 2 threads: one for the scene reconstruction and the other one for 3D scene graph prediction. The scene reconstruction requires 28 ms on average while the graph prediction sums up to 133ms running the GNN and fusing the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we presented SceneGraphFusion, a 3D scene graph method that incrementally fuses partial graph predictions from a geometric segmentation into a globally consistent semantic map. Our network outperforms other 3D scene graph prediction methods; FAT works better than any other attention mechanism in handling missing graph information and the semantic/panoptic segmentation -the by-product of our method -achieves performance on par with other incremental methods while running in 35Hz. Due to this efficiency, incremental semantic scene graphs could be beneficial in future work, when retrieving camera poses or detecting loop-closures in a SLAM framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Network Architecture</head><p>We use FC(in, out) denote a fully-connected layer, and MLP(·, ..., ·) as a set of FC layers with a ReLU activation between each FC layer. Our PointNet encoder f p , is a shared-weight MLP(64, 128, 512) followed by a maximum pooling operation to obtain a global feature. The other layers are listed in Tbl. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Training Details</head><p>All the models in our evaluation section (Sec. 6) were trained with the same set up but with different training data for 150 epochs. We use AdamW <ref type="bibr" target="#b34">[28]</ref> optimizer with Amsgrad <ref type="bibr" target="#b49">[43]</ref> and an adaptive learning rate, inverse proportional to the log of the number of edges. Given a training batch with n edges and lr base = 1e −3 , the base learning rate in AdamW is adjusted as follows lr = lr base 1 ln n .</p><p>The training data are the 3D reconstructions created from RGB-D sequences. In order to train our network to handle partial data, subgraphs are randomly extracted during training time. In each iteration, two segments are randomly selected together with their four-hop neighbor segments. We further randomly discard edges with a dropout rate of 50%.</p><p>In addition, we randomly sample points in each segment. The properties described in Sec 3.1 are computed based on sampled points. For the training loss, we follow the approach in <ref type="bibr" target="#b63">[57]</ref> with a weight factor of 0.1 between the object and predicate loss. We use two message passing layers, each with 8 heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Experiment Details</head><p>In this section, we detailed the training dataset and the hyper-parameters used in the experiment section (Sec 6.1 Geometric Segments) of our main paper. As mentioned in the main paper, 20 NYUv2 <ref type="bibr" target="#b40">[34]</ref> object classes are used. For predicates, we focus on support relationships. We further filter out rare relationship. A predicate is discarded if it occurs less than 10 times in the training data or less then 5 times in the test data. This leaves us with 8 predicates, i.e. supported by, attached to, standing on, hanging on, connected to, part of, build in, and same part.</p><p>The geometrical segmentation method <ref type="bibr" target="#b56">[50]</ref> in our framework uses the pyramid level of 2, which scales the input image with a factor of 2, for image segmentation. Further, we filter out segments with less than 512 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">3D Panoptic Segmentation</head><p>On Tbl. 12 we report the complete panoptic segmentation evaluation on Tbl. 4. With respect to the panoptic quality (PQ), our method outperforms PanopticFusion in 7 out of 20 classes. The PQ can be broken down into segmentation quality (SQ) and recognition quality (RQ). The SQ evaluates only the matched segments, via an intersection over union (IoU) score over 50%. RQ is known as the F 1 score. Our method has a similar SQ performance as Panop-ticFusion while performing worse when compared with the RQ metric. This is likely due to missing scene geometry caused by the incremental segmentation <ref type="bibr" target="#b56">[50]</ref> that our approach relies on. By using a metric that is less influenced by missing points, i.e. SQ, or ignoring the missing points in the evaluation, our method has equivalent or slightly better performance compare to PanopticFusion <ref type="bibr" target="#b39">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.">Robustness against Missing Information</head><p>Tbl. 11 shows the complete experiment mentioned in Sec 6.2 of the main paper. We use our network architecture with different attention methods. The update of the node feature v i in equation 7 can be re-written as follows:</p><formula xml:id="formula_11">v +1 i = g e [v i , Φ j∈N (i) (Ψ (·))] ,<label>(12)</label></formula><p>where Φ is a permutation in-variance function, e.g. sum, mean or max, and Ψ(·) represents an attention function. For without, we set</p><formula xml:id="formula_12">Ψ(·) to Ψ(v j ) = v j with Φ = . For SDPA, the Ψ(·) is set to f sdpa (v i , v j ) with Φ = ,</formula><p>where f sdps is the multi-head attention method <ref type="bibr" target="#b60">[54]</ref> and for GAT, we directly use the Pytorch implementation <ref type="bibr" target="#b17">[11]</ref> to update nodes respectively.</p><p>The proposed attention method consistently outperforms others even when the edges are dropped by 50%. There are some classes that are more robust to missing information, such as a chair, curtain, desk, floor, and wall while some classes are more dependant on others such as bathtub, bed, shower curtain, and window. We visualize this effect by plotting the difference of the confusion matrix with and without dropping of edges, see <ref type="figure" target="#fig_5">Fig. 7</ref>. It can be seen that beds and pictures are easier to predict when full edges are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.">Runtime Analysis</head><p>In the following a more detailed runtime analysis is given in terms of the number of segments and data re-usage and graph structure update.</p><p>We report the analysis using scene scene0645 01 which consists of 5230 paired RGB and depth images. The average update of node, edge, GNN features and the class predictions are listed on Tbl. 9. The computation time overtime is reported on <ref type="figure" target="#fig_6">Fig. 8</ref>. By updating node and edge features with our graph structure, the computation time is significantly reduced. Again, our scene graph prediction method runs in a different thread and will only block the main thread in the data copy and fusion stage.   <ref type="table">Table 9</ref>. The average number and time of computation on each feature computation process on the sequence of scene0645 01</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7.">Qualitative Result</head><p>We demonstrate more qualitative results in the 3D scene graph prediction on both 3RScan <ref type="bibr" target="#b62">[56]</ref> and ScanNet <ref type="bibr" target="#b4">[5]</ref>. Note that ScanNet does not have ground truth relationships. We therefore use the trained model with 3RScan to do inference on ScanNet scenes. Our method is able to handle the domain gap across these two datasets and predicts reasonable 3D scene graphs on ScanNet scenes.</p><p>The results are shown on <ref type="figure">Fig. 9, Fig. 10</ref> and <ref type="figure">Fig. 11</ref>. Segments are represented by circles and estimated object instances are drawn as rectangles. In our visualization, the class prediction of a segment is correct if no label is shown in the circle and wrong otherwise.</p><p>As for relationship prediction, we use green, red and blue to indicate the correct, wrong, and unknown predictions respectively. An unknown prediction is a case where no ground truth data is available. The label on an edge without bracket is the predicted label, with bracket is its ground truth label. To simplify the visualization, we ignore nonerelationships and merge segments with same part relationships in the same box. We also group up predictions of segments with the same label within the same box. The indication of such a grouped prediction is shown by connecting box to box. As for the wrongly predicted segments, their predicted probability remains individual. This indicated with an edge from a circle to a box. Function Layer Definition g v , g a MLP(768, 768, 512) g e MLP(1280, 768, 256) g q ,ĝ e FC(512, 512) g τ FC(256, 256) <ref type="table">Table 10</ref>. Parameters of the layers in our GNN. FC(·, ·) represents fully connected layer, and MLP(·, ..., ·) represent FC(·, ·) layers with ReLU activation between them.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative evaluation of our incremental graph prediction. Node (circle) colors represent geometric segments shown on the left, the corresponding predicted semantics (panoptic segmentation) is visualized in the center, corresponding to the boxes in the right. For visualization purposes, we only show the biggest segments and filter out small ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative semantic and panoptic segmentation results of SceneGraphFusion. Note that for 3D panoptic segmentation (top row), random colors are used for object instances, while walls are and the floor is .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The difference of the confusion matrix without and with (50%) dropping of edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>The computation time of the scene graph prediction over time. bath bed bkshf cab. chair cntr. curt. desk door floor ofurn pic. refri. show. sink sofa table toil wall wind. avg without 50.0 3.9 0.0 27.0 51.7 16.7 62.2 20.0 16.4 96.2 15.4 8.0 4.3 11.1 52.5 45.9 54.2 41.7 67.0 26.2 33.5 SDPA[54] 50.0 11.1 2.3 26.0 45.7 17.7 65.2 3.9 18.7 87.4 11.2 4.8 2.5 29.4 38.1 60.8 36.8 65.0 60.1 24.0 33.0 GAT[55] 22.0 5.7 0.0 10.9 22.8 11.0 37.6 1.8 9.4 19.8 3.1 1.3 0.0 0.0 10.4 33.0 12.0 8.7 8.7 11.9 11.5 ours 83.3 24.3 0.0 43.4 69.8 30.0 68.7 4.5 29.6 98.1 26.6 10.0 34.5 66.7 65.0 74.7 54.2 86.5 75.3 41.7 49.3 without@p50 37.5 3.7 0.0 24.8 49.9 13.3 52.3 17.3 14.9 86.8 19.9 5.0 5.8 6.2 46.7 36.4 37.3 46.0 63.5 22.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc><ref type="bibr" target="#b2">3</ref> and SDPA<ref type="bibr" target="#b60">[54]</ref> <ref type="bibr" target="#b3">4</ref> for 3D semantic Evaluation of the semantic scene graph prediction on geometric segments of 3RScan/3DSSG<ref type="bibr" target="#b63">[57]</ref> with 20 objects and 8 predicate classes. (f ) indicates a prediction on the full 3D scene while (i) is the incremental result from the RGB-D sequence.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Relationship Object</cell><cell>Predicate</cell></row><row><cell cols="2">Method (Attention)</cell><cell cols="3">R@1 R@3 R@1 R@3 R@1 R@2</cell></row><row><cell>1 3DSSG (none)</cell><cell></cell><cell cols="3">(f ) 0.38 0.59 0.61 0.85 0.83 0.98</cell></row><row><cell>2 Ours (none)</cell><cell></cell><cell cols="3">(f ) 0.41 0.62 0.62 0.88 0.84 0.98</cell></row><row><cell>3 Ours (GAT)</cell><cell></cell><cell cols="3">(f ) 0.12 0.22 0.25 0.64 0.85 0.98</cell></row><row><cell>4 Ours (SDPA)</cell><cell></cell><cell cols="3">(f ) 0.39 0.62 0.62 0.87 0.85 0.98</cell></row><row><cell>5 Ours (FAT)</cell><cell></cell><cell cols="3">(f ) 0.55 0.78 0.75 0.93 0.86 0.98</cell></row><row><cell>6 Ours (FAT)</cell><cell></cell><cell cols="3">(i) 0.51 0.67 0.78 0.94 0.77 0.98</cell></row><row><cell cols="5">7 Ours Fusion (FAT) (i) 0.52 0.70 0.79 0.94 0.78 0.98</cell></row><row><cell></cell><cell cols="2">Relationship</cell><cell>Object</cell><cell>Predicate</cell></row><row><cell></cell><cell cols="4">R@1 R@3 R@1 R@3 R@1 R@2</cell></row><row><cell cols="3">Ours without L pred 0.26 0.36</cell><cell>0.62 0.87</cell><cell>0.59 0.75</cell></row><row><cell>Ours with L pred</cell><cell cols="2">0.55 0.78</cell><cell>0.75 0.93</cell><cell>0.86 0.98</cell></row><row><cell cols="5">Table 3. Ablation Study: Comparison of training with and without</cell></row><row><cell cols="5">predicate loss L pred on 3RScan/3DSSG [57] with 20 object and</cell></row><row><cell cols="5">8 predicate classes. Note that the comparison is based on graphs</cell></row><row><cell cols="3">computed from the full 3D scene (f ).</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table Door</head><label>Door</label><figDesc></figDesc><table><row><cell></cell><cell>good job -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>haha</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Window</cell><cell>Bookshelf</cell><cell>Refrigerator</cell></row><row><cell>Picture</cell><cell>Counter</cell><cell>Desk</cell><cell>Curtain</cell><cell>Toilet</cell><cell>Sink</cell><cell>Bathtub</cell><cell>Other Furniture</cell><cell>Shower Curtain</cell></row><row><cell></cell><cell>scene0643_00</cell><cell></cell><cell></cell><cell></cell><cell>scene0015_00</cell><cell></cell><cell></cell><cell>scene0693_00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc></figDesc><table><row><cell cols="5">Ablation study: Segment classification of InSeg [50] on</cell></row><row><cell cols="5">3RScan [56] reporting avg. IoU on segment-level. The complete</cell></row><row><cell cols="5">per-class evaluation can be found in the supplementary material.</cell></row><row><cell></cell><cell cols="2">Segmentation Node</cell><cell>Edge</cell><cell>GNN</cell></row><row><cell>Mean [ms]</cell><cell>28</cell><cell>8</cell><cell>17</cell><cell>108</cell></row><row><cell cols="5">Table 8. Runtime [ms] of the different components of our method.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 .</head><label>11</label><figDesc>Ablation study: Segment classification of InSeg [50] on 3RScan [56] reporting avg. IoU on segment-level.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors thank Anees B. Kazi and Georam Vivar for fruitfull discussions. This work is supported by the German Research Foundation (DFG, project number 407378162) and the Bavarian State Ministry of Education, Science and the Arts in the framework of the Centre Digitisation Bavaria (ZD.B).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D SceneGgraph: A Structure for Unified Semantics, 3D Space, and Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scene Graph Prediction With Limited Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paroma</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent-Graph Learning for Disease Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Volumetric Method for Building Complex Models from Range Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">Xuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting Visual Relationships with Deep Relational Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic Image Manipulation Using Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helisa</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azade</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual Relationships as Functions: Enabling Few-Shot Scene Graph Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorva</forename><surname>Dornadula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Narcomey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation. In metric all things stuff bath bed bkshf cab. chair cntr. curt. desk door floor ofurn pic. refri. show. sink sofa table toil wall wind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<idno>NN mapping) PQ 31.5 30.2 43</idno>
	</analytic>
	<monogr>
		<title level="j">Ours</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">67</biblScope>
		</imprint>
	</monogr>
	<note>6 25.4 13.9 22.2 47.2 10.5 16.4 12.6 26.4 56.4 22.9 31.3 28.0 38.3 38.0 32.3 34.8 63.2 30.4 11</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ours (skip missing) PQ 36</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<idno>NN mapping) SQ 72.9 73.0 72.6 80.6 68.2 66.9 71.1 76.5 61.7 75.1 63.8 77.4 74.8 71.6 81.5 77.8 79.1 75.4 65.3 73.3 80.2 70.4 68.2 Ours (skip missing) SQ 76</idno>
	</analytic>
	<monogr>
		<title level="j">Ours</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<idno>NN mapping) RQ 42.2 40.3 59.3 83.9</idno>
	</analytic>
	<monogr>
		<title level="j">Ours</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page">61</biblScope>
		</imprint>
	</monogr>
	<note>1 21.8 19.7 34.1 75.4 31.9 38.5 36.0 48.5 50.3 49.5 47.5 78.9 43.2 17.1</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Ours (skip missing) RQ</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The full 3D panoptic segmentation results on the ScanNet v2 open test set</title>
		<imprint/>
	</monogr>
	<note>Table 12</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference Computer Vision Workshops</title>
		<imprint>
			<publisher>ICCVW</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Alexander Hermans, and Bastian Leibe</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshops (ICLRW)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incremental Object Database: Building 3D Models from Multiple Partial Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadri</forename><surname>Furrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonci</forename><surname>Novkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Fehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gawel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margarita</forename><surname>Grinvald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual Graphs from Motion (VGfM): Scene Understanding with Object Geometry Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Gay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio Del</forename><surname>Bue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grinvald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Furrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Novkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RAL)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scene Graph Generation with External Knowledge and Image Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">OccuSeg: Occupancy-aware 3D Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">Tex-tureNet: Consistent Local Parametrizations for Learning from High-Resolution Signals on Meshes. IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image Generation from Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-Time 3D Reconstruction in Dynamic Scenes Using Point-Based Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maik</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Lefloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weyrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kolb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Panoptic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalanditis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D Instance Segmentation via Multi-Task Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Incremental Scene Understanding on Dense SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene Graph Generation from Objects, Phrases and Region Captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Manessi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Manzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fusion++: Volumetric Object-Level SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast and Accurate Semantic Mapping through Geometric-based Incremental Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikatsu</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideo</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff and Things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaku</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Seno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohsuke</forename><surname>Kaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Pushmeet Kohli Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">KinectFusion: Real-Time Dense Surface Mapping and Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Mixed and Augmented Reality</title>
		<meeting><address><addrLine>Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Pushmeet Kohli</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pixels to Graphs by Associative Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Real-time 3D Reconstruction at Scale using Voxel Hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time Progressive 3D Semantic Segmentation for Indoor Scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference Applications Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep Hough Voting for 3D Object Detection in Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attentive Relational Networks for Mapping Images to Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengshi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the Convergence of Adam and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully-Convolutional Point Networks for Large-Scale Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosinol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans. Computing Research Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Rosinol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingnan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Carlone</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hauke</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">SuperGlue: Learning Feature Matching with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">CNN-SLAM: Real-Time Dense Monocular SLAM with Learned Depth Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Real-Time and Scalable Incremental Segmentation on Dense SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">When 2.5D is not enough: Simultaneous Reconstruction, Segmentation and Recognition on dense SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph-Structured Representations for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">SemanticPaint: Interactive 3D Labeling and Learning at your Fingertips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">RIO: 3D Object Instance Re-Localization in Changing Indoor Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Avetisyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning 3D Semantic Scene Graphs from 3D Indoor Reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helisa</forename><surname>Dhamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Real-Time Fully Incremental Scene Understanding on Mobile Platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RAL)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">ElasticFusion: Dense SLAM Without A Pose Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">Salas</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Graph R-CNN for Scene Graph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
