<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Domain Adaptation with Manifold Embedded Distribution Alignment *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
							<email>yqchen@ict.ac.cnfengwenjie@soware.ict.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
							<email>han.yu@ntu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
							<email>huangmeiyu@qxslab.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">NTU Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Qian Xuesen Lab. of Space Technology, CAST</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UIC Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">with Institute for Data Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Domain Adaptation with Manifold Embedded Distribution Alignment *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3240508.3240512</idno>
					<note>978-1-4503-5665-7/18/10. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain Adaptation</term>
					<term>Transfer Learning</term>
					<term>Distribution Alignment</term>
					<term>Subspace Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual domain adaptation aims to learn robust classi ers for the target domain by leveraging knowledge from a source domain. Existing methods either a empt to align the cross-domain distributions, or perform manifold subspace learning. However, there are two signi cant challenges: (1) degenerated feature transformation, which means that distribution alignment is o en performed in the original feature space, where feature distortions are hard to overcome. On the other hand, subspace learning is not su cient to reduce the distribution divergence. (2) unevaluated distribution alignment, which means that existing distribution alignment methods only align the marginal and conditional distributions with equal importance, while they fail to evaluate the di erent importance of these two distributions in real applications. In this paper, we propose a Manifold Embedded Distribution Alignment (MEDA) approach to address these challenges. MEDA learns a domain-invariant classi er in Grassmann manifold with structural risk minimization, while performing dynamic distribution alignment to quantitatively account for the relative importance of marginal and conditional distributions. To the best of our knowledge, MEDA is the rst a empt to perform dynamic distribution alignment for manifold domain adaptation. Extensive experiments demonstrate that MEDA shows signi cant improvements in classi cation accuracy compared to state-of-the-art traditional and deep methods. *   e rst two authors contributed equally. † J. Wang and Y. Chen are also a liated with Beijing Key Lab. of Mobile Computing and Pervasive Devices. W. Feng is also with CAS Key Lab. of Network Data Science &amp; Technology. J. Wang and W. Feng are also a liated with University of Chinese Academy of Sciences. ‡ P. Yu is also a liated</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>e rapid growth of online media and content sharing applications has stimulated a great demand for automatic recognition and analysis for images and other multimedia data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref>. Unfortunately, it is o en expensive and time-consuming to acquire su cient labeled data to train machine learning models. us, it is o en necessary to leverage the abundant labeled samples in some existing domains to facilitate learning in a new target domain. Domain adaptation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref> has been a promising approach to solve such cross-domain learning problems.</p><p>Since the distributions of the source and target domains are di erent, the key to successful adaptation is to reduce the distribution divergence. To this end, existing work can be summarized into two main categories: (a) instance reweighting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39]</ref>, which reuses samples from the source domain according to some weighting technique; and (b) feature matching, which either performs subspace learning by exploiting the subspace geometrical structure <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref>, or distribution alignment to reduce the marginal or conditional distribution divergence between domains <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40]</ref>. Our focus is on feature matching methods. ere are two signi cant challenges in existing methods, i.e. degenerated feature transformation and unevaluated distribution alignment.</p><p>Degenerated feature transformation means that both subspace learning and distribution alignment can only reduce, but not remove the distribution divergence <ref type="bibr" target="#b0">[1]</ref>. Speci cally, subspace learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref> conducts subspace transformation to obtain be er feature representations. However, feature divergence is not eliminated a er subspace transformation <ref type="bibr" target="#b21">[22]</ref> since subspace learning only utilizes the subspace or manifold structure, but fails to perform feature alignment. On the other hand, distribution alignment <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref> usually reduces the distribution distance in the original feature space, where features are o en distorted <ref type="bibr" target="#b2">[3]</ref> which makes it hard to reduce the divergence between domains. erefore, it is critical to exploit both the advantages of subspace learning and distribution alignment to further facilitate domain adaptation. Unevaluated distribution alignment means that existing work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref> only a empted to align the marginal and conditional distributions with equal weights. But they failed to evaluate the relative importance of these two distributions. For example, when two domains are very dissimilar <ref type="figure" target="#fig_0">(Figure 1</ref>(a) → 1(b)), the marginal distribution is more important to align. When the marginal distributions are close <ref type="figure" target="#fig_0">(Figure 1(a) → 1(c)</ref>), the conditional distribution should be given more weight. However, there is no alignment method which can quantitatively account for the importance of these two distributions in conjunction.</p><p>As far as we know, there has been no previous work that tackle these two challenges together. In this paper, we propose a novel Manifold Embedded Distribution Alignment (MEDA) method to address the challenges of both degenerated feature transformation and unevaluated distribution alignment. MEDA learns a domain-invariant classi er in Grassmann manifold with structural risk minimization, while performing dynamic distribution alignment by considering the di erent importance of marginal and conditional distributions. We also provide a feasible solution to quantitatively evaluate the importance of distributions. To the best of our knowledge, MEDA is the rst a empt to reveal the relative importance of marginal and conditional distributions in domain adaptation.</p><p>is work makes the following contributions: 1) We propose the MEDA approach for domain adaptation. MEDA is capable of addressing both the challenges of degenerated feature transformation and unevaluated distribution alignment.</p><p>2) We provide the rst quantitative evaluation of the relative importance of marginal and conditional distributions in domain adaptation. is is signi cantly useful in future research on transfer learning.</p><p>3) Extensive experiments on 7 real-world image datasets demonstrate that compared to several state-of-the-art traditional and deep methods, MEDA achieves a signi cant improvement of 3.5% in average classi cation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>MEDA substantially distinguishes from existing feature matching domain adaptation methods in several aspects:</p><p>Subspace learning. Subspace Alignment (SA) <ref type="bibr" target="#b12">[13]</ref> aligned the base vectors of both domains, but failed to adapt feature distributions. Subspace distribution alignment (SDA) <ref type="bibr" target="#b30">[31]</ref> extended SA by adding the subspace variance adaptation. However, SDA did not consider the local property of subspaces and ignored conditional distribution alignment. CORAL <ref type="bibr" target="#b29">[30]</ref> aligned subspaces in secondorder statistics, but it did not consider the distribution alignment. Sca er component analysis (SCA) <ref type="bibr" target="#b13">[14]</ref> converted the samples into a set of subspaces (i.e. sca ers) and then minimized the divergence between them. GFK <ref type="bibr" target="#b14">[15]</ref> extended the idea of sampled points in manifold <ref type="bibr" target="#b15">[16]</ref> and proposed to learn the geodesic ow kernel between domains. e work of <ref type="bibr" target="#b3">[4]</ref> used a Hellinger distance to approximate the geodesic distance in Riemann space. <ref type="bibr" target="#b2">[3]</ref> proposed to use Grassmann for domain adaptation, but they ignored the conditional distribution alignment. Di erent from these approaches, MEDA can learn a domain-invariant classi er in the manifold and align both marginal and conditional distributions.</p><p>Distribution alignment. MEDA substantially di ers from existing work that only align marginal or conditional distribution <ref type="bibr" target="#b25">[26]</ref>. Joint distribution adaptation (JDA) <ref type="bibr" target="#b22">[23]</ref> proposed to match both distributions with equal weights. Others extended JDA by adding regularization <ref type="bibr" target="#b21">[22]</ref>, sparse representation <ref type="bibr" target="#b37">[38]</ref>, structural consistency <ref type="bibr" target="#b18">[19]</ref>, domain invariant clustering <ref type="bibr" target="#b32">[33]</ref>, and label propagation <ref type="bibr" target="#b39">[40]</ref>. e main di erences between MEDA and these methods are: 1) ese work treats the two distributions equally. However, when there is a greater discrepancy between both distributions, they cannot evaluate their relative importance and thus lead to undermined performance. Our work is capable of evaluating the quantitative importance of each distribution via considering their di erent e ects. 2) ese methods are designed only for the original space, where feature distortion will hinder the performance. MEDA can align the distributions in the manifold to overcome the feature distortions.</p><p>Domain-invariant classi er learning. e recent work of ARTL <ref type="bibr" target="#b21">[22]</ref>, DIP <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, and DMM <ref type="bibr" target="#b6">[7]</ref> also aimed to build a domain-invariant classi er. However, ARTL and DMM can be undermined by feature distortion in original space, and they failed to leverage the di erent importance of distributions. DIP mainly focused on feature transformation and only aligned marginal distributions. MEDA is able to avoid the feature distortion and quantitatively evaluate the importance of marginal and conditional distribution alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MANIFOLD EMBEDDED DISTRIBUTION ALIGNMENT</head><p>In this section, we present the Manifold Embedded distribution alignment (MEDA) approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem De nition</head><p>Given a labeled source domain</p><formula xml:id="formula_0">D s = {x s i , s i } n i=1 and an unlabeled target domain D t = {x t j } n+m j=n+1 , assume the feature space X s = X t , label space Y s = Y t , but marginal probability P s (x s ) P t (x t ) with conditional probability Q s ( s |x s ) Q t ( t |x t )</formula><p>. e goal of domain adaptation is to learn a classi er f : x t → y t to predict the labels y t ∈ Y t for the target domain D t using labeled source domain D s .</p><p>According to the structural risk minimization (SRM) <ref type="bibr" target="#b34">[35]</ref>, f = arg min f ∈H K (f (x), y) + R(f ), where the rst term indicates the loss on data samples, the second term denotes the regularization term, and H K is the Hilbert space induced by kernel function K(·, ·).</p><p>Since there is no labels on D t , we can only perform SRM on D s . Moreover, due to the di erent distributions between D s and D t , it is necessary to add other constraints to maximize the distribution consistency while learning f . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Main Idea</head><p>MEDA consists of two fundamental steps. Firstly, MEDA performs manifold feature learning to address the challenge of degenerated feature transformation. Secondly, MEDA performs dynamic distribution alignment to quantitatively account for the relative importance of marginal and conditional distributions to address the challenge of unevaluated distribution alignment. Eventually, a domain-invariant classi er f can be learned by summarizing these two steps with the principle of SRM. <ref type="figure" target="#fig_1">Figure 2</ref> presents the main idea of the proposed MEDA approach. Formally, if we denote (·) the manifold feature learning functional, then f can be represented as</p><formula xml:id="formula_1">f = arg min f ∈ n i =1 H K (f ( (x i )), i ) + η|| f || 2 K + λD f (D s , D t ) + ρR f (D s , D t )<label>(1)</label></formula><p>where || f || 2 K is the squared norm of f . e term D f (·, ·) represents the proposed dynamic distribution alignment. Additionally, we introduce R f (·, ·) as a Laplacian regularization to further exploit the similar geometrical property of nearest points in manifold G <ref type="bibr" target="#b4">[5]</ref>. η, λ, and ρ are regularization parameters accordingly. e overall learning process of MEDA is in Algorithm 1. In next sections, we rst introduce manifold feature learning (learn (·)). en, we present the dynamic distribution alignment (learn D f (·, ·)). Eventually, we articulate the learning of f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Manifold Feature Learning</head><p>Manifold feature learning serves as the preprocessing step to eliminate the threat of degenerated feature transformation. MEDA learns (·) in the Grassmann manifold G(d) <ref type="bibr" target="#b17">[18]</ref> since features in the manifold have some geometrical structures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> that can avoid distortion in the original space. And G can facilitate classi er learning by treating the original d-dimensional subspace (i.e. feature vector) as its basic element <ref type="bibr" target="#b3">[4]</ref>. Additionally, feature transformation and distribution alignment o en have e cient numerical forms and can thus facilitate domain adaptation on G(d) <ref type="bibr" target="#b17">[18]</ref>. ere are several approaches to transform the features into G <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>, among which we embed Geodesic Flow Kernel (GFK) <ref type="bibr" target="#b14">[15]</ref> to learn (·) for its computational e ciency. We only introduce the main idea of GFK and the details can be found in its original paper.</p><p>When learning manifold features, MEDA tries to model the domains with d-dimensional subspaces and then embed them into G. Let S s and S t denote the PCA subspaces for the source and target domain, respectively. G can thus be regarded as a collection of all d-dimensional subspaces. Each original subspace can be seen as a point in G. erefore, the geodesic ow {Φ(t) : 0 ≤ t ≤ 1} between two points can draw a path for the two subspaces. If we let S s = Φ(0) and S t = Φ(1), then nding a geodesic ow from Φ(0) to Φ(1) equals to transforming the original features into an in nite-dimensional feature space, which eventually eliminates the domain shi . is kind of approach can be seen as an incremental way of 'walking' from Φ(0) to Φ(1). Speci cally, the new features can be represented as z = (x) = Φ(t) T x. From <ref type="bibr" target="#b14">[15]</ref>, the inner product of transformed features z i and z j gives rise to a positive semide nite geodesic ow kernel:</p><formula xml:id="formula_2">z i , z j = ∫ 1 0 (Φ(t) T x i ) T (Φ(t) T x j ) dt = x T i Gx j<label>(2)</label></formula><p>us, the feature in original space can be transformed into Grassmann manifold with z = (x) = √ Gx. G can be computed e ciently by singular value decomposition <ref type="bibr" target="#b14">[15]</ref>. Note that √ G is only an expression form and cannot be computed directly, while its square root is calculated by Denman-Beavers algorithm <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dynamic Distribution Alignment</head><p>e purpose of dynamic distribution alignment is to quantitatively evaluate the importance of aligning marginal (P) and conditional (Q) distributions in domain adaptation. Existing methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40]</ref> failed in this evaluation by only assuming that both distributions are equally important. However, this assumption may not be realistic for real applications. For instance, when transferring from <ref type="figure" target="#fig_0">Figure  1</ref>(a) to 1(b), there is a large di erence between datasets. erefore, the divergence between P s and P t is more dominant. In contrast, from <ref type="figure" target="#fig_0">Figure 1</ref>(a) to 1(c), the datasets are similar. erefore, the distribution divergence in each class (Q s and Q t ) is more dominant. e adaptive factor: In view of this phenomenon, we introduce an adaptive factor to dynamically leverage the importance of these two distributions. Formally, the dynamic distribution alignment D f is de ned as</p><formula xml:id="formula_3">D f (D s , D t ) = (1 − µ)D f (P s , P t ) + µ C c=1 D (c) f (Q s , Q t )<label>(3)</label></formula><p>where µ ∈ [0, 1] is the adaptive factor and c ∈ {1, · · · , C} is the class indicator. D f (P s , P t ) denotes the marginal distribution alignment, and D</p><p>(c) f (Q s , Q t ) denotes the conditional distribution alignment for class c.</p><p>When µ → 0, it means that the distribution distance between the source and the target domains is large. us, marginal distribution alignment is more important <ref type="figure" target="#fig_0">(Figure 1</ref>(a) → 1(b)). When µ → 1, it means that feature distribution between domains is relatively small, so the distribution of each class is dominant. us, the conditional distribution alignment is more important <ref type="figure" target="#fig_0">(Figure 1</ref>(a) → 1(c)). When µ = 0.5, both distributions are treated equally as in existing methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40]</ref>. Hence, the existing methods can be regarded as the special cases of MEDA. By learning the optimal adaptive factor µ opt (which we will discuss later), MEDA can be applied to di erent domain adaptation problems.</p><p>We use the maximum mean discrepancy (MMD) <ref type="bibr" target="#b5">[6]</ref> to empirically calculate the distribution divergence between domains. As a nonparametric measurement, MMD has been widely applied in many existing methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>, and its theoretical e ectiveness has been veri ed in <ref type="bibr" target="#b16">[17]</ref>. e MMD distance between distributions p and q is de ned as</p><formula xml:id="formula_4">d 2 (p, q) = (E p [ϕ(z s )] − E q [ϕ(z t )]) 2 H K</formula><p>where H K is the reproducing kernel Hilbert space (RKHS) induced by feature map ϕ(·). Here, E[·] denotes the mean of the embedded samples. In order to compute an MMD associated with f , we adopt projected MMD <ref type="bibr" target="#b27">[28]</ref> and compute the marginal distribu-</p><formula xml:id="formula_5">tion alignment as D f (P s , P t ) = E[f (z s )] − E[f (z t )] 2 H K . Sim- ilarly, the conditional distribution alignment is D (c) f (Q s , Q t ) = E[f (z (c) s )] − E[f (z (c) t )] 2 H K</formula><p>. en, dynamic distribution alignment can be expressed as</p><formula xml:id="formula_6">D f (D s , D t ) =(1 − µ) E[f (z s )) − E[f (z t )] 2 H K + µ C c=1 E[f (z (c) s )] − E[f (z (c) t )] 2 H K (4)</formula><p>Note that since D t has no labels, it is not feasible to evaluate the conditional distribution Q t = Q t ( t |z t ). Instead, we follow the idea in <ref type="bibr" target="#b36">[37]</ref> and use the class conditional distribution Q t (z t | t ) to approximate Q t . In order to evaluate Q t (z t | t ), we apply prediction to D t using a base classi er trained on D s to obtain so labels for D t . e so labels may be less reliable, so we iteratively re ne the prediction. Note that we only use the base classi er in the rst iteration. A er that, MEDA can automatically re ne the labels for D t using results from previous iteration. e quantitative evaluation of the adaptive factor µ: We can treat µ as a parameter and tune its value by crossvalidation techniques. However, there is no labels for the target domain in unsupervised domain adaptation problems. It is extremely hard to calculate the value of µ. In this work, we made the rst a empt towards calculating µ (i.e.μ) by exploiting the global and local structure of domains. We adopted the A-distance <ref type="bibr" target="#b5">[6]</ref> as the basic measurement. e A-distance is de ned as the error of building a linear classi er to discriminate two domains (i.e. a binary classi cation). Formally, we denote ϵ(h) the error of a linear classi er h discriminating the two domains D s and D t . en, the A-distance can be de ned as</p><formula xml:id="formula_7">µ ≈ 1 − d M d M + C c=1 d c<label>(6)</label></formula><p>is estimation has to be conducted at every iteration of the dynamic distribution adaptation, since the feature distribution may vary a er evaluating the conditional distribution each time. To be noticed, this is the rst solution to quantitatively estimate the relative importance of each distribution. In fact, this estimation can be of signi cant help in future research on transfer learning and domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Learning Classi er f</head><p>A er manifold feature learning and dynamic distribution alignment, f can be learned by summarizing SRM over D s and distribution alignment. Adopting the square loss l 2 , f can be represented as</p><formula xml:id="formula_8">f = arg min f ∈H K n i=1 ( i − f (z i )) 2 + η|| f || 2 K + λD f (D s , D t ) + ρR f (D s , D t )<label>(7)</label></formula><p>In order to perform e cient learning, we now reformulate each term in detail.</p><p>SRM on the Source Domain: Using the representer theorem <ref type="bibr" target="#b4">[5]</ref>, f admits the expansion</p><formula xml:id="formula_9">f (z) = n+m i=1 β i K(z i , z)<label>(8)</label></formula><p>where β = (β 1 , β 2 , · · · ) T ∈ R (n+m)×1 is the coe cients vector and K is a kernel. en, SRM on D s can be</p><formula xml:id="formula_10">n i=1 ( i − f (z i )) 2 + η|| f || 2 K = n+m i=1 A ii ( i − f (z i )) 2 + η|| f || 2 K = ||(Y − β T K)A|| 2 F + ηtr(β T Kβ)<label>(9)</label></formula><p>where || · || F is the Frobenious norm. K ∈ R (n+m)×(n+m) is the kernel matrix with</p><formula xml:id="formula_11">K i j = K(z i , z j ), and A ∈ R (n+m)×(n+m) is a diagonal domain indicator matrix with A ii = 1 if i ∈ D s , otherwise A ii = 0. Y = [ 1 , · · ·</formula><p>, n+m ] is the label matrix from source and the target domains. tr(·) denotes the trace operation. Although the labels for D t are unavailable, they can be ltered out by the indicator matrix A. Dynamic distribution alignment: Using the representer theorem and kernel tricks, dynamic distribution alignment in equation (4) becomes</p><formula xml:id="formula_12">D f (D s , D t ) = tr β T KMKβ<label>(10)</label></formula><p>where M = (1 − µ)M 0 + µ C c=1 M c is the MMD matrix with its element calculated by</p><formula xml:id="formula_13">(M 0 ) i j =          1 n 2 , z i , z j ∈ D s 1 m 2 , z i , z j ∈ D t − 1 mn , otherwise<label>(11)</label></formula><p>Visual Domain Adaptation with Manifold Embedded Distribution Alignment MM '18, Oct. <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr">2018</ref>, Seoul, Republic of Korea</p><formula xml:id="formula_14">(M c ) i j =                      1 n 2 c , z i , z j ∈ D (c) s 1 m 2 c , z i , z j ∈ D (c) t − 1 m c n c , z i ∈ D (c) s , z j ∈ D (c) t z i ∈ D (c) t , z j ∈ D (c) s 0, otherwise<label>(12)</label></formula><p>where n c = |D (c) s | and m c = |D (c) t |. Laplacian Regularization: Additionally, we add a Laplacian regularization term to further exploit the similar geometrical property of nearest points in manifold G <ref type="bibr" target="#b4">[5]</ref>. We denote the pair-wise a nity matrix as</p><formula xml:id="formula_15">W i j = sim(z i , z j ), z i ∈ N p (z j ) or z j ∈ N p (z i ) 0, otherwise<label>(13)</label></formula><p>where sim(·, ·) is a similarity function (such as cosine distance) to measure the distance between two points. N p (z i ) denotes the set of p-nearest neighbors to point z i . p is a free parameter and must be set in the method. By introducing Laplacian matrix L = D − W with diagonal matrix D ii = n+m j=1 W i j , the nal regularization can be expressed by</p><formula xml:id="formula_16">R f (D s , D t ) = n+m i, j=1 W i j (f (z i ) − f (z j )) 2 = n+m i, j=1 f (z i )L i j f (z j ) = tr β T KLKβ<label>(14)</label></formula><p>Overall Reformulation: Substituting with equations (9), (10) and <ref type="bibr" target="#b13">(14)</ref>, f in equation <ref type="formula" target="#formula_8">(7)</ref> can be reformulated as</p><formula xml:id="formula_17">f = arg min f ∈H K ||(Y − β T K)A|| 2 F + η tr(β T Kβ) + tr β T K(λM + ρL)Kβ<label>(15)</label></formula><p>Se ing derivative ∂ f /∂β = 0, we obtain the solution</p><formula xml:id="formula_18">β = ((A + λM + ρL)K + ηI) −1 AY T<label>(16)</label></formula><p>MEDA has a nice property: it can learn the cross-domain function directly without the need of explicit classi er training. is makes it signi cantly di erent from most existing work such as JGSA <ref type="bibr" target="#b39">[40]</ref> and CORAL <ref type="bibr" target="#b29">[30]</ref> that further needs to learn a certain classi er.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND EVALUATIONS</head><p>In this section, we evaluate the performance of MEDA through extensive experiments on large-scale public datasets. e source code for MEDA is available at h p://transferlearning.xyz/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preparation</head><p>We adopted seven publicly image datasets: O ce+Caltech10, USPS + MNIST, ImageNet + VOC2007, and O ce-31. ese datasets are popular for benchmarking domain adaptation algorithms and have been widely adopted in most existing work such as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. <ref type="table" target="#tab_0">Table 1</ref> lists the statistics of the seven datasets.</p><p>O ce-31 <ref type="bibr" target="#b28">[29]</ref> consists of three real-world object domains: Amazon (A), Webcam (W) and DSLR (D). It has 4,652 images with 31</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Manifold Embedded Distribution Alignment</head><p>Input: Data matrix X = [X s , X t ], source domain labels y s , manifold subspace dimension d, regularization parameters λ, η, ρ, and #neighbor p. Output: Classi er f . <ref type="bibr">1:</ref> Learn manifold feature transformation kernel G via equation (2), and get manifold feature Z = √ GX. 2: Train a base classi er using D s , then apply prediction on D t to get its so labelsˆ t . 3: Construct kernel K using transformed features Z s = Z 1:n,: and Z t = Z n+1:n+m,: . 4: repeat <ref type="bibr">5:</ref> Calculate the adaptive factorμ using equation <ref type="bibr" target="#b5">(6)</ref>. and compute M 0 and M c by equations <ref type="formula" target="#formula_1">(11)</ref> and <ref type="bibr" target="#b11">(12)</ref>. <ref type="bibr">6:</ref> Compute β by solving equation <ref type="formula" target="#formula_1">(16)</ref> and obtain f via the representer theorem in equation <ref type="bibr" target="#b7">(8)</ref>. <ref type="bibr">7:</ref> Update the so labels of D t :ˆ t = f (Z t ). 8: until Convergence 9: return Classi er f . ere are 10 common classes in the two datasets. For our experiments, we adopted the O ce+Caltech10 datasets from <ref type="bibr" target="#b14">[15]</ref> which contains 12 tasks: A → D, A → C,…, C → W. In the rest of the paper, we use A → B to denote the knowledge transfer from source domain A to the target domain B.</p><p>USPS (U) and MNIST (M) are standard digit recognition datasets containing handwri en digits from 0-9. Since the same digits across two datasets follow di erent distributions, it is necessary to perform domain adaptation. USPS consists of 7,291 training images and 2,007 test images of size 16 × 16. MNIST consists of 60,000 training images and 10,000 test images of size 28 × 28. We construct two tasks: U → M and M → U.</p><p>ImageNet (I) and VOC2007 (V) are large standard image recognition datasets. Each dataset can be treated as one domain. e images from the same classes of two domains follow di erent distributions. In our experiments, we adopt the sub-datasets presented in <ref type="bibr" target="#b11">[12]</ref> to construct cross-domain tasks. Five common classes are extracted from both datasets: bird, cat, chair, dog, and person. Eventually, we have two tasks: I → V and V → I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">State-of-the-art Comparison Methods</head><p>We compared the performance of MEDA with several state-of-theart traditional and deep domain adaptation approaches.</p><p>Traditional learning methods:   • 1NN, SVM, and PCA • Transfer Component Analysis (TCA) <ref type="bibr" target="#b25">[26]</ref>, which performs marginal distribution alignment • Geodesic Flow Kernel (GFK) <ref type="bibr" target="#b14">[15]</ref>, which performs manifold feature learning • Joint distribution alignment (JDA) <ref type="bibr" target="#b22">[23]</ref>, which adapts both marginal and conditional distribution • Transfer Joint Matching (TJM) <ref type="bibr" target="#b23">[24]</ref>, which adapts marginal distribution with source sample selection • Adaptation Regularization (ARTL) <ref type="bibr" target="#b21">[22]</ref>, which learns domain classi er in original space • CORrelation Alignment (CORAL) <ref type="bibr" target="#b29">[30]</ref>, which performs second-order subspace alignment • Sca er Component Analysis (SCA) <ref type="bibr" target="#b13">[14]</ref>, which adapts scatters in subspace • Joint Geometrical and Statistical Alignment (JGSA) <ref type="bibr" target="#b39">[40]</ref>, which aligns marginal &amp; conditional distributions with label propagation • Distribution Matching Machine (DMM) <ref type="bibr" target="#b6">[7]</ref>, which learns a transfer SVM to align distributions And deep domain adaptation methods:</p><p>• AlexNet <ref type="bibr" target="#b20">[21]</ref>, which is a standard convnet • Deep Domain Confusion (DDC) <ref type="bibr" target="#b33">[34]</ref>, which is a singlelayer deep adaptation method with MMD loss • Deep Adaptation Network (DAN) <ref type="bibr" target="#b24">[25]</ref>, which is a multilayer adaptation method with multiple kernel MMD • Deep CORAL (DCORAL) <ref type="bibr" target="#b31">[32]</ref>, which is a deep neural network with CORAL loss • Deep Unsupervised Convolutional Domain Adaptation (DUCDA) <ref type="bibr" target="#b40">[41]</ref>, which is based on a ention and CORAL loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Setup</head><p>For fair comparison, we follow the same protocols as <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>   Parameter se ing: e optimal parameters of all comparison methods are set according to their original papers. As for MEDA, we set the manifold feature dimension d = 20, 30, 40 for O ce+Caltech10, USPS+MNIST, and ImageNet+VOC datasets, respectively. e iteration number are set to T = 10. We use the RBF kernel with the bandwidth set to be the variance of inputs. e regularization parameters are set as p = 10, λ = 10, η = 0.1, and ρ = 1. e approach of se ing these parameters are in the supplementary le. Additionally, the experiments on parameter sensitivity and convergence analysis in later experiments (Section 4.6 and 4.7) indicate that MEDA stays robust with a wide range of parameter choices.</p><p>We adopt classi cation Accuracy on D t as the evaluation metric, which is widely used in existing literatures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref>: Accurac = |x:x∈ D t ∧ˆ (x)= (x) | |x:x∈ D t | , where (x) andˆ (x) are the truth and predicted labels for target domain, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results and Analysis</head><p>e classi cation accuracy results on the aforementioned datasets are shown in Tables 2, 3, and 4, respectively 1 . From those results, we can make several observations as follows.</p><p>Firstly, MEDA outperformed all other traditional and deep comparison methods in most tasks (21/28 tasks). e average classication accuracy of MEDA on 28 tasks was 73.2%. Compared to the best baseline method JGSA (69.7%), the average performance improvement was 3.5%, which showed a signi cant average error reduction of 11.6%. Note that the results on O ce-31 dataset were in the supplementary le 2 due to space constraints, and the observations are the same. Since these results were obtained from a  <ref type="table">Table 6</ref>: Performance comparison between µ opt andμ. wide range of image datasets, it demonstrates that MEDA is capable of signi cantly reducing the distribution divergence in domain adaptation problems. Secondly, the performances of distribution alignment methods (TCA, JDA, ARTL, TJM, JGSA, and DMM) and subspace learning methods (GFK, CORAL, and SCA) were generally worse than MEDA. Each kind of methods has its limitations and cannot handle domain adaptation in speci c tasks.</p><formula xml:id="formula_19">Task C → A W → D C → A (DeCaf) W → C (DeCaf) M → U I → V</formula><p>is indicates the disadvantages of those methods to cope with degenerated feature transformation and unevaluated distribution alignment. A er manifold or subsapce learning, there still exists large domain shi <ref type="bibr" target="#b2">[3]</ref>; while feature distortion will undermine the distribution alignment methods.</p><p>irdly, MEDA also outperformed the deep methods (AlexNet, DDC, DAN, DCORAL, and DUCDA) on O ce+Caltech10 datasets. Deep methods o en have to tune a lot of hyperparameters before obtaining the optimal results. Compared to them, MEDA only involves several parameters that can easily be set by human experience or cross-validation. is implies the accuracy and e ciency of MEDA in domain adaptation problems over other deep methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">E ectiveness Analysis</head><p>4.5.1 Manifold Feature Learning. We investigate the e ectiveness of manifold feature learning in handling the degenerated feature transformation challenge. To this end, we ran MEDA with and without manifold feature learning on randomly selected tasks. <ref type="table" target="#tab_5">Table 5</ref> showed the mean, standard deviation, and performance improvement of classi cation accuracy with µ ∈ {0, 0.1, · · · , 1}. For instance, the improvement of mean accuracy on task C → A was: (56.5 − 44.9)/44.9 × 100% = 25.8%. From these results, we can observe that: 1) e performance of all the tasks were improved with manifold feature learning, indicating that transforming features into the manifold alleviates domain shi to some extent and facilitates distribution alignment; 2) e standard deviation of methods that adopted manifold learning with di erent µ could be dramatically reduced. 3) MEDA can also reach a comparable performance without manifold learning, while adding manifold learning would produce be er results. is reveals the e ectiveness of manifold feature learning to alleviate degenerated feature transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Dynamic Distribution Alignment.</head><p>We verify the e ectiveness of dynamic distribution alignment in handling the unevaluated distribution alignment challenge. We ran MEDA by searching µ ∈ {0, 0.1, · · · , 0.9, 1.0} and compared the performances with the best baseline method (JGSA). From the results in <ref type="figure">Figure 3</ref>, we can  clearly observe that the classi cation accuracy varied with di erent choice of µ. is indicates the necessity to consider the di erent e ects between marginal and conditional distributions. We can also observe that the optimal µ value varied on di erent tasks (µ = 0.2, 0, 1 for three tasks, respectively). us, it is necessary to dynamically adjust the distribution alignment between domains according to di erent tasks. Moreover, the optimal value of µ is not unique on certain task. e classi cation results may be the same even for di erent µ. e estimation of µ: We evaluate our solution of estimating µ (equation <ref type="formula" target="#formula_7">(6)</ref>). Since the optimal µ is not unique, we can not directly compare the value of µ opt andμ to evaluate our solution. Instead, we compare the performances (accuracy values) achieved by µ opt andμ. e results in <ref type="table">Table 6</ref> indicated that the performance of estimatedμ was very close to µ opt , and sometimes it is be er than grid search (M → U). For instance, the performance variation of C → A was (57.0 − 56.5)/57.0 × 100% = 0.9%. is demonstrates that the e ectiveness in estimating µ. is estimation solution can be directly applied to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Evaluation of Each Component.</head><p>When learning the nal classi er f , MEDA involves three components: the structural risk minimization (SRM), the dynamic distribution alignment (DA), and Laplacian regularization (Lap). We empirically evaluated the importance of each component. We randomly selected several tasks and reported the results in <ref type="figure">Figure 4</ref>. Note that we did not run this experiment on the Decaf features of O ce+Caltech10 dataset since its results are already satis ed.</p><p>ose results clearly indicated that each component is important in MEDA, and they are indispensable. Moreover, we observe that in all tasks, it is more important to align the distributions. e reason is that there exists large distribution divergence between two domains. e results also suggests that adding Laplacian regularization is more bene cial in capturing the manifold structure. Additionally, combining the e ectiveness of manifold feature learning (Section 4.5.1), it is clear that all components are important for improving the accuracy in domain adaptation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Parameter Sensitivity</head><p>As with other state-of-the-art domain adaptation algorithms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40]</ref>, MEDA also involves several parameters. In this section, we evaluate the parameter sensitivity. Due to lack of space, we only report the main results in this paper. Other results can be found in the supplementary le. Experimental results demonstrated the robustness of MEDA under a wide range of parameter choices. erefore, the parameters do not need to be ne-tuned in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Subspace Dimension and #neighbor.</head><p>We investigated the sensitivity of manifold subspace dimension d and #neighbor p through experiments with a wide range of d ∈ {10, 20, · · · , 100} and p ∈ {2, 4, · · · , 64} on randomly selected tasks. From the results in <ref type="figure" target="#fig_4">Figure 5</ref>(a) and 5(b), it can be observed that MEDA was robust with regard to di erent values of d and p. erefore, they can be selected without knowledge in real applications. 4.6.2 Regularization Parameters. We ran MEDA with a wide range of values for regularization parameters λ, η, and ρ on several random tasks and compare its performance with the best baseline method. For the lack of space, we only report the results of λ in <ref type="figure" target="#fig_4">Figure 5</ref>(c), and the results of ρ and η can be found in the supplementary le. We observed that MEDA can achieve a robust performance with regard to a wide range of parameter values. Speci cally, the best choices of these parameters are: λ ∈ [0.5, 1, 000], η ∈ [0.01, 1], and ρ ∈ [0.01, 5]. To sum up, the performance of MEDA stays robust with a wide range of regularization parameter choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Convergence and Time Complexity</head><p>We validated the convergence of MEDA through empirical analysis. From the results in <ref type="figure" target="#fig_4">Figure 5(d)</ref>, it can be observed that MEDA can reach a steady performance in only a few (T &lt; 10) iterations. It indicates the training advantage of MEDA in cross-domain tasks.</p><p>We also empirically checked the time complexity of MEDA and compared it with other top two baselines ARTL and JGSA on different tasks. e environment was an Intel Core i7-4790 CPU with 24 GB memory. Note that the time complexity of deep methods are not comparable with MEDA since they require a lot of backpropagations. e results in <ref type="table" target="#tab_7">Table 7</ref> reveal that except its superiority in classi cation accuracy, MEDA also achieved a running time complexity comparable to top two best baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we propose a novel Manifold Embedded Distribution Alignment (MEDA) approach for visual domain adaptation. Compared to existing work, MEDA is the rst a empt to handle the challenges of both degenerated feature transformation and unevaluated distribution alignment. MEDA can learn the domain-invariant classi er with the principle of structural risk minimization while performing dynamic distribution alignment. We also provide a feasible solution to quantitatively calculate the adaptive factor. We conducted extensive experiments on several large-scale publicly available image classi cation datasets. e results demonstrate the superiority of MEDA against other state-of-the-art traditional and deep domain adaptation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>arXiv:1807.07258v2 [cs.CV] 28 Jul 2018 (a) Source (b) Target: Type I (c) Target: Type II Examples of two di erent target domains w.r.t. the same source domain during distribution adaption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>e main idea of MEDA. 1 Features in the original space are transformed into manifold space by learning the manifold kernel G. 2 Dynamic distribution alignment (by learning µ) with SRM is performed in manifold to learn the nal domain-invariant classi er f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Accuracy in original (le ) and manifold space (right) with di erent µ. Dashed lines are best baseline. Evaluation of each component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>(a)∼(c): classi cation accuracy w.r.t. d, p, and λ, respectively. (d) convergence analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the seven benchmark datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Sample #Feature #Class</cell><cell>Domain</cell></row><row><cell>O ce-10</cell><cell>1,410</cell><cell>800 (4,096)</cell><cell>10</cell><cell>A, W, D</cell></row><row><cell>Caltech-10</cell><cell>1,123</cell><cell>800 (4,096)</cell><cell>10</cell><cell>C</cell></row><row><cell>O ce-31</cell><cell>4,652</cell><cell>4,096</cell><cell>31</cell><cell>A, W, D</cell></row><row><cell>USPS</cell><cell>1,800</cell><cell>256</cell><cell>10</cell><cell>USPS (U)</cell></row><row><cell>MNIST</cell><cell>2,000</cell><cell>256</cell><cell>10</cell><cell>MNIST (M)</cell></row><row><cell>ImageNet</cell><cell>7,341</cell><cell>4,096</cell><cell>5</cell><cell>ImageNet (I)</cell></row><row><cell>VOC2007</cell><cell>3,376</cell><cell>4,096</cell><cell>5</cell><cell>VOC (V)</cell></row><row><cell cols="5">categories. Caltech-256 (C) contains 30,607 images and 256 cat-</cell></row><row><cell cols="5">egories. Since the objects in O ce and Caltech follow di erent</cell></row><row><cell cols="5">distributions, domain adaptation can help to perform cross-domain</cell></row><row><cell>recognition.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) on O ce+Caltech10 datasets using SURF features. C → W 25.8 41.7 34.6 39.3 37.0 39.3 39.0</figDesc><table><row><cell>Task</cell><cell cols="6">1NN SVM PCA TCA GFK JDA TJM CORAL SCA ARTL JGSA MEDA</cell></row><row><cell>C → A</cell><cell>23.7 53.1 39.5 45.6 46.0 43.1 46.8</cell><cell>52.1</cell><cell>45.6</cell><cell>44.1</cell><cell>51.5</cell><cell>56.5</cell></row><row><cell></cell><cell></cell><cell>46.4</cell><cell>40.0</cell><cell>31.5</cell><cell>45.4</cell><cell>53.9</cell></row><row><cell>C → D</cell><cell>25.5 47.8 44.6 45.9 40.8 49.0 44.6</cell><cell>45.9</cell><cell>47.1</cell><cell>39.5</cell><cell>45.9</cell><cell>50.3</cell></row><row><cell>A → C</cell><cell>26.0 41.7 39.0 42.0 40.7 40.9 39.5</cell><cell>45.1</cell><cell>39.7</cell><cell>36.1</cell><cell>41.5</cell><cell>43.9</cell></row><row><cell cols="2">A → W 29.8 31.9 35.9 40.0 37.0 38.0 42.0</cell><cell>44.4</cell><cell>34.9</cell><cell>33.6</cell><cell>45.8</cell><cell>53.2</cell></row><row><cell cols="2">A → D 25.5 44.6 33.8 35.7 40.1 42.0 45.2</cell><cell>39.5</cell><cell>39.5</cell><cell>36.9</cell><cell>47.1</cell><cell>45.9</cell></row><row><cell cols="2">W → C 19.9 28.8 28.2 31.5 24.8 33.0 30.2</cell><cell>33.7</cell><cell>31.1</cell><cell>29.7</cell><cell>33.2</cell><cell>34.0</cell></row><row><cell cols="2">W → A 23.0 27.6 29.1 30.5 27.6 29.8 30.0</cell><cell>36.0</cell><cell>30.0</cell><cell>38.3</cell><cell>39.9</cell><cell>42.7</cell></row><row><cell cols="2">W → D 59.2 78.3 89.2 91.1 85.4 92.4 89.2</cell><cell>86.6</cell><cell>87.3</cell><cell>87.9</cell><cell>90.5</cell><cell>88.5</cell></row><row><cell>D → C</cell><cell>26.3 26.4 29.7 33.0 29.3 31.2 31.4</cell><cell>33.8</cell><cell>30.7</cell><cell>30.5</cell><cell>29.9</cell><cell>34.9</cell></row><row><cell cols="2">D → A 28.5 26.2 33.2 32.8 28.7 33.4 32.8</cell><cell>37.7</cell><cell>31.6</cell><cell>34.9</cell><cell>38.0</cell><cell>41.2</cell></row><row><cell cols="2">D → W 63.4 52.5 86.1 87.5 80.3 89.2 85.4</cell><cell>84.7</cell><cell>84.4</cell><cell>88.5</cell><cell>91.9</cell><cell>87.5</cell></row><row><cell cols="2">Average 31.4 41.1 43.6 46.2 43.1 46.8 46.3</cell><cell>48.8</cell><cell>45.2</cell><cell>44.3</cell><cell>50.0</cell><cell>52.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%) on USPS+MNIST and ImageNet+VOC2007 datasets.</figDesc><table><row><cell>Task</cell><cell cols="6">1NN SVM PCA TCA GFK JDA TJM CORAL SCA ARTL JGSA MEDA</cell></row><row><cell cols="2">U → M 44.7 62.2 45.0 51.2 46.5 59.7 52.3</cell><cell>30.5</cell><cell>48.0</cell><cell>67.7</cell><cell>68.2</cell><cell>72.1</cell></row><row><cell cols="2">M → U 65.9 68.2 66.2 56.3 61.2 67.3 63.3</cell><cell>49.2</cell><cell>65.1</cell><cell>88.8</cell><cell>80.4</cell><cell>89.5</cell></row><row><cell>I → V</cell><cell>50.8 52.4 58.4 63.7 59.5 63.4 63.7</cell><cell>59.6</cell><cell>-</cell><cell>62.4</cell><cell>52.3</cell><cell>67.3</cell></row><row><cell>V → I</cell><cell>38.2 42.7 65.1 64.9 73.8 70.2 73.0</cell><cell>70.3</cell><cell>-</cell><cell>72.2</cell><cell>70.6</cell><cell>74.7</cell></row><row><cell cols="2">Average 49.9 56.3 58.7 59.0 60.2 65.1 63.1</cell><cell>52.4</cell><cell>-</cell><cell>72.8</cell><cell>67.9</cell><cell>75.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy (%) on O ce+Caltech10 datasets using DeCaf6 features.</figDesc><table><row><cell>Task</cell><cell cols="17">Traditional Methods 1NN SVM PCA TCA GFK JDA TJM SCA ARTL JGSA CORAL DMM AlexNet DDC DAN DCORAL DUCDA Deep Methods</cell><cell>MEDA</cell></row><row><cell>C → A</cell><cell>87.3</cell><cell>91.6</cell><cell>88.1</cell><cell>89.8</cell><cell>88.2</cell><cell>89.6</cell><cell>88.8</cell><cell>89.5</cell><cell>92.4</cell><cell>91.4</cell><cell>92.0</cell><cell>92.4</cell><cell>91.9</cell><cell>91.9</cell><cell>92.0</cell><cell>92.4</cell><cell>92.8</cell><cell>93.4</cell></row><row><cell cols="2">C → W 72.5</cell><cell>80.7</cell><cell>83.4</cell><cell>78.3</cell><cell>77.6</cell><cell>85.1</cell><cell>81.4</cell><cell>85.4</cell><cell>87.8</cell><cell>86.8</cell><cell>80.0</cell><cell>87.5</cell><cell>83.7</cell><cell>85.4</cell><cell>90.6</cell><cell>91.1</cell><cell>91.6</cell><cell>95.6</cell></row><row><cell>C → D</cell><cell>79.6</cell><cell>86.0</cell><cell>84.1</cell><cell>85.4</cell><cell>86.6</cell><cell>89.8</cell><cell>84.7</cell><cell>87.9</cell><cell>86.6</cell><cell>93.6</cell><cell>84.7</cell><cell>90.4</cell><cell>87.1</cell><cell>88.8</cell><cell>89.3</cell><cell>91.4</cell><cell>91.7</cell><cell>91.1</cell></row><row><cell>A → C</cell><cell>71.7</cell><cell>82.2</cell><cell>79.3</cell><cell>82.6</cell><cell>79.2</cell><cell>83.6</cell><cell>84.3</cell><cell>78.8</cell><cell>87.4</cell><cell>84.9</cell><cell>83.2</cell><cell>84.8</cell><cell>83.0</cell><cell>85.0</cell><cell>84.1</cell><cell>84.7</cell><cell>84.8</cell><cell>87.4</cell></row><row><cell cols="2">A → W 68.1</cell><cell>71.9</cell><cell>70.9</cell><cell>74.2</cell><cell>70.9</cell><cell>78.3</cell><cell>71.9</cell><cell>75.9</cell><cell>88.5</cell><cell>81.0</cell><cell>74.6</cell><cell>84.7</cell><cell>79.5</cell><cell>86.1</cell><cell>91.8</cell><cell>-</cell><cell>-</cell><cell>88.1</cell></row><row><cell cols="2">A → D 74.5</cell><cell>80.9</cell><cell>82.2</cell><cell>81.5</cell><cell>82.2</cell><cell>80.3</cell><cell>76.4</cell><cell>85.4</cell><cell>85.4</cell><cell>88.5</cell><cell>84.1</cell><cell>92.4</cell><cell>87.4</cell><cell>89.0</cell><cell>91.7</cell><cell>-</cell><cell>-</cell><cell>88.1</cell></row><row><cell cols="2">W → C 55.3</cell><cell>67.9</cell><cell>70.3</cell><cell>80.4</cell><cell>69.8</cell><cell>84.8</cell><cell>83.0</cell><cell>74.8</cell><cell>88.2</cell><cell>85.0</cell><cell>75.5</cell><cell>81.7</cell><cell>73.0</cell><cell>78.0</cell><cell>81.2</cell><cell>79.3</cell><cell>80.2</cell><cell>93.2</cell></row><row><cell cols="2">W → A 62.6</cell><cell>73.4</cell><cell>73.5</cell><cell>84.1</cell><cell>76.8</cell><cell>90.3</cell><cell>87.6</cell><cell>86.1</cell><cell>92.3</cell><cell>90.7</cell><cell>81.2</cell><cell>86.5</cell><cell>83.8</cell><cell>84.9</cell><cell>92.1</cell><cell>-</cell><cell>-</cell><cell>99.4</cell></row><row><cell cols="11">W → D 98.1 100.0 99.4 100.0 100.0 100.0 100.0 100.0 100.0 100.0</cell><cell>100.0</cell><cell>98.7</cell><cell>100.0</cell><cell cols="2">100.0 100.0</cell><cell>-</cell><cell>-</cell><cell>99.4</cell></row><row><cell>D → C</cell><cell>42.1</cell><cell>72.8</cell><cell>71.7</cell><cell>82.3</cell><cell>71.4</cell><cell>85.5</cell><cell>83.8</cell><cell>78.1</cell><cell>87.3</cell><cell>86.2</cell><cell>76.8</cell><cell>83.3</cell><cell>79.0</cell><cell>81.1</cell><cell>80.3</cell><cell>82.8</cell><cell>82.5</cell><cell>87.5</cell></row><row><cell cols="2">D → A 50.0</cell><cell>78.7</cell><cell>79.2</cell><cell>89.1</cell><cell>76.3</cell><cell>91.7</cell><cell>90.3</cell><cell>90.0</cell><cell>92.7</cell><cell>92.0</cell><cell>85.5</cell><cell>90.7</cell><cell>87.1</cell><cell>89.5</cell><cell>90.0</cell><cell>-</cell><cell>-</cell><cell>93.2</cell></row><row><cell cols="2">D → W 91.5</cell><cell>98.3</cell><cell>98.0</cell><cell>99.7</cell><cell>99.3</cell><cell>99.7</cell><cell>99.3</cell><cell cols="3">98.6 100.0 99.7</cell><cell>99.3</cell><cell>99.3</cell><cell>97.7</cell><cell>98.2</cell><cell>98.5</cell><cell>-</cell><cell>-</cell><cell>97.6</cell></row><row><cell cols="2">Average 71.1</cell><cell>82.0</cell><cell>81.7</cell><cell>85.6</cell><cell>81.5</cell><cell>88.2</cell><cell>86.0</cell><cell>85.9</cell><cell>90.7</cell><cell>90.0</cell><cell>84.7</cell><cell>89.4</cell><cell>86.1</cell><cell>88.2</cell><cell>90.1</cell><cell>-</cell><cell>-</cell><cell>92.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Mean and standard deviation of accuracy in feature learning in both original and manifold space.</figDesc><table><row><cell>Task</cell><cell cols="3">Original Space Manifold Space Improvement</cell></row><row><cell>C → A</cell><cell>44.9 (2.1)</cell><cell>56.5 (0.5)</cell><cell>25.8% (-76.9%)</cell></row><row><cell>C → W</cell><cell>33.5 (4.5)</cell><cell>54.0 (0.4)</cell><cell>61.4% (-90.9%)</cell></row><row><cell>C → A (DeCaf)</cell><cell>92.5 (0.2)</cell><cell>93.4 (0.1)</cell><cell>1.0% (-58.3%)</cell></row><row><cell>C → W (DeCaf)</cell><cell>88.4 (1.7)</cell><cell>95.5 (0.3)</cell><cell>8.1% (-82.6%)</cell></row><row><cell>U → M</cell><cell>64.1 (9.2)</cell><cell>71.2 (4.2)</cell><cell>11.1% (-54.5%)</cell></row><row><cell>I → V</cell><cell>63.0 (2.5)</cell><cell>63.7 (2.2)</cell><cell>1.1% (-13.2%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Running time (s) of ARTL, JGSA, and MEDA.</figDesc><table><row><cell>Task</cell><cell cols="2">#Sample × #Feature ARTL</cell><cell>JGSA</cell><cell>MEDA</cell></row><row><cell>C → A</cell><cell>2,081 × 800</cell><cell>29.2</cell><cell>95.2</cell><cell>32.3</cell></row><row><cell>M → U</cell><cell>3,800 × 256</cell><cell>29.1</cell><cell>14.6</cell><cell>31.4</cell></row><row><cell>I → V</cell><cell>10,717 × 4,096</cell><cell cols="3">2,648.8 &gt; 10,000 2,931.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">d A (D s , D t ) = 2(1 − 2ϵ(h))(5)We can directly compute the marginal A-distance using above equation, which is denoted as d M . For the A-distance between conditional distributions, we denote d c as the A-distance for the cth class. It can be calculated as d c = d A (D(c) s , D (c) t ), where D (c) s and D (c) t denote samples from class c in D s and D t , respectively. Eventually, µ can be estimated aŝ</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Symbol '-' denotes the result is not available since there is no code or results.<ref type="bibr" target="#b1">2</ref> Supplementary le is at h ps://www.jianguoyun.com/p/DRuWOFkQjKnsBRjkr2E.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENTS is work is supported in part by National Key R &amp; D Plan of China (2016YFB1001200), NSFC (61572471,61702520,61672313), and NSF through grants IIS-1526499, IIS-1763325, CNS-1626432, and Nanyang Assistant Professorship (NAP) of Nanyang Technological University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Landmarks-based kernelized subspace alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distribution-matching embedding for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3760" to="3789" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation on the statistical manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2481" to="2488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation with Distribution Matching Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI International Conference on Arti cial Intelligence</title>
		<meeting>the 2018 AAAI International Conference on Arti cial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pa ern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Boosting for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning (ICML)</title>
		<meeting>the 24th international conference on Machine learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">e matrix sign function and computations in systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eugene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">N</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beavers</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied mathematics and Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="63" to="94" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Ho Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for so ening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sca er component analysis: A uni ed framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1414" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geodesic ow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pa ern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gre On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Grassmann discriminant analysis: a unifying view on subspace-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Hamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation With Label and Structural Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng-An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ren</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5552" to="5562" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Datasets column: diversity and credibility for social images and image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maia</forename><surname>Rohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><forename type="middle">Lucian</forename><surname>Gînsca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMultimedia Records</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classi cation with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo Rey E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptation regularization: A general framework for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1076" to="1089" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain invariant transfer kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1519" to="1532" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>A survey on transfer learning</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large margin transductive transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1327" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Return of Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Subspace Distribution Alignment for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="24" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual domain adaptation via transfer feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jafar</forename><surname>Tahmoresnezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hashemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Ho Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Vladimir Naumovich Vapnik and Vlamimir Vapnik</title>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Statistical learning theory</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Everything about Transfer Learning and Domain Adapation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="//transferlearning.xyz." />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Balanced distribution adaptation for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuji</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1129" to="1134" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discriminative transfer subspace learning via low-rank and sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="850" to="863" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Uni ed Framework for Metric Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Huaqing Min, and Hengjie Song</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint Geometrical and Statistical Alignment for Visual Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Convolutional Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="269" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
