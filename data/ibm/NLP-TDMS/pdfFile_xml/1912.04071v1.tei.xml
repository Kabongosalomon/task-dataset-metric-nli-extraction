<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepFuse: An IMU-Aware Network for Real-Time 3D Human Pose Estimation from Multi-View Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
							<email>fyhuang@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
							<email>alzeng@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
							<email>mhliu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
							<email>qxlai@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepFuse: An IMU-Aware Network for Real-Time 3D Human Pose Estimation from Multi-View Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a two-stage fully 3D network, namely DeepFuse, to estimate human pose in 3D space by fusing body-worn Inertial Measurement Unit (IMU) data and multi-view images deeply. The first stage is designed for pure vision estimation. To preserve data primitiveness of multi-view inputs, the vision stage uses multi-channel volume as data representation and 3D soft-argmax as activation layer. The second one is the IMU refinement stage which introduces an IMU-bone layer to fuse the IMU and vision data earlier at data level. without requiring a given skeleton model a priori, we can achieve a mean joint error of 28.9mm on TotalCapture dataset and 13.4mm on Hu-man3.6M dataset under protocol 1, improving the SOTA result by a large margin. Finally, we discuss the effectiveness of a fully 3D network for 3D pose estimation experimentally which may benefit future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As a fundamental technique for many applications (e.g., Virtual Reality (VR), Human-Computer Interaction (HCI), and animation making), human pose estimation is a longstanding research problem and has received significant attention from both academia and industry.</p><p>While 2D human pose estimation has been extensively studied in the literature (thanks to the availability of large manually-annotated datasets), existing 3D human pose estimation techniques still have many limitations. Markerbased vision solutions (e.g., Vicon <ref type="bibr" target="#b48">[49]</ref>) are able to achieve high accuracy in recovering 3D human pose and position, but they require sophisticated setup for the surrounding cameras as well as carefully-calibrated markers on human body. Markerless vision solutions (e.g., Kinect and Leap-Motion) are handier, but they can only capture human pose within a near range and fail when there is occlusion. Alternatively, body-worn IMUs (e.g., Xsens <ref type="bibr" target="#b55">[56]</ref>) show remarkable stability and accuracy in capturing bone orientation, but they cannot tell the accurate joint positions.</p><p>Considering the pros and cons of the two types of sen-sors, an interesting problem is whether we could fuse the IMU data and vision data to achieve better results. One challenging issue for such fusion is that the vision input is in pixel/voxel format while the IMU input is in quaternion form. The difference in feature spaces makes it difficult to directly concatenate them into a single network. Trumble et al. <ref type="bibr" target="#b46">[47]</ref> simply fuse the results from the two kinds of sensors with a fully connected layer at the end of the network for regression. Such a straightforward solution does not realize the potential benefits of combining the two modalities. To tackle this problem, some optimization-based solutions try to fuse the data by introducing pre-defined skeleton lengths <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b50">51]</ref>. These solutions, however, requires pre-defined skeleton data, and thus they cannot be well generalized for unknown subjects.</p><p>To overcome the limitations of the above fusion solutions, we propose DeepFuse, a novel IMU-aware network that can fuse the two modalities deeply by introducing a soft-argmax layer and an IMU-bone layer in the network. DeepFuse does not require a given skeleton model a priori, and hence it can be well generalized to unknown subjects. Moreover, to make full use of the geometric-related frame data from multi-view images and preserve data primitiveness, we propose a new data representation: multi-channel volume for multi-view representation. Finally, we propose a new data augmentation technique, namely Random Shut, to enhance the generalization capability of our network for multi-channel volume data.</p><p>We test our fusion solution on TotalCapture <ref type="bibr" target="#b46">[47]</ref> dataset, featuring synchronized camera data from 8 viewpoints, 13 body-worn IMUs and high-quality ground truth. Experimental results show that the proposed approach not only improves the estimation accuracy but also makes the two modalities of sensors mutually complementary. In addition, we test our vision-only network on a popular dataset Hu-man3.6M <ref type="bibr" target="#b15">[16]</ref> and achieve state-of-the-art results as well.</p><p>The main contributions of this work include:</p><p>• We propose a new vision-IMU data fusion technique namely DeepFuse for learning-based 3D human pose estimation, which deeply fuses data from the two kinds  </p><formula xml:id="formula_0">P rh = R(θ 3 , t 3 ) · R(θ 2 , t 2 ) · R(θ 1 , t 1 ) · P root , where R(θ, t)</formula><p>is the rotation matrix made from IMU orientation θ and bone length t. See §2.2 for more details. of sensors. Unlike previous works, the pre-defined skeleton model is not required in our method, making it well generalized to unknown users.</p><p>• We propose a new data format namely multi-channel volume with a corresponding data augmentation algorithm namely Random Shut to process multi-view images. This data format is able to preserve the geometric information of cameras and data primitiveness. Ablation study shows that Random Shut is effective in enhancing model generalization capability.</p><p>• To the best of our knowledge, this is the first work that applies soft-argmax layer in a fully 3D CNN network with volumetric input for human pose estimation. Our method outperforms state-of-the-art result by a large margin and we provide a detailed analysis to show effectiveness of 3D soft-argmax with volumetric representation by conducting rigorous experiments.</p><p>The remainder of this paper is organized as follows. In Section 2, we review the literature on human pose estimation. Section 3 details our method. Next, we present comparative study and ablation study with state-of-the-art works in Section 4. Limitations is discussed in Section 5. Finally, Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There are mainly three types of human pose estimation methods: vision-based, IMU-based and hybrid approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vision-based human pose estimation</head><p>We can generally divide the vision-based tasks into 2D and 3D human pose estimation.</p><p>2D human pose estimation has been extensively explored by adopting either heatmap-based approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6]</ref> or regression-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b2">3]</ref> that regress 2D image to joint coordinates directly. Specifically, Newell et al. <ref type="bibr" target="#b28">[29]</ref> conduct deep conv-deconv hourglass models which has been widely used as a backbone network by previous works <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Despite of the success towards 2D pose estimation, 3D pose estimation is yet under explored. Most methods for 3D pose estimation originate from 2D estimation work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b3">4]</ref>. Generally speaking, heatmap-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46]</ref> show superior performance to that of direct regression work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>. Heatmap-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> regress volumetric heatmaps from 2D image. Recently, many multi-view based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19]</ref> try to get more effective and accurate information from different views. To reduce quantization error, some work <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref> introduce soft-argmax layer to replace hard-argmax. The effectiveness of soft-argmax on networks with 2D images input has been extensively studied in <ref type="bibr" target="#b38">[39]</ref>. However, the effectiveness of soft-argmax layer in a fully 3D CNN network with volumetric input for pose estimation has not been explored yet. We, therefore, conduct rigorous ablation study to demonstrate the effectiveness of soft-argmax layer in a fully 3D CNN network, shown in section 4.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">IMU-based human pose estimation</head><p>IMUs measure bone orientation accurately when they are attached to the human body. According to state-of-the-art results, the mean measurement error of bone orientation produced by body-worn IMUs is about 1.65 • <ref type="bibr" target="#b32">[33]</ref>, while that by the vision-based method is about 12.1 • [51]. Consequently, they are widely used in applications wherein recovering bone orientation is sufficient <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>However, the IMU-based approach has several critical limitations when used to estimate human joint positions.</p><p>• A pre-defined skeleton model is required to solve a kinematic chain as shown in <ref type="figure" target="#fig_0">Fig 1</ref> to recover joint positions. Therefore, manual calibration of the skeleton model is mandatory for each subject. Marcard et al. <ref type="bibr" target="#b52">[53]</ref> combine IMUs with a skinned multi-person linear model <ref type="bibr" target="#b21">[22]</ref> to recover the joint positions.</p><p>• Body skeleton is modeled as a tree-like structure in the kinematic model. Even if you have obtained correct limb lengths, the position of end effector node, such as hand, is determined by all its ancestor nodes, making estimation error accumulated dramatically.</p><p>• Last but not the least, IMU is unable to determine the position of subjects in world space. Although theoretically the subject's positions can be derived by a double integral of the acceleration data in the IMU <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b37">38]</ref>, such measurement error dramatically accumulates over time, making it almost impossible to calculate subject's positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Hybrid approach for human pose estimation</head><p>From the above, vision-based approaches are good at acquiring joint positions, but they are sensitive to body occlusion and illumination changes. IMU-based approaches, on the other hand, are capable of capturing accurate bone orientation stably, but fail to obtain joint positions. A hybrid solution that is able to fuse the two modalities effectively would have great potential.</p><p>Malleson et al. <ref type="bibr" target="#b25">[26]</ref> propose a real-time optimization approach to fuse multi-view data and IMU data by combining position term, orientation term, pose prior term, and acceleration term. Particle-based optimization is used in <ref type="bibr" target="#b35">[36]</ref> to constrain orientation cues from IMU and lowdimensional manifold images cues on an inverse kinematic model. Trumble et al. <ref type="bibr" target="#b46">[47]</ref> propose a learning-based method to fuse volumetric data and IMU data in deep neural network. Bone orientations captured from IMU are converted to joint position by applying forward kinematics. And then, joint positions obtained from the two sources are fused at the very end of the network by fully connected layers. Consequently, the fusion layer makes limited contributions to vision tensors. Marcard et al. <ref type="bibr" target="#b50">[51]</ref> achieve state-of-the-art result by solving a graph-based optimization that jointly optimizes vision data and IMU data on a SMPL model. They jointly optimize their model over all frames simultaneously, making it not applicable for a real-time system. Moreover, the overwhelming majority of current fusion work use predefined skeleton model, restricting the generalization capability of models to unknown subjects.</p><p>In terms of sensor fusion, tight coupling (fusion in data layer) shows overall better performance than loose coupling (fusion in result layer) <ref type="bibr" target="#b24">[25]</ref>. We argue that for learningbased approach, the original data, instead of the estimation results, from the two data sources should be fused at the early stage of network so that the network could capture a deeper relationship of the two modalities to make them mutually complementary. Additionally, pre-defined skeleton model should not be introduced, as it would restrict the generalization capability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Solution</head><p>We define IMU-vision hybrid 3D human pose estimation as a learning-based two-stage regression problem. The network, namely DeepFuse, takes vision and IMU data as input, and regresses 3D human joint positions directly. As shown in <ref type="figure">Fig 2,</ref> the left part of the figure defines the estimation stage which infers 3D human pose from vision data only. The right part defines the refinement stage, introducing IMU-bone layer to refine the result from the previous stage. The whole network is trained in an end-to-end manner, making the two modalities mutually complementary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-processing</head><p>The vision data from TotalCapture <ref type="bibr" target="#b47">[48]</ref> is captured by 8 cameras. We use the provided binary matte images as input. To make full use of the geometric correlation of multi-view images and preserve data primitiveness, we propose a new data format called multi-channel volume and an accompanied data augmentation algorithm named Random Shut. The bone orientations read from IMU are transformed from local coordination to global coordination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Multi-channel volume</head><p>In terms of data representation in 3D human pose estimation, researchers either regard multi-view images as synchronized 2D images <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b30">31]</ref> without considering multiview geometry of cameras, or transform them into 3D volume <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46]</ref> by introducing probability visual hull (PVH). However, PVH does not well preserve data primitiveness since only one volume is generated by applying Bayes probability operator over all images. Accordingly, we propose a multi-channel volume format to overcome the limitations.</p><p>To build multi-channel volume for a single frame, we first define binary matte images I 1 , I 2 . . . I k from K cameras with intrinsic parameter M int k and extrinsic parameter M ext k . For each camera k, we initialize a volume V k centred on the performer with resolution 64 × 64 × 64. The voxel size is set to 35mm to make sure that all body parts are within the volume. The center position C of each voxel in volume V k is transformed from world coordination to pixel coordination by camera parameter P x,y = M int k · M ext k · C. The voxel value is set to 1 if its corresponding pixel in image I k is occupied and to 0 otherwise. Instead of fusing the K volumes into one volume by applying summation or multiplication <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46]</ref> , we simply regard each volume as a single channel of input to preserve original information as much as possible. Finally, the shape of the multi-channel volume is 8 × 64 × 64 × 64, serving as the vision input of our network. <ref type="figure" target="#fig_1">Fig 3 shows</ref> a sample multi-channel volume and its respective matte images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Data augmentation</head><p>In order to prevent overfitting, the volumes are augmented by performing a random rotation around the vertical axis within the range [−π, π]. Additionally, as shown in section 4.2.2, the estimation accuracy decreases when the subject is not captured by all the 8 cameras at the same time. In order to make our model well adapted to this situation, chances are that a random volume channel is set to zero to augment training data. We set the chance of randomly 'shutting down' a camera to 20% and name it as Random Shut. This data augmentation algorithm is proved to be effective for multi-view data in ablation study <ref type="bibr" target="#b3">4</ref>  <ref type="figure">Figure 2</ref>: To simplify the illustration, all the 3D modules are visualized with 2D shapes. In the estimation stage, vision data is first down-sampled and passes through the hourglass network, Residual network 3D (Res3D) and soft-argmax layer. The first mean square error (MSE) loss between estimation result and ground truth is computed at the end of this stage. In the refinement stage, bone orientations from IMUs are transformed to 13 × 32 × 32 × 32 volume by IMU-bone layer, which are then concatenated with vision volume and heatmap volume. See for more details. The first row is the given matte data and the second row is its respective channel of volume (See §3.1.1). Method Error (mm) Video Inertial Poser (VIP) <ref type="bibr" target="#b50">[51]</ref> 26.0 † Frame-by-Frame Optimization <ref type="bibr" target="#b25">[26]</ref> 62.0 FC IMU+3D PVH <ref type="bibr" target="#b46">[47]</ref> 70.0 DeepFuse 28.9 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">IMU orientation</head><p>According to <ref type="bibr" target="#b46">[47]</ref>, IMUs are assumed rigid attached to human bones. The orientation data for sensor k is measured in local frames and employed in quaternion representation (as in: <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1]</ref>), noted as local q k . By multiplying wearing offset wear q k and local-global transform quaternion global local q k , the bone orientation in global frame global q k is calculated as:</p><formula xml:id="formula_1">global q k = local q k ⊗ global local q k (1) global q k = wear q * k ⊗ global q k<label>(2)</label></formula><p>where " * " denote the quaternion conjugate. The bone orientations in global frame global q k are then transformed into IMU-bone layer in the network as discussed in section 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network structure</head><p>As shown in <ref type="figure">Fig 2,</ref> DeepFuse consists of an estimation stage and a refinement stage. The backbone network used is Hourglass Network <ref type="bibr" target="#b28">[29]</ref>. The network takes multi-channel volume as vision input, so we modify original Hourglass Network to the 3D version with 3D CNN. The left part of <ref type="figure">Fig 2 represents</ref> the estimation stage which takes vision data only as input and outputs 3D voxel heatmaps of each joint. Considering the large GPU memory consumption of 3D CNN, the volume input resolution for one channel is 64 × 64 × 64 and the voxel heatmap is 32 × 32 × 32. By adding a soft-argmax layer to the end of the hourglass network, the 3D positions of each joint can be directly regressed from voxel heatmap. Soft-argmax is differentiable so that the estimated positions are able to further propagate to generate IMU-bone layer. Thus, the entire network can be trained in an end-to-end manner.</p><p>The right part is the refinement stage, taking IMU data and output from the previous stage as input. Specifically, the IMU data and estimated joint positions from the last stage consist the IMU-bone layers which turn quaternion data into a multi-channel volume. Thus, IMU-bones layers, voxel heatmap and original vision data, can be concatenated in the same feature space. In the following stack of hourglass network, IMU data and vision data are fused implicitly to produce a set of refined voxel heatmaps. A soft-argmax layer is appended to the last layer as well, from which the final refined joint positions are estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Soft-argmax layer</head><p>Heatmap-based approaches are proved to be effective in both human pose and hand pose estimation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14]</ref>. The voxel heatmap is discrete, while the joint positions are continuous in world coordination. So if we assume that volume  To overcome the limitation of low resolution of voxel heatmap, we introduce soft-argmax layer into our network. Instead of simply picking the largest voxel as joint position, the soft-argmax layer is able to learn a weighted average of multiple voxels to predict joint positions.</p><p>Specifically, soft-argmax shares similar idea with softmax algorithm. Softmax value of voxel x i is defined as:</p><formula xml:id="formula_2">f sof tmax (x i ) = e θxi N j e θxj<label>(3)</label></formula><p>The sum of softmax values equals to 1. Thus, the coordinate of max value among all voxels is the sum of softmax values multiplied by indices along each axis:</p><formula xml:id="formula_3">f (x) = i e θxi N j e θxj i<label>(4)</label></formula><p>where i is the x-,y-,z-coordinate of voxel x i for each axis and N is the total voxel number in the volume. The larger θ will enlarge the voxels with big value and lower the smaller ones, which means if θ is large enough, soft-argmax will return the coordinate of voxel with maximum value. However, we expect that the joint position should be the coordinate of the weighted average of several large voxels. We set θ = 3 empirically (see ablation study in section 4.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">IMU-bone layer and sensor fusion</head><p>As explained in section 2.2, IMU sensor measures bone orientation only. Thus, the pre-defined skeleton model was introduced to estimate joint positions as <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52</ref>] did in their work. We claim that the pre-defined skeleton model should not be used as prior knowledge in order to make the estimation model well generalized to unknown subjects.</p><p>Therefore, the challenge of sensor fusing lies in that how to use measured bone orientation from IMU to refine joint position estimation result from vision. Existing learning-based fusion work <ref type="bibr" target="#b46">[47]</ref> tries to fuse the estimation results from two modalities instead of origin inputs through a dense layer at the end of the network. As a result, the original data from two sensors does not propagate to its counterpart and fusion is relatively shallow. So we are going to fuse the original input of two modalities at the early stage of the network for a deeper fusion.</p><p>According to the skeleton model, one limb L consists of two joints [J 1 , J 2 ] and one piece of bone. If the position of one joint, say J 1 , and bone orientation q b are given, a ray f ray (J 1 , q b ) can be cast from J 1 along q b direction. Thus, J 2 must be located somewhere along the ray. So if we can obtain the estimated joint positions J est from estimation stage and the bone orientation q b from IMU, a cluster of rays can be obtained respectively to describe the possible locations of joints. Since J est is the estimated result, the generated rays are not 100% accurate. The rays are transformed to directed cylinders by introducing a radius r around the rays. Finally, we define a channel of volume</p><formula xml:id="formula_4">V i = f cylinder (J est [i], q b [i]</formula><p>, r) for each IMU i. The voxels are set to 1 if occupied by the 'bone cylinder' and to 0 otherwise. Finally, we stack these volumes together to form IMU-bone layers.</p><p>Since we have volumetric representation for original vision data, voxel heatmap for each joint, and volumes for IMU-bone layer at this stage, the three batches of volumes can be concatenated together and then passed to next stage to refine the result from estimation stage. In this way, the original data from two modalities are deeply fused by a new stack of hourglass network in the refinement stage. Similarly, a soft-argmax layer is appended to the last layer to make final estimation of joint positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Training target</head><p>By introducing soft-argmax layer, joint positions can be directly recovered. So there is no need to involve in heatmap  loss as the voxel heatmap can be learned implicitly by the mean squared error between the estimated joint positions and ground truth positions in world coordination. Specifically, the loss function L is the sum of estimation stage loss L est and refinement stage loss L ref :</p><formula xml:id="formula_5">L = L est + L ref = K i |J i est − J i gt | 2 + K i |J i ref − J i gt | 2<label>(5)</label></formula><p>where J i est and J i ref are the estimated joint positions for i th joint in estimation stage and refinement stage, respectively. J i gt are the ground truth. K is the #joints to be estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In our experiments, RMSProp optimizer is used for training. The learning rate is initially set to be 1e-5 and decays 0.2 every 5 epochs. Our system is able to run at 25 Hz with a single NVIDIA 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparative study</head><p>In this section, we present the comparative result of DeepFuse with recent works on two public 3D human pose estimation datasets: TotalCapure <ref type="bibr" target="#b46">[47]</ref> and Human3.6M <ref type="bibr" target="#b15">[16]</ref>. The TotalCapture dataset features 1.9M video frames  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">TotalCapture evaluation</head><p>TotalCapture <ref type="bibr" target="#b46">[47]</ref> is the only dataset including synchronized body-worn IMU data and multi-view video frames with high-quality ground truth. There are three pieces of work <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b50">51]</ref> evaluated on this dataset by using both vision and IMU data. Specifically, Malleson et al. <ref type="bibr" target="#b25">[26]</ref> optimizes the two modalities in a frame-by-frame manner while Marcard et al. <ref type="bibr" target="#b50">[51]</ref> optimizes the whole video sequence simultaneously. Learning-based method <ref type="bibr" target="#b46">[47]</ref> uses fully connected layer to fuse the IMU data and 3D vision data.</p><p>As shown in <ref type="table" target="#tab_2">Table 1</ref>, DeepFuse outperforms <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b25">26]</ref> by a large margin and shows close performance with <ref type="bibr" target="#b50">[51]</ref>. However, pre-defined skeleton model is not employed in our method, but it was used by all the other three methods. Consequently, DeepFuse is more friendly to unknown subjects. Moreover, Video Inertial Poser (VIP) <ref type="bibr" target="#b50">[51]</ref> achieved the lowest estimation error by optimizing over all frames of a given video sequence and hence it is not applicable for real-time estimation. DeepFuse and the other two methods are framebased estimations without such limitation.</p><p>To sum up, our method achieves the state-of-the-art result in terms of real-time frame-based 3D human pose estimation by fusing IMU and vision data on TotalCapture, showing that even if there is no pre-defined skeleton model as input, vision data and IMU data can still be fused in a complementary way to produce better fusion result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Human3.6M evaluation</head><p>To validate the performance of our proposed multi-channel volume data representation and soft-argmax on volumetric data, we remove the IMU-bone layer of our network and test it on Human3.6M dataset <ref type="bibr" target="#b15">[16]</ref>. It consists of 3.6M frames captured from 11 subjects with 4 synchronized cameras.</p><p>Followed by the setting of <ref type="bibr" target="#b38">[39]</ref>, protocol 1 uses subjects (S1, S5, S6, S7, S8, S9) for training and S11 for test. The measured result, mean per joint position error ( MPJPE ), is aligned by Procrustes Analysis ( PA MPJPE ). Protocol 2 uses subjects (S1, S5, S6, S7, S8) for training and subjects (S9, S11) for test without PA. To remove data redundancy, only every 5th frames in training sequences and every 64th frames in test sequences are used. Meanwhile, we use the provided foreground matte images from all the 4 cameras as input. Because the number of cameras is only 4 and subject always move within area captured by all cameras, Random Shut introduces more noise on this dataset in our experiment, which is not used in training as a result. From table 2, our method significantly outperforms other methods by large margins including singles-view methods and multi-view methods. Specifically, it improves the SOTA <ref type="bibr" target="#b42">[43]</ref> by 31.2mm (relative 70% lower than <ref type="bibr" target="#b42">[43]</ref>) under protocol 1, and by 11.6mm (relative 24.0% lower than the state-of-the-art method <ref type="bibr" target="#b18">[19]</ref>) under protocol 2, and it also is clearly ahead by over 40% as the same matte data used in <ref type="bibr" target="#b45">[46]</ref>. By observing the failure cases, we find that the matte data of 3 actions, Greeting, Sitting Down and Waiting, are incomplete due to truncations, leading to unexpected high error. After remove these there actions, our result reaches 32.5mm. Furthermore, we will show the qualitative examples in the supplementary materials.  Finally, recent methods with volumetric input show overall better performance than that with 2D image input, showing the advantage of volumetric data representation. Also, our method outperforms other volume-based method by a large margin, showing the effectiveness of soft-argmax layer on volumetric data for 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>In this section, we try to answer the following three questions: <ref type="bibr" target="#b0">(1)</ref> To what extent can IMU-bone layer contribute to the final estimation? (2) Can the proposed data augmentation algorithm, Random Shut, improve the generalization capability of our model? And <ref type="formula" target="#formula_2">(3)</ref> To what extent and why 3D soft-argmax over volumetric data representation improve the estimation accuracy? All the experiments of ablation study are evaluated on the TotalCapture dataset <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sensor fusion</head><p>The first study explores the effectiveness of sensor fusion. Results of four training strategies are listed in <ref type="table" target="#tab_6">Table 3</ref>. To alleviate the possible influence of Random Shut (RS) on this ablation study, we perform comparison experiment between vision-IMU network and vision-only network w. or w/o. RS, respectively. Both of the vision-IMU networks outperform their vision-only counterparts, supporting the effectiveness of our data fusion solution quantitatively.</p><p>In addition, we want to find out how IMU data influence the estimation after refinement. We plot the per-joint estimation error on test set. As shown in <ref type="figure">Fig 4,</ref> the joints around limbs including foot and hand show more improvement by fusing IMU sensors compared to other joints. The main reason is that the IMU-bone layer constructs cylinder volume for each bone and it simulates volume around limbs much better than that around torso due to visual similarity. This argument is also supported by the qualitative result in <ref type="figure" target="#fig_2">Fig.5</ref>. As can be observed, fusion result is superior to vision-only result. The joints around limbs show better refinement than that around torsp, especially under heavy self-occlusion.</p><p>As discussed before, the measurement of IMU sensors is more stable than that of vision sensors. Therefore, in supplementary material, we show that fusion approach shows better sequential stability than its vision-only counterpart.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Random Shut</head><p>The second study shows the motivation and effectiveness of the proposed data augmentation approach, Random Shut (RS), for multi-view data. <ref type="figure" target="#fig_4">Fig 6 demonstrates</ref> the per-frame estimation accuracy on a test sequence. We find that the estimation error increases when subject is partially-captured (Frame 600 and 1500), which is also proved by statistic data shown in <ref type="table" target="#tab_8">Table 4</ref>. Motivated by this finding, Random Shut simulates the situation when subject randomly missed by a certain camera during training. As demonstrated in <ref type="table" target="#tab_8">Table 4</ref> and in <ref type="figure" target="#fig_4">Fig. 6</ref>, we conclude that RS shows improvement on generalization capability of the model for multi-view data, especially for the partially-captured frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Soft-argmax and volumetric representation</head><p>The last study aims to validate the effectiveness of 3D softargmax layer and volumetric representation. Although softargmax layer has already been used in human pose estimation by several works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>, the effectiveness of this layer has not yet been evaluated in a fully 3D CNN-based network with volumetric input.</p><p>To show the effectiveness of 3D soft-argmax, we compare it with two widely used post-processing techniques: hard-argmax which directly picks highest voxel position, and direct regression which does regression via a dense layer. For fair comparison, we evaluate the three kinds of methods on the proposed fully 3D CNN-based network with volumetric data as input only. The comparative results in <ref type="table" target="#tab_9">Table 5</ref> show that soft-argmax significantly outperforms the other approaches. The main reason is that the low resolution of voxel heatmap limits the performance of hard-argmax as stated in section 3.2.1 and the dense layer is not well capable of learning argmax operator which is highly non-linear.</p><p>To show the effectiveness of volumetric representation, we first have following finding in <ref type="table" target="#tab_4">Table 2</ref> that our volumetric representation achieves mean error of 37.5mm, an improvement of 23.6% and 28.9% compared to its 2D soft-argmax multiview counterparts 49.1mm <ref type="bibr" target="#b16">[17]</ref> and 52.8mm <ref type="bibr" target="#b42">[43]</ref>, showing the advantage of volumetric representation in 3D human pose estimation. Second, our model converges very fast with 3D soft-argmax. As shown in θ 1 2 3 5 10 100 Error (mm) 33.8 33.4 32.7 34.2 38.9 41.0 <ref type="table">Table 6</ref>: Different θ of soft-argmax and their respective estimation errors in the vision-only network (See §4.2.3). <ref type="figure" target="#fig_5">Fig. 7</ref>, we achieve state-of-the-art result only after the first epoch, indicating that fully 3D CNN-based network would not waste its capability on learning unnecessary space mapping, which explains the effectiveness of volumetric representation.</p><p>Meanwhile, the model converges too fast, indicating potential severe overfitting. We therefore explore how to tune the parameter θ in the soft-argmax <ref type="bibr">Equation 4</ref>, which may influence the convergence speed. As discussed in Sec. 3.2.1, soft-argmax pays more attention to large voxel when θ enlarges. If θ is too large, soft-argmax will degenerate to hardargmax. If θ is too small, too many voxels contribute to the final result, which may lead to overfitting. So we list some possible values of θ and their respective estimation errors in <ref type="table">Table 6</ref>. In our experiment, θ = 3 achieves the best performance in our experiment setting. We believe the above findings of using soft-argmax in 3D space are novel and would be beneficial for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitation and future work</head><p>One limitation is that, like many multi-view methods, the proposed multi-channel volume is biased to camera configuration. To make it fair in more general configurations, the triangulation should be trainable instead of fixed, making model adaptive. Another one is that performance of our method depends on the quality of foreground silhouette. The popular segmentation networks, like maskrcnn <ref type="bibr" target="#b12">[13]</ref>, show low quality of segmentation near human edgee so we use the given silhouette ground truth. In the future work, we will try to make this step into an end-to-end pipeline for simplifying the input representation and improve the silhouette quality simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose an IMU-aware network to fuse IMU data and multi-view images for 3D human pose estimation. To fully utilize the multi-view geometric information , we re-project it into a multi-channel volume format and apply Random Shut for data augmentation. To deeply fuse IMU orientation and multiple views, we then propose an IMU-bone layer to transform the original data from two modalities into a same feature space at early stage of network. Rigorous ablation shows the effectiveness of the multi-channel volume, Random Shut and IMUbone layer. Finally, our method achieves the state-of-the-art performance on both TotalCapture and Human3.6M dataset with real-time capability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of kinematics. The position of end effector P rh , right hand, can be derived through kinematic chain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Sample multi-channel volume of a single frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Left to right: camera, ground truth, vision-only estimation, fusion estimation. See §4.2.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>from 8 cameras performed by 5 subjects and synchronized orientation data from 13 body-worn IMUs. Human3.6M is a more popular 3D human pose estimation dataset with only vision data as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Per-frame mean joint error of test sequence S4 F3. The four matte images at bottom are captured by camera 4 at frame 400, 600, 1200 and 1500. Partially-captured frames, e.g. frame 600 and 1500, show inferior performances. RS for Random Shut. F denotes frame number. (See §4.2.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Mean error over epoch Baselines Hard-argmax Soft-argmax Direct regression (FC) Mean Error (mm)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.2.2.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Estimation Stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IMU</cell><cell>Refinement Stage</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Joint loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MSE loss for</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">joint positions</cell></row><row><cell cols="2">kernel:3</cell><cell cols="2">kernel:3</cell><cell cols="2">kernel:3</cell><cell cols="2">kernel:1</cell><cell cols="2">kernel:1</cell><cell cols="2">kernel:1</cell><cell></cell><cell></cell></row><row><cell cols="2">stride: 2</cell><cell cols="2">stride: 1</cell><cell cols="2">stride: 1</cell><cell cols="2">stride: 1</cell><cell cols="2">stride: 1</cell><cell cols="2">stride: 1</cell><cell>in: 13 IMUs</cell><cell>Element-wise</cell></row><row><cell>in:</cell><cell>8</cell><cell>in:</cell><cell>128</cell><cell>in:</cell><cell>128</cell><cell>in:</cell><cell>128</cell><cell>in:</cell><cell>128</cell><cell>in:</cell><cell>4</cell><cell>out: 13*32*32*32</cell><cell>Addition</cell></row><row><cell cols="2">out: 128</cell><cell cols="2">out: 128</cell><cell cols="2">out: 128</cell><cell cols="2">out: 128</cell><cell cols="2">out: 21</cell><cell cols="2">out: 128</cell><cell>voxel map</cell><cell></cell></row><row><cell>Conv3D</cell><cell>Hourglass3D</cell><cell></cell><cell>Res3D</cell><cell></cell><cell>Res3D</cell><cell></cell><cell>Res3D</cell><cell></cell><cell cols="2">Res3D</cell><cell>Soft-argmax</cell><cell>IMU-bone layer</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison results regarding mean joint error on TotalCapture dataset. † indicates sequence-based work. See §4.1.1 for detail. (Best in bold; same for other tables).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison results regarding mean joint error following protocol 1 and 2 of Human3.6M with 17 keypoints. † use provided matte data. × use multi-view images. (FS: fully supervised baseline). See §4.1.2 for details.</figDesc><table /><note>length is 2000mm and heatmap resolution is 32 × 32 × 32, the average estimation error within a single voxel is about 2000/32/2 = 31.25mm at least. So even if we obtained a perfect voxel heatmap, the accuracy of joint positions in world coordination would be far from satisfactory by sim- ply picking the largest voxel in a voxel heatmap.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Per-joint mean error on test set. * indicates right/left foot and hand joints. See §4.2.1 for details.</figDesc><table><row><cell></cell><cell>40 50</cell><cell>vision+IMU vision</cell></row><row><cell>Mean Error (mm)</cell><cell>10 20 30</cell></row><row><cell></cell><cell>0</cell><cell>Hips Spine Spine1 Spine2 Spine3 Neck Head Shoulder.R Arm.R ForeArm.R Hand.R* Shoulder.L Arm.L ForeArm.L Hand.L* UpLeg.R Leg.R Foot.R* UpLeg.L Leg.L Foot.L* Mean</cell></row><row><cell cols="3">Figure 4: Methods</cell><cell>Error (mm)</cell></row><row><cell></cell><cell></cell><cell>Vision-IMU w/ RS (proposed)</cell><cell>28.9</cell></row><row><cell></cell><cell></cell><cell>Vision-only w/ RS</cell><cell>32.7</cell></row><row><cell></cell><cell></cell><cell>Vision-IMU w/o RS</cell><cell>32.4</cell></row><row><cell></cell><cell></cell><cell>Vision-only w/o RS</cell><cell>35.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Mean joint error results for 4 different experiment settings. RS is short for Random Shut. Vision-IMU w/ RS is identical to the proposed DeepFuse (See §4.2.1).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Mean joint error of our proposed method trained w. or w/o. Random Shut (RS). Full frames are frames that are captured by all the 8 cameras while partial frames are not. All errors are measured in millimeter (mm) (See §4.2.2).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Mean joint error results for three kinds of output layers. FC stands for fully connected layer (See §4.2.3).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Orientation tracking for humans and robots using inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Duman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Usta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Mcghee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zyda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation. CIRA&apos;99 (Cat. No. 99EX375)</title>
		<meeting>1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation. CIRA&apos;99 (Cat. No. 99EX375)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7035" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weaklysupervised discovery of geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10895" to="10904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">30413048</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Novel methods for attitude determination using vector observations. Technion-Israel Institute of Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choukroun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Faculty of Aerospace Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Powell. Design of multi-sensor attitude determination systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gebre-Egziabher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hayward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on aerospace and electronic systems</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="627" to="649" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structureaware 3d hourglass network for hand pose estimation from single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">289</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A generalizable approach for multi-view 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno>abs/1804.10462</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02330</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5543" to="5552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02322</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep coupling autoencoder for fault diagnosis with multimodal sensory data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1137" to="1145" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time full-body motion capture from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2017 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="449" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01484</idno>
		<title level="m">3d human pose estimation with 2d marginal heatmaps</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<title level="m">Numerical coordinate regression with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Xsens mtw awinda: Miniature wireless inertial-magnetic motion tracker for highly accurate 3d kinematic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paulich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schepers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rudigkeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bellusci</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno>abs/1611.07828</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12531262</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Outdoor human motion capture using inverse kinematics and von mises-fisher sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spörri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action capture with accelerometers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Slyper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</title>
		<meeting>the 2008 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Synthetic occlusion augmentation with volumetric heatmaps for the 2018 eccv posetrack challenge on 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Srndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Motion reconstruction using sparse accelerometer data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tautges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Helten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eberhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno>number EPFL-CONF-230311</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep autoencoder for combined human pose estimation and body model upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 28th British Machine Vision Conference</title>
		<meeting>28th British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Vicon motion systems ltd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vicon</surname></persName>
		</author>
		<ptr target="http://www.vicon.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Practical motion capture in everyday surroundings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adelsberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vannucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barnwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Human pose estimation from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1533" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Sparse inertial poser: Automatic 3d human pose estimation from sparse imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="349" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Xsens motion technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xsens</surname></persName>
		</author>
		<ptr target="http://www.xsens.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4948" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
