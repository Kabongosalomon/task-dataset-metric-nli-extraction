<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Matching the Blanks: Distributional Similarity for Relation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><forename type="middle">Baldini</forename><surname>Soares</surname></persName>
							<email>liviobs@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ling</surname></persName>
							<email>jeffreyling@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Matching the Blanks: Distributional Similarity for Relation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris' distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task's training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on Se-mEval 2010 Task 8, KBP37, and TACRED.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reading text to identify and extract relations between entities has been a long standing goal in natural language processing <ref type="bibr" target="#b6">(Cardie, 1997)</ref>. Typically efforts in relation extraction fall into one of three groups. In a first group, supervised <ref type="bibr" target="#b15">(Kambhatla, 2004;</ref><ref type="bibr" target="#b9">GuoDong et al., 2005;</ref><ref type="bibr" target="#b28">Zeng et al., 2014)</ref>, or distantly supervised relation extractors <ref type="bibr" target="#b18">(Mintz et al., 2009</ref>) learn a mapping from text to relations in a limited schema. Forming a second group, open information extraction removes the limitations of a predefined schema by instead representing relations using their surface forms <ref type="bibr" target="#b2">(Banko et al., 2007;</ref><ref type="bibr" target="#b8">Fader et al., 2011;</ref><ref type="bibr" target="#b23">Stanovsky et al., 2018)</ref>, which increases scope but also leads * Work done as part of the Google AI residency.</p><p>to an associated lack of generality since many surface forms can express the same relation. Finally, the universal schema <ref type="bibr" target="#b22">(Riedel et al., 2013)</ref> embraces both the diversity of text, and the concise nature of schematic relations, to build a joint representation that has been extended to arbitrary textual input <ref type="bibr" target="#b24">(Toutanova et al., 2015)</ref>, and arbitrary entity pairs <ref type="bibr" target="#b26">(Verga and McCallum, 2016)</ref>. However, like distantly supervised relation extractors, universal schema rely on large knowledge graphs (typically Freebase <ref type="bibr" target="#b4">(Bollacker et al., 2008)</ref>) that can be aligned to text.</p><p>Building on <ref type="bibr" target="#b16">Lin and Pantel (2001)</ref>'s extension of Harris' distributional hypothesis <ref type="bibr" target="#b12">(Harris, 1954)</ref> to relations, as well as recent advances in learning word representations from observations of their contexts <ref type="bibr" target="#b17">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b21">Peters et al., 2018;</ref><ref type="bibr" target="#b7">Devlin et al., 2018)</ref>, we propose a new method of learning relation representations directly from text. First, we study the ability of the Transformer neural network architecture <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref> to encode relations between entity pairs, and we identify a method of representation that outperforms previous work in supervised relation extraction. Then, we present a method of training this relation representation without any supervision from a knowledge graph or human annotators by matching the blanks.</p><p>[BLANK], inspired by Cale's earlier cover, recorded one of the most acclaimed versions of "[BLANK]"</p><p>[BLANK]'s rendition of "[BLANK]" has been called "one of the great songs" by Time, and is included on Rolling Stone's list of "The 500 Greatest Songs of All Time". Following <ref type="bibr" target="#b22">Riedel et al. (2013)</ref>, we assume access to a corpus of text in which entities have been linked to unique identifiers and we define a rela-tion statement to be a block of text containing two marked entities. From this, we create training data that contains relation statements in which the entities have been replaced with a special <ref type="bibr">[BLANK]</ref> symbol, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Our training procedure takes in pairs of blank-containing relation statements, and has an objective that encourages relation representations to be similar if they range over the same pairs of entities. After training, we employ learned relation representations to the recently released FewRel task <ref type="bibr" target="#b11">(Han et al., 2018)</ref> in which specific relations, such as 'original language of work' are represented with a few exemplars, such as The Crowd (Italian: La Folla) is a 1951 Italian film. <ref type="bibr" target="#b11">Han et al. (2018)</ref> presented FewRel as a supervised dataset, intended to evaluate models' ability to adapt to relations from new domains at test time. We show that through training by matching the blanks, we can outperform <ref type="bibr" target="#b11">Han et al. (2018)</ref>'s top performance on FewRel, without having seen any of the FewRel training data. We also show that a model pre-trained by matching the blanks and tuned on FewRel outperforms humans on the FewRel evaluation. Similarly, by training by matching the blanks and then tuning on labeled data, we significantly improve performance on the SemEval 2010 Task 8 <ref type="bibr" target="#b13">(Hendrickx et al., 2009</ref>), KBP-37 <ref type="bibr" target="#b29">(Zhang and Wang, 2015)</ref>, and TACRED <ref type="bibr" target="#b30">(Zhang et al., 2017)</ref> relation extraction benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>Task definition In this paper, we focus on learning mappings from relation statements to relation representations. Formally, let x = [x 0 . . . x n ] be a sequence of tokens, where x 0 = [CLS] and x n = [SEP] are special start and end markers. Let s 1 = (i, j) and s 2 = (k, l) be pairs of integers such that 0 &lt; i &lt; j − 1, j &lt; k, k ≤ l − 1, and l ≤ n. A relation statement is a triple r = (x, s 1 , s 2 ), where the indices in s 1 and s 2 delimit entity mentions in x: the sequence [x i . . . x j−1 ] mentions an entity, and so does the sequence [x k . . . x l−1 ]. Our goal is to learn a function h r = f θ (r) that maps the relation statement to a fixed-length vector h r ∈ R d that represents the relation expressed in x between the entities marked by s 1 and s 2 .</p><p>Contributions This paper contains two main contributions. First, in Section 3.1 we investigate different architectures for the relation encoder f θ , all built on top of the widely used Transformer se-quence model <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr" target="#b25">Vaswani et al., 2017)</ref>. We evaluate each of these architectures by applying them to a suite of relation extraction benchmarks with supervised training.</p><p>Our second, more significant, contributionpresented in Section 4-is to show that f θ can be learned from widely available distant supervision in the form of entity linked text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architectures for Relation Learning</head><p>The primary goal of this work is to develop models that produce relation representations directly from text. Given the strong performance of recent deep transformers trained on variants of language modeling, we adopt <ref type="bibr" target="#b7">Devlin et al. (2018)</ref>'s BERT model as the basis for our work. In this section, we explore different methods of representing relations with the Transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relation Classification and Extraction Tasks</head><p>We evaluate the different methods of representation on a suite of supervised relation extraction benchmarks. The relation extractions tasks we use can be broadly categorized into two types: fully supervised relation extraction, and few-shot relation matching.</p><p>For the supervised tasks, the goal is to, given a relation statement r, predict a relation type t ∈ T where T is a fixed dictionary of relation types and t = 0 typically denotes a lack of relation between the entities in the relation statement. For this type of task we evaluate on SemEval 2010 Task 8 <ref type="bibr" target="#b13">(Hendrickx et al., 2009</ref>), KBP-37 <ref type="bibr" target="#b29">(Zhang and Wang, 2015)</ref> and TACRED <ref type="bibr" target="#b30">(Zhang et al., 2017)</ref>. More formally,</p><p>In the case of few-shot relation matching, a set of candidate relation statements are ranked, and matched, according to a query relation statement. In this task, examples in the test and development sets typically contain relation types not present in the training set. For this type of task, we evaluate on the FewRel <ref type="bibr" target="#b11">(Han et al., 2018)</ref> dataset. Specifically, we are given K sets of N labeled relation statements S k = {(r 0 , t 0 ) . . . (r N , t N )} where t i ∈ {1 . . . K} is the corresponding relation type. The goal is to predict the t q ∈ {1 . . . K} for a query relation statement r q .     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation Representations from Deep Transformers Model</head><p>In all experiments in this section, we start with the BERT LARGE model made available by <ref type="bibr" target="#b7">Devlin et al. (2018)</ref> and train towards task-specific losses. Since BERT has not previously been applied to the problem of relation representation, we aim to answer two primary modeling questions: (1) how do we represent entities of interest in the input to BERT, and (2) how do we extract a fixed length representation of a relation from BERT's output. We present three options for both the input encoding, and the output relation representation. Six combinations of these are illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Entity span identification</head><p>Recall, from Section 2, that the relation statement r = (x, s 1 , s 2 ) contains the sequence of tokens x and the entity span identifiers s 1 and s 2 . We present three different options for getting information about the focus spans s 1 and s 2 into our BERT encoder.</p><p>Standard input First we experiment with a BERT model that does not have access to any explicit identification of the entity spans s 1 and s 2 . We refer to this choice as the STANDARD input. This is an important reference point, since we believe that BERT has the ability to identify entities in x, but with the STANDARD input there is no way of knowing which two entities are in focus when x contains more than two entity mentions.</p><p>Positional embeddings For each of the tokens in its input, BERT also adds a segmentation embedding, primarily used to add sentence segmentation information to the model. To address the STANDARD representation's lack of explicit entity identification, we introduce two new segmentation embeddings, one that is added to all tokens in the span s 1 , while the other is added to all tokens in the span s 2 . This approach is analogous to previous work where positional embeddings have been applied to relation extraction <ref type="bibr" target="#b30">(Zhang et al., 2017;</ref><ref type="bibr" target="#b3">Bilan and Roth, 2018)</ref>.</p><p>Entity marker tokens Finally, we augment x with four reserved word pieces to mark the begin and end of each entity mention in the relation statement. We introduce the [E1 start ], [E1 end ],</p><p>[E2 start ] and [E2 end ] and modify x to givẽ</p><formula xml:id="formula_0">x =[x 0 . . . [E1 start ] x i . . . x j−1 [E1 end ] . . . [E2 start ] x k . . . x l−1 [E2 end ] . . . x n ].</formula><p>and we feed this token sequence into BERT instead of x. We also update the entity indicess 1 = (i + 1, j + 1) ands 2 = (k + 3, l + 3) to account for the inserted tokens. We refer to this representation of the input as ENTITY MARKERS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fixed length relation representation</head><p>We now introduce three separate methods of extracting a fixed length relation representation h r from the BERT encoder. The three variants rely on extracting the last hidden layers of the transformer network, which we define as H = [h 0 , ...h n ] for n = |x| (or |x| if entity marker tokens are used).</p><p>[CLS] token Recall from Section 2 that each x starts with a reserved [CLS] token. BERT's output state that corresponds to this token is used by <ref type="bibr" target="#b7">Devlin et al. (2018)</ref> as a fixed length sentence representation. We adopt the [CLS] output, h 0 , as our first relation representation.</p><p>Entity mention pooling We obtain h r by maxpooling the final hidden layers corresponding to the word pieces in each entity mention, to get two vectors h e 1 = MAXPOOL([h i ...h j−1 ]) and h e 2 = MAXPOOL([h k ...h l−1 ]) representing the two entity mentions. We concatenate these two vectors to get the single representation h r = h e 1 |h e 2 where a|b is the concatenation of a and b. We refer to this architecture as MENTION POOLING.</p><p>Entity start state Finally, we propose simply representing the relation between two entities with the concatenation of the final hidden states corresponding their respective start tokens, when EN-TITY MARKERS are used. Recalling that ENTITY MARKERS inserts tokens in x, creating offsets in s 1 and s 2 , our representation of the relation is r h = h i |h j+2 . We refer to this output representation as ENTITY START output. Note that this can only be applied to the ENTITY MARKERS input. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates a few of the variants we evaluated in this section. In addition to defining the model input and output architecture, we fix the training loss used to train the models (which is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>). In all models, the output representation from the Transformer network is fed into a fully connected layer that either <ref type="formula">(1)</ref> contains a linear activation, or (2) performs layer normalization <ref type="bibr">(Ba et al., 2016)</ref> on the representation. We treat the choice of post Transfomer layer as a hyper-parameter and use the best performing layer type for each task.</p><p>For the supervised tasks, we introduce a new classification layer W ∈ R KxH where H is the size of the relation representation and K is the number of relation types. The classification loss is the standard cross entropy of the softmax of h r W T with respect to the true relation type.</p><p>For the few-shot task, we use the dot product between relation representation of the query statement and each of the candidate statements as a similarity score. In this case, we also apply a cross entropy loss of the softmax of similarity scores with respect to the true class.</p><p>We perform task-specific fine-tuning of the BERT model, for all variants, with the following set of hyper-parameters:</p><p>•  <ref type="table">Table 1</ref> shows the results of model variants on the three supervised relation extraction tasks and the 5-way-1-shot variant of the few-shot relation classification task. For all four tasks, the model using the ENTITY MARKERS input representation and ENTITY START output representation achieves the best scores.</p><p>From the results, it is clear that adding positional information in the input is critical for the model to learn useful relation representations. Unlike previous work that have benefited from positional embeddings <ref type="bibr" target="#b30">(Zhang et al., 2017;</ref><ref type="bibr" target="#b3">Bilan and Roth, 2018)</ref>, the deep Transformers benefits the most from seeing the new entity boundary word pieces (ENTITY MARKERS). It is also worth noting that the best variant outperforms previous published models on all four tasks. For the remainder of the paper, we will use this architecture when further training and evaluating our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning by Matching the Blanks</head><p>So far, we have used human labeled training data to train our relation statement encoder f θ . Inspired by open information extraction <ref type="bibr" target="#b2">(Banko et al., 2007;</ref><ref type="bibr" target="#b0">Angeli et al., 2015)</ref>, which derives relations directly from tagged text, we now introduce a new method of training f θ without a predefined ontology, or relation-labeled training data. Instead, we declare that for any pair of relation statements r and r , the inner product f θ (r) f θ (r ) should be high if the two relation statements, r and r , express semantically similar relations. And, this inner product should be low if the two relation statements express semantically different relations.</p><p>Unlike related work in distant supervision for information extraction <ref type="bibr" target="#b14">(Hoffmann et al., 2011;</ref><ref type="bibr" target="#b18">Mintz et al., 2009</ref>), we do not use relation labels at training time. Instead, we observe that there is a high degree of redundancy in web text, and each relation between an arbitrary pair of entities is likely to be stated multiple times. Subsequently, r = (x, s 1 , s 2 ) is more likely to encode the same semantic relation as r = (x , s 1 , s 2 ) if s 1 refers to the same entity as s 1 , and s 2 refers to the same entity as s 2 . Starting with this observation, we introduce a new method of learning f θ from entity linked text. We introduce this method of learning by matching the blanks (MTB). In Section 5 we show that MTB learns relation representations that can be used without any further tuning for relation extraction-even beating previous work that trained on human labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning Setup</head><p>Let E be a predefined set of entities. And let D = [(r 0 , e 0 1 , e 0 2 ) . . . (r N , e N 1 , e N 2 )] be a corpus of relation statements that have been labeled with two entities e i 1 ∈ E and e i 2 ∈ E. Recall, from Section 2, that r i = (x i , s i 1 , s i 2 ), where s i 1 and s i 2 delimit entity mentions in x i . Each item in D is created by pairing the relation statement r i with the two entities e i 1 and e i 2 corresponding to the spans s i 1 and s i 2 , respectively. We aim to learn a relation statement encoder f θ that we can use to determine whether or not two relation statements encode the same relation. To do this, we define the following binary classifier</p><formula xml:id="formula_1">p(l = 1|r, r ) = 1 1 + exp f θ (r) f θ (r )</formula><p>to assign a probability to the case that r and r encode the same relation (l = 1), or not (l = 0). We will then learn the parameterization of f θ that rA In 1976, e1 (then of Bell Labs) published e2, the first of his books on programming inspired by the Unix operating system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>rB</head><p>The "e2" series spread the essence of "C/Unix thinking" with makeovers for Fortran and Pascal. e1's Ratfor was eventually put in the public domain. rC e1 worked at Bell Labs alongside e3 creators Ken Thompson and Dennis Ritchie. Mentions e1 = Brian Kernighan, e2 = Software Tools, e3 = Unix <ref type="table">Table 2</ref>: Example of "matching the blanks" automatically generated training data. Statement pairs rA and rB form a positive example since they share resolution of two entities. Statement pairs rA and rC as well as rB and rC form strong negative pairs since they share one entity in common but contain other non-matching entities. minimizes the loss L(D) = − 1 |D| 2 (r,e 1 ,e 2 )∈D (r ,e 1 ,e 2 )∈D (1) δ e 1 ,e 1 δ e 2 ,e 2 · log p(l = 1|r, r )+ (1 − δ e 1 ,e 1 δ e 2 ,e 2 ) · log(1 − p(l = 1|r, r ))</p><p>where δ e,e is the Kronecker delta that takes the value 1 iff e = e , and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Introducing Blanks</head><p>Readers may have noticed that the loss in Equation 1 can be minimized perfectly by the entity linking system used to create D. And, since this linking system does not have any notion of relations, it is not reasonable to assume that f θ will somehow magically build meaningful relation representations. To avoid simply relearning the entity linking system, we introduce a modified corpus D = [(r 0 , e 0 1 , e 0 2 ) . . . (r N , e N 1 , e N 2 )]</p><p>where eachr i = (x i , s i 1 , s i 2 ) contains a relation statement in which one or both entity mentions may have been replaced by a special [BLANK] symbol. Specifically,x contains the span defined by s 1 with probability α. Otherwise, the span has been replaced with a single [BLANK] symbol. The same is true for s 2 . Only α 2 of the relation statements inD explicitly name both of the entities that participate in the relation. As a result, minimizing L(D) requires f θ to do more than simply identifying named entities in r. We hypothesize that training onD will result in a f θ that encodes the semantic relation between the two possibly elided entity spans. Results in Section 5 support this hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Matching the Blanks Training</head><p>To train a model with matching the blank task, we construct a training setup similar to BERT, where two losses are used concurrently: the masked language model loss and the matching the blanks loss. For generating the training corpus, we use English Wikipedia and extract text passages from the HTML paragraph blocks, ignoring lists, and tables. We use an off-the-shelf entity linking system 1 to annotate text spans with a unique knowledge base identifier (e.g., Freebase ID or Wikipedia URL). The span annotations include not only proper names, but other referential entities such as common nouns and pronouns. From this annotated corpus we extract relation statements where each statement contains at least two grounded entities within a fixed sized window of tokens 2 . To prevent a large bias towards relation statements that involve popular entities, we limit the number of relation statements that contain the same entity by randomly sampling a constant number of relation statements that contain any given entity.</p><p>We use these statements to train model parameters to minimize L(D) as described in the previous section. In practice, it is not possible to compare every pair of relation statements, as in Equation 1, and so we use a noise-contrastive estimation <ref type="bibr" target="#b10">(Gutmann and Hyvärinen, 2012;</ref><ref type="bibr" target="#b19">Mnih and Kavukcuoglu, 2013)</ref>. In this estimation, we consider all positive pairs of relation statements that contain the same entity, so there is no change to the contribution of the first term in Equation 1-where δ e 1 ,e 1 δ e 2 ,e 2 = 1. The approximation does, however, change the contribution of the second term.</p><p>Instead of summing over all pairs of relation statements that do not contain the same pair of entities, we sample a set of negatives that are either randomly sampled uniformly from the set of all relation statement pairs, or are sampled from the set of relation statements that share just a single 5-way 5-way 10-way 10-way 1-shot 5-shot  <ref type="table">Table 3</ref>: Test results for FewRel few-shot relation classification task. Proto Net is the best published system from <ref type="bibr" target="#b11">Han et al. (2018)</ref>. At the time of writing, our BERTEM +MTB model outperforms the top model on the leaderboard (http: //www.zhuhao.me/fewrel/) by over 10% on the 5-way-1-shot and over 15% on the 10-way-1-shot configurations.</p><p>entity. We include the second set 'hard' negatives to account for the fact that most randomly sampled relation statement pairs are very unlikely to be even remotely topically related, and we would like to ensure that the training procedure sees pairs of relation statements that refer to similar, but different, relations. Finally, we probabilistically replace each entity's mention with [BLANK] symbols, with a probability of α = 0.7, as described in Section 3.2, to ensure that the model is not confounded by the absence of [BLANK] symbols in the evaluation tasks. In total, we generate 600 million relation statement pairs from English Wikipedia, roughly split between 50% positive and 50% strong negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>In this section, we evaluate the impact of training by matching the blanks. We start with the best BERT based model from Section 3.3, which we call BERT EM , and we compare this to a variant that is trained with the matching the blanks task (BERT EM +MTB). We train the BERT EM +MTB model by initializing the Transformer weights to the weights from BERT LARGE and use the following parameters:</p><p>• We report results on all of the tasks from Section 3.1, using the same task-specific training methodology for both BERT EM and BERT EM +MTB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Few-shot Relation Matching</head><p>First, we investigate the ability of BERT EM +MTB to solve the FewRel task without any task-specific training data. Since FewRel is an exemplar-based approach, we can just rank each candidate rela-  tion statement according to its representation's inner product with the exemplars' representations. <ref type="figure">Figure 4</ref> shows that the task agnostic BERT EM and BERT EM +MTB models outperform the previous published state of the art on FewRel task even when they have not seen any FewRel training data. For BERT EM +MTB, the increase over <ref type="bibr" target="#b11">Han et al. (2018)</ref>'s supervised approach is very significant-8.8% on the 5-way-1-shot task and 12.7% on the 10-way-1-shot task. BERT EM +MTB also significantly outperforms BERT EM in this unsupervised setting, which is to be expected since there is no relation-specific loss during BERT EM 's training.</p><p>To investigate the impact of supervision on BERT EM and BERT EM +MTB, we introduce increasing amounts of FewRel's training data. <ref type="figure">Figure 4</ref> shows the increase in performance as we either increase the number of training examples for each relation type, or we increase the number of relation types in the training data. When given access to all of the training data, BERT EM approaches BERT EM +MTB's performance. However, when we keep all relation types during training, and vary the number of types per example, BERT EM +MTB only needs 6% of the training data to match the performance of a BERT EM model trained on all of the training data. We observe that maintaining a diversity of relation types, and reducing the number of examples per type, is the most effective way to reduce annotation effort for this task. The results in <ref type="figure">Figure 4</ref> show that MTB training could be used to significantly reduce effort in implementing an exemplar based relation extraction system.</p><p>Finally, we report BERT EM +MTB's performance on all of FewRel's fully supervised tasks in Table 3. We see that it outperforms the human upper bound reported by <ref type="bibr" target="#b11">Han et al. (2018)</ref>, and it significantly outperforms all other submissions to the FewRel leaderboard, published or unpublished.  outperform previously published results for these three tasks. The additional MTB based training further increases F1 scores for all tasks. We also analyzed the performance of our two models while reducing the amount of supervised task specific tuning data. The results displayed in <ref type="table" target="#tab_9">Table 5</ref> show the development set performance when tuning on a random subset of the task specific training data. For all tasks, we see that MTB based training is even more effective for low-resource cases, where there is a larger gap in performance between our BERT EM and BERT EM +MTB based classifiers. This further supports our argument that training by matching the blanks can significantly reduce the amount of human input required to create relation extractors, and populate a knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Supervised Relation Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper we study the problem of producing useful relation representations directly from text. We describe a novel training setup, which we call matching the blanks, which relies solely on entity resolution annotations. When coupled with a new architecture for fine-tuning relation representations in BERT, our models achieves state-ofthe-art results on three relation extraction tasks, and outperforms human accuracy on few-shot relation matching. In addition, we show how the new model is particularly effective in low-resource regimes, and we argue that it could significantly reduce the amount of human effort required to create relation extractors.</p><p>In future work, we plan to work on relation discovery by clustering relation statements that have similar representations according to BERT EM +MTB. This would take us some of the way toward our goal of truly general purpose relation identification and extraction. We will also study representations of relations and entities that can be used to store relation triples in a distributed knowledge base. This is inspired by recent work in knowledge base embedding <ref type="bibr" target="#b5">(Bordes et al., 2013;</ref><ref type="bibr" target="#b20">Nickel et al., 2016)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>"Matching the blanks" example where both relation statements share the same two entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of losses used in our models. The left figure depicts a model suitable for supervised training, where the model is expected to classify over a predefined dictionary of relation types. The figure on the right depicts a pairwise similarity loss used for few-shot classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Variants of architectures for extracting relation representations from deep Transformers network. Figure (a) depicts a model with STANDARD input and [CLS] output, Figure (b) depicts a model with STANDARD input and MENTION POOLING output and Figure (c) depicts a model with POSITIONAL EMBEDDINGS input and MENTION POOLING output. Figures (d), (e), and (f) use ENTITY MARKERS input while using [CLS], MENTION POOLING, and ENTITY START output, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Relation Statement Per class representation Softmax Deep Transformer Encoder Linear or Norm Layer Similarity score Deep Transformer Encoder Linear or Norm Layer Deep Transformer Encoder Linear or Norm Layer Query Relation Statement Candidate Relation Statement</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>F1 scores of BERTEM +MTB and BERTEM based relation classifiers on the respective test sets. Details of the SOTA systems are given inTable 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>contains results for our classifiers tuned on supervised relation extraction data. As was established in Section 3.2, our BERT EM based classifiers Comparison of classifiers tuned on FewRel. Results are for the development set while varying the amount of annotated examples available for fine-tuning. On the left, we display accuracies while varying the number of examples per relation type, while maintaining all 64 relations available for training. On the right, we display accuracy on the development set of the two models while varying the total number of relation types available for tuning, while maintaining all 700 examples per relation type. In both graphs, results for the 10-way-1-shot variant of the task are displayed.</figDesc><table><row><cell></cell><cell>85</cell><cell></cell><cell cols="3">BERTᴇᴍ</cell><cell cols="2">BERTᴇᴍ+MTB</cell><cell></cell><cell></cell><cell>85</cell><cell cols="2">BERTᴇᴍ</cell><cell>BERTᴇᴍ+MTB</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell></row><row><cell>Accuracy</cell><cell>70 75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>70 75</cell><cell></cell></row><row><cell></cell><cell>65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>65</cell><cell></cell></row><row><cell></cell><cell>60</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>40</cell><cell cols="3">80 160 320 700</cell><cell>60</cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">examples per relation type (log scale)</cell><cell></cell><cell></cell><cell></cell><cell>number of relation types</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">5 way 1 shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5 way 1 shot</cell></row><row><cell cols="5"># examples per type 0</cell><cell></cell><cell>5</cell><cell cols="3">20 80 320 700</cell><cell cols="2"># training types</cell><cell>0</cell><cell>5</cell><cell>16 32 64</cell></row><row><cell cols="4">Prot.Net. (CNN)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-71.6</cell><cell cols="3">Prot.Net. (CNN) -</cell><cell>-</cell><cell>-</cell><cell>-71.6</cell></row><row><cell></cell><cell cols="2">BERTEM</cell><cell></cell><cell cols="6">72.9 81.6 85.1 86.9 88.8 88.9</cell><cell></cell><cell>BERTEM</cell><cell>72.9 78.4 81.2 83.4 88.9</cell></row><row><cell cols="4">BERTEM +MTB</cell><cell cols="6">80.4 85.5 88.4 89.6 89.6 90.1</cell><cell cols="3">BERTEM +MTB 80.4 84.04 85.5 86.8 90.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">10 way 1 shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 way 1 shot</cell></row><row><cell cols="5"># examples per type 0</cell><cell></cell><cell>5</cell><cell cols="3">20 80 320 700</cell><cell cols="2"># training types</cell><cell>0</cell><cell>5</cell><cell>16 32 64</cell></row><row><cell cols="4">Prot.Net. (CNN)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-58.8</cell><cell cols="3">Prot.Net. (CNN) -</cell><cell>-</cell><cell>-</cell><cell>-58.8</cell></row><row><cell></cell><cell cols="2">BERTEM</cell><cell></cell><cell cols="6">62.3 72.8 76.9 79.0 81.4 82.8</cell><cell></cell><cell>BERTEM</cell><cell>62.3 68.9 71.9 74.3 81.4</cell></row><row><cell cols="4">BERTEM +MTB</cell><cell cols="6">71.5 78.1 81.2 82.9 83.7 83.4</cell><cell cols="3">BERTEM +MTB 71.5 76.2 76.9 78.5 83.7</cell></row><row><cell cols="4">Figure 4: % of training set</cell><cell cols="6">1% 10% 20% 50% 100%</cell><cell></cell><cell></cell></row><row><cell cols="5">SemEval 2010 Task 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">BERTEM</cell><cell></cell><cell cols="6">28.6 66.9 75.5 80.3 82.1</cell><cell></cell><cell></cell></row><row><cell cols="4">BERTEM +MTB</cell><cell cols="6">31.2 70.8 76.2 80.4 82.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">KBP-37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">BERTEM</cell><cell></cell><cell cols="6">40.1 63.6 65.4 67.8 69.5</cell><cell></cell><cell></cell></row><row><cell cols="4">BERTEM +MTB</cell><cell cols="6">44.2 66.3 67.2 68.8 70.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">TACRED</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">BERTEM</cell><cell></cell><cell cols="6">32.8 59.6 65.6 69.0 70.1</cell><cell></cell><cell></cell></row><row><cell cols="4">BERTEM +MTB</cell><cell cols="6">43.4 64.8 67.2 69.9 70.6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>F1 scores on development sets for supervised relation extraction tasks while varying the amount of tuning data available to our BERTEM and BERTEM +MTB models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the public Google Cloud Natural Language API to annotate our corpus extracting the "entity analysis" results -https://cloud.google.com/natural-language/ docs/basics#entity analysis .2  We use a window of 40 tokens, which we observed provides some coverage of long range entity relations, while avoiding a large number of co-occurring but unrelated entities.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Leveraging linguistic structure for open domain information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin Jose Johnson</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI&apos;07</title>
		<meeting>the 20th International Joint Conference on Artifical Intelligence, IJCAI&apos;07<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Position-aware self-attention with relative positional encodings for slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Bilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<idno>abs/1807.03052</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1145/1376616.1376746</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Empirical methods in information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="80" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuidó</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DIRT: Discovery of Inference Rules from Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<idno type="DOI">10.1145/502512.502559</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;01)</title>
		<meeting>the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;01)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="323" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervised open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="885" to="895" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Row-less universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Automated Knowledge Base Construction</title>
		<meeting>the 5th Workshop on Automated Knowledge Base Construction<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1123</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Relation classification via recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1508.01006</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
