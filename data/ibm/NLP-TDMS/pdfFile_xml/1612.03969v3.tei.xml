<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-05-10">10 May 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Courant Institute</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
							<email>aszlam@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
							<email>abordes@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Courant Institute</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-05-10">10 May 2017</date>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network <ref type="bibr" target="#b20">(Sukhbaatar et al., 2015)</ref>. Like a Neural Turing Machine or Differentiable Neural Computer <ref type="bibr" target="#b5">(Graves et al., 2014;</ref><ref type="bibr" target="#b24">2016)</ref> it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The essence of intelligence is the ability to predict. An intelligent agent must be able to predict unobserved facts about their environment from limited percepts <ref type="bibr">(visual, auditory, textual, or otherwise)</ref>, combined with their knowledge of the past. In order to reason and plan, they must be able to predict how an observed event or action will affect the state of the world. Arguably, the ability to maintain an estimate of the current state of the world, combined with a forward model of how the world evolves, is a key feature of intelligent agents.</p><p>A natural way for an agent to represent the world is to maintain a set of high-level concepts or entities together with their properties, which are updated as new information is received. For example, if a percept is the textual description of an event, such as "John walks out of the kitchen", the agent should learn to update its estimate of John's location, as well as the list (and number) of people present in each room. If John was carrying a bag, the location of the bag and the list of objects in the kitchen must also be updated. When we read a story, each sentence we read or hear causes us to update our internal representation of the current state of the world within the story. The flow of the story is captured by the evolution of this state of the world. At any given time, an agent typically receives limited information about the state of the world, and should therefore be able to infer new information through partial observation. In this paper, we investigate this problem through a simple story understanding scenario, in which the agent is given a sequence of textual statements and events, and then given another series of statements about the final state of the world. If the second series of statements is given in the form of questions about the final state of the world together with their correct answers, the agent should be able to learn from them and its performance can be measured by the accuracy of its answers.</p><p>Even with this weak form of supervision, the system may learn basic dynamical constraints about the world. For example, it may learn that a person or object cannot be in two locations at the same time, or may learn simple update rules such as incrementing and decrementing the number of persons or objects in a room. It may also learn basic rules of approximate (logical) inference, such as the fact that objects belonging to the same category tend to have similar properties (light objects can be carried over from rooms to rooms for instance).</p><p>We propose to handle this scenario with a new kind of memory-augmented neural network that uses a distributed memory and processor architecture: the Recurrent Entity Network (EntNet). The model consists of a fixed number of dynamic memory cells, each containing a vector key w j and a vector value (or content) h j . Each cell is associated with its own "processor", a simple gated recurrent network that may update the cell value given an input. If each cell learns to represent a concept or entity in the world, one can imagine a gating mechanism that, based on the key and content of the memory cells, will only modify the cells that concern the entities mentioned in the input. In the current version of the model, there is no direct interaction between the memory cells, hence the system can be seen as multiple identical processors functioning in parallel, with distributed local memory. Alternatively, the EntNet can be seen as a bank of gated RNNs (all sharing the same parameters), whose hidden states correspond to latent concepts and attributes, and whose parameters describe the laws of the world according to which the attributes of objects are updated. The sharing of these parameters reflects an invariance of these laws across object instances, similarly to how the weight tying scheme in a CNN reflects an invariance of image statistics across locations. Their hidden state is updated only when new information relevant to their concept is received, and remains otherwise unchanged. The keys used in the addressing/gating mechanism also correspond to concepts or entities, but are modified only during learning, not during inference.</p><p>The EntNet is able to solve all 20 bAbI question-answering tasks , a popular benchmark of story understanding, which to our knowledge sets a new state-of-the-art. Our experiments also indicate that the model indeed maintains an internal representation of the simplified world in which the stories take place, and that the model does not limit itself to storing the aspects of the world required to answer a specific question. We also introduce a new reasoning task which, unlike the bAbI tasks, requires a model to use a large number of supporting facts to answer the question, and show that the EntNet outperforms both LSTMs and Memory Networks <ref type="bibr" target="#b20">(Sukhbaatar et al., 2015)</ref> by a significant margin. It is also able to generalize to sequences longer than those seen during training. Finally, our model also obtains competitive results on the Childrens Book Test <ref type="bibr" target="#b10">(Hill et al., 2016)</ref>, and performs best among models that read the text in a single pass before receiving knowledge of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL</head><p>Our model is designed to process data in sequential form, and consists of three main parts: an input encoder, a dynamic memory and an output layer, which we now describe in detail. We developed it in the context of question answering on short stories where the inputs are word sequences, but the model could be adapted to many other contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">INPUT ENCODER</head><p>The encoding layer summarizes an element of the input sequence with a vector of fixed length. Typically the input element at time t is a sequence of words, e.g. a sentence or window of words. One is free to choose the encoding module to be any standard sequence encoder, which is an active area of research. Typical choices include a bag-of-words (BoW) representation or the final state of a recurrent neural net (RNN) run over the sequence. In this work, we use a simple encoder consisting of a learned multiplicative mask followed by a summation. More precisely, let the input at time t be a sequence of words with embeddings {e 1 , ..., e k }. The vector representation of this input is then:</p><formula xml:id="formula_0">s t = i f i ⊙ e i<label>(1)</label></formula><p>The same set of vectors {f 1 , ..., f k } are used at each time step and are learned jointly with the other parameters of the model. Note that the model can choose to adopt a standard BoW representation by setting all weights in the multiplicative mask to 1, or can choose a positional encoding model as used in <ref type="bibr" target="#b20">(Sukhbaatar et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DYNAMIC MEMORY</head><p>The dynamic memory is a gated recurrent network with a (partially) block structured weight tying scheme. We divide the hidden states of the network into blocks h 1 , ..., h m ; the full hidden state is the concatenation of the h j . In the experiments below, m is of the order of 5 to 20, and each block h j is of the order of 20 to 100 units.</p><p>At each time step t, the content of the hidden states {h j } (which we will call the jth memory) are updated using a set of key vectors {w j } and the encoded input s t . In its most general form, the update equations of our model are given by:</p><formula xml:id="formula_1">g j ← σ(s T t h j + s T t w j ) (2) h j ← φ(U h j + V w j + W s t ) (3) h j ← h j + g j ⊙h j (4) h j ← h j ||h j ||<label>(5)</label></formula><p>Here σ represents a sigmoid, g j is a gating function which determines how much the j th memory should be updated, andh j is the new candidate value of the memory to be combined with the existing memory h j . The function φ can be chosen from any number of activation functions, in our experiments we use either parametric ReLU non-linearities <ref type="bibr" target="#b9">(He et al., 2015)</ref> or the identity. The matrices U, V, W are typically trainable parameters of the model, and are shared between all the blocks. They can also be fixed to certain values, such as the identity or zero, to yield a simpler model which we use in some of our experiments.</p><p>The gating function g j contains two terms: a "content" term s T t h j which causes the gate to open for memory slots whose content matches the input, and a "location" term s T t w j which causes the gate to open for memory slots whose key matches the input. The final normalization step allows the model to forget previous information. To see this, note that since the memories lie on the unit sphere, all information is contained in their phase. Adding any vector to a given memory (other than the memory itself) will decrease the cosine distance between the original memory and the updated one. Therefore, as new information is added, old information is forgotten.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">OUTPUT MODULE</head><p>Whenever the model is required to produce an output, it is presented with a query vector q. Specifically, the output is computed using the following equations:</p><formula xml:id="formula_2">p j = Softmax(q T h j ) u = j p j h j y = Rφ(q + Hu)<label>(6)</label></formula><p>The matrices H and R are additional trainable parameters of the model. The output module can be viewed as a one-hop Memory Network <ref type="bibr" target="#b20">(Sukhbaatar et al., 2015)</ref> with an additional non-linearity φ between the internal state and the decoder matrix. If the memory slots correspond to specific words (as we will describe in the following section) which contain the answer, p can be viewed as a distribution over potential answers and can be used to make a prediction directly or fed into a loss function, removing the need for the last two steps.</p><p>The entire model (all three components described above) is trained via backpropagation through time, receiving gradients from any time steps where the reader is required to produce an output, which are then propagated through the unrolled network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATING EXAMPLE OF OPERATION</head><p>We now describe a motivating example of how our model can perform reasoning on-the-fly as it is ingesting input sequences. Let us suppose our model is reading a story, so the inputs are natural language sentences, and then it is required to answer questions about the story it has just read.</p><p>Our model is free to learn the key vectors w j for each memory j. One choice the model could make is to associate a single memory (via the key) with each entity in the story. The memory slot corresponding to a person could encode that person's location, the objects they are carrying, or the people they are with, depending on what information is relevant for the task at hand. As new information is received indicating that objects are acquired or discarded, or the person changes location, their memory slot will change accordingly. Similarly useful updates can be made for memories corresponding to object and location entities as well.</p><p>In fact, we could encode this choice of memories directly into our model, which we consider as a type of prior knowledge. By tying the weights of the key vectors with the embeddings of specific words, we can encourage the model to record information about certain words occuring in the text which we believe to be important. For example, given a list of named entities (which could be produced by a standard tagger), we could make the model have a separate memory slot for each entity. We consider this "tied" variant in our experiments. Since the list of entities is independent of the training data, this variant can handle entities not seen in the training set, as long as their embeddings can be initialized in a reasonable way (such as pre-training on a larger corpus). Now, consider that the model reads the following two sentences, and the desired behavior of the gating function and update function at each memory as they are seen:</p><p>• Mary picked up the ball.</p><p>• Mary went to the garden.</p><p>As the first sentence s t is ingested, and assuming memories encode entities, we would like the gates of the memories corresponding to both "Mary" and "ball" to activate. This is possible due to the location addressing term s T t w j which uses the key w j . We expect that a well trained model would learn to do this. The model would hence modify both the entry corresponding to "Mary" to indicate that she is now carrying the ball, and also the entry corresponding to "ball", to indicate that it is being carried by Mary. When the second sentence is seen, we would like the model to again modify the "Mary" entry to indicate that she is now in the garden, and also modify the "ball" entry to reflect its new location as well. Assuming the information for "Mary" is contained in the "ball" memory as described before, the gate corresponding to "ball" can activate due to the content addressing term s T t h j , even though the word "ball" does not occur in the second sentence. As before, the gate corresponding to the "Mary" entry can open due to the second term.</p><p>If the gating function and update function have weights such that the steps above are executed, then the memory will be in a state where questions such as "Where is the ball?" or "Where is Mary?" can be answered from the values of relevant memories, without the need for further complex reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>The EntNet is related to gated recurrent models such as the LSTM <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997)</ref> and GRU <ref type="bibr" target="#b2">(Cho et al., 2014)</ref>, which also use gates to fix or modify the information stored in the hidden state. However, these models use scalar memory cells with full interactions between them, whereas ours has separate memory slots which could be seen as groups of hidden units with tied weights in the gating and update functions. Another important difference is the content-based matching term between the input and hidden state, which is not present in these models.</p><p>Our model also shares some similarities with the DNC/NTM framework of <ref type="bibr" target="#b5">(Graves et al., 2014;</ref><ref type="bibr" target="#b24">2016)</ref>. There, as in our model, a block of hidden states acts as a set of read-writeable memories. On the other hand, the DNC has a relatively sophisticated controller network (such as an LSTM) which reads an input and outputs a number of interface vectors (such as keys and weightings) which are then combined via a softmax to read from and write to the external memory matrix. In contrast, our model can be viewed as a set of separate recurrent models whose hidden states store the memory slots. These hidden states are either fixed by the gates, or modified through a simple RNN-style update. The bulk of the reasoning is thus performed by these parallel recurrent models, rather than through a central controller. Moreover, instead of using a softmax, our model uses an independent gate for writing to each memory.</p><p>Our model is similar to a Memory Network and its variants <ref type="bibr" target="#b25">(Weston et al., 2014;</ref><ref type="bibr" target="#b20">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b1">Chandar et al., 2016;</ref><ref type="bibr" target="#b18">Miller et al., 2016)</ref> in the way it produces an output using a softmax over blocks of hidden states, and our encoding layer is inspired by techniques used in those works. However, Memory Networks explicitly store the entire input sequence in memory, and then sequentially update a controller's hidden state via a softmax gating over the memories. In contrast, our model keeps a fixed number of blocks of hiddens as memories and updates each block with an independent gated RNN. The Dynamic Memory Network of <ref type="bibr" target="#b27">(Xiong et al., 2016)</ref> also performs updates via a recurrent model, however it links memories to input tokens and updates them sequentially rather than in parallel.</p><p>The weight tying scheme and the parallel gated RNNs recall the gated graph network of <ref type="bibr" target="#b16">(Li et al., 2015)</ref>. If we interpret our work in that context, the "graph" is just a set of vertices with no edges; our gating mechanism is also somewhat different than the one they use. The CommNN model of <ref type="bibr" target="#b21">(Sukhbaatar et al., 2016)</ref>, the Interaction Network of (?), the Neural Physics Engine of (?) and the model of (?) also use a set of parallel recurrent models with tied weights, but differ from our model in their use of inter-network communication and the lack of a gating mechanism.</p><p>Finally, there is another class of recent models that have a writeable memory arranged as (unbounded) stacks, linked lists or queues <ref type="bibr" target="#b12">(Joulin &amp; Mikolov, 2015;</ref><ref type="bibr" target="#b7">Grefenstette et al., 2015)</ref>. Our model is different from these in that we use a key-value pair array instead of a stack, and in the experiments in this work, the array is of fixed size.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section we evaluate our model on three different datasets. Training details common to all experiments can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SYNTHETIC WORLD MODEL TASK</head><p>We first study our model's properties on a toy task designed to measure the ability to keep a world model in memory. In this task two agents are initially placed randomly on an 10×10 grid, and at each time step a randomly chosen agent either changes direction or moves ahead. After a certain number of time steps, the model is required to provide the locations of each of the agents, thus revealing its internal world model (details can be found in Appendix B). This task is challenging because the model must combine up to T − 2 supporting facts in order to answer the question correctly, and must also keep the locations of both agents in memory and update them at different times.</p><p>We compared the performance of a MemN2N, LSTM and EntNet. For the MemN2N, we set the number of hops equal to T −2 and the embedding dimension to d = 20. The EntNet had embedding dimension d = 20 and 5 memory slots, and the LSTM had 50 hidden units which resulted in it having significantly more parameters than the other two models. For each model, we repeated the experiment with 5 different initializations and reported the best performance. All models were trained with ADAM <ref type="bibr" target="#b14">(Kingma &amp; Ba, 2014)</ref> with initial learning rates set by grid search over {0.1, 0.01, 0.001} and divided by 2 every 10,000 updates. <ref type="table" target="#tab_1">Table 1a</ref> shows the results. The MemN2N has the worst performance, which degrades quickly as the length of the sequence increases. The LSTM performs better, but still loses accuracy as the length of the sequence increases. In contrast, the EntNet is able to solve the task in all cases.</p><p>The ability to generalize to sequences longer than those seen during training is a desirable property, which suggests that the network has learned the dynamics of the world it is trying to model. It also means the model can be trained less expensively. To study this, we trained an EntNet on variable length sequences between 1 and 20, and evaluated it on different length sequences longer than 20. Results are shown in <ref type="table" target="#tab_1">Table 1b</ref>. We see that the model is able to achieve good performance several times past its training horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">BABI TASKS</head><p>We next evaluate our model on the bAbI tasks, which are a collection of 20 synthetic questionanswering datasets first introduced in  designed to test a wide variety of reasoning abilities. They have since become a benchmark for memory-augmented neural networks and most of the related methods described in Section 4 have been tested on them. Performance is measured using two metrics: the average error across all tasks, and the number of failed tasks (more than 5% error). We used version 1.2 of the dataset with 10k samples. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>We used a similar training setup as <ref type="bibr" target="#b20">(Sukhbaatar et al., 2015)</ref>. All models were trained with ADAM using a learning rate of η = 0.01, which was divided by 2 every 25 epochs until 200 epochs were reached. Copying previous works <ref type="bibr" target="#b20">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b27">Xiong et al., 2016)</ref>, the capacity of the memory was limited to the most recent 70 sentences, except for task 3 which was limited to 130 sentences. Due to the high variance in model performance for some tasks, for each task we conducted 10 runs with different initializations and picked the best model based on performance on the validation set, as it has been done in previous work. In all experiments, our model had embedding dimension size d = 100 and 20 memory slots.</p><p>In <ref type="table" target="#tab_2">Table 2</ref> we compare our model to various other state-of-the-art models in the literature: the larger MemN2N reported in the appendix of <ref type="bibr" target="#b20">(Sukhbaatar et al., 2015)</ref>, the Dynamic Memory Network of <ref type="bibr" target="#b27">(Xiong et al., 2016)</ref>, the Dynamic Neural Turing Machine <ref type="bibr" target="#b8">(Gulcehre et al., 2016)</ref>, the Neural Turing Machine <ref type="bibr" target="#b5">(Graves et al., 2014)</ref> and the Differentiable Neural Computer <ref type="bibr" target="#b6">(Graves et al., 2016)</ref>. Our model is able to solve all the tasks, outperforming the other models in terms of both the number of solved tasks and the average error.</p><p>To analyze what kind of representations our model can learn, we conducted an additional experiment on Task 2 using a simple BoW sentence encoding and key vectors which were tied to entity embeddings. This was designed to make the model more interpretable, since the weight tying forces memory slots to encode information about specific entities. 2 After training, we ran the model over a story and computed the cosine distance between φ(Hh j ) and each row r i of the decoder matrix R. This gave us a score which measures the affinity between a given memory slot and each word in the vocabulary. <ref type="table" target="#tab_3">Table 3</ref> shows the nearest neighboring words for each memory slot (which itself corresponds to an entity). We see that the model has indeed stored locations of all of the objects and characters in its memory slots which reflect the final state of the story. In particular, it has the correct answer readily stored in the memory slot of the entity being inquired about (the milk). It also has correct location information about all other non-location entities stored in the appropriate memory slots. Note that it does not store useful or correct information in the memory slots corresponding to locations, most likely because this task does not contain questions about locations (such as "who is in the kitchen?").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">CHILDREN'S BOOK TEST (CBT)</head><p>We next evaluated our model on the Children's Book Test <ref type="bibr" target="#b10">(Hill et al., 2016)</ref>, which is a semantic language modeling (sentence completion) benchmark built from children's books that are freely available from Project Gutenberg 3 . Models are required to read 20 consecutive sentences from a given story and use this context to fill in a missing word from the 21st sentence. More specifically, each sample consists of a tuple (S, q, C, a) where S is the story consisting of 20 sentences, Q is the 21st sentence with one word replaced by a special blank token, C is a set of 10 candidate answers of the same type as the missing word (for example, common nouns or named entities), and a is the true answer (which is always contained in C).</p><p>It was shown in <ref type="bibr" target="#b10">(Hill et al., 2016)</ref> that methods with limited memory such as LSTMs perform well on more frequent, syntax based words such as prepositions and verbs, being similar to human performance, but poorly relative to humans on more semantically meaningful words such as named entities and common nouns. Therefore, most recent methods have been evaluated on the Named Entity and Common Noun subtasks, since they better test the ability of a model to make use of wider contextual information.</p><p>Training Details We adopted the same window memory approach used in <ref type="bibr" target="#b10">(Hill et al., 2016)</ref>, where each input corresponds to a window of text from</p><formula xml:id="formula_3">{w (i−b−1/2) ...w i ...w (i+(b−1)/2) } centered at a can- didate w i ∈ C.</formula><p>In our experiments we set b = 5. All models were trained using standard stochastic gradient descent (SGD) with a fixed learning rate of 0.001. We used separate input encodings for the update and gating functions, and applied a dropout rate of 0.5 to the word embedding dimensions. Key embeddings were tied to the embeddings of the candidate words, resulting in 10 hidden blocks, one per member of C. Due to the weight tying, we did not need a decoder matrix and used the distribution over candidates to directly produce a prediction, as described in Section 3.</p><p>We found that a simpler version of the model worked best, with U = V = 0, W = I and φ equal to the identity. We also removed the normalization step in this simplified model, which we found to hurt performance. This can be explained by the fact that the maximum frequency baseline model in <ref type="bibr" target="#b10">(Hill et al., 2016)</ref> has performance which is significantly higher than random, and including the normalization step hides this useful frequency-based information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We draw a distinction between two setups: the single-pass setup, where the model must read the story and query in order and immediately produce an output, and the multi-pass setup, where the model can use the query to perform attention over the story. The first setup is more challenging  <ref type="bibr" target="#b13">(Kadlec et al., 2016)</ref> 0.686 0.634 Gated-Attention Reader (Bhuwan <ref type="bibr" target="#b0">Dhingra &amp; Salakhutdinov, 2016)</ref> 0.690 0.639 EpiReader <ref type="bibr" target="#b22">(Trischler et al., 2016)</ref> 0.697 0.674 AoA Reader <ref type="bibr" target="#b4">(Cui et al., 2016)</ref> 0.720 0.694 NSE Adaptive Computation <ref type="bibr" target="#b19">(Munkhdalai &amp; Yu, 2016)</ref> 0.732 0.714 because the model does not know beforehand which query it will be presented with, and must learn to retain information which is useful for a wide variety of potential queries. For this reason it can be viewed as a test of the model's ability to construct a general-purpose representation of the current state of the story. The second setup leverages all available information, and allows the model to use knowledge of which question will be asked when it reads the story.</p><p>In <ref type="table" target="#tab_4">Table 4</ref>, we show the performance of the general EntNet, the simplified EntNet, as well as other single-pass models taken from <ref type="bibr" target="#b10">(Hill et al., 2016)</ref>. The general EntNet performs better than the LSTMs and n-gram model on the Named Entities Task, but lags behind on the Common Nouns task. The simplified EntNet outperforms all other single-pass models on both tasks, and also performs better than the Memory Network which does not use the self-supervision heuristic. However, there is still a performance gap when compared to more sophisticated machine comprehension models, many of which perform multiple layers of attention over the story using query knowledge. The fact that the simplified EntNet is able to obtain decent performance is encouraging since it indicates that the model is able to build an internal representation of the story which it can then use to answer a relatively diverse set of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Two closely related challenges in artificial intelligence are designing models which can maintain an estimate of the state of a world with complex dynamics over long timescales, and models which can predict the forward evolution of the state of the world from partial observation. In this paper, we introduced the Recurrent Entity Network, a new model that makes a promising step towards the first goal. Our model is able to accurately track the world state while reading text stories, which enables it to set a new state-of-the-art on the bAbI tasks, the competitive benchmark of story understanding, by being the first model to solve them all. We also showed that our model is able to capture simple dynamics over long timescales, and is able to perform competitively on a real-world dataset.</p><p>Although our model was able to solve all the bAbI tasks using 10k training samples, we found that performance dropped considerably when using only 1k samples (see Appendix). Most recent work on the bAbI tasks has focused on the 10k samples setting, and we would like to emphasize that solving them in the 1k samples setting remains an open problem which will require improving the sample efficiency of reasoning models, including ours.</p><p>Recent works have made some progress towards the second goal of forward modeling, for instance in capturing simple physics <ref type="bibr" target="#b15">(Lerer et al., 2016)</ref>, predicting future frames in video <ref type="bibr" target="#b17">(Mathieu et al., 2015)</ref> or responses in dialog . Although we have only applied our model to tasks with textual inputs in this work, the architecture is general and future work should investigate how to combine the EntNet's tracking abilities with such predictive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TRAINING DETAILS</head><p>All models were implemented using Torch <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref>. In all experiments, we initialized our model by drawing weights from a Gaussian distribution with mean zero and standard deviation 0.1, except for the PReLU slopes and encoder weights which were initialized to 1. Note that the PReLU initialization is related to two of the heuristics used in <ref type="bibr" target="#b20">(Sukhbaatar et al., 2015)</ref>, namely starting training with a purely linear model, and adding non-linearities to half of the hidden units.</p><p>Our initialization allows the model to choose when and how much to enter the non-linear regime. Initializing the encoder weights to 1 corresponds to beginning with a BoW encoding, which the model can then choose to modify. The initial values of the memory slots were initialized to the key values, which we found to help performance. Optimization was done with SGD or ADAM using minibatches of size 32, and gradients with norm greater than 40 were clipped to 40. A null symbol whose embedding was constrained to be zero was used to pad all sentences or windows to a fixed size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DETAILS OF WORLD MODEL EXPERIMENTS</head><p>Two agents are initially placed at random on a 10 × 10 grid with 100 distinct locations {(1, 1), (1, 2), ...(9, 10), (10, 10)}. At each time step an agent is chosen at random. There are two types of actions: the agent can face a given direction, or can move a number of steps ahead. Actions are sampled until a legal action is found by either choosing to change direction or move with equal probability. If they change direction, the direction is chosen between north, south, east and west with equal probability. If they move, the number of steps is randomly chosen between 1 and 5. A legal action is one which does not place the agent off the grid. Stories are given to the network in textual form, an example of which is below. The first action after each agent is placed on the grid is to face a given direction. Therefore, the maximum number of actions made by one agent is T − 2. The network learns word embeddings for all words in the vocabulary such as locations, agent identifiers and actions. At question time, the model must predict the correct answer (which will always be a location) from all the tokens in the vocabulary.</p><p>agent1 is at (2,8) agent1 faces-N agent2 is at (9,7) agent2 faces-N agent2 moves-2 agent2 faces-E agent2 moves-1 agent1 moves-1 agent2 faces-S agent2 moves-5 Q1: where is agent1 ? Q2: where is agent2 ? A1: (2,9) A2: (10,4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL RESULTS ON BABI TASKS</head><p>We provide some additional experiments on the bAbI tasks, in order to better understand the influence of architecture, weight tying, and amount of training data. <ref type="table" target="#tab_5">Table 5</ref> shows results when a simple BoW encoding is used for the inputs. Here, the EntNet still performs better than a MemN2N which uses the same encoding scheme, indicating that the architecture has an important effect. Tying the key vectors to entities did not help, and hurt performance for some tasks. <ref type="table" target="#tab_6">Table 6</ref> shows results when using only 1k training samples. In this setting, the EntNet performs worse than the MemN2N. <ref type="table">Table 7</ref> shows results for the EntNet and the DNC when models are trained on all tasks jointly. We report results for the mean performance across different random seeds (20 for the DNC, 5 for the EntNet), as well as the performance for the single best seed (measured by validation error). The DNC results for mean performance were taken from the appendix of <ref type="bibr" target="#b6">Graves et al. (2016)</ref>. The DNC has better performance in terms of the best seed, but also exhibits high variation across seeds, indicating that many different runs are required to achieve good performance. The EntNet exhibits less variation across runs and is able to solve more tasks consistently. Note that <ref type="table" target="#tab_2">Table 2</ref> reports DNC results with joint training, since results when training on each task separately were not available.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Diagram of the Recurrent Entity Network's dynamic memory. Update equations 1 and 2 are represented by the module f θ , where θ is the set of trainable parameters. Equations 3 and 4 are represented by the gate, since they fullfill a similar function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>: a) Error of different models on the World Model Task. b) Generalization of an EntNet trained up to T = 20. All errors range from 0 to 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on bAbI Tasks with 10k training samples.</figDesc><table><row><cell>Task</cell><cell cols="6">NTM D-NTM MemN2N DNC DMN+ EntNet</cell></row><row><cell>1: 1 supporting fact</cell><cell>31.5</cell><cell>4.4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>2: 2 supporting facts</cell><cell>54.5</cell><cell>27.5</cell><cell>0.3</cell><cell>0.4</cell><cell>0.3</cell><cell>0.1</cell></row><row><cell>3: 3 supporting facts</cell><cell>43.9</cell><cell>71.3</cell><cell>2.1</cell><cell>1.8</cell><cell>1.1</cell><cell>4.1</cell></row><row><cell>4: 2 argument relations</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>5: 3 argument relations</cell><cell>0.8</cell><cell>1.7</cell><cell>0.8</cell><cell>0.8</cell><cell>0.5</cell><cell>0.3</cell></row><row><cell>6: yes/no questions</cell><cell>17.1</cell><cell>1.5</cell><cell>0.1</cell><cell>0</cell><cell>0</cell><cell>0.2</cell></row><row><cell>7: counting</cell><cell>17.8</cell><cell>6.0</cell><cell>2.0</cell><cell>0.6</cell><cell>2.4</cell><cell>0</cell></row><row><cell>8: lists/sets</cell><cell>13.8</cell><cell>1.7</cell><cell>0.9</cell><cell>0.3</cell><cell>0.0</cell><cell>0.5</cell></row><row><cell>9: simple negation</cell><cell>16.4</cell><cell>0.6</cell><cell>0.3</cell><cell>0.2</cell><cell>0.0</cell><cell>0.1</cell></row><row><cell>10: indefinite knowledge</cell><cell>16.6</cell><cell>19.8</cell><cell>0</cell><cell>0.2</cell><cell>0</cell><cell>0.6</cell></row><row><cell>11: basic coreference</cell><cell>15.2</cell><cell>0</cell><cell>0.0</cell><cell>0</cell><cell>0.0</cell><cell>0.3</cell></row><row><cell>12: conjunction</cell><cell>8.9</cell><cell>6.2</cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell>0</cell></row><row><cell cols="2">13: compound coreference 7.4</cell><cell>7.5</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.3</cell></row><row><cell>14: time reasoning</cell><cell>24.2</cell><cell>17.5</cell><cell>0.2</cell><cell>0.4</cell><cell>0.2</cell><cell>0</cell></row><row><cell>15: basic deduction</cell><cell>47.0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>16: basic induction</cell><cell>53.6</cell><cell>49.6</cell><cell>51.8</cell><cell>55.1</cell><cell>45.3</cell><cell>0.2</cell></row><row><cell>17: positional reasoning</cell><cell>25.5</cell><cell>1.2</cell><cell>18.6</cell><cell>12.0</cell><cell>4.2</cell><cell>0.5</cell></row><row><cell>18: size reasoning</cell><cell>2.2</cell><cell>0.2</cell><cell>5.3</cell><cell>0.8</cell><cell>2.1</cell><cell>0.3</cell></row><row><cell>19: path finding</cell><cell>4.3</cell><cell>39.5</cell><cell>2.3</cell><cell>3.9</cell><cell>0.0</cell><cell>2.3</cell></row><row><cell>20: agent's motivation</cell><cell>1.5</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Failed Tasks (&gt; 5% error): 16</cell><cell>9</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>0</cell></row><row><cell>Mean Error:</cell><cell>20.1</cell><cell>12.8</cell><cell>4.2</cell><cell>3.8</cell><cell>2.8</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>On the left, the network's final "world model" after reading the story on the right. First and second nearest neighbors from each memory slot are shown, along with their cosine distance.</figDesc><table><row><cell>Key</cell><cell>1-NN</cell><cell>2-NN</cell><cell>Story</cell></row><row><cell>football</cell><cell cols="2">hallway (0.135) dropped (0.056)</cell><cell>mary got the milk there</cell></row><row><cell>milk</cell><cell>garden (0.111)</cell><cell>took (0.011)</cell><cell>john moved to the bedroom</cell></row><row><cell>john</cell><cell cols="2">kitchen (0.501) dropped (0.027)</cell><cell>sandra went back to the kitchen</cell></row><row><cell>mary</cell><cell>garden (0.442)</cell><cell>took (0.034)</cell><cell>mary travelled to the hallway</cell></row><row><cell>sandra</cell><cell cols="2">hallway (0.394) kitchen (0.121)</cell><cell>john got the football there</cell></row><row><cell>daniel</cell><cell cols="2">hallway (0.689) to (0.076)</cell><cell>john went to the hallway</cell></row><row><cell cols="3">bedroom hallway (0.367) dropped (0.075)</cell><cell>john put down the football</cell></row><row><cell>kitchen</cell><cell cols="2">kitchen (0.483) daniel (0.029)</cell><cell>mary went to the garden</cell></row><row><cell>garden</cell><cell>garden (0.281)</cell><cell>where (0.026)</cell><cell>john went to the kitchen</cell></row><row><cell>hallway</cell><cell cols="2">hallway (0.475) left (0.060)</cell><cell>sandra travelled to the hallway</cell></row><row><cell></cell><cell></cell><cell></cell><cell>daniel went to the hallway</cell></row><row><cell></cell><cell></cell><cell></cell><cell>mary discarded the milk</cell></row><row><cell></cell><cell></cell><cell></cell><cell>where is the milk ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>answer: garden</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy on CBT test set. Single-pass models encode the document before seeing the query, multi-pass models have access to the query at read time.</figDesc><table><row><cell>Model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Error rates on bAbI Tasks with inputs are encoded using BoW. "Tied" refers to the case where key vectors are tied with entity embeddings.</figDesc><table><row><cell>Task</cell><cell cols="3">MemN2N EntNet-tied EntNet</cell></row><row><cell>1: 1 supporting fact</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>2: 2 supporting facts</cell><cell>0.6</cell><cell>3.0</cell><cell>1.2</cell></row><row><cell>3: 3 supporting facts</cell><cell>7</cell><cell>9.6</cell><cell>9.0</cell></row><row><cell>4: 2 argument relations</cell><cell>32.6</cell><cell>33.8</cell><cell>31.8</cell></row><row><cell>5: 3 argument relations</cell><cell>10.2</cell><cell>1.7</cell><cell>3.5</cell></row><row><cell>6: yes/no questions</cell><cell>0.2</cell><cell>0</cell><cell>0</cell></row><row><cell>7: counting</cell><cell>10.6</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>8: lists/sets</cell><cell>2.6</cell><cell>0.1</cell><cell>0.3</cell></row><row><cell>9: simple negation</cell><cell>0.3</cell><cell>0</cell><cell>0</cell></row><row><cell>10: indefinite knowledge</cell><cell>0.5</cell><cell>0</cell><cell>0</cell></row><row><cell>11: basic coreference</cell><cell>0</cell><cell>0.3</cell><cell>0</cell></row><row><cell>12: conjunction</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">13: compound coreference 0</cell><cell>0.2</cell><cell>0.4</cell></row><row><cell>14: time reasoning</cell><cell>0.1</cell><cell>6.2</cell><cell>0.1</cell></row><row><cell>15: basic deduction</cell><cell>11.4</cell><cell>12.5</cell><cell>12.1</cell></row><row><cell>16: basic induction</cell><cell>52.9</cell><cell>46.5</cell><cell>0</cell></row><row><cell>17: positional reasoning</cell><cell>39.3</cell><cell>40.5</cell><cell>40.5</cell></row><row><cell>18: size reasoning</cell><cell>40.5</cell><cell>44.2</cell><cell>45.7</cell></row><row><cell>19: path finding</cell><cell>74.4</cell><cell>75.1</cell><cell>74.0</cell></row><row><cell>20: agent's motivation</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Failed Tasks (&gt; 5%):</cell><cell>9</cell><cell>8</cell><cell>6</cell></row><row><cell>Mean Error:</cell><cell>15.6</cell><cell>13.7</cell><cell>10.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on bAbI Tasks with 1k samples.</figDesc><table><row><cell>Task</cell><cell cols="2">MemN2N EntNet</cell></row><row><cell>1: 1 supporting fact</cell><cell>0</cell><cell>0.7</cell></row><row><cell>2: 2 supporting facts</cell><cell>8.3</cell><cell>56.4</cell></row><row><cell>3: 3 supporting facts</cell><cell>40.3</cell><cell>69.7</cell></row><row><cell>4: 2 argument relations</cell><cell>2.8</cell><cell>1.4</cell></row><row><cell>5: 3 argument relations</cell><cell>13.1</cell><cell>4.6</cell></row><row><cell>6: yes/no questions</cell><cell>7.6</cell><cell>30.0</cell></row><row><cell>7: counting</cell><cell>17.3</cell><cell>22.3</cell></row><row><cell>8: lists/sets</cell><cell>10.0</cell><cell>19.2</cell></row><row><cell>9: simple negation</cell><cell>13.2</cell><cell>31.5</cell></row><row><cell>10: indefinite knowledge</cell><cell>15.1</cell><cell>15.6</cell></row><row><cell>11: basic coreference</cell><cell>0.9</cell><cell>8.0</cell></row><row><cell>12: conjunction</cell><cell>0.2</cell><cell>0.8</cell></row><row><cell cols="2">13: compound coreference 0.4</cell><cell>9.0</cell></row><row><cell>14: time reasoning</cell><cell>1.7</cell><cell>62.9</cell></row><row><cell>15: basic deduction</cell><cell>0</cell><cell>57.8</cell></row><row><cell>16: basic induction</cell><cell>1.3</cell><cell>53.2</cell></row><row><cell>17: positional reasoning</cell><cell>51.0</cell><cell>46.4</cell></row><row><cell>18: size reasoning</cell><cell>11.1</cell><cell>8.8</cell></row><row><cell>19: path finding</cell><cell>82.8</cell><cell>90.4</cell></row><row><cell>20: agent's motivation</cell><cell>0</cell><cell>2.6</cell></row><row><cell>Failed Tasks (&gt; 5%):</cell><cell>11</cell><cell>15</cell></row><row><cell>Mean Error:</cell><cell>13.9</cell><cell>29.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code to reproduce these experiments can be found at https://github.com/facebook/MemNN/tree/master/EntNet-babi.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For most tasks including this one, tying key vectors did not significantly change performance, although it hurt in a few cases (see Appendix C). Therefore we did not apply it inTable 2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">www.gutenberg.org</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gatedattention readers for text comprehension. CoRR, abs/1606.01549</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.01549" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sungjin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07427</idno>
		<title level="m">Hierarchical memory networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W/W14/W14-4012.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clment</forename><surname>Farabet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attentionover-attention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhipeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shijin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno>abs/1607.04423</idno>
		<ptr target="http://arxiv.org/abs/1607.04423" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural Turing Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dnihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.5401" />
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malcolm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agnieszka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to transduce with unbounded memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1828" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dynamic neural turing machines with soft and hard addressing schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1607.00036</idno>
		<ptr target="http://arxiv.org/abs/1607.00036" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inferring algorithmic patterns with stack-augmented recurrent nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01007</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kleindienst</forename></persName>
		</author>
		<idno>abs/1603.01547</idno>
		<ptr target="http://arxiv.org/abs/1603.01547" />
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning physical intuition of block towers by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v48/lerer16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
		<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="430" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1511.05493</idno>
		<ptr target="http://arxiv.org/abs/1511.05493" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error. CoRR, abs/1511.05440</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.05440" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reasoning with memory augmented neural networks for language comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1610.06454</idno>
		<ptr target="https://arxiv.org/abs/1610.06454" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Endto-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sainbayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiagent communication with backpropagation. CoRR, abs/1605.07736</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sainbayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.07736" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Natural language comprehension with the epireader. CoRR, abs/1606.02270</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.02270" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dialog-based language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1604.06045</idno>
		<ptr target="http://arxiv.org/abs/1604.06045" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<ptr target="http://arxiv.org/abs/1410.3916" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1502.05698</idno>
		<ptr target="http://arxiv.org/abs/1502.05698" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
