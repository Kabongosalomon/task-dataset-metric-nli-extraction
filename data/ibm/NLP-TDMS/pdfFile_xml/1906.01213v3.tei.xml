<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Self-Supervised Attention Learning for Aspect-Level Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialong</forename><surname>Tang</surname></persName>
							<email>jialong2019@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Ge</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Self-Supervised Attention Learning for Aspect-Level Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect-level sentiment classification (ASC), as an indispensable task in sentiment analysis, aims at inferring the sentiment polarity of an input sentence in a certain aspect. In this regard, pre- * Equal contribution † Corresponding author 1 https://github.com/DeepLearnXMU/PSSAttention</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In aspect-level sentiment classification (ASC), it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. However, such a mechanism tends to excessively focus on a few frequent words with sentiment polarities, while ignoring infrequent ones. In this paper, we propose a progressive self-supervised attention learning approach for neural ASC models, which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms. Specifically, we iteratively conduct sentiment predictions on all training instances. Particularly, at each iteration, the context word with the maximum attention weight is extracted as the one with active/misleading influence on the correct/incorrect prediction of every instance, and then the word itself is masked for subsequent iterations. Finally, we augment the conventional training objective with a regularization term, which enables ASC models to continue equally focusing on the extracted active context words while decreasing weights of those misleading ones. Experimental results on multiple datasets show that our proposed approach yields better attention mechanisms, leading to substantial improvements over the two stateof-the-art neural ASC models. Source code and trained models are available. 1 vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine <ref type="bibr" target="#b5">(Kiritchenko et al., 2014;</ref><ref type="bibr" target="#b18">Wagner et al., 2014)</ref>. Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models <ref type="bibr" target="#b17">(Tang et al., 2016b;</ref><ref type="bibr" target="#b20">Wang et al., 2016;</ref><ref type="bibr" target="#b16">Tang et al., 2016a;</ref><ref type="bibr" target="#b11">Ma et al., 2017;</ref><ref type="bibr" target="#b19">Wang et al., 2018)</ref>, which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models.</p><p>However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low-frequency ones. As a result, the performance of attentional neural ASC models is still far from satisfaction. We speculate that this is because there exist widely "apparent patterns" and "inapparent patterns". Here, "apparent patterns" are interpreted as high-frequency words with strong sentiment polarities and "inapparent patterns" are referred to as low-frequency ones in training data. As mentioned in <ref type="bibr" target="#b22">Xu et al., 2018;</ref><ref type="bibr">Lin et al., 2017)</ref>, NNs are easily affected by these two modes: "apparent patterns" tend to be overly learned while "inapparent patterns" often can not be fully learned.</p><p>Here we use sentences in <ref type="table" target="#tab_1">Table 1</ref> to explain this defect. In the first three training sentences, given the fact that the context word "small" occurs frequently with negative sentiment, the atten-  tion mechanism pays more attention to it and directly relates the sentences containing it with negative sentiment. This inevitably causes another informative context word "crowded" to be partially neglected in spite of it also possesses negative sentiment. Consequently, a neural ASC model incorrectly predicts the sentiment of the last two test sentences: in the first test sentence, the neural ASC model fails to capture the negative sentiment implicated by "crowded"; while, in the second test sentence, the attention mechanism directly focuses on "small" though it is not related to the given aspect. Therefore, we believe that the attention mechanism for ASC still leaves tremendous room for improvement.</p><p>One potential solution to the above-mentioned issue is supervised attention, which, however, is supposed to be manually annotated, requiring labor-intense work. In this paper, we propose a novel progressive self-supervised attention learning approach for neural ASC models. Our method is able to automatically and incrementally mine attention supervision information from a training corpus, which can be exploited to guide the training of attention mechanisms in ASC models. The basic idea behind our approach roots in the following fact: the context word with the maximum attention weight has the greatest impact on the sentiment prediction of an input sentence. Thus, such a context word of a correctly predicted training instance should be taken into consideration during the model training. In contrast, the context word in an incorrectly predicted training instance ought to be ignored. To this end, we iteratively conduct sentiment predictions on all training instances. Particularly, at each iteration, we extract the context word with the maximum attention weight from each training instance to form attention supervision information, which can be used to guide the training of attention mechanism: in the case of correct prediction, we will remain this word to be considered; otherwise, the attention weight of this word is expected to be decreased. Then, we mask all extracted context words of each training instance so far and then refollow the above process to discover more supervision information for attention mechanisms. Finally, we augment the standard training objective with a regularizer, which enforces attention distributions of these mined context words to be consistent with their expected distributions.</p><p>Our main contributions are three-fold: (1) Through in-depth analysis, we point out the existing drawback of the attention mechanism for ASC.</p><p>(2) We propose a novel incremental approach to automatically extract attention supervision information for neural ASC models. To the best of our knowledge, our work is the first attempt to explore automatic attention supervision information mining for ASC. (3) We apply our approach to two dominant neural ASC models: Memory Network (MN) <ref type="bibr" target="#b17">(Tang et al., 2016b;</ref><ref type="bibr" target="#b19">Wang et al., 2018)</ref> and Transformation Network (TNet) . Experimental results on several benchmark datasets demonstrate the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we give brief introductions to MN and TNet, which both achieve satisfying performance and thus are chosen as the foundations of our work. Here we introduce some notations to facilitate subsequent descriptions: x= (x 1 , x 2 , ..., x N ) is the input sentence, t= (t 1 , t 2 , ..., t T ) is the given target aspect, y, y p ∈{Positive, Negative, Neutral} denote the ground-truth and the predicted sentiment, respectively.  MN <ref type="bibr" target="#b17">(Tang et al., 2016b;</ref><ref type="bibr" target="#b19">Wang et al., 2018)</ref>. The framework illustration of MN is given in <ref type="figure" target="#fig_0">Figure  1</ref>. We first introduce an aspect embedding matrix converting each target aspect word t j into a vector representation, and then define the final vector representation v(t) of t as the averaged aspect embedding of its words. Meanwhile, another embedding matrix is used to project each context word x i to the continuous space stored in memory, denoted by m i . Then, an internal attention mechanism is applied to generate the aspectrelated semantic representation o of the sentence</p><formula xml:id="formula_0">… " # " $ " % … ℎ # ℎ % … ' # ' % ((*) ' $ Attention ℎ $ , - … CPT CPT CPT … , - " # " $ " % Bi-LSTM L× ℎ # (./#) ℎ $ (./#) ℎ # (.) ℎ $ (.) CNN/Attention … * # * $ * 0 … … … … … … * # * $ * 1 ℎ 2 (./#) Bi-LSTM Attention ((*)</formula><formula xml:id="formula_1">… " # " $ " % … ℎ # ℎ % … ' # ' % ((*) ' $ Attention ℎ $ , - … CPT CPT CPT … , - " # " $ " % Bi-LSTM ×L ℎ # (./#) ℎ $ (./#) ℎ # (.) ℎ $ (.) CNN/Attention … * # * $ * 0 … … … … … … … * # * $ * 1 ℎ 2 (./#) Bi-LSTM Attention ((*) Gating Fully-connected CPM ℎ 2 (.) ℎ % (./#) ℎ % (.)</formula><formula xml:id="formula_2">x: o = i softmax(v T t M m i )h i ,</formula><p>where M is an attention matrix and h i is the final semantic representation of x i , induced from a context word embedding matrix. Finally, we use a fully connected output layer to conduct classification based on o and v(t).</p><p>TNet . <ref type="figure" target="#fig_1">Figure 2</ref> provides the framework illustrations of TNet, which mainly consists of three components:</p><p>(1) The bottom layer is a Bi-LSTM that transforms the input x into the contextualized word representations</p><formula xml:id="formula_3">h (0) (x)=(h (0) 1 , h (0) 2 , ..., h (0) N ) (i.e. hidden states of Bi-LSTM).</formula><p>(2) The middle part, as the core of the whole model, contains L layers of Context-Preserving Transformation (CPT), where word representations are updated as h (l+1) (x)=CPT(h (l) (x)). The key operation of CPT layers is Target-Specific Transformation. It contains another Bi-LSTM for generating v(t) via an attention mechanism, and then incorporates v(t) into the word representations. Besides, CPT layers are also equipped with a Context-Preserving Mechanism (CPM) to preserve the context information and learn more abstract word-level features. In the end, we obtain the word-level semantic representations</p><formula xml:id="formula_4">h(x)=(h 1 ,h 2 ...,h N ), with h i =h (L) i . (3)</formula><p>The topmost part is a CNN layer used to produce the aspect-related sentence representation o for the sentiment classification.</p><p>In this work, we consider another alternative to the original TNet, which replaces its topmost CNN with an attention mechanism to produce the aspect-related sentence representation as o=Atten(h(x), v(t)). In Section 4, we will investigate the performance of the original TNet and its variant equipped with an attention mechanism, denoted by TNet-ATT.</p><p>Training Objective. Both of the abovementioned models take the negative log-likelihood of the gold-truth sentiment tags as their training objectives:</p><formula xml:id="formula_5">J(D; θ) = − (x,t,y)∈D J(x, t, y; θ) = (x,t,y)∈D d(y) · logd(x, t; θ),<label>(1)</label></formula><p>where D is the training corpus, d(y) is the one-hot vector of y, d(x, t; θ) is the model-predicted sentiment distribution for the pair (x,t), and · denotes the dot product of two vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>In this section, we first describe the basic intuition behind our approach and then provide its details. Finally, we elaborate how to incorporate the mined supervision information for attention mechanisms into neural ASC models. It is noteworthy that our method is only applied to the training optimization of neural ASC models, without any impact on the model testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Intuition</head><p>The basic intuition of our approach stems from the following fact: in attentional ASC models, the importance of each context word on the given aspect mainly depends on its attention weight. Thus, the context word with the maximum attention weight has the most important impact on the sentiment prediction of the input sentence. Therefore, for a training sentence, if the prediction of ASC model is correct, we believe that it is reasonable to continue focusing on this context word. Conversely, the attention weight of this context word should be decreased. However, as previously mentioned, the context word with the maximum attention weight is often the one with strong sentiment polarity. It usually occurs frequently in the training corpus and thus tends to be overly considered during model training. This simultaneously leads to the insufficient learning of other context words, especially low-frequency ones with sentiment polarities. To address this problem, one intuitive and feasible method is to first shield the influence of this most important context word before reinvestigating effects of remaining context words of the training instance. In that case, other low-frequency context words with sentiment polarities can be discovered according to their attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Details of Our Approach</head><p>Based on the above analysis, we propose a novel incremental approach to automatically mine influential context words from training instances, which can be then exploited as attention supervision information for neural ASC models.</p><p>As shown in Algorithm 1, we first use the initial training corpus D to conduct model training, and then obtain the initial model parameters θ (0) (Line 1). Then, we continue training the model for K iterations, where influential context words of all training instances can be iteratively extracted (Lines 6-25). During this process, for each training instance (x, t, y), we introduce two word sets initialized as ∅ (Lines 2-5) to record its extracted context words: (1) s a (x) consists of context words with active effects on the sentiment prediction of x. Each word of s a (x) will be encouraged to remain considered in the refined model training, and (2) s m (x) contains context words with misleading Algorithm 1 : Neural ASC Model Training with Automatically Mined Attention Supervision Information.</p><p>Input: D: the initial training corpus;</p><p>θ init : the initial model parameters;</p><p>α: the entropy threshold of attention weight distribution; K: the maximum number of training iterations; 1:</p><formula xml:id="formula_6">θ (0) ← Train(D; θ init ) 2: for (x, t, y) ∈ D do 3: sa(x) ← ∅ 4:</formula><p>sm(x) ← ∅ 5: end for 6: for k = 1, 2..., K do 7:</p><formula xml:id="formula_7">D (k) ← ∅ 8:</formula><p>for (x, t, y) ∈ D do 9:</p><p>v(t) ← GenAspectRep(t, θ (k−1) ) 10:</p><p>x ← MaskWord(x, sa(x), sm(x)) 11:</p><p>h</p><formula xml:id="formula_8">(x ) ← GenWordRep(x , v(t), θ (k−1) ) 12: yp, α(x ) ← SentiPred(h(x ), v(t), θ (k−1) ) 13: E(α(x )) ← CalcEntropy(α(x )) 14: if E(α(x )) &lt; α then 15: m ← argmax 1≤i≤N α(x i ) 16:</formula><p>if yp == y then 17:</p><p>sa</p><formula xml:id="formula_9">(x) ← sa(x) ∪ {x m } 18: else 19: sm(x) ← sm(x) ∪ {x m } 20: end if 21: end if 22: D (k) ← D (k) ∪ (x , t, y) 23:</formula><p>end for 24: θ (k) ← Train(D (k) ; θ (k−1) ) 25: end for 26: Ds ← ∅ 27: for (x, t, y) ∈ D do 28:</p><p>Ds ← Ds ∪ (x, t, y, sa(x), sm(x)) 29: end for 30: θ ← Train(Ds) Return: θ; effects, whose attention weights are expected to be decreased. Specifically, at the k-th training iteration, we adopt the following steps to deal with (x, t, y):</p><p>In</p><p>Step 1, we first apply the model parameters θ (k−1) of the previous iteration to generate the aspect representation v(t) (Line 9). Importantly, according to s a (x) and s m (x), we then mask all previously extracted context words of x to create a new sentence x , where each masked word is replaced with a special token " mask " (Line 10). In this way, the effects of these context words will be shielded during the sentiment prediction of x , and thus other context words can be potentially extracted from x . Finally, we generate the word representations h(x )={h(x i )} N i=1 (Line 11). In Step 2, on the basis of v(t) and h(x ), we   <ref type="figure">E(α(x )</ref>) denotes the entropy of the attention weight distribution α(x ), α is entropy threshold set as 3.0, and x m indicates the context word with the maximum attention weight. Note that all extracted words are replaced with " mask " and their background colors are labeled as white.</p><p>leverage θ (k−1) to predict the sentiment of x as y p (Line 12), where the word-level attention weight distribution</p><formula xml:id="formula_10">α(x )={α(x 1 ), α(x 2 ), ..., α(x N )} subjecting to N i=1 α(x i ) = 1 is induced. In</formula><p>Step 3, we use the entropy E(α(x )) to measure the variance of α(x ) (Line 13), which contributes to determine the existence of an influential context word for the sentiment prediction of x ,</p><formula xml:id="formula_11">E(α(x )) = − N i=1 α(x i ) log(α(x i )). (2)</formula><p>If E(α(x )) is less than a threshold α (Line 14), we believe that there exists at least one context word with great effect on the sentiment prediction of x . Hence, we extract the context word x m with the maximum attention weight (Line 15-20) that will be exploited as attention supervision information to refine the model training. Specifically, we adopt two strategies to deal with x m according to different prediction results on x : if the prediction is correct, we wish to continue focusing on x m and add it into s a (x) (Lines 16-17); otherwise, we expect to decrease the attention weight of x m and thus include it into s m (x) (Lines 18-19).</p><p>In</p><p>Step 4, we combine x , t and y as a triple, and merge it with the collected ones to form a new training corpus D (k) (Line 22). Then, we leverage D (k) to continue updating model parameters for the next iteration (Line 24). In doing so, we make our model adaptive to discover more influential context words.</p><p>Through K iterations of the above steps, we manage to extract influential context words of all training instances. <ref type="table" target="#tab_3">Table 2</ref> illustrates the context word mining process of the first sentence shown in <ref type="table" target="#tab_1">Table 1</ref>. In this example, we iteratively extract three context words in turn: "small", "crowded" and "quick". The former two words are included in s a (x), while the last one is contained in s m (x).</p><p>Finally, the extracted context words of each training instance will be included into D, forming a final training corpus D s with attention supervision information (Lines 26-29), which will be used to carry out the last model training (Line 30). The details will be provided in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training with Attention Supervision Information</head><p>To exploit the above extracted context words to refine the training of attention mechanisms for ASC models, we propose a soft attention regularizer (α(s a (x) ∪ s m (x)),α(s a (x) ∪ s m (x)); θ) to jointly minimize the standard training objective, where α( * ) andα( * ) denotes the model-induced and expected attention weight distributions of words in s a (x)∪s m (x), respectively. More specifically, (α( * ),α( * ); θ) is an Euclidean Distance style loss that penalizes the disagreement between α( * ) andα( * ).</p><p>As previously analyzed, we expect to equally continue focusing on the context words of s a (x) during the final model training. To this end, we set their expected attention weights to the same value 1 |sa(x)| . By doing so, the weights of words extracted first will be reduced, and those of words extracted later will be increased, avoiding the over-fitting of high-frequency context words with sentiment polarities and the under-fitting of lowfrequency ones. On the other hand, for the words in s m (x) with misleading effects on the sentiment prediction of x, we want to reduce their effects and thus directly set their expected weights as 0. Back to the sentence shown in <ref type="table" target="#tab_3">Table 2</ref>, both "small" and "crowded"∈s a (x) are assigned the same expected weight 0.5, and the expected weight of "quick"∈s m (x) is 0.</p><p>Finally, our objective function on the training corpus D s with attention supervision information  </p><formula xml:id="formula_12">γ (α(s a (x) ∪ s m (x)),α(s a (x) ∪ s m (x)); θ)},</formula><p>where J(x, t, y; θ) is the conventional training objective defined in Equation 1, and γ&gt;0 is a hyperparameter that balances the preference between the conventional loss function and the regularization term. In addition to the utilization of attention supervision information, our method has a further advantage: it is easier to address the vanishing gradient problem by adding such information into the intermediate layers of the entire network <ref type="bibr" target="#b15">(Szegedy et al., 2015)</ref>, because the supervision ofα( * ) is closer to α( * ) than y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We applied the proposed approach into MN <ref type="bibr" target="#b17">(Tang et al., 2016b;</ref><ref type="bibr" target="#b19">Wang et al., 2018)</ref> and TNet-ATT ) (see Section 2), and conducted experiments on three benchmark datasets: LAPTOP, REST <ref type="bibr" target="#b14">(Pontiki et al., 2014)</ref> and TWITTER <ref type="bibr" target="#b1">(Dong et al., 2014)</ref>. In our datasets, the target aspect of each sentence has been provided. Besides, we removed a few instances with conflict sentiment labels as implemented in . The statistics of the final datasets are listed in <ref type="table" target="#tab_5">Table 3</ref>. Contrast Models. We referred to our two enhanced ASC models as MN(+AS) and TNet-ATT(+AS), and compared them with MN, TNet, and TNet-ATT. Note our models require additional K+1-iteration training, therefore, we also compared them with the above models with additional K+1-iteration training, which are denoted as MN(+KT), TNet(+KT) and TNet-ATT(+KT). Moreover, to investigate effects of different kinds of attention supervision information, we also listed the performance of MN(+AS a ) and MN(+AS m ), which only leverage context words of s a (x) and s m (x), respectively, and the same for TNet-ATT(+AS a ) and TNet-ATT(+AS m ). Training Details. We used pre-trained GloVe vectors <ref type="bibr" target="#b13">(Pennington et al., 2014)</ref> to initialize the word embeddings with vector dimension 300. For out-of-vocabulary words, we randomly sampled their embeddings from the uniform distribution <ref type="bibr">[-0.25, 0.25]</ref>, as implemented in <ref type="bibr" target="#b3">(Kim, 2014)</ref>. Besides, we initialized the other model parameters uniformly between [-0.01, 0.01]. To alleviate overfitting, we employed dropout strategy (Hinton et al., 2012) on the input word embeddings of the LSTM and the ultimate aspect-related sentence representation. Adam (Kingma and Ba, 2015) was adopted as the optimizer with the learning rate 0.001.</p><p>When implementing our approach, we empirically set the maximum iteration number K as 5, γ in Equation 3 as 0.1 on LAPTOP data set, 0.5 on REST data set and 0.1 on TWITTER data set, respectively. All hyper-parameters were tuned on 20% randomly held-out training data. Finally, we used F1-Macro and accuracy as our evaluation Model LAPTOP REST TWITTER Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy MN <ref type="bibr" target="#b19">(Wang et al., 2018)</ref> 62   <ref type="bibr" target="#b19">(Wang et al., 2018;</ref>. * * and * means significant at p &lt;0.01 and p &lt;0.05 over the baselines (MN, TNet) on each test set, respectively. Here we conducted 1,000 bootstrap tests <ref type="bibr" target="#b6">(Koehn, 2004)</ref> to measure the significance in metric score differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>measures.</head><p>4.1 Effects of α α is a very important hyper-parameter that controls the iteration number of mining attention supervision information (see Line 14 of Algorithm 1). Thus, in this group of experiments, we varied α from 1.0 to 7.0 with an increment of 1 each time, so as to investigate its effects on the performance of our models on the validation sets. <ref type="figure">Figure 3</ref> and 4 show the experimental results of different models. Specifically, MN(+AS) with α =3.0 achieves the best performance, meanwhile, the optimal performance of TNet-ATT(+AS) is obtained when α =4.0. We observe the increase of α does not lead to further improvements, which may be due to more noisy extracted context words. Because of these results, we set α for MN(+AS) and TNet-ATT(+AS) as 3.0 and 4.0 in the following experiments, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Results</head><p>Table 4 provides all the experimental results. To enhance the persuasiveness of our experimental results, we also displayed the previously reported scores of MN <ref type="bibr" target="#b19">(Wang et al., 2018)</ref> and TNet  on the same data set. According to the experimental results, we can come to the following conclusions:</p><p>First, both of our reimplemented MN and TNet are comparable to their original models reported in <ref type="bibr" target="#b19">(Wang et al., 2018;</ref>. These results show that our reimplemented baselines are competitive. When we replace the CNN of TNet with an attention mechanism, TNet-ATT is slightly inferior to TNet. Moreover, when we perform additional K+1-iteration of training on these models, their performance has not changed significantly, suggesting simply increasing training time is unable to enhance the performance of the neural ASC models.</p><p>Second, when we apply the proposed approach into both MN and TNet-ATT, the context words in s a (x) are more effective than those in s m (x). This is because the proportion of correctly predicted training instances is larger than that of incorrectly ones. Besides, the performance gap between MN(+AS a ) and MN(+AS m ) is larger than that between two variants of TNet-ATT. One underlying reason is that the performance of TNet-ATT is better than MN, which enables TNet-ATT to produce more correctly predicted training instances. This in turn brings more attention supervision to TNet-ATT than MN.</p><p>Finally, when we use both kinds of attention supervision information, no matter for which metric, MN(+AS) remarkably outperforms MN on all test sets. Although our TNet-ATT is slightly in-  ferior to TNet, TNet-ATT(+AS) still significantly surpasses both TNet and TNet-ATT. These results strongly demonstrate the effectiveness and generality of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study</head><p>In order to know how our method improves neural ASC models, we deeply analyze attention results of TNet-ATT and TNet-ATT(+AS). It has been found that our proposed approach can solve the above-mentioned two issues well. <ref type="table" target="#tab_9">Table 5</ref> provides two test cases. TNet-ATT incorrectly predicts the sentiment of the first test sentence as neutral. This is because the context word "uncomfortable" only appears in two training instances with negative polarities, which distracts attention from it. When using our approach, the average attention weight of "uncomfortable" is increased to 2.6 times than that of baseline in these two instances. Thus, TNet-ATT(+AS) is capable of assigning a greater attention weight (0.0056→0.2940) to this context word, leading to the correct prediction of the first test sentence. For the second test sentence, since the context word "cute" occurs in training instances mostly with positive polarity, TNet-ATT directly focuses on this word and then incorrectly predicts the sentence sentiment as positive. Adopting our method, attention weights of "cute" in training instances with neural or negative polarity are significantly decreased. Specifically, in these instances, the average weight of "cute" is reduced to 0.07 times of the original. Hence, TNet-ATT(+AS) assigns a smaller weight (0.1090→0.0062) to "cute" and achieves the correct sentiment prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Recently, neural models have been shown to be successful on ASC. For example, due to its multiple advantages, such as being simpler and faster, MNs with attention mechanisms <ref type="bibr" target="#b17">(Tang et al., 2016b;</ref><ref type="bibr" target="#b19">Wang et al., 2018)</ref> have been widely used. Another prevailing neural model is LSTM that also involves an attention mechanism to explicitly capture the importance of each context word <ref type="bibr" target="#b20">(Wang et al., 2016)</ref>. Overall, attention mechanisms play crucial roles in all these models.</p><p>Following this trend, researchers have resorted to more sophisticated attention mechanisms to refine neural ASC models.  proposed a multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. An interactive attention network has been designed by <ref type="bibr" target="#b11">Ma et al., (2017)</ref> for ASC, where two attention networks were introduced to model the target and context interactively.  proposed to leverage multiple attentions for ASC: one obtained from the left context and the other one acquired from the right context of a given aspect. Very recently, transformation-based model has also been explored for ASC , and the attention mechanism is replaced by CNN.</p><p>Different from these work, our work is in line with the studies of introducing attention supervision to refine the attention mechanism, which have become hot research topics in several NNbased NLP tasks, such as event detection , machine translation <ref type="bibr" target="#b9">(Liu et al., 2016)</ref>, and police killing detection <ref type="bibr" target="#b12">(Nguyen and Nguyen, 2018)</ref>. However, such supervised attention acquisition is labor-intense. Therefore, we mainly commits to automatic mining supervision information for attention mechanisms of neural ASC models. Theoretically, our approach is orthogonal to these models, and we leave the adaptation of our approach into these models as future work.</p><p>Our work is inspired by two recent models: one is  proposed to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems, and the other one is <ref type="bibr" target="#b22">(Xu et al., 2018)</ref> where a dropout method integrating with global information is presented to encourage the model to mine inapparent features or patterns for text classification. To the best of our knowledge, our work is the first one to explore automatic mining of attention supervision information for ASC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we have explored how to automatically mine supervision information for attention mechanisms of neural ASC models. Through indepth analyses, we first point out the defect of the attention mechanism for ASC: a few frequent words with sentiment polarities are tend to be over-learned, while those with low frequency often lack sufficient learning. Then, we propose a novel approach to automatically and incrementally mine attention supervision information for neural ASC models. These mined information can be further used to refine the model training via a regularization term. To verify the effectiveness of our approach, we apply our approach into two dominant neural ASC models, where experimental results demonstrate our method significantly improves the performance of these two models.</p><p>Our method is general for attention mechanisms. Thus, we plan to extend our approach to other neural NLP tasks with attention mechanisms, such as neural document classification <ref type="bibr">(Yang et al., 2016)</ref> and neural machine translation <ref type="bibr" target="#b24">(Zhang et al., 2018)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The framework architecture of MN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The framework architecture of TNet/TNet-ATT. Note that TNet-ATT is the variant of TNet replacing CNN with an attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Effects of α on the validation sets using MN(+AS). Effects of α on the validation sets using TNet-ATT(+AS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>TrainThe [place] is small and crowded but the service is quick . Neg / -TrainThe [place] is a bit too small for live music . Neg / -Train The service is decent even when this small [place] is packed .</figDesc><table><row><cell>Type</cell><cell>Sentence</cell><cell>Ans./Pred.</cell></row><row><cell></cell><cell></cell><cell>Neg / -</cell></row><row><cell>Test</cell><cell>At lunch time , the [place] is crowded .</cell><cell>Neg / Pos</cell></row><row><cell>Test</cell><cell>A small area makes for quiet [place] to study alone .</cell><cell>Pos / Neg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The example of attention visualization for five sentences, where the first three are training instances and the last two are test ones. The bracketed bolded words are target aspects. Ans./Pred. = ground-truth/predicted sentiment label. Words are highlighted with different degrees according to attention weights.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The [place] is small and crowded but the service is quick . The [place] is mask and crowded but the service is quick . The [place] is mask and mask but the service is quick . The [place] is mask and mask but the service is mask .</figDesc><table><row><cell>Iter</cell><cell>Sentence</cell><cell cols="2">Ans./Pred. E(α(x ))</cell><cell>x m</cell></row><row><cell>1</cell><cell></cell><cell>Neg / Neg</cell><cell>2.38</cell><cell>small</cell></row><row><cell>2</cell><cell></cell><cell>Neg / Neg</cell><cell>2.59</cell><cell>crowded</cell></row><row><cell>3</cell><cell></cell><cell>Neg / Pos</cell><cell>2.66</cell><cell>quick</cell></row><row><cell>4</cell><cell></cell><cell>Neg / Neg</cell><cell>3.07</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The example of mining influential context words from the first training sentence inTable 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Datasets in our experiments. #Pos, #Neg and #Neu denotes the number of instances with Positive, Negative and Neutral sentiment, respectively.</figDesc><table><row><cell>becomes</cell><cell></cell><cell></cell></row><row><cell>J s (D s ; θ) = −</cell><cell>{J(x, t, y; θ)+</cell><cell>(3)</cell></row><row><cell>(x,t,y)∈Ds</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on various datasets. We directly cited the best experimental results of MN and TNet reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The [folding chair] i was seated at was uncomfortable . Neg / Neg TNet-ATT The [food] did take a few extra minutes ... the cute waiters ... The [food] did take a few extra minutes ... the cute waiters ...</figDesc><table><row><cell>Model</cell><cell>Sentence</cell><cell>Ans./Pred.</cell></row><row><cell>TNet-ATT</cell><cell>The [folding chair] i was seated at was uncomfortable .</cell><cell>Neg / Neu</cell></row><row><cell>TNet-ATT(+AS)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Neu / Pos</cell></row><row><cell>TNet-ATT(+AS)</cell><cell></cell><cell>Neu / Neu</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Two test cases predicted by TNet-ATT and TNet-ATT(+AS).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors were supported by National Natural Science Foundation of China (Nos. 61433015, 61672440), NSF Award <ref type="figure">(</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Ruslan Salakhutdinov</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Nrc-canada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In SemEval</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dolla r. 2017. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural machine translation with supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting argument information to improve event detection via supervised attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Who is killed by police: Introducing supervised attention for hierarchical lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Going deeper with convolutions</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DCU: aspect-based polarity classification for semeval task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utsab</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasha</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamia</forename><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Target-sensitive memory networks for aspect sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahisnu</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From random to supervised: A novel dropout mechanism integrated with global information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengru</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renfen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CONLL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural machine translation with deep attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention modeling for targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
