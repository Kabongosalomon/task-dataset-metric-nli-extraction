<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EPE-NAS: Efficient Performance Estimation Without Training for Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasco</forename><surname>Lopes</surname></persName>
							<email>vasco.lopes@ubi.pt</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NOVA LINCS</orgName>
								<orgName type="institution" key="instit2">Universidade da Beira Interior † C4-Cloud Computing Competence Center</orgName>
								<orgName type="institution" key="instit3">Universidade da Beira Interior</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Alirezazadeh</surname></persName>
							<email>saeid.alirezazadeh@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NOVA LINCS</orgName>
								<orgName type="institution" key="instit2">Universidade da Beira Interior † C4-Cloud Computing Competence Center</orgName>
								<orgName type="institution" key="instit3">Universidade da Beira Interior</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><forename type="middle">A</forename><surname>Alexandre</surname></persName>
							<email>luis.alexandre@ubi.pt</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NOVA LINCS</orgName>
								<orgName type="institution" key="instit2">Universidade da Beira Interior † C4-Cloud Computing Competence Center</orgName>
								<orgName type="institution" key="instit3">Universidade da Beira Interior</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EPE-NAS: Efficient Performance Estimation Without Training for Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS) has shown excellent results in designing architectures for computer vision problems. NAS alleviates the need for human-defined settings by automating architecture design and engineering. However, NAS methods tend to be slow, as they require large amounts of GPU computation. This bottleneck is mainly due to the performance estimation strategy, which requires the evaluation of the generated architectures, mainly by training them, to update the sampler method. In this paper, we propose EPE-NAS, an efficient performance estimation strategy, that mitigates the problem of evaluating networks, by scoring untrained networks and creating a correlation with their trained performance. We perform this process by looking at intra and inter-class correlations of an untrained network. We show that EPE-NAS can produce a robust correlation and that by incorporating it into a simple random sampling strategy, we are able to search for competitive networks, without requiring any training, in a matter of seconds using a single GPU. Moreover, EPE-NAS is agnostic to the search method, since it focuses on the evaluation of untrained networks, making it easy to integrate into almost any NAS method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In the past years, deep learning algorithms have been extensively researched, and efficiently applied to various tasks with excellent results <ref type="bibr" target="#b7">[1]</ref>, <ref type="bibr" target="#b8">[2]</ref>, especially those related to computer vision <ref type="bibr" target="#b9">[3]</ref>. The great success in computer vision tasks is mainly attributed to the advent of Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b10">[4]</ref>, given their robust feature extraction capability and transferability between different problems. Different CNNs architectures have gradually been proposed, incrementally showing that CNNs can be improved, by revising the architecture itself, adding additional components such as residual connections, reducing the number of parameters, the size or inference time <ref type="bibr" target="#b11">[5]</ref>, <ref type="bibr" target="#b12">[6]</ref>, <ref type="bibr" target="#b13">[7]</ref>, <ref type="bibr" target="#b14">[8]</ref>, <ref type="bibr" target="#b15">[9]</ref>, <ref type="bibr" target="#b16">[10]</ref>. However, designing efficient architectures is extremely time-consuming. It requires expert knowledge and trial and error. Deep neural networks can have many design choices, such as layers, their combination and sequence, parameters associated with the layers, architecture, and the training procedure as well as optimization rules. Therefore, an automated way to conduct neural architectures' design came as a natural process <ref type="bibr" target="#b17">[11]</ref>.</p><p>Neural Architecture Search (NAS) aims to automate architecture engineering and design, by autonomously designing high performance architectures for a given problem <ref type="bibr" target="#b18">[12]</ref>. NAS methods for computer vision problems have been successfully applied to various tasks, such as image classification, semantic segmentation, object detection, and others <ref type="bibr" target="#b18">[12]</ref>, <ref type="bibr" target="#b19">[13]</ref>. Since the incipience proposal <ref type="bibr" target="#b20">[14]</ref>, NAS methods broadly focused on designing architectures using a similar flow. A controller, using a specified search strategy, being the most common Reinforcement Learning or Evolutionary Strategies, samples an architecture A from the space of possible architectures A, which is defined by the search space, that comprises the possible operations (e.g., convolution, pooling) and the architecture type. The generated architecture is evaluated, and the result is given as a reward to the controller to update its parameters. This process is repeated thousands of times, whereby the controller learns to sample better architectures over time. A visualization of this process can be seen in <ref type="figure" target="#fig_0">Fig.  1</ref>.</p><p>Although NAS methods have shown excellent results, the computational cost of most methods is extremely high, which in some cases can be in the order of months of GPU computation <ref type="bibr" target="#b20">[14]</ref>, <ref type="bibr" target="#b21">[15]</ref>, <ref type="bibr" target="#b22">[16]</ref>. This is mainly associated with the performance estimation strategy, which evaluates the generated architectures based on regular training, either from scratch until convergence or partial training <ref type="bibr" target="#b23">[17]</ref>, <ref type="bibr" target="#b24">[18]</ref>. Recent approaches, attempt to smooth the training process, by sharing parameters <ref type="bibr" target="#b25">[19]</ref>, applying mutations to already trained networks <ref type="bibr" target="#b26">[20]</ref>, or by using one-shot NAS, where the controller generates architectures and corresponding weights <ref type="bibr" target="#b27">[21]</ref>, <ref type="bibr" target="#b28">[22]</ref>, <ref type="bibr" target="#b29">[23]</ref>. However, some NAS proposals have shown to be overfitting the search space and not allowing exploration due to the introducing of design bias <ref type="bibr" target="#b30">[24]</ref>.</p><p>To mitigate the aforementioned problems, in this paper, we propose EPE-NAS, a performance estimation strategy that scores generated networks at initialization stage, without requiring any training. By evaluating how the gradients of the network behave with respect to the input, it is possible to score untrained networks, eliminating the need to train generated architectures to update parameters. The proposed method is extremely fast, allowing the analysis of thousands of networks in seconds. We show that this method can be used to guide the search over the search space due to its fast inference of a network trained accuracy from its untrained state. The proposed method can be easily integrated into almost any NAS method, by entirely replacing the performance estimation strategy, or complementing it, by creating a multi estimation strategy. We show this by incorporating the proposed method into a random search strategy, achieving competitive results in seconds. The code for the proposed method is also available 1 .</p><p>The main contributions of this paper can be summarized as follows:</p><p>• We propose a novel performance estimation strategy that can evaluate the trained performance of an untrained network, which can be easily integrated into almost any NAS method. • We analyze the impact of the proposed method when coupled with random search, showing that it can achieve competitive results in a few seconds. • We compare the proposed method with different NAS methods, as well as with a surrogate performance estimator in NAS-Bench-201. <ref type="bibr">•</ref> We show that the proposed method allows the search space to be quickly analyzed without the need to train networks, allowing bad candidates to be weed out, by analyzing the relationship between the score and the network performance when trained. The remainder of this work is organized as follows. Section II, contextualizes the related work. Section III, describes the proposed method in detail. In Section IV, we present the experiments performed, the datasets and benchmark used, the results and discussion. Finally, in Section V, a conclusion is drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Generally, NAS methods attempt to automatically design optimal CNNs using a sample-evaluate-update scheme, where a controller generates an architecture and is updated using the generated architecture performance. The problem with this is that evaluating the generated architectures is very costly. Zoph and Le <ref type="bibr" target="#b20">[14]</ref> initially formulated NAS as a reinforcement learning problem, where a controller was trained over-time to sample more efficient architectures. The problem was that this <ref type="bibr" target="#b7">1</ref> Code publicly available on GitHub: www.github.com/VascoLopes/EPENAS method required more than 60 years of GPU computation, as it trained all generated architectures to convergence. As followup work, the authors tackle this problem by performing a cellbased search in a search space with 13 operations <ref type="bibr" target="#b22">[16]</ref>. By focusing on designing two types of cells: normal cells (perform convolutional operations) and reduction cells (reduce input size), the authors could reduce the GPU computation to 2000 days. More, they found that cell-based architectures searched in CIFAR-10 can be transferred to ImageNet by stacking more cells. In this method, more than 20000 networks were trained and evaluated.</p><p>Similar to <ref type="bibr" target="#b20">[14]</ref>, in <ref type="bibr" target="#b31">[25]</ref> the authors present MetaQNN, a method based on reinforcement learning and Q-learning, where the learning agent was trained to sequentially sample CNN layers. Using a similar reinforcement learning approach, BlockQNN <ref type="bibr" target="#b32">[26]</ref> focuses on sampling blocks of operations used to form entire networks. However, BlockQNN still required 96 GPU days of computation. ENAS <ref type="bibr" target="#b25">[19]</ref>, used a controller, trained with policy gradient, to discover architectures by searching for an optimal subgraph within a large computational graph. By constructing a sizeable computational graph, where each subgraph represents a network, ENAS forced all generated architectures to share their parameters. In this way, efficient search was enabled in less than one GPU computational day. The authors of <ref type="bibr" target="#b27">[21]</ref> proposed DARTS, a gradientbased method, that by performing continuous relaxation of the search space to be continuous, it optimized architectures using gradient descent. The authors propose a bilevel gradient optimization, which jointly learns the architecture and the weights in a few GPU days. This paper served as foundation for many other one-shot methods. In <ref type="bibr" target="#b33">[27]</ref>, the authors improve DARTS generated architecture performances by introducing regularization mechanisms. Also using a differentiable approach, GDAS <ref type="bibr" target="#b34">[28]</ref> is a method that makes the search procedure differentiable, so that sub-graphs can be sampled from the directed acyclic graph representing the search space, which can be trained end-to-end to sample efficient networks. GDAS' controller is optimized based on the validation loss of the trained sampled architecture. SETN <ref type="bibr" target="#b35">[29]</ref>, also uses a differentiable approach, but uses an evaluator trained to indicate the probability of each architecture to have a low validation loss, which allows selective sampling of networks. REA <ref type="bibr" target="#b36">[30]</ref>, instead of reinforcement learning or gradient-based methods, focuses on using evolutionary tournament selection algorithms with an age property that favors younger architectures.</p><p>To mitigate the bottleneck of the performance estimation strategy, surrogate methods have also been proposed to extrapolate the learning curve with a partial train <ref type="bibr" target="#b37">[31]</ref>, <ref type="bibr" target="#b38">[32]</ref>, or by learning a HyperNet that generates weights based on the architecture <ref type="bibr" target="#b39">[33]</ref>. In <ref type="bibr" target="#b40">[34]</ref>, the authors propose BOBH, that focuses on hyperparameter optimization, which includes architecture design, by combining Bayesian optimization and bandit-based methods. However, it still required 33 GPU days to design an optimal network in CIFAR-10.</p><p>Our work differentiates from the aforementioned, being closer to NAS-WOT <ref type="bibr" target="#b41">[35]</ref>, as our focus is to evaluate a generated network, without requiring any training, neither for the performance estimation strategy, nor for the generated networks. Thus creating a score that correlates an untrained network to its performance once trained, in efficient time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this paper, we propose EPE-NAS, a novel performance estimation strategy, whose goal is to estimate the performance of generated networks without requiring any training, neither for the generated networks nor for the performance estimator. To do this, we score untrained networks as an indicator of their accuracy when trained.</p><p>We base our approach on the idea proposed in <ref type="bibr" target="#b41">[35]</ref>, which states that different networks can be compared by evaluating their behavior using local linear operators at different data points. The local linear operators are obtained by multiplying the linear maps at each layer interspersed with the binary rectification units. To do this, one can define a linear map,</p><formula xml:id="formula_0">w i = f (x i ), which maps the input x i ∈ R D , through the network, f (x i )</formula><p>, where x i represents an image that belongs to a batch X, and D is the input dimension. Then, the linear map can be computed using:</p><formula xml:id="formula_1">Jacobian w i = ∂f (x i ) ∂x i</formula><p>In order to evaluate how a network behaves with different data points, we calculate the Jacobian matrix w i for different data points, f (x i ), of the batch X, i ∈ 1, · · · , N :</p><formula xml:id="formula_2">J = ∂f (x1) ∂x1 ∂f (x2) ∂x2 · · · ∂f (x N ) ∂x N</formula><p>The Jacobian Matrix J contains information about the network output with respect to the input for several data points. We then can evaluate how points belonging to the same class correlate with each other, where the goal is to see if an untrained network is capable of modeling complex functions. Explicitly, a flexible network should simultaneously be able to distinguish local linear operators for each data point, but also have similar results for similar data points, which in a supervised approach means that the data points belong to the same class. The perfect scenario would be to have an untrained network with low correlation between different data points, where data points of the same category are closer to each other, which means that the network would easily learn to distinguish the two data points during training. To evaluate this behavior, we evaluate the correlation of J values with respect to their class, by computing a covariance matrix for each class present in J: C Jc = (J − M Jc )(J − M Jc ) , where M J is the matrix with elements:</p><formula xml:id="formula_3">(M Jc ) i,j = 1 N n∈{1,...,N }, xi∈class c J i,n ,</formula><p>where c represents the class, c ∈ 1, ..., C, and C is the number of classes present in the batch. Then, it is possible to calculate the correlation matrix per class, Σ Jc , for each</p><formula xml:id="formula_4">covariance matrix C Jc : (Σ J c ) i,j = (C Jc )i,j √ (C Jc )i,i * (C Jc )j,j</formula><p>, where (i, j) represents the (i, j) th element of the matrices. Each individual correlation matrix allows the analysis of how the untrained network behaves for each class, which may be an indication of the ability of the local linear operators to perceive differences between classes.</p><p>To allow comparison between the different individual correlation matrices, as they may have different sizes due to the number of data points per class, they are individually evaluated:</p><formula xml:id="formula_5">E c =        N i=0 N j=0 log(|(Σ J c ) i,j | + k), if C ≤ 100 N i=0 N j=0 log(|(Σ J c )i,j )|+k ||Σ J c || , otherwise</formula><p>where k is a small-constant with the value of 1 × 10 −5 , and C is the number of classes in batch X. To avoid confusion with absolute value operation, we denote ||X|| as the number of elements of the set X. The normalization based on the size of the correlation matrix is due to the fact that for a constant batch size, as the number of classes increases, the size of the individual correlation matrices becomes smaller, a correlation matrix with a larger size would obtain a larger value. Then, a network is scored based on the individual evaluations of the correlation matrices by:</p><formula xml:id="formula_6">S =        C t=0 |E t |, if C ≤ 100 C i=0 C j=i |Ei−Ej | ||E|| , otherwise</formula><p>where E is the vector containing all the correlation matrices' scores. Depending on the number of classes present in the batch, the final score is either a sum of the individual correlation matrices' scores or a normalized pair-wise difference. Normalization serves to mitigate the class difference when evaluating networks in datasets with a high number of classes and noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate the effectiveness of EPE-NAS on three datasets: CIFAR-10, CIFAR-100 and ImageNet16-120 from NAS-Bench-201, using a batch size of 256. As the proposed method is a performance estimation strategy that does not require any training, we also evaluate EPE-NAS by combining it with a random search strategy <ref type="bibr" target="#b42">[36]</ref>, where a candidate network from the search space is randomly proposed and scored using the proposed performance estimation method, instead of training the network. This evaluation is done for different sample sizes of N networks.</p><p>The setup for all the experiments conducted was a desktop computer, with a single 1080Ti GPU and 32GB of ram. In the following sections, we comment on NAS-Bench-201, and individually detail the experiments, results and provide a discussion. I: Comparison of several search methods evaluated using the NAS-Bench-201 benchmark. Performance shown in accuracy with mean±std, on CIFAR-10, CIFAR-100 and ImageNet-16-120. Methods are divided into 4 blocks, depending on their approach: weight sharing, non-weight sharing, training-free approaches (with a direct comparison between the proposed method and NAS-WOT), and a baseline using an SVM as a surrogate estimator. Search times are the mean time required to search for cells in CIFAR-10, using a single 1080Ti GPU. Search time includes the time taken to train networks as part of the process where applicable. The performances of the training-free approaches are given for different sample size N. For each sample size, we also report the optimal network. Table adapted from <ref type="bibr" target="#b41">[35]</ref>, with reported results for non-weight and weight sharing methods from <ref type="bibr" target="#b43">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Search NAS methods tend to be hard to reproduce, compare with other methods, and evaluate their real performance on common search spaces <ref type="bibr" target="#b44">[38]</ref>. Increases in search spaces size, result in increasing the number of possible networks that can be generated, which using performance estimation strategies that require some type of training makes exhaustively evaluating the performance of NAS methods extremely hard, ultimately resulting in evaluations using subsets of the whole search space (thousands of networks in a search space that can ultimately be unbounded). It is crucial that methods smooth the reproducibility process by adopting common training procedures and settings <ref type="bibr" target="#b44">[38]</ref>.</p><p>Recently, NAS benchmarks have been proposed, where the goal is to have a controlled setting, where information about the training and final performance of possible networks under the proposed search space is provided, allowing rapid prototyping and comparison between different NAS methods using the same search space, training procedures and hyperparameters <ref type="bibr" target="#b45">[39]</ref>, <ref type="bibr" target="#b46">[40]</ref>, <ref type="bibr" target="#b47">[41]</ref>, <ref type="bibr" target="#b48">[42]</ref>.</p><p>In this work, we used NAS-Bench-201 <ref type="bibr" target="#b48">[42]</ref> to evaluate the proposed method. NAS-Bench-201 provides information about trained networks in three different datasets: CIFAR-10, CIFAR-100 and ImageNet16-120, with fixed splits, and also provide results of several NAS methods under its constraints, allowing direct comparison. In this benchmark, the goal is to design cell-based architectures, where each cell is comprised of 6 edges and 4 nodes. All nodes receive an input edge from all the preceding nodes. The edges represent the possible operations, which are selected from a pool of 5 operations: <ref type="bibr" target="#b7">(1)</ref> zeroize, which zeros the information, (2) skip connection, (3) 1 × 1 convolution, (4) 3 × 3 convolution, and (5) 3 × 3 average pooling layer. The number of possible operations and edges means that there are 5 6 = 15625 possible cells. The final networks are comprised of a fixed macro skeleton, where a cell is a replicated block in the network, meaning that there are as many networks as possible cells, as the only change in the macro skeleton is the cell to be replicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results and Discussion</head><p>First, we evaluate the effectiveness of the proposed method, by randomly sampling 1000 networks from each dataset of NAS-Bench-201 and score them to see the correlation between the score and the networks' performance when trained. This evaluation can be seen in <ref type="figure">Fig. 3</ref>, where the first row refers to CIFAR-10, the second to CIFAR-100 and the third to ImageNet16-120. On the left, it is possible to see a strong correlation between the score given by EPE-NAS and the network's accuracy once trained. This validates the proposed method by showing that scoring untrained networks highly is indicative of a higher performance than the networks scored lower. From this, it is also possible to see that by defining thresholds, e.g., 2 × 10 4 in CIFAR-10, we can efficiently weed out bad candidates, which is of utmost importance to methods based on evolution, where this method can be used to select the best networks from a pool of generated networks that will serve the purpose of generating the next iteration of the evolution. More, this information can also serve to guide the search of large search spaces, by directly indicating which network configurations are better. This is important because searching for networks in large search spaces, possibly unbounded, is extremely difficult and prone to converge to local minimas, mainly due to lack of information about the search space which ultimately leads search methods to converge fast to the best networks initially sampled.</p><p>Then, by combining EPE-NAS with a random search strategy, we can compare the effectiveness of a simple search strategy coupled with the proposed performance estimation strategy against other NAS methods. To perform this experiment, a network is randomly proposed, and instead of training it, we evaluate its performance by scoring the network. This setup requires no training, and we can perform this for different sample sizes (N , where N represents the number of networks evaluated). <ref type="table">Table I</ref> shows the results for EPE-NAS with random search, and compares it with several methods. Methods that perform the search without weight sharing are shown in the first block, whereas weight sharing methods are shown in the second block. In the third block, we present the results for the proposed method and directly compare it with NAS-WOT <ref type="bibr" target="#b41">[35]</ref>, while also showing the optimal network in each setting where our method and NAS-WOT were evaluated. Finally, in the last block, we show a baseline method based on a Support Vector Machine (SVM). The SVM approach is indicative of a possible surrogate model that is trained with information of the 100 trained networks performances. The SVM input was created by computing a single correlation matrix of the batch and then calculating the eigenvalues of the matrix. Denote that for training the 100 networks, 4.16 days of GPU computation were required. After finalizing the SVM training, there is no need to further train any network, as the SVM infers the performance based on untrained networks' correlation matrix.</p><p>From this table, it is possible to see that our proposed method requires orders of magnitude less time to search for efficient networks, while both non-weight sharing and weight sharing incur in a large search time cost. Our method also achieves better results throughout all datasets than weight sharing methods, except for GDAS on CIFAR-10 and CIFAR-100. However, our method is more than 12500× faster. The non-weight sharing methods outperform our method (random search coupled with the proposed performance estimation strategy), but our method is still on pair with them, being capable of achieving competitive results in all datasets. As for the direct comparison with NAS-WOT, in <ref type="table">Table I</ref> it is also shown that the proposed method outperforms NAS-WOT both in terms of inference, being faster in all settings, and in terms of accuracy, being capable of selecting high performant networks in CIFAR-10 and CIFAR-100 in the settings where the sample size is 10 and 100, and CIFAR-10, CIFAR-100 and ImageNet-16-120 for higher sample sizes (500 and 1000). It is important to note that for NAS-WOT, as sample size increases, it increasingly suffers from noise, increasing the gap between the chosen network accuracy and the optimal result and decreasing the performance compared to smaller sample sizes. The opposite happens with our method. As the sample size increases, our method is capable of selecting high performant networks without losing precision, which is of extreme importance, as it is improbable that optimal networks are present in small sample sizes. More focused on ImageNet-16-120, which is a dataset with more noise, due to the image sizes <ref type="bibr">(16 × 16)</ref> and the high number of classes, our method can select networks that attain excellent test accuracies, when compared with no weight sharing methods and NAS-WOT.</p><p>As can be seen in the second column of <ref type="table">Table I</ref>, the execution time of our method is a great advantage, as it is capable of evaluating 1000 networks in 206 seconds. To further evaluate the gains in terms of execution time compared to NAS-WOT, we explored how both methods behave in scoring a network, with a batch size of increasingly different image sizes, which can be seen in <ref type="figure" target="#fig_1">Fig. 2</ref>. This evaluation shows that the proposed method consistently outperforms NAS-WOT, and that it is capable of evaluating images with sizes 256 * 256 * 3 in approximately 5 seconds, meaning that the proposed method can also serve as an improvement for current NAS methods that solely search for networks in CIFAR-10, due to the reduced image size, and then transfer the best networks to ImageNet settings. Thus, NAS methods that were incapable of searching using larger datasets due to time complexity, can use EPE-NAS to search networks in larger datasets directly.</p><p>The reason why the proposed method is capable of outperforming NAS-WOT in terms of time is directly linked with the time complexity of creating a correlation matrix, which is highly dependant on the number of data points and features. By evaluating individual correlation matrices, one per class, we reduce each correlation matrix's size, allowing for faster computations.</p><p>Considering the mean time required to evaluate 1000 networks by our method <ref type="table">(Table I)</ref>, EPE-NAS also allows exhaustive exploration of a search space, as the proposed method is capable of evaluating over 1 million architectures in just 2 days of GPU computing, under these settings. Therefore, this could be used to evaluate a search space's behaviour, giving information to the search method on how to start and proceed, which is a significant benefit when considering large, possibly unbounded, search spaces where information about their shape is limited.</p><p>An important property of the proposed method is that it can easily be incorporated in almost any NAS method either as the sole method that evaluates networks or as a complementary method to perform mixed training, where the reward to update the controller parameters <ref type="figure" target="#fig_0">(Fig. 1)</ref> is a combination of complementary evaluations (e.g., EPE-NAS score combined with the inference/latency of the network in a mobile setting <ref type="bibr" target="#b49">[43]</ref>, <ref type="bibr" target="#b50">[44]</ref>). More, EPE-NAS is agnostic to the search method, as it focuses on the evaluation of networks, being the perfect addition to search methods that rely on information about generated networks or to guide the search, allowing the analysis of thousands of networks in seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we propose EPE-NAS, a performance estimation strategy that scores untrained networks with a high correlation to their trained performance. By leveraging information about the gradients of the output of a network with regards to its input, our method can accurately infer if the generated network is good in less than one second, being capable of evaluating thousands of networks in a matter of seconds. More, in this work, we have shown that using a simple random search coupled with the proposed estimation strategy, it is possible to sample high performant networks, in seconds, that can outperform many current NAS methods.</p><p>Our proposal can also contribute to allow NAS methods to search large search spaces, by providing an efficient way of extracting information about generated networks without requiring any training, and large databases, as our method is still very fast even in the presence of large image sizes. Furthermore, the proposed method is agnostic to the search strategy, allowing it to be integrated into almost any NAS method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by 'FCT -Fundação para a Ciência e Tecnologia' through the research grant '2020.04588.BD', partially supported by NOVA LINCS under grant 'UID/EEA/50008/2019' and and partially supported by operation Centro-01-0145-FEDER-000019 -C4 -Centro de Competencias em Cloud Computing, cofinanced by the European Regional Development Fund (ERDF) through the Programa Operacional Regional do Centro (Centro 2020), in the scope of the Sistema de Apoioà Investigação Cientifíca e Tecnologica -Programas Integrados de IC&amp;DT. CIFAR-10 Accuracy (%)  <ref type="figure">Fig. 3</ref>: Plots of scoring 1000 random untrained networks using the proposed method against the final accuracy when the networks are trained, using the 3 different datasets. On the right, the cell with the highest score, in each setting, is shown.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>General Neural Architecture Search flow. A controller generates an architecture A from the search space of possible operations and architectures, A, which is then evaluated, and its performance is used as reward to update the controller. Our method acts in the performance estimation block, by obtaining estimates without training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Comparison of the time, in seconds, required to score 1 network using our proposed method (in blue) against NAS-WOT, for different image sizes (x-axis). The image size represents the image's width and height, as the images evaluated are square and with 3 channels (RGB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ours</surname></persName>
		</author>
		<idno>N=1000) 206.2 87.87±0.85 91.31±1.69 69.44±0.83 69.58±0.83 41.86±2.33 41.84±2.06</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optimal (N=10)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optimal (N=100)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Optimal (N=500)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Optimal (N=1000)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">† Results obtained by running the author&apos;s publicly available code 3 times with the same settings as the proposed method</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">‡ Includes the time required to train, which was done using information of the performance of 100 fully trained networks</title>
		<imprint/>
	</monogr>
	<note>which collectively required 4.16 training days to train. REFERENCES</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning: methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="197" to="387" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning for computer vision: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voulodimos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Protopapadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence and neuroscience</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of the recent architectures of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sohail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Zahoora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5455" to="5516" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>ICLR, Y. Bengio and Y. LeCun</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>ICML, K. Chaudhuri and R. Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automated Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pedapati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to design RNA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aging evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<meeting>the 35th International Conference on Machine Learning, ser. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient multi-objective neural architecture search via lamarckian evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nas evaluation is frustratingly hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Esperança</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">One-Shot Neural Architecture Search via Self-Evaluated Template Network</title>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3680" to="3689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularized Evolution for Image Classifier Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-fourth international joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SMASH: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">BOHB: Robust and Efficient Hyperparameter Optimization at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<editor>J. G. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning, ser. Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1436" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural Architecture Search without Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence. PMLR</title>
		<imprint>
			<biblScope unit="page" from="367" to="377" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Best practices for scientific research on neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">243</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">NAS-Bench-101: Towards Reproducible Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>ICML, K. Chaudhuri and R. Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Nas-bench-1shot1: Benchmarking and dissecting one-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">NAS-Bench-301 and the Case for Surrogate Benchmarks for Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="734" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
