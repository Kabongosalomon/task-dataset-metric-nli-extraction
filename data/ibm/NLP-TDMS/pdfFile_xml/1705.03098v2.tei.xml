<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
							<email>rayat137@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
							<email>javier.romero@bodylabs.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Body Labs Inc</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
							<email>little@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3dimensional positions.</p><p>With the goal of understanding these sources of error, we set out to build a system that given 2d joint locations predicts 3d positions. Much to our surprise, we have found that, with current technology, "lifting" ground truth 2d joint locations to 3d space is a task that can be solved with a remarkably low error rate: a relatively simple deep feedforward network outperforms the best reported result by about 30% on Human3.6M, the largest publicly available 3d pose estimation benchmark. Furthermore, training our system on the output of an off-the-shelf state-of-the-art 2d detector (i.e., using images as input) yields state of the art results -this includes an array of systems that have been trained end-to-end specifically for this task. Our results indicate that a large portion of the error of modern deep 3d pose estimation systems stems from their visual analysis, and suggests directions to further advance the state of the art in 3d human pose estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The vast majority of existing depictions of humans are two dimensional, e.g. video footage, images or paintings. These representations have traditionally played an important role in conveying facts, ideas and feelings to other people, and this way of transmitting information has only been possible thanks to the ability of humans to understand complex spatial arrangements in the presence of depth ambiguities. For a large number of applications, including virtual and augmented reality, apparel size estimation or even autonomous driving, giving this spatial reasoning power to machines is crucial. In this paper, we will focus on a particular instance of this spatial reasoning problem: 3d human pose estimation from a single image.</p><p>More formally, given an image -a 2-dimensional representation -of a human being, 3d pose estimation is the task of producing a 3-dimensional figure that matches the spatial position of the depicted person. In order to go from an image to a 3d pose, an algorithm has to be invariant to a number of factors, including background scenes, lighting, clothing shape and texture, skin color and image imperfections, among others. Early methods achieved this invariance through features such as silhouettes <ref type="bibr" target="#b0">[1]</ref>, shape context <ref type="bibr" target="#b27">[28]</ref>, SIFT descriptors <ref type="bibr" target="#b5">[6]</ref> or edge direction histograms <ref type="bibr" target="#b39">[40]</ref>. While data-hungry deep learning systems currently outperform approaches based on human-engineered features on tasks such as 2d pose estimation (which also require these invariances), the lack of 3d ground truth posture data for images in the wild makes the task of inferring 3d poses directly from colour images challenging.</p><p>Recently, some systems have explored the possibility of directly inferring 3d poses from images with end-to-end deep architectures <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>, and other systems argue that 3d reasoning from colour images can be achieved by training on synthetic data <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48]</ref>. In this paper, we explore the power of decoupling 3d pose estimation into the well studied problems of 2d pose estimation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50]</ref>, and 3d pose estimation from 2d joint detections, focusing on the latter. Separating pose estimation into these two problems gives us the possibility of exploiting existing 2d pose estimation systems, which already provide invariance to the previously mentioned factors. Moreover, we can train data-hungry algorithms for the 2d-to-3d problem with large amounts of 3d mocap data captured in controlled environments, while working with low-dimensional representations that scale well with large amounts of data.</p><p>Our main contribution to this problem is the design and analysis of a neural network that performs slightly better than state-of-the-art systems (increasing its margin when the detections are fine-tuned, or ground truth) and is fast (a forward pass takes around 3ms on a batch of size 64, allowing us to process as many as 300 fps in batch mode), while being easy to understand and reproduce. The main reason for this leap in accuracy and performance is a set of simple ideas, such as estimating 3d joints in the camera coordinate frame, adding residual connections and using batch normalization. These ideas could be rapidly tested along with other unsuccessful ones (e.g. estimating joint angles) due to the simplicity of the network.</p><p>The experiments show that inferring 3d joints from groundtruth 2d projections can be solved with a surprisingly low error rate -30% lower than state of the art -on the largest existing 3d pose dataset. Furthermore, training our system on noisy outputs from a recent 2d keypoint detector yields results that slightly outperform the state-of-the-art on 3d human pose estimation, which comes from systems trained end-to-end from raw pixels.</p><p>Our work considerably improves upon the previous best 2d-to-3d pose estimation result using noise-free 2d detections in Human3.6M, while also using a simpler architecture. This shows that lifting 2d poses is, although far from solved, an easier task than previously thought. Since our work also achieves state-of-the-art results starting from the output of an off-the-shelf 2d detector, it also suggests that current systems could be further improved by focusing on the visual parsing of human bodies in 2d images. Moreover, we provide and release a high-performance, yet lightweight and easy-to-reproduce baseline that sets a new bar for future work in this task. Our code is publicly available at https://github.com/una-dinosauria/ 3d-pose-baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous work</head><p>Depth from images The perception of depth from purely 2d stimuli is a classic problem that has captivated the attention of scientists and artists at least since the Renaissance, when Brunelleschi used the mathematical concept of perspective to convey a sense of space in his paintings of Florentine buildings.</p><p>Centuries later, similar perspective cues have been exploited in computer vision to infer lengths, areas and distance ratios in arbitrary scenes <ref type="bibr" target="#b56">[57]</ref>. Apart from perspective information, classic computer vision systems have tried to use other cues like shading <ref type="bibr" target="#b52">[53]</ref> or texture <ref type="bibr" target="#b24">[25]</ref> to recover depth from a single image. Modern systems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref> typically approach this problem from a supervised learning perspective, letting the system infer which image features are most discriminative for depth estimation.</p><p>Top-down 3d reasoning One of the first algorithms for depth estimation took a different approach: exploiting the known 3d structure of the objects in the scene <ref type="bibr" target="#b36">[37]</ref>. It has been shown that this top-down information is also used by humans when perceiving human motion abstracted into a set of sparse point projections <ref type="bibr" target="#b7">[8]</ref>. The idea of reasoning about 3d human posture from a minimal representation such as sparse 2d projections, abstracting away other potentially richer image cues, has inspired the problem of 3d pose estimation from 2d joints that we are addressing in this work.</p><p>2d to 3d joints The problem of inferring 3d joints from their 2d projections can be traced back to the classic work of Lee and Chen <ref type="bibr" target="#b22">[23]</ref>. They showed that, given the bone lengths, the problem boils down to a binary decision tree where each split correspond to two possible states of a joint with respect to its parent. This binary tree can be pruned based on joint constraints, though it rarely resulted in a single solution. Jiang <ref type="bibr" target="#b19">[20]</ref> used a large database of poses to resolve ambiguities based on nearest neighbor queries. Interestingly, the idea of exploiting nearest neighbors for refining the result of pose inference has been recently revisited by Gupta et al. <ref type="bibr" target="#b13">[14]</ref>, who incorporated temporal constraints during search, and by Chen and Ramanan <ref type="bibr" target="#b8">[9]</ref>. Another way of compiling knowledge about 3d human pose from datasets is by creating overcomplete bases suitable for representing human poses as sparse combinations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, lifting the pose to a reproducible kernel Hilbert space (RHKS) <ref type="bibr" target="#b17">[18]</ref> or by creating novel priors from specialized datasets of extreme human poses <ref type="bibr" target="#b1">[2]</ref>.</p><p>Deep-net-based 2d to 3d joints Our system is most related to recent work that learns the mapping between 2d and 3d with deep neural networks. Pavlakos et al. <ref type="bibr" target="#b32">[33]</ref> introduced a deep convolutional neural network based on the stacked hourglass architecture <ref type="bibr" target="#b29">[30]</ref> that, instead of regressing 2d joint probability heatmaps, maps to probability distributions in 3d space. Moreno-Noguer <ref type="bibr" target="#b26">[27]</ref> learns to predict a pairwise distance matrix (DM) from 2-to-3dimensional space. Distance matrices are invariant up to rotation, translation and reflection; therefore, multidimensional scaling is complemented with a prior of human poses <ref type="bibr" target="#b1">[2]</ref> to rule out unlikely predictions.</p><p>A major motivation behind Moreno-Noguer's DM regression approach, as well as the volumetric approach of Pavlakos et al., is the idea that predicting 3d keypoints from 2d detections is inherently difficult. For example, Pavlakos et al. <ref type="bibr" target="#b32">[33]</ref> present a baseline where a direct 3d joint representation (such as ours) is used instead ( <ref type="table" target="#tab_0">Table 1</ref> in <ref type="bibr" target="#b32">[33]</ref>), with much less accurate results than using volumetric regression 1 Our work contradicts the idea that regressing 3d keypoints from 2d joint detections directly should  <ref type="figure">Figure 1</ref>. A diagram of our approach. The building block of our network is a linear layer, followed by batch normalization, dropout and a RELU activation. This is repeated twice, and the two blocks are wrapped in a residual connection. The outer block is repeated twice. The input to our system is an array of 2d joint positions, and the output is a series of joint positions in 3d.</p><p>be avoided, and shows that a well-designed and simple network can perform quite competitively in the task of 2d-to-3d keypoint regression.</p><p>2d to 3d angular pose There is a second branch of algorithms for inferring 3d pose from images which estimate the body configuration in terms of angles (and sometimes body shape) instead of directly estimating the 3d position of the joints <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b53">54]</ref>. The main advantages of these methods are that the dimensionality of the problem is lower due to the constrained mobility of human joints, and that the resulting estimations are forced to have a human-like structure. Moreover, constraining human properties such as bone lengths or joint angle ranges is rather simple with this representation <ref type="bibr" target="#b50">[51]</ref>. We have also experimented with such approaches; however in our experience the highly nonlinear mapping between joints and 2d points makes learning and inference harder and more computationally expensive. Consequently, we opted for estimating 3d joints directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Solution methodology</head><p>Our goal is to estimate body joint locations in 3dimensional space given a 2-dimensional input. Formally, our input is a series of 2d points x ∈ R 2n , and our output is a series of points in 3d space y ∈ R 3n . We aim to learn a function f * : R 2n → R 3n that minimizes the prediction error over a dataset of N poses:</p><formula xml:id="formula_0">f * = min f 1 N N i=1 L (f (x i ) − y i ) .<label>(1)</label></formula><p>In practice, x i may be obtained as ground truth 2d joint locations under known camera parameters, or using a 2d joint detector. It is also common to predict the 3d positions relative to a fixed global space with respect to its root joint, resulting in a slightly lower-dimensional output.</p><p>We focus on systems where f * is a deep neural network, and strive to find a simple, scalable and efficient architecture that performs well on this task. These goals are the main rationale behind the design choices of our network. <ref type="figure">Figure 1</ref> shows a diagram with the basic building blocks of our architecture. Our approach is based on a simple, deep, multilayer neural network with batch normalization <ref type="bibr" target="#b16">[17]</ref>, dropout <ref type="bibr" target="#b43">[44]</ref> and Rectified Linear Units (RE-LUs) <ref type="bibr" target="#b28">[29]</ref>, as well as residual connections <ref type="bibr" target="#b15">[16]</ref>. Not depicted are two extra linear layers: one applied directly to the input, which increases its dimensionality to 1024, and one applied before the final prediction, that produces outputs of size 3n. In most of our experiments we use 2 residual blocks, which means that we have 6 linear layers in total, and our model contains between 4 and 5 million trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Our approach -network design</head><p>Our architecture benefits from multiple relatively recent improvements on the optimization of deep neural networks, which have mostly appeared in the context of very deep convolutional neural networks and have been the key ingredient of state-of-the-art systems submitted to the ILSVRC (Imagenet <ref type="bibr" target="#b9">[10]</ref>) benchmark. As we demonstrate, these contributions can also be used to improve generalization on our 2d-to-3d pose estimation task.</p><p>2d/3d positions Our first design choice is to use 2d and 3d points as inputs and outputs, in contrast to recent work that has used raw images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref> or 2d probability distributions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b55">56]</ref> as inputs, and 3d probabilities <ref type="bibr" target="#b32">[33]</ref>, 3d motion parameters <ref type="bibr" target="#b53">[54]</ref> or basis pose coefficients and camera parameter estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> as outputs. While 2d detections carry less information, their low dimensionality makes them very appealing to work with; for example, one can easily store the entire Hu-man3.6M dataset in the GPU while training the network, which reduces overall training time, and considerably allowed us to accelerate the search for network design and training hyperparameters.</p><p>Linear-RELU layers Most deep learning approaches to 3d human pose estimation are based on convolutional neural networks, which learn translation-invariant filters that can be applied to entire images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref>, or 2dimensional joint-location heatmaps <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b55">56]</ref>. However, since we are dealing with low-dimensional points as inputs and outputs, we can use simpler and less computationally expensive linear layers. RELUs <ref type="bibr" target="#b28">[29]</ref> are a standard choice to add non-linearities in deep neural networks.</p><p>Residual connections We found that residual connections, recently proposed as a technique to facilitate the training of very deep convolutional neural networks <ref type="bibr" target="#b15">[16]</ref>, improve generalization performance and reduce training time. In our case, they helped us reduce error by about 10%.</p><p>Batch normalization and dropout While a simple network with the three components described above achieves good performance on 2d-to-3d pose estimation when trained on ground truth 2d positions, we have discovered that it does not perform well when trained on the output of a 2d detector, or when trained on 2d ground truth and tested on noisy 2d observations. Batch normalization <ref type="bibr" target="#b16">[17]</ref> and dropout <ref type="bibr" target="#b43">[44]</ref> improve the performance of our system in these two cases, while resulting in a slight increase of trainand test-time.</p><p>Max-norm constraint We also applied a constraint on the weights of each layer so that their maximum norm is less than or equal to 1. Coupled with batch normalization, we found that this stabilizes training and improves generalization when the distribution differs between training and test examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data preprocessing</head><p>We apply standard normalization to the 2d inputs and 3d outputs by subtracting the mean and dividing by the standard deviation. Since we do not predict the global position of the 3d prediction, we zero-centre the 3d poses around the hip joint (in line with previous work and the standard protocol of Human3.6M).</p><p>Camera coordinates In our opinion, it is unrealistic to expect an algorithm to infer the 3d joint positions in an arbitrary coordinate space, given that any translation or rotation of such space would result in no change in the input data. A natural choice of global coordinate frame is the camera frame <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref> since this makes the 2d to 3d problem similar across different cameras, implicitly enabling more training data per camera and preventing overfitting to a particular global coordinate frame. We do DMR <ref type="bibr" target="#b26">[27]</ref> Ours ∆ GT/GT 62. <ref type="bibr" target="#b16">17</ref>  this by rotating and translating the 3d ground-truth according to the inverse transform of the camera. A direct effect of inferring 3d pose in an arbitrary global coordinate frame is the failure to regress the global orientation of the person, which results in large errors in all joints. Note that the definition of this coordinate frame is arbitrary and does not mean that we are exploiting pose ground truth in our tests.</p><p>2d detections We obtain 2d detections using the stateof-the-art stacked hourglass network of Newell et al. <ref type="bibr" target="#b29">[30]</ref>, pre-trained on the MPII dataset <ref type="bibr" target="#b2">[3]</ref>. Similar to previous work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref>, we use the bounding boxes provided with H3.6M to estimate the centre of the person in the image. We crop a square of size 440 × 440 pixels around this computed centre to the detector (which is then resized to 256 × 256 by stacked hourglass). The average error between these detections and the ground truth 2d landmarks is 15 pixels, which is slightly higher than the 10 pixels reported by Moreno-Noguer <ref type="bibr" target="#b26">[27]</ref> using CPM <ref type="bibr" target="#b49">[50]</ref> on the same dataset. We prefer stacked hourglass over CPM because (a) it has shown slightly better results on the MPII dataset, and (b) it is about 10 times faster to evaluate, which allowed us to compute detections over the entire H3.6M dataset.</p><p>We have also fine-tuned the stacked hourglass model on the Human3.6M dataset (originally pre-trained on MPII), which obtains more accurate 2d joint detections on our target dataset and further reduces the 3d pose estimation error. We used all the default parameters of stacked hourglass, except for minibatch size which we reduced from 6 to 3 due to memory limitations on our GPU. We set the learning rate to 2.5 × 10 −4 , and train for 40 000 iterations.</p><p>Training details We train our network for 200 epochs using Adam <ref type="bibr" target="#b20">[21]</ref>, a starting learning rate of 0.001 and exponential decay, using mini-batches of size 64. Initially, the weights of our linear layers are set using Kaiming initialization <ref type="bibr" target="#b14">[15]</ref>. We implemented our code using Tensorflow, which takes around 5ms for a forward+backward pass, and  <ref type="table">Table 2</ref>. Detailed results on Human3.6M <ref type="bibr" target="#b18">[19]</ref> under Protocol #1 (no rigid alignment in post-processing). SH indicates that we trained and tested our model with Stacked Hourglass <ref type="bibr" target="#b29">[30]</ref> detections as input, and FT indicates that the 2d detector model was fine-tuned on H3.6M. GT detections denotes that the groundtruth 2d locations were used. SA indicates that a model was trained for each action, and MA indicates that a single model was trained for all actions.</p><p>around 2ms for a forward pass on a Titan Xp GPU. This means that, coupled with a state-of-the-art realtime 2d detector (e.g., <ref type="bibr" target="#b49">[50]</ref>), our network could be part of full pixelsto-3d system that runs in real time.</p><p>One epoch of training on the entire Human3.6M dataset can be done in around 2 minutes, which allowed us to extensively experiment with multiple variations of our architecture and training hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental evaluation</head><p>Datasets and protocols We focus our numerical evaluation on two standard datasets for 3d human pose estimation: HumanEva <ref type="bibr" target="#b41">[42]</ref> and Human3.6M <ref type="bibr" target="#b18">[19]</ref>. We also show qualitative results on the MPII dataset <ref type="bibr" target="#b2">[3]</ref>, for which the ground truth 3d is not available.</p><p>Human3.6M is, to the best of our knowledge, currently the largest publicly available datasets for human 3d pose estimation. The dataset consists of 3.6 million images featuring 7 professional actors performing 15 everyday activities such as walking, eating, sitting, making a phone call and engaging in a discussion. 2d joint locations and 3d ground truth positions are available, as well as projection (camera) parameters and body proportions for all the actors. HumanEva, on the other hand, is a smaller dataset that has been largely used to benchmark previous work over the last decade. MPII is a standard dataset for 2d human pose estimation based on thousands of short youtube videos.</p><p>On Human3.6M we follow the standard protocol, using subjects 1, 5, 6, 7, and 8 for training, and subjects 9 and 11 for evaluation. We report the average error in millimetres between the ground truth and our prediction across all joints and cameras, after alignment of the root (central hip) joint. Typically, training and testing is carried out indepen-dently in each action. We refer to this as protocol #1. However, in some of our baselines, the prediction has been further aligned with the ground truth via a rigid transformation (e.g. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref>). We call this post-processing protocol #2. Similarly, some recent methods have trained one model for all the actions, as opposed to building action-specific models. We have found that this practice consistently improves results, so we report results for our method under these two variations. In HumanEva, training and testing is done on all subjects and in each action separately, and the error is always computed after a rigid transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative results</head><p>An upper bound on 2d-to-3d regression Our method, based on direct regression from 2d joint locations, naturally depends on the quality of the output of a 2d pose detector, and achieves its best performance when it uses ground-truth 2d joint locations.</p><p>We followed Moreno-Noguer <ref type="bibr" target="#b26">[27]</ref> and tested under different levels of Gaussian noise a system originally trained with 2d ground truth. The results can be found in Table 1. Our method largely outperforms the Distance-Matrix method <ref type="bibr" target="#b26">[27]</ref> for all levels of noise, and achieves a peak performance of 37.10 mm of error when it is trained on ground truth 2d projections. This is about 43% better than the best result we are aware of reported on ground truth 2d joints <ref type="bibr" target="#b26">[27]</ref>. Moreover, note that this result is also about 30% better than the 51.9 mm reported by Pavlakos et al. <ref type="bibr" target="#b32">[33]</ref>, which is the best result on Human3.6M that we aware ofhowever, their result does not use ground truth 2d locations, which makes this comparison unfair.</p><p>Although every frame is evaluated independently, and we make no use of time, we note that the predictions produced by our network are quite smooth. A video with  <ref type="table">Table 3</ref>. Detailed results on Human3.6M <ref type="bibr" target="#b18">[19]</ref> under protocol #2 (rigid alignment in post-processing). The 14j (17j) annotation indicates that the body model considers 14 <ref type="bibr" target="#b16">(17)</ref> body joints. The results of all approaches are obtained from the original papers, except for (*), which were obtained from <ref type="bibr" target="#b6">[7]</ref>.</p><p>Walking Jogging S1 S3 S3 S1 S2 S3 Avg Radwan et al. <ref type="bibr" target="#b34">[35]</ref> 75. these and more qualitative results can be found at https: //youtu.be/Hmi3Pd9x1BE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to detector noise</head><p>To further analyze the robustness of our approach, we also experimented with testing the system (always trained with ground truth 2d locations) with (noisy) 2d detections from images. These results are also reported at the bottom of <ref type="table" target="#tab_0">Table 1</ref>. <ref type="bibr" target="#b1">2</ref> In this case, we also outperform previous work, and demonstrate that our network can perform reasonably well when trained on ground truth and tested on the output of a 2d detector.</p><p>Training on 2d detections While using 2d ground truth at train and test time is interesting to characterize the performance of our network, in a practical application our system has to work with the output of a 2d detector. We report our results on protocol #1 of Human3.6M in <ref type="table">Table 2</ref>. Here, our closest competitor is the recent volumetric prediction method of Pavlakos et al. <ref type="bibr" target="#b32">[33]</ref>, which uses a stacked-hourglass architecture, is trained end-to-end on Human3.6M, and uses a single model for all actions. Our method outperforms this state-of-the-art result by 4.4 mm even when using out-of-the-box stacked-hourglass detections, and more than doubles the gap to 9.0 mm when the 2d detector is fine-tuned on H3.6M. Our method also consistently outperforms previous work in all but one of the 15 actions of H3.6M.</p><p>Our results on Human3.6M under protocol #2 (using a rigid alignment with the ground truth), are shown in <ref type="table">Table 3</ref>. Although our method is slightly worse than previous work with out-of-the-box detections, it comes first when we use fine-tuned detections.</p><p>Finally, we report results on the HumanEva dataset in <ref type="table" target="#tab_3">Table 4</ref>. In this case, we obtain the best result to date in 3 out of 6 cases, and overall the best average error for actions Jogging and Walking. Since this dataset is rather small, and the same subjects show up on the train and test set, we do not consider these results to be as significant as those obtained by our method in Human3.6M.</p><p>Ablative and hyperparameter analysis We also performed an ablative analysis to better understand the impact of the design choices of our network. Taking as a basis our non-fine tuned MA model, we present those results in <ref type="table">Table 5</ref>. Removing dropout or batch normalization leads to 3-to8 mm of increase in error, and residual connections account for a gain of about 8 mm in our result. However, not pre-processing the data to the network in camera coordinates results in error above 100 mm -substantially worse than state-of-the-art performance.</p><p>Last but not least, we analyzed the sensitivity of our network to depth and width. Using a single residual block results in a loss of 6 mm, and performance is saturated after 2 blocks. Empirically, we observed that decreasing the layers to 512 dimensions gave worse performance, while layers with 2 048 units were much slower and did not seem to increase the accuracy.   <ref type="table">Table 5</ref>. Ablative and hyperparameter sensitivity analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative results</head><p>Finally, we show some qualitative results on Hu-man3.6M in <ref type="figure" target="#fig_1">Figure 2</ref>, and from images "in the wild" from the test set of MPII in <ref type="figure" target="#fig_2">Figure 3</ref>. Our results on MPII reveal some of the limitations of our approach; for example, our system cannot recover from a failed detector output, and it has a hard time dealing with poses that are not similar to any examples in H3.6M (e.g. people upside-down). Finally, in the wild most images of people do not feature full bodies, but are cropped to some extent. Our system, trained on full body poses, is currently unable to deal with such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Looking at <ref type="table">Table 2</ref>, we see a generalized increase in error when training with SH detections as opposed to training with ground truth 2d across all actions -as one may well expect. There is, however, a particularly large increase in the classes taking photo, talking on the phone, sitting and sitting down. We hypothesize that this is due to the severe self-occlusions in these actions -for example, in some phone sequences, we never get to see one of the hands of the actor. Similarly, in sitting and sitting down, the legs are often aligned with the camera viewpoint, which results in large amounts of foreshortening.</p><p>Further improvements The simplicity of our system suggests multiple directions of improvement in future work. For example, we note that stacked hourglass produces final joint detection heatmaps of size 64 × 64, and thus a larger output resolution might result in more fine-grained detections, moving our system closer to its performance when trained on ground truth. Another interesting direction is to use multiple samples from the 2d stacked hourglass heatmaps to estimate an expected gradient -à la policy gradients, commonly used in reinforcement learningso as to train a network end-to-end. Yet another idea is to emulate the output of 2d detectors using 3-dimensional mocap databases and "fake" camera parameters for data augmentation, perhaps following the adversarial approach of Shrivastava et al. <ref type="bibr" target="#b40">[41]</ref>. Learning to estimate coherently the depth of each person in the scene is an interesting research path, since it would allow our system to work on 3d pose estimation of multiple people. Finally, our architecture is simple, and it is likely that further research into network design could lead to better results on 2d-to-3d systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implications of our results</head><p>We have demonstrated that a relatively simple deep feedforward neural network can achieve a remarkably low error rate on 3d human pose estimation. Coupled with a state-ofthe-art 2d detector, our system obtains the best results on 3d pose estimation to date. Our results stand in contrast to recent work, which has focused on deep, end-to-end systems trained from pixels to 3d positions, and contradicts the underlying hypothesis that justify the complexity of recent state-of-the-art approached to 3d human pose estimation. For example, the volumetric regression approach of <ref type="bibr" target="#b32">[33]</ref> et al. is based on the hypothesis that directly regressing 3d points is inherently difficult, and regression in a volumetric space would provide easier gradients for the network (see <ref type="table" target="#tab_0">Table 1</ref> in <ref type="bibr" target="#b32">[33]</ref>). Although we agree that image content should help to resolve challenging ambiguous cases (consider for example the classic turning ballerina optical illusion), competitive 3d pose estimation from 2d points can be achieved with simple high capacity systems. This might be related to the latent information about subtle body and motion traits existing in 2d joint stimuli, such as gender, which can be perceived by people <ref type="bibr" target="#b46">[47]</ref>. Similarly, the use of a distance matrix as a body representation in <ref type="bibr" target="#b26">[27]</ref> is justified by the claim that invariant, human-designed features should boost the accuracy of the system. However, our results show that well trained systems can outperform these particular features in a simple manner. It would be interesting to see whether a combination of joint distances and joint positions boost the performance even further -we leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>We have shown that a simple, fast and lightweight deep neural network can achieve surprisingly accurate results in the task of 2d-to-3d human pose estimation; and coupled with a state-of-the-art 2d detector, our work results in an easy-to-reproduce, yet high-performant baseline that outperforms the state of the art in 3d human pose estimation.</p><p>Our accuracy in 3d pose estimation from 2d ground truth suggest that, although 2d pose estimation is considered a close to solved problem, it remains as one of the main causes for error in the 3d human pose estimation task. Moreover, our work represents poses in simple 2d and 3d coordinates, which suggests that finding invariant (and more complex) representations of the human body, as has been the focus of recent work, might either not be crucial, or have not been exploited to its full potential.</p><p>Finally, given its simplicity and the rapid development in the field, we like to think of our work as a future baseline, rather than a full-fledged system for 3d pose estimation. This suggests multiple directions of future work. For one, our network currently does not have access to visual evidence; we believe that adding this information to our pipeline, either via fine-tuning of the 2d detections or through multi-sensor fusion will lead to further gains in performance. On the other hand, our architecture is similar to a multi-layer perceptron, which is perhaps the simplest architecture one may think of. We believe that a further exploration of the network architectures will result in improved performance. These are all interesting areas of future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Example output on the test set of Human3.6M. Left: 2d observation. Middle: 3d ground truth. Right (green): our 3d predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results on the MPII test set. Observed image, 2d detection with Stacked Hourglass<ref type="bibr" target="#b29">[30]</ref>, (in green) our 3d prediction. The bottom 3 examples are typical failure cases, where either the 2d detector has failed badly (left), or slightly (right). In the middle, the 2d detector does a fine job, but the person is upside-down and Human3.6M does not provide any similar examples -the network still seems to predict an average pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>37.10 25.07 GT/GT + N (0, 5) 67.11 46.65 20.46 GT/GT + N (0, 10) 79.12 52.84 26.28 GT/GT + N (0, 15) 96.08 59.97 36.11 GT/GT + N (0, 20) 115.55 70.24 45.31 Performance of our system on Human3.6M under protocol #2. (Top) Training and testing on ground truth 2d joint locations plus different levels of additive gaussian noise. (Bottom) Training on ground truth and testing on the output of a 2d detector.</figDesc><table><row><cell>GT/CPM [50]</cell><cell>76.47</cell><cell>-</cell><cell>-</cell></row><row><cell>GT/SH [30]</cell><cell cols="2">-60.52</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Protocol #1 Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SitingD Smoke Wait WalkD Walk WalkT Avg LinKDE [19] (SA) 132.7 183.6 132.3 164.4 162.1 205.9 150.6 171.3 151.6 243.0 162.1 170.7 177.1 96.6 127.9 162.1 Li et al. Tekin et al. [46] (SA) 102.4 147.2 88.8 125.3 118.0 182.7 112.4 129.2 138.9 224.9 118.4 138.8 126.3 55.1 65.8 125.0 Zhou et al. [56] (MA) 87.4 109.3 87.1 103.2 116.2 143.3 106.9 99.8 124.5 199.2 107.4 118.1 114.2 79.4 97.7 113.0 Tekin et al.</figDesc><table><row><cell>[24] (MA)</cell><cell cols="2">-136.9 96.9 124.7</cell><cell cols="2">-168.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-132.2 70.0</cell><cell>-</cell><cell>-</cell></row><row><cell>[45] (SA)</cell><cell cols="2">-129.1 91.4 121.7</cell><cell cols="2">-162.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-130.5 65.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Ghezelghieh et al. [13] (SA)</cell><cell>80.3</cell><cell>80.4 78.1 89.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-95.1 82.2</cell><cell>-</cell></row><row><cell>Du et al. [11] (SA)</cell><cell cols="13">85.1 112.7 104.9 122.1 139.1 135.9 105.9 166.2 117.5 226.9 120.0 117.7 137.4 99.3 106.5 126.5</cell></row><row><cell>Park et al. [32] (SA)</cell><cell cols="13">100.3 116.2 90.0 116.5 115.3 149.5 117.6 106.9 137.2 190.8 105.8 125.1 131.9 62.6 96.2 117.3</cell></row><row><cell>Zhou et al. [54] (MA)</cell><cell cols="13">91.8 102.4 96.7 98.8 113.4 125.2 90.0 93.8 132.2 159.0 107.0 94.4 126.0 79.0 99.0 107.3</cell></row><row><cell>Pavlakos et al. [33] (MA)</cell><cell>67.4</cell><cell cols="6">71.9 66.7 69.1 72.0 77.0 65.0 68.3 83.7</cell><cell>96.5</cell><cell cols="2">71.7 65.8</cell><cell cols="3">74.9 59.1 63.2 71.9</cell></row><row><cell>Ours (SH detections) (SA)</cell><cell>61.6</cell><cell cols="7">73.4 63.3 58.3 91.8 93.6 66.3 62.0 91.7 109.4</cell><cell cols="2">75.7 86.5</cell><cell cols="3">67.2 51.2 52.3 73.6</cell></row><row><cell>Ours (SH detections) (MA)</cell><cell>53.3</cell><cell cols="6">60.8 62.9 62.7 86.4 82.4 57.8 58.7 81.9</cell><cell>99.8</cell><cell cols="2">69.1 63.9</cell><cell cols="3">67.1 50.9 54.8 67.5</cell></row><row><cell>Ours (SH detections FT) (MA)</cell><cell>51.8</cell><cell cols="6">56.2 58.1 59.0 69.5 78.4 55.2 58.1 74.0</cell><cell>94.6</cell><cell cols="2">62.3 59.1</cell><cell cols="3">65.1 49.5 52.4 62.9</cell></row><row><cell>Ours (GT detections) (MA)</cell><cell>37.7</cell><cell cols="6">44.4 40.3 42.1 48.2 54.9 44.4 42.1 54.6</cell><cell>58.0</cell><cell cols="2">45.1 46.4</cell><cell cols="3">47.6 36.4 40.4 45.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Protocol #2 Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SitingD Smoke Wait WalkD Walk WalkT Avg Akhter &amp; Black [2]* (MA) 14j 199.2 177.6 161.8 197.8 176.2 186.5 195.4 167.3 160.7 173.7 177.8 181.9 176.2 198.6 192.7 181.1 Ramakrishna et al. [36]* (MA) 14j 137.4 149.3 141.6 154.3 157.7 158.9 141.8 158.1 168.6 175.6 160.4 161.7 150.0 174.8 150.2 157.3 Zhou et al. [55]* (MA) 14j 99.7 95.8 87.9 116.8 108.3 107.3 93.5 95.3 109.1 137.5 106.0 102.2 106.5 110.4 115.2 106.7 Bogo et al. [7] (MA) 14j 62.0 60.2 67.8 76.5 92.1 77.0 73.0 75.3 100.3 137.3 83.4 77.3 86.8 79.7 87.7 82.3 Moreno-Noguer [27] (MA) 14j 66.1 61.7 84.5 73.7 65.2 67.2 60.9 67.3 103.5 74.6 92.6 69.6 71.5 78.0 73.2 74.0 Pavlakos et al.</figDesc><table><row><cell>[33] (MA) 17j</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-51.9</cell></row><row><cell>Ours (SH detections) (SA) 17j</cell><cell>50.1</cell><cell cols="8">59.5 51.3 56.9 68.5 67.5 51.0 47.2 68.5</cell><cell cols="6">85.6 61.2 67.0 55.1 41.1 45.5 58.5</cell></row><row><cell>Ours (SH detections) (MA) 17j</cell><cell>42.2</cell><cell cols="8">48.0 49.8 50.8 61.7 60.7 44.2 43.6 64.3</cell><cell cols="6">76.5 55.8 49.1 53.6 40.8 46.4 52.5</cell></row><row><cell>Ours (SH detections FT) (MA) 17j</cell><cell>39.5</cell><cell cols="8">43.2 46.4 47.0 51.0 56.0 41.4 40.6 56.5</cell><cell cols="6">69.4 49.2 45.0 49.5 38.0 43.1 47.7</cell></row><row><cell>Ours (SH detections) (SA) 14j</cell><cell>44.8</cell><cell cols="8">52.0 44.4 50.5 61.7 59.4 45.1 41.9 66.3</cell><cell cols="6">77.6 54.0 58.8 49.0 35.9 40.7 52.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Kostrikov et al. [22] 44.0 30.9 41.7 57.2 35.0 33.3 40.3 Yasin et al. [52] 35.8 32.4 41.6 46.6 41.4 35.4 38.9 Moreno-Noguer [27] 19.7 13.0 24.9 39.7 20.0 21.0 26.9 Pavlakos et al. [33] 22.1 21.9 29.0 29.8 23.6 26.0 25.5 Ours (SH detections) 19.7 17.4 46.8 26.9 18.2 18.6 24.6 Results on the HumanEva [42] dataset, and comparison with previous work.</figDesc><table><row><cell></cell><cell>1 99.8 93.8 79.2 89.8 99.4 89.5</cell></row><row><cell>Wang et al. [49]</cell><cell>71.9 75.7 85.3 62.6 77.7 54.4 71.3</cell></row><row><cell cols="2">Simo-Serra et al. [43] 65.1 48.6 73.5 74.2 46.6 32.2 56.7</cell></row><row><cell>Bo et al. [5]</cell><cell>46.4 30.3 64.9 64.5 48.0 38.2 48.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This approach, however, is slightly different from ours, as the input is still image pixels, and the intermediate 2d body representation is a series of joint heatmaps -not joint 2d locations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This was, in fact, the protocol used in the main result of<ref type="bibr" target="#b26">[27]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments The authors thank NVIDIA for the donation of GPUs used in this research. Julieta was supported in part by the Perceiving Systems group at the Max Planck Institute for Intelligent Systems. This research was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D human pose from silhouettes by relevance vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Estimating anthropometry and pose from a single uncalibrated image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="269" to="284" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Twin Gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>-2)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast algorithms for large scale conditional 3D prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Top-down influences on stereoscopic depth-perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bülthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bülthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="257" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D human pose estimation = 2D pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Marker-less 3d human motion capture with monocular image sequence and height-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning camera viewpoint using cnn to improve 3d body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Ghezelghieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3D Pose from Motion for Cross-view Action Recognition via Non-linear Circulant Temporal Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human 3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d human pose reconstruction using millions of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1674" to="1677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3d human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Determination of 3D human body postures from a single view. Computer Vision, Graphics and Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="148" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shape from texture from a multi-scale perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recovering 3D human body configurations using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1052" to="1062" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">View independent human body pose estimation from a single perspective image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Monocular image 3d human pose estimation under self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reconstructing 3D Human Pose from 2D Image Landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Machine perception of three-dimensional solids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TR</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<date type="published" when="1963-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3D pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning 3-D scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast pose estimation with parameter-sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>-2)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A joint model for 2d and 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptation aftereffects in the perception of gender from biological motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sadr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="7" to="7" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modeling 3D human poses from uncalibrated monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Shape from shading: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Single view metrology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
