<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iterative Crowd Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
							<email>vranjan@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
							<email>minhhoai@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Iterative Crowd Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>crowd counting</term>
					<term>density estimation</term>
					<term>multi-stage CNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we tackle the problem of crowd counting in images. We present a Convolutional Neural Network (CNN) based density estimation approach to solve this problem. Predicting a high resolution density map in one go is a challenging task. Hence, we present a two branch CNN architecture for generating high resolution density maps, where the first branch generates a low resolution density map, and the second branch incorporates the low resolution prediction and feature maps from the first branch to generate a high resolution density map. We also propose a multi-stage extension of our approach where each stage in the pipeline utilizes the predictions from all the previous stages. Empirical comparison with the previous state-of-the-art crowd counting methods shows that our method achieves the lowest mean absolute error on three challenging crowd counting benchmarks: Shanghaitech, World-Expo'10, and UCF datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gathering of large crowds is commonplace nowadays, and estimating the size of a crowd is an important problem for different purposes ranging from journalism to public safety. Without turnstiles to provide a precise count, the media and crowd safety specialists must estimate the size of the crowd based on images and videos of the crowd. Manual visual estimation, however, is difficult and laborious for humans. Humans are good at subitizing, i.e., predicting fast and accurate counts for small number of items, but the accuracy with which humans count deteriorates as the number of items increase <ref type="bibr" target="#b6">[7]</ref>. Furthermore, the addition of each new item beyond a few adds an extra processing time of around 250 to 300 milliseconds <ref type="bibr" target="#b16">[17]</ref>. As a result, any crowd monitoring system that relies on humans for counting people in crowded scenes will be slow and unreliable. There is a need for an automatic computer vision algorithm that can accurately count the number of people in crowded scenes based on images and videos of the crowds.</p><p>There exist a number of computer vision algorithms for crowd counting, and the current state-of-the-art methods are based on density estimation rather than detection-then-counting. Density-estimation methods use Convolutional Neural arXiv:1807.09959v1 [cs.CV] 26 Jul 2018 <ref type="figure">Fig. 1</ref>. Crowd counting can be posed as a CNN-based density estimation problem, but this problem can be challenging for a single CNN due to the huge variation of density values across pixels of different images. This figure shows two images from the Shanghaitech dataset that have very different crowd densities. As can be seen, crowd count could vary from a few to a few thousand.</p><p>Networks (CNNs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> to output a map of density values, one for each pixel of the input image. The final count estimate can be obtained by summing over the predicted density map. Unlike the detection-then-counting approach (e.g., <ref type="bibr" target="#b4">[5]</ref>), the output of the density estimation approach at each pixel is not necessarily binary. Density estimation has been proved to be more robust than the detectionthen-counting approach because the former does not have to commit to binarized decisions at an early stage.</p><p>Estimating the crowd density per pixel is a challenging task due to the large variation of the crowd density values. As shown in <ref type="figure">Figure 1</ref>, some images contain hundreds of people, while others have only a few. It is difficult for a single CNN to handle the entire spectrum of crowd densities. Earlier works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15]</ref> have tackled this challenge by using a multi-column or a switching CNN architecture. These CNN architectures consist of three parallel CNN branches with different receptive field sizes. In such architectures, a branch with smaller receptive fields could handle the high density images well, while a branch with larger receptive fields could handle the low density images. More recently, a five-branch CNN architecture was proposed <ref type="bibr" target="#b15">[16]</ref> where three of the branches resembled the previous multi-column CNN <ref type="bibr" target="#b19">[20]</ref>, while the remaining two branches acted as global and local context estimators. These context estimator branches were trained beforehand on the related task of classifying the image into different density categories. Some of the key takeaways from these previous approaches are: <ref type="bibr" target="#b0">(1)</ref> using a multi-column CNN model with varying kernel sizes improves the performance of crowd density estimation; and (2) augmenting the feature set with the ones learned from a task related to density estimation, such as count range classification, improves the performance of the density estimation task.</p><p>In this work, we propose iterative counting Convolutional Neural Networks (ic-CNN), a CNN-based iterative approach for crowd counting. Unlike previous approaches, where three <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15]</ref> or more <ref type="bibr" target="#b15">[16]</ref> columns are needed to achieve good performance, our ic-CNN approach has a simpler architecture comprising of two On the top is the Low Resolution CNN branch (LR-CNN) and at the bottom is the High Resolution CNN branch (HR-CNN). LR-CNN predicts a density map at a lower resolution (LR). It passes the predicted density map and the convolutional feature maps to HR-CNN. HR-CNN fuses its feature maps with the feature maps and predicted density map from LR-CNN, and predicts a high resolution density map (HR) at the size of the original image. LR and HR are low and high resolution prediction maps respectively.</p><p>columns/branches. The first branch predicts a density map at a lower resolution of <ref type="bibr">1 4</ref> the size of the original image, and passes the predicted map and a set of convolutional features to the second branch. The second branch predicts a high resolution density map at the size of the original image. Density maps contain information about the spatial distribution of crowd in an image. Hence, the first stage map serves as an important feature for the high resolution density map prediction task. We also propose a multi-stage extension of ic-CNN where we combine multiple ic-CNNs sequentially to further improve the quality of the predicted density map. Each ic-CNN in the multi-stage pipeline provides both the low and high resolution density predictions to all subsequent stages. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the schematic architecture for ic-CNN. ic-CNN has two branches: Low Resolution CNN (LR-CNN) and High Resolution CNN (HR-CNN). LR-CNN predicts the density map at a low resolution while HR-CNN predicts the density map at the original image resolution. The key highlights of our work are: 1. We propose ic-CNN, a two-stage CNN framework for crowd density estimation and counting. 2. ic-CNN achieves state of the art results on multiple crowd counting datasets.</p><p>On Shanghaitech Part B dataset, ic-CNN yields 48.3% improvement in terms of mean absolute error over the previously published results <ref type="bibr" target="#b15">[16]</ref>. 3. We also propose a multi-stage extension of ic-CNN, which can combine predictions from multiple ic-CNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Crowd counting is an important research problem and a number of approaches have been proposed by the computer vision community. Earlier work tackled crowd counting as an object detection problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Lin et al. <ref type="bibr" target="#b11">[12]</ref> extracted Haar features for head like contours and used an SVM classifier to classify these features as the contour of a head or not. Li et al. <ref type="bibr" target="#b10">[11]</ref> proposed a detection based approach where the input image was first segmented into foreground-background regions and a HOG feature based head-shoulder detector was used to detect each person in the crowd. These detection based methods often fail to accurately count people in extremely dense scenes. To handle images of dense crowds, some methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> proposed to use a regression approach to avoid the harder detection problem. They instead extracted local patch level features and learned a regression function to directly estimate the total count for an input image patch. These regression approaches, however, do not fully utilize the available annotation associated with training data; they ignore the spatial density and distribution of people in training images. Several researchers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref> proposed to use a density estimation approach to take advantage of the provided crowd density annotation maps of training images. Lempitsky &amp; Zisserman <ref type="bibr" target="#b9">[10]</ref> learned a linear mapping between the crowd images and the corresponding ground truth density maps.</p><p>Pham et al. <ref type="bibr" target="#b13">[14]</ref> learned a more robust mapping by using a random decision forest to estimate the crowd density map. These density-based methods solve some of the challenges faced by the earlier detection and regression based approaches, by avoiding the harder detection problem and also utilizing the spatial annotation and correlation. All aforementioned methods predated the deep-learning era, and they used hand crafted features for crowd counting.</p><p>More recent methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref> used CNNs to tackle crowd counting. Wang et al. <ref type="bibr" target="#b17">[18]</ref> posed crowd counting as a regression problem, and used a CNN model to map the input crowd image to its corresponding count. Instead of predicting the overall count, Fu et al. <ref type="bibr" target="#b3">[4]</ref> classified an image into five broad crowd density categories and used a cascade of two CNNs in a boosting like strategy where the second CNN was trained on the images misclassified by the first CNN. These methods also overlooked the benefits provided by the crowd density annotation maps.</p><p>The methods that are most related to our work are <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Zhang et al. <ref type="bibr" target="#b19">[20]</ref> proposed a CNN-based method to predict crowd density maps. To handle the large variation in crowd densities and sizes across different images, Zhang et al. <ref type="bibr" target="#b19">[20]</ref> proposed a multi-column CNN architecture (MCNN) with filters and receptive fields of various sizes. The CNN column with smaller receptive field and filter sizes were responsible for the denser crowd images, while the CNN columns with larger receptive fields and filter sizes were meant for the less dense crowd images. The features from the three columns were concatenated and processed by a 1×1 convolution layer to predict the final density map. To handle the variations in density and size within an image, the authors divided each image into non-overlapping patches, and trained the MCNN architecture on these patches. Given that the number of training samples in annotated crowd counting datasets is much smaller in comparison to the datasets pertaining to image classification and segmentation tasks, training a CNN from scratch on full images might lead to overfitting. Hence, patch-based training of MCNN was essential in preventing overfitting and also improving the overall performance by serving as a data augmentation strategy. One issue with MCNN was that it fused the features form three CNN columns for predicting the density map. For a given patch, it is expected that the counting performance can be made more accurate by choosing the right CNN column that specializes in analyzing images of similar density values. Sam et al. <ref type="bibr" target="#b14">[15]</ref> built on this idea and decoupled the three columns into separate CNNs, each focused on a subset of the training patches. To decide which CNN to assign a patch to, the authors trained a CNN-based switch classifier. However, since the ground truth label needed to train the switch classifier was unavailable, the authors resorted to a multi-stage training strategy: 1) training the three density predicting CNNs on the entire set of training patches, 2) training the switch classifier using the count from the previous stage to decide the switch labels, and 3) retraining the three CNNs using the patches assigned by the switch classifier. In a more recent work, Sindagi et al. <ref type="bibr" target="#b15">[16]</ref> further modified the MCNN architecture by adding two more branches for estimating global and local context maps. The global/local context prediction branches were trained beforehand for the related task of classifying an image/patch into five different count categories. The classification scores were used to create a feature map of the same size as the image/patch, which served as the global/local context map. These context maps were fused with the convolutional feature maps obtained using a three branch multi-column CNN, and the resulting features were further processed by convolutional layers and a 1×1 convolution layer to obtain the final density map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>In this section, we describe the architecture of ic-CNN, its multi-stage extension and the training strategy. ic-CNN is discussed in Section 3.1. The multi stage extension of ic-CNN is discussed in Section 3.2, and the training details are discussed in Sec 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Iterative Counting CNN</head><formula xml:id="formula_0">Let D = {(X 1 , Y 1 , Z 1 ), .</formula><p>. . , (X n , Y n , Z n )} be the training set of n (image, high resolution density map, low resolution density map) triplets, where X i is the i th image, Y i is the corresponding crowd density map at the same resolution as the image X i , and Z i is a low resolution version of the crowd density map. Y i and Z i have the same overall count. Let f l and f h be the mapping functions which transform the image into the low resolution and high resolution density maps, respectively. Let the parameters of the low resolution branch (LR-CNN) and high resolution branch (HR-CNN) be θ l and θ h respectively. Note that f l depends on only θ l , while f h depends on both θ l and θ h . Given an input image X i , the low resolution density mapẐ i can be obtained by a doing a forward pass through the LR-CNN branch:Ẑ</p><formula xml:id="formula_1">i = f l (X i ; θ l ).<label>(1)</label></formula><p>The inputs to the high resolution branch HR-CNN are: the image X i , the features computed by the low resolution branch LR-CNN, and the low resolution predictionẐ i . HR-CNN predicts a high resolution density map of the same size as the original image:Ŷ</p><formula xml:id="formula_2">i = f h (X i ,Ẑ i ; θ l , θ h ).<label>(2)</label></formula><p>The low resolution predictionẐ i contains information about the spatial distribution of the crowd in the image X i . It serves as an important feature map for the high resolution prediction task. We can learn the parameters θ l and θ h by minimizing the loss function L(θ l , θ h ):</p><formula xml:id="formula_3">L(θ l , θ h ) = 1 n n i=1 (λ l L(f l (X i ; θ l ), Z i ) + λ h L(f h (X i ,Ẑ i ; θ l , θ h ), Y i )),<label>(3)</label></formula><p>where L(·, ·) denotes the loss function, and a reasonable choice is to use the squared error between the estimated and ground truth values. λ l and λ h are scalar hyperparameters which can be used to give more importance to one of the loss terms. Using Equations <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula" target="#formula_2">(2)</ref>, the right hand side can be further simplified as:</p><formula xml:id="formula_4">L(θ l , θ h ) = 1 n n i=1 (λ l L(Ẑ i , Z i ) + λ h L(Ŷ i , Y i )).<label>(4)</label></formula><p>At test time, given an image X i , we first obtain the low resolution output Z i by doing a forward pass through LR-CNN and then pass the convolutional features and the low resolution mapẐ i to HR-CNN, which will predict the high resolution mapŶ i . We use the high resolution output predicted by HR-CNN as the final output of ic-CNN. The overall crowd count is obtained by summing over all the pixels in the density mapŶ i . Below we provide the architecture details for the LR-CNN and HR-CNN branches. LR-CNN. The LR-CNN branch takes as input an image, and predicts a density map at 1 4 the size of the original image. LR-CNN has the following architecture: Conv3-64, Conv3-64, MaxPool, Conv3-128, Conv3-128, MaxPool, Conv3-256, Conv3-256, Conv3-256, Conv7-196, Conv5-96, Conv3-32, Conv1-1. Here, ConvX-Y implies a convolution layer having Y filters with X×X kernel size. MaxPool is the max pooling layer. We use a ReLU nonlinearity after each convolutional layer.</p><p>HR-CNN. The HR-CNN branch predicts the high resolution density map at the same size as the input image. HR-CNN has the following architecture: Conv7-16, MaxPool, Conv5-24, MaxPool, Conv3-48, Conv3-48, Conv3-24, Conv7-196, Conv5-96, Upsampling-2, Conv3-32, Upsampling-2, Conv1-1. Here, Upsampling-2 is a bilinear interpolation layer which upsamples the input to twice its size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-stage Crowd Counting</head><p>A multi-stage ic-CNN is a network that combines multiple building blocks of ic-CNN described in the previous section. Each ic-CNN block inputs the low and high resolution prediction maps from all the previous blocks. Given an input image X i , the low resolution branch of the k th block, represented by the function f k l , outputs the low resolution prediction:</p><formula xml:id="formula_5">Z k i = f k l (X i ,Ẑ 1:k−1 i ,Ŷ 1:k−1 i , θ k l ),<label>(5)</label></formula><p>where θ k l represents the parameters of LR-CNN,Ẑ 1:k−1 i andŶ 1:k−1 i represent the set of low and high level predictions from the first k − 1 blocks for the input X i . The high resolution branch of the k th block, represented by the function f k h , takes as input the image X i , the feature maps computed by the low resolution branch f k l , the low resolution predictionẐ k i , and the entire set of low and high resolution prediction maps from the first k − 1 blocks. Hence, the output of the k th HR-CNN can be computed using:</p><formula xml:id="formula_6">Y k i = f k h (X i ,Ẑ 1:k i ,Ŷ 1:k−1 i , θ k l , θ k h ).<label>(6)</label></formula><p>Note that f k l and f k h do not depend on the parameters for the first k − 1 blocks, andẐ 1:k−1 i andŶ 1:k−1 i are treated as fixed inputs (i.e., the parameters of the corresponding network blocks are frozen). We can learn the parameters θ k l and θ k h by minimizing the loss function L(θ k l , θ k h ):</p><formula xml:id="formula_7">L(θ k l , θ k h ) = λ l n n i=1 L(f k l (X i ,Ẑ 1:k−1 i ,Ŷ 1:k−1 i , θ k l ), Z i ) + λ h n n i=1 L(f k h (X i ,Ẑ 1:k i ,Ŷ 1:k−1 i , θ k l , θ k h ), Y i ).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Details</head><p>An ic-CNN is trained by minimizing the loss function L(θ l , θ h ) from Equation <ref type="formula" target="#formula_3">(3)</ref>. We use the Stochastic Gradient Descent algorithm with the following hyper parameters (unless specified otherwise): learning rate 10 −4 , momentum 0.9, batch size 1. We give more importance to the high resolution loss term in Equation <ref type="formula" target="#formula_3">(3)</ref> and set λ l and λ h to 10 −2 and 10 2 , respectively. We train a multi-stage ic-CNN in multiple stages. In the k th stage, we train the k th ic-CNN block by minimizing the loss function given in Equation <ref type="formula" target="#formula_7">(7)</ref>, using the Stochastic Gradient Descent algorithm with the same hyper parameters as above. Once the training for the k th stage has converged, we freeze the parameters for the k th stage and proceed to the next stage.</p><p>The training data consists of crowd images and corresponding ground truth annotation files. A ground truth annotation for an image specifies the location of each person in the image with a single dot on the person. We convert this annotation into a binary map consisting of 0's at all locations, except for the annotated points which are assigned the value of 1. We convolve this binary map with a Gaussian filter of standard deviation 5. We use the resulting density map for training the networks. <ref type="table">Table 1</ref>. Count errors of different methods on the Shanghaitech dataset. This dataset has two parts: A and B. We compare ic-CNN with the previous state-of-the-art approaches, using two metrics: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). ic-CNN (one stage) is the single stage ic-CNN with two branches HR-CNN and LR-CNN. ic-CNN (two stages) is the two-stage variant of ic-CNN. Both ic-CNN networks outperform the previous approaches in 3 out of 4 cases. On the Shanghaitech Part B dataset, using the one-stage ic-CNN, which has a simpler architecture than CP-CNN <ref type="bibr" target="#b15">[16]</ref>, we improve on the previously reported state of the art results by 48.3% using the MAE metric and 46.8% using the RMSE metric </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on three challenging datasets: Shanghaitech <ref type="bibr" target="#b19">[20]</ref>, World-Expo'10 <ref type="bibr" target="#b18">[19]</ref>, and UCF Crowd Counting Dataset <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metrics</head><p>Following previous works for crowd counting, we use the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to evaluate the performance of our proposed method. If the predicted count for image i isĈ i and the ground truth count is C i , the MAE and RMSE can be computed as:</p><formula xml:id="formula_8">M AE = 1 n n i=1 |C i −Ĉ i |, RM SE = 1 n n i=1 (C i −Ĉ i ) 2 (8)</formula><p>where n is the number of test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on the Shanghaitech Dataset</head><p>The Shanghaitech dataset <ref type="bibr" target="#b19">[20]</ref> consists of 1198 annotated crowd images. The dataset is divided into two parts, Part-A containing 482 images and Part-B containing 716 images. Part-A is split into train and test subsets consisting of 300 and 182 images, respectively. Part-B is split into train and test subsets consisting of 400 and 316 images. Each person in a crowd image is annotated with one point close to the center of the head. In total, the dataset consists of 330,165 annotated  <ref type="table">Table 1</ref>, we compare ic-CNNs with the previous state-of-the-art approaches. ic-CNNs outperform the previous approaches in three out of four cases by a large margin. On Part-B of the Shanghaitech dataset, using the one-stage ic-CNN which has a simpler architecture than the five-branch CP-CNN <ref type="bibr" target="#b15">[16]</ref>, we improve on the previously reported state of the art results by 48.3% for MAE metric and 46.8% for the RMSE metric. On Part A of the Shanghaitech dataset, we achieve a 5.1 absolute improvement in MAE over CP-CNN. Furthermore, for Part A data, the two-stage ic-CNN results in an improvement of 1.3 MAE over the one-stage ic-CNN. We also trained a three-stage ic-CNN on Part A data, which resulted in MAE = 69.4 and RMSE = 116.0. Since adding the 3 rd stage did not yield a significant performance gain, we did not experiment with more than three stages.</p><p>In <ref type="table">Table 2</ref>, we analyze the effects of varying the resolution of the intermediate prediction on the overall performance. Using any resolution other than <ref type="bibr">1 4</ref> leads to a drop in the performance.</p><p>In <ref type="table">Table 3</ref>, we analyze the effects of varying the hyperparameter λ h on performance of ic-CNN. We use Shanghaitech Part-A dataset for this experiment. We show the MAE of the high and low resolution branches as the scalar weight λ h is varied. λ l is kept fixed at 10 −2 . We can see that the LR-CNN branch performs better when λ l is comparable with λ h , and its performance degrades when λ h is too large. The performance of HR-CNN improves as λ h is varied from 10 −4 to 10 2 . In the extreme case when λ h is set to 10 4 , there is a large degradation in the performance of the LR-CNN branch, which affects the performance of the HR-CNN branch. When λ h is 10 4 , the low resolution prediction task is possibly ignored, and the network solely focuses on solving the high resolution task. In such a scenario, the low resolution prediction does not contain any useful information, which affects the performance of the high resolution branch HR-CNN. We obtain the best results for the HR-CNN branch when λ h is set to 10 2 . In this case, the high resolution loss does not force the network to completely ignore the low resolution task.</p><p>In <ref type="table" target="#tab_2">Table 4</ref>, we show the training time and the number of parameters of ic-CNN, MCNN, Switching CNN, and CP-CNN. An ic-CNN takes 10 hours to train, while a Switching CNN takes around 22 hours. An ic-CNN has significantly fewer parameters than a CP-CNN and a Switching CNN. We contacted the authors of MCNN and CP-CNN, but we did not get a response for the training time of these networks.</p><p>In <ref type="table">Table 5</ref>, we analyze the importance of each of the components of our proposed ic-CNN model. We see that both the feature sharing and the feedback of the low resolution prediction are important for ic-CNN. Removing any of these two components leads to significant drop in performance.  <ref type="figure">Fig. 3</ref>. Performance across different crowd density: We divide the 182 test images from Shanghaitech Part A into 10 groups on the basis of the crowd count. Each group except the last has 18 test images. We average the crowd count across a group to obtain the average count. GT is the ground truth, ic-CNN is prediction from the high resolution branch. For majority of the count groups, the difference between the average counts for ic-CNN and GT is small.</p><p>In <ref type="figure">Figure 3</ref>, we analyze the performance of ic-CNN across different groups of images with varying crowd counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on the WorldExpo'10 Dataset</head><p>The WorldExpo'10 dataset consists of 1132 annotated video sequences captured by 108 surveillance cameras. Annotated frames from 103 cameras are used for training and the annotated frames from the remaining 5 cameras are used for testing. We trained ic-CNN networks using random crops of sizes H 2 × W 2 . We used the networks trained on Shanghaitech Part A for initializing the models for the experiments on the WorldExpo dataset. In <ref type="table">Table 6</ref>, we compare ic-CNN with other state of art approaches. ic-CNN outperforms these previous approaches on three out of five cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments on the UCF Dataset</head><p>The UCF Crowd Counting dataset <ref type="bibr" target="#b5">[6]</ref> consists of 50 crowd images collected from the web. Each person in the dataset is annotated with a single dot annotation. The numbers of people in the images vary from 94 to 4545 with an average of 1280 people per image. The average count for the UCF dataset is much larger than the previous two datasets. Following previous works using this dataset, we performe five-fold cross validation and report the MAE and RMSE values. We trained ic-CNN networks using random crops of sizes H 3 × W 3 . We compare ic-CNN with previous approaches and show the results in <ref type="table">Table 7</ref>. Since the dataset is small, adding multiple stages to ic-CNN could lead to overfitting. Hence we only use one-stage ic-CNN on the UCF dataset. ic-CNN achieves the best MAE on this dataset, outperforming CP-CNN by a large margin.  <ref type="bibr" target="#b5">[6]</ref> 419.5 487.1 Crowd CNN <ref type="bibr" target="#b18">[19]</ref> 467.0 498.5 Crowdnet <ref type="bibr" target="#b0">[1]</ref> 452.5 -MCNN <ref type="bibr" target="#b19">[20]</ref> 377.6 509.1 Hydra2s <ref type="bibr" target="#b12">[13]</ref> 333.7 425.6 Switch CNN <ref type="bibr" target="#b14">[15]</ref> 318.1 439.2 CP-CNN <ref type="bibr" target="#b15">[16]</ref> 295.8 320.9 ic-CNN (proposed) 260.9 365.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>In <ref type="figure">Figure 4</ref>, we show some qualitative results on images from the Shanghaitech Part-A dataset obtained using ic-CNN. The first three are success cases for ic-CNN, while the last two are failure cases. In the failure cases, we see that ic-CNN sometimes misclassify tree leaves as tiny people in a crowd. In <ref type="figure" target="#fig_2">Figure 5</ref>, we show some qualitative results on images from Shanghaitech Part-B dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have proposed ic-CNN, a two-branch architecture for crowd counting via crowd density estimation based. We have also proposed a multistage pipeline comprising of multiple ic-CNNs, where each stage takes into account the predictions of all the previous stages. We performed experiments on three challenging crowd counting benchmark datasets and observed the effectiveness of our iterative approach.  Underneath each density map is the total count, rounded to the nearest integer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Figure shows the ic-CNN architecture which consists of two columns/branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results on the Shanghaitech Part B dataset. The four columns show the input image, the ground truth annotation map, the low resolution prediction (LR output), and the high resolution prediction map (HR output).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>MAE and RMSE on Shanghaitech Part-A dataset as we vary the resolution being used for the low resolution branch LR-CNN of ic-CNN. The resolution of HR-CNN is fixed at 1, the size of the input image. Effect of varying hyper parameter λ h : Mean absolute error on Shanghaitech Part A dataset. λ l is kept fixed at 10 −2 .people. Images from Part-A were collected from the Internet, while images from Part-B were collected on the busy streets of Shanghai. To avoid the risk of overfitting to the small number of training images, we trained ic-CNNs on random crops of size H 3 × W 3 , where H and W are the height and width of a training image. In</figDesc><table><row><cell>LR-Resolution</cell><cell>HR-Resolution</cell><cell>MAE</cell><cell>RMSE</cell></row><row><cell>1/8</cell><cell>1</cell><cell>74.9</cell><cell>131.6</cell></row><row><cell>1/4</cell><cell>1</cell><cell>69.8</cell><cell>117.3</cell></row><row><cell>1/2</cell><cell>1</cell><cell>73.3</cell><cell>124.4</cell></row><row><cell>1</cell><cell>1</cell><cell>74.4</cell><cell>128.3</cell></row><row><cell>λ h</cell><cell>LR-CNN</cell><cell>HR-CNN</cell><cell></cell></row><row><cell>10 −4</cell><cell>73.7</cell><cell>78.8</cell><cell></cell></row><row><cell>10 −2</cell><cell>73.0</cell><cell>73.6</cell><cell></cell></row><row><cell>1</cell><cell>75.1</cell><cell>73.3</cell><cell></cell></row><row><cell>10 2</cell><cell>79.9</cell><cell>69.8</cell><cell></cell></row><row><cell>10 4</cell><cell>432.6</cell><cell>74.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Training time, number of parameters, and MAE on Part A of the Shanghaitech dataset. ic-CNN was trained on a single GPU machine (Nvidia GTX 1080 TI).</figDesc><table><row><cell>Model</cell><cell>Training Time</cell><cell cols="2">Number of Parameters</cell><cell>MAE</cell></row><row><cell>MCNN [20]</cell><cell>unknown</cell><cell>1.27 × 10 5</cell><cell></cell><cell>110.2</cell></row><row><cell>Switching CNN [15]</cell><cell>22 hrs</cell><cell>1.2 × 10 7</cell><cell></cell><cell>90.4</cell></row><row><cell>CP-CNN [16]</cell><cell>unknown</cell><cell>6.3 × 10 7</cell><cell></cell><cell>73.6</cell></row><row><cell>ic-CNN (proposed)</cell><cell>10 hrs</cell><cell>7.9 × 10 6</cell><cell></cell><cell>69.8</cell></row><row><cell cols="5">Table 5. Ablation study on Shanghaitech Part A data. HR-CNN is the high</cell></row><row><cell cols="5">resolution branch, LR-CNN is the low resolution branch. LR-CNN alone and HR-CNN</cell></row><row><cell cols="5">alone refer to a counting network that contains either LR-CNN or HR-CNN only.</cell></row><row><cell cols="5">ic-CNN is our proposed approach, where both the features and the low resolution</cell></row><row><cell cols="5">prediction map from LR-CNN are shared with HR-CNN. We also compared with two</cell></row><row><cell cols="5">variants where either the low resolution map or the convolutional feature maps from</cell></row><row><cell cols="2">LR-CNN is not shared with the HR-CNN.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="2">MAE</cell><cell>RMSE</cell></row><row><cell>LR-CNN alone</cell><cell></cell><cell></cell><cell>78.5</cell><cell>133.2</cell></row><row><cell>HR-CNN alone</cell><cell></cell><cell cols="2">136.2</cell><cell>204.0</cell></row><row><cell cols="3">HR-CNN + LR-CNN features (no low-res prediction)</cell><cell>75.1</cell><cell>129.0</cell></row><row><cell cols="3">HR-CNN + LR-CNN low-res prediction (no features)</cell><cell>77.4</cell><cell>130.4</cell></row><row><cell>ic-CNN (proposed)</cell><cell></cell><cell cols="2">69.8</cell><cell>117.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Performance of different methods on the WorldExpo'10 dataset. Switch CNN(with perspective) refers to the case when perspective maps are used to obtain the crowd density map, while Switch CNN(sans perspective) refers to the case when the perspective map isn't used.ic-CNN is our proposed two branch approach. We outperform other approaches on 3 of 6 cases. Performance of various methods on the UCF Crowd Counting dataset. The proposed method ic-CNN achieves the best MAE.</figDesc><table><row><cell>Method</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S4</cell><cell>S5</cell><cell>Avg</cell></row><row><cell>Crowd CNN [19]</cell><cell>9.8</cell><cell>14.1</cell><cell>14.3</cell><cell>22.2</cell><cell>3.7</cell><cell>12.9</cell></row><row><cell>MCNN [20]</cell><cell>3.4</cell><cell>20.6</cell><cell>12.9</cell><cell>13.0</cell><cell>8.1</cell><cell>11.6</cell></row><row><cell>Switching CNN (sans perspective) [15]</cell><cell>4.4</cell><cell>15.7</cell><cell>10.0</cell><cell>11.0</cell><cell>5.9</cell><cell>9.4</cell></row><row><cell>Switching CNN (with perspective) [15]</cell><cell>4.2</cell><cell>14.9</cell><cell>14.2</cell><cell>18.7</cell><cell>4.3</cell><cell>11.2</cell></row><row><cell>CP-CNN[16]</cell><cell>2.9</cell><cell>14.7</cell><cell>10.5</cell><cell>10.4</cell><cell>5.8</cell><cell>8.8</cell></row><row><cell>ic-CNN (proposed)</cell><cell>17.0</cell><cell>12.3</cell><cell>9.2</cell><cell>8.1</cell><cell>4.7</cell><cell>10.3</cell></row><row><cell>Method</cell><cell></cell><cell>MAE</cell><cell></cell><cell cols="2">RMSE</cell><cell></cell></row><row><cell>Lempitsky &amp; Zisserman [10]</cell><cell></cell><cell>493.4</cell><cell></cell><cell cols="2">487.1</cell><cell></cell></row><row><cell>Idrees et. al</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Fig. 4. Qualitative results, some success and failure cases. The four columns show the input image, ground truth annotation map, the low resolution prediction (LR output), and the high resolution prediction map (HR output). The total counts are shown below each density map. The first three rows are success cases for ic-CNN, while the last two are failure cases. ic-CNN sometimes misclassifies tree leaves as people.</figDesc><table><row><cell>Image</cell><cell>Ground truth</cell><cell>LR output</cell><cell>HR output</cell></row><row><cell>Image</cell><cell>Ground truth</cell><cell>LR output</cell><cell>HR output</cell></row><row><cell></cell><cell>23</cell><cell>26</cell><cell>24</cell></row><row><cell></cell><cell>502</cell><cell>793</cell><cell>512</cell></row><row><cell></cell><cell>252</cell><cell>257</cell><cell>252</cell></row><row><cell></cell><cell>270</cell><cell>346</cell><cell>280</cell></row><row><cell></cell><cell>183</cell><cell>191</cell><cell>186</cell></row><row><cell></cell><cell>86</cell><cell>114</cell><cell>89</cell></row><row><cell></cell><cell>181</cell><cell>167</cell><cell>164</cell></row><row><cell></cell><cell>172</cell><cell>493</cell><cell>317</cell></row><row><cell></cell><cell>84</cell><cell>109</cell><cell>103</cell></row><row><cell></cell><cell>566</cell><cell>961</cell><cell>744</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by SUNY2020 Infrastructure Transportation Security Center. The authors would like to thank Boyu Wang for participating on the discussions and experiments related to an earlier version of the proposed technique. The authors would like to thank NVIDIA for their GPU donation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowdnet: A deep convolutional network for dense crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast crowd density estimation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="81" to="88" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Talking heads: Detecting humans and recognizing their interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The discrimination of visual number</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Reese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Volkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American journal of psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="525" />
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimation of number of people in crowded scenes using perspective transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="645" to="654" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kozakaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why are small and large numbers enumerated differently? a limited-capacity preattentive stage in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Trick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
