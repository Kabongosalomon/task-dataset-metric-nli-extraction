<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
							<email>rad@icg.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
							<email>lepetit@icg.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratoire Bordelais de Recherche en Informatique</orgName>
								<orgName type="institution">Université de Bordeaux</orgName>
								<address>
									<settlement>Bordeaux</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel method for 3D object detection and pose estimation from color images only. We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a "holistic" approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes. This, however, is not sufficient for handling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this problem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional additional step that refines the predicted poses. We improve the state-of-the-art on the LINEMOD dataset from 73.7% [2] to 89.3% of correctly registered RGB frames. We are also the first to report results on the Occlusion dataset [1] using color images only. We obtain 54% of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, compared to the 67% of the state-of-the-art [10] on the same sequences which uses both color and depth. The full approach is also scalable, as a single network can be trained for multiple objects simultaneously.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D pose estimation of object instances has recently become a popular problem again, because of its application in robotics, virtual and augmented reality. Many recent approaches rely on depth maps, sometimes in conjunction with color images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10]</ref>. However, it is not always possible to use depth cameras, as they fail (a) (b) (c) (d) <ref type="figure">Figure 1</ref>. Zooms on estimated poses for (a) the Ape of the LINEMOD dataset <ref type="bibr" target="#b6">[7]</ref>, (b) the Driller of the Occlusion dataset <ref type="bibr" target="#b0">[1]</ref>, (c) and (d) three objects of the T-LESS <ref type="bibr" target="#b9">[10]</ref> dataset. The green bounding boxes correspond to the ground truth poses, and the blue bounding boxes to the poses estimated with our method. The two boxes often overlap almost perfectly, showing the accuracy of our estimated poses. The parts of the bounding boxes occluded by the object were removed using the object mask rendered from our estimated pose. In (b), we can still obtain a good pose despite the large occlusion by the bench vise. In (c) and (d), we also obtain very good estimates despite large occlusions, the similarities between the objects, and the fact that the symmetries challenge the learning algorithms.</p><p>outdoor or on specular objects. In addition, they drain the batteries of mobile devices, being an active sensor. It is therefore desirable to rely only on color images for 3D pose estimation, even if it is more challenging. Recent methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2]</ref> work by identifying the 'object coordinates' of the pixels, which are the pixels' 3D coordinates in a coordinate system related to the object <ref type="bibr" target="#b18">[19]</ref>. The object 3D pose can then be estimated using a PnP algorithm from these 2D-3D correspondences. <ref type="bibr" target="#b2">[3]</ref> obtain similar correspondences by associating some pixels in selected parts of the object with virtual 3D points. However, obtaining these 2D-3D correspondences from local patches is difficult and the output is typically very noisy for these methods. A robust optimization is then needed to estimate the pose.</p><p>In this paper, we argue for a "holistic" approach, in the sense that we predict the pose of an object directly from its appearance, instead of identifying its individual surface points. As we will show, this approach provides significantly better results.</p><p>We first detect the target objects in 2D. We show that using object segmentation performs better for this task compared to a standard sliding window detector, in particular in presence of partial occlusion. We then apply a CNN to predict the 3D pose of the detected objects. While the predicted 3D pose can be represented directly by a translation and a rotation, we achieve better accuracy by using a representation similar to the one used in <ref type="bibr" target="#b2">[3]</ref> for object parts: We predict the 2D projections of the corners of the object's bounding box, and compute the 3D pose from these 2D-3D correspondences with a PnP algorithm. Compared to the object coordinate approaches the predictions are typically outlier-free, and no robust estimation is thus needed. Compared to the direct prediction of the pose, this also avoids the need for a meta-parameter to balance the translation and rotation terms.</p><p>Unfortunately, this simple approach performs badly on the recent and challenging T-LESS dataset. This dataset is made of manufactured objects that are not only similar to each other, but also have one axis of rotational symmetry. For example, the squared box of <ref type="figure">Fig. 1</ref>(c) has an angle of symmetry of 90 • and the other object has an angle of symmetry of 0 • since it is an object of revolution; Object #5 in <ref type="figure">Fig. 1(d)</ref> is not perfectly symmetrical but only because of the small screw on the top face.</p><p>The approach described above fails on these objects because it tries to learn a mapping from the image space to the pose space. Since two images of a symmetrical object under two different poses look identical, the image-pose correspondence is in fact a one-to-many relationship. This issue is actually not restricted to our approach. For example, <ref type="bibr" target="#b1">[2]</ref>, which relies on object coordinates, does not provide results on the Bowl object of the LINEMOD dataset, an object with an axis of symmetry: It is not clear which coordinates should be assigned to the 3D points of this object, as all the points on a circle orthogonal to the axis of symmetry have the same appearance.</p><p>To solve this problem, we train the method described above using images of the object under rotation in a restricted range, such that the training set does not contain ambiguous images. In order to recover the object pose under a larger range of rotation, we train a classifer to tell under which range the object rotation is. Again, this is easy to do with a "holistic" approach, and this classifier takes an image of the entire object as input. As we will explain in more details, we can then always use the CNN trained on the restricted range to estimate any pose. In addition, we will show how to adapt this idea to handle "approximatively symmetrical" objects like Object #5. This approach allows us to obtain good performance on the T-LESS dataset.</p><p>Finally, we show that we can add an optional last step to refine the pose estimates by using the "feedback loop" proposed in <ref type="bibr" target="#b16">[17]</ref> for hand detection in depth images: We train a network to improve the prediction of the 2D projections by comparing the input image and a rendering of the object for the initial pose estimate. This allows us to improve even more our results on the LINEMOD and Occlusion datasets.</p><p>Our full approach, which we call BB8, for the 8 corners of the bounding box, is also very fast, as it only requires to apply Deep Networks to the input image a few times. In the remainder of the paper, we first discuss related work, describe our approach, and compare it against the state-ofthe-art on the three available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The literature on 3D object detection is very large, thus we will focus only on recent works. Keypoint-based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref> were popular for a long time and perform well but only on very textured objects. The apparition of inexpensive 3D cameras favored the development of methods suitable for untextured objects: <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref> rely on depth data only and use votes from pairs of 3D points and their normals to detect 3D objects. <ref type="bibr" target="#b13">[14]</ref> uses a decision tree applied to RGB-D images to simultaneously recognize the objects and predict their poses. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref> consider a template-based representation computed from RGB-D or RGB data, which allows for large scale detection <ref type="bibr" target="#b10">[11]</ref>. However, this template approach is sensitive to partial occlusions.</p><p>To tackle clutter and partial occlusions, <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b20">[21]</ref> rely on local patches recognition performed with Random Forests. In particular, <ref type="bibr" target="#b0">[1]</ref> considers '3D object coordinates': A Random Forest is trained to predict the 3D location in the object coordinate system of each image location. The prediction of this forest is integrated in an energy function together with a term that compares the depth map with a rendering of the object and a term that penalizes pixels that lie on the object rendering but predicted by the forest to not be an object point. This energy function is optimized by a RANSAC procedure. <ref type="bibr" target="#b12">[13]</ref> replaces this energy function by an energy computed from the output of a CNN trained to compare observed image features and features computed from a 3D rendering of the potentially detected object. This makes the approach very robust to partial occlusions.</p><p>These works, however, are designed for RGB-D data. <ref type="bibr" target="#b1">[2]</ref> extends this work and relies on RGB data only, as we do. They use Auto-Context <ref type="bibr" target="#b21">[22]</ref> to obtain better predictions from the Random Forests, estimate a distribute over the object coordinates to handle the prediction uncertainties better, and propose a more sophisticated RANSAC-like method that scales with the number of objects. This results in an efficient and accurate method, however, robustness to partial occlusions are not demonstrated.</p><p>[3] is related to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2]</ref> but focuses on providing sparse 2D-3D correspondences from reliable object parts. Unfortunately, it provides results on its own dataset only, not on more broadly available datasets.</p><p>Like us, <ref type="bibr" target="#b11">[12]</ref> relies on a CNN to directly predict a 3D pose, but in the form of a translation and a rotation. It considers camera relocalisation in urban environment rather than 3D object detection, and uses the full image as input to the CNN. By predicting the 2D projections of the corners of the bounding box, we avoid the need for a meta-parameter to balance the position and orientation errors. As shown in our experiments, the pose appears to be more accurate when predicted in this form. Intuitively, this should not be surprising, as predicting 2D locations from a color images seems easier than predicting a 3D translation and a quaternion, for example.</p><p>[6] also uses a CNN to predict the 3D pose of generic objects but from RGB-D data. It first segments the objects of interest to avoid the influence of clutter. We tried segmenting the objects before predicting the pose as well, however, this performed poorly on the LINEMOD dataset, because the segmented silhouttes were not very accurate, even with state-of-the-art segmentation methods.</p><p>In summary, our method appears to be one of the first to deal with RGB data only to detect 3D objects and estimate their poses on recent datasets. As we will show in the experiments, it outperforms the accuracy of the state-of-theart <ref type="bibr" target="#b1">[2]</ref> by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In our approach, we first find the objects in 2D, we obtain a first estimate of the 3D poses, including objects with a rotational symmetry, and we finally refine the initial pose estimates. We describe each step in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Localizing the Objects in 2D</head><p>We first identify the 2D centers of the objects of interest in the input images. We could use a standard 2D object detector, but we developed an approach based on segmentation that resulted in better performance as it can provide accurate locations even under partial occlusions. Compared to our initial tests using a sliding window, this ap- proach improved our 2D detection results from about 75% to 98.8% correct detection rate based on a IoU of 0.5. We only need a low resolution segmentation and thus do not need a hourglass-shaped architecture <ref type="bibr" target="#b14">[15]</ref>, which makes our segmentation more efficient. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our approach performs a two-level coarse-to-fine object segmentation. For each level, we train a single network for all the objects. The first network is obtained by replacing the last layer of VGG <ref type="bibr" target="#b19">[20]</ref> by a fully connected layer with the required number of output required by each step, and fine-tune it. The second network has a simple, ad hoc architecture.</p><formula xml:id="formula_0">(a) (b) (c) (d)</formula><p>More exactly, the first network is trained to provide a very low resolution binary segmentation of the objects given an image region J of size 128 × 128 by minimizing the following objective function:</p><formula xml:id="formula_1">(J,S,o)∈Ts (f 1 φ (J))[o] − S 2 ,<label>(1)</label></formula><p>where T s is a training set made of image regions J, and the corresponding segmentations S for object o,</p><formula xml:id="formula_2">(f 1 φ (J))[o]</formula><p>is the output of network f 1 φ for region J and object o. φ denotes the network's parameters, optimized during training. For the LINEMOD and Occlusion datasets, there is at most one object for a given region J, but more objects can be present for the T-LESS dataset. At run-time, to get the segmentations, we compute: </p><formula xml:id="formula_3">s 1,o (J) = (f 1 φ (J))[o] &gt; τ 1 ,<label>(2)</label></formula><formula xml:id="formula_4">s 2,o (P ) = (f 2 ψ (P ))[o] &gt; τ 2 ,<label>(3)</label></formula><p>using notations similar to the ones in Eq. (2). Since the input to f 2 ψ (P ) has a low resolution, we do not need a complex network such as VGG <ref type="bibr" target="#b19">[20]</ref>, and we use a much simpler architecture with 2 convolutional layers and 2 pooling layers. We finally obtain a segmentation S 2,o with resolution 64 × 48 for the full input image and each visible object o. We therefore get the identities o of the visible object(s), and for these objects, we use the segmentation centroids as their 2D centers, to compute the 3D poses of the objects as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Predicting the 3D Pose</head><p>We predict the 3D pose of an object by applying a Deep Network to an image window W centered on the 2D object center estimated as described in the previous section. As for the segmentation, we use VGG <ref type="bibr" target="#b19">[20]</ref> as a basis for this network. This allows us to handle all the objects of the target dataset with a single network.</p><p>It is possible to directly predict the pose in the form of a 3-vector and an exponential map for example, as in <ref type="bibr" target="#b11">[12]</ref>. However, a more accurate approach was proposed in <ref type="bibr" target="#b2">[3]</ref> for predicting the poses of object parts. To apply it here, we minimize the following cost function over the parameters Θ of network g Θ :</p><formula xml:id="formula_5">(W,e,t,o)∈T i Proj e,t (M o i ) − m i ((g Θ (W ))[o]) 2 ,<label>(4)</label></formula><p>where T is a training set made of image windows W containing object o under a pose defined by an exponential map e and a 3-vector t. The  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Handling Objects with an Axis of Symmetry</head><p>If we apply the method described so far to the T-LESS dataset, the performances are significantly lower than the performances on the LINEMOD dataset. As mentioned in the introduction, this is because training images W in Eq. (4) for the objects of this dataset can be identical while having very different expected predictions Proj e,t (M o i ), because of the rotational symmetry of the objects.</p><p>We first remark that for an object with an angle of symmetry α, its 3D rotation around its axis of symmetry can be defined only modulo α, not 2π. For an object with an angle of symmetry α, we can therefore restrict the poses used for training to the poses where the angle of rotation around the symmetry axis is within the range [0; α[, to avoid the ambiguity between images. However, this solves our problem only partially: Images at one extremity of this range of poses and the images at the other extremity, while not identical, still look very similar. As a result, for input images with an angle of rotation close to 0 modulo α, the pose prediction can still be very bad, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>To explain our solution, let us first denote by β the rotation angle, and introduce the intervals r 1 = [0; α/2[ and r 2 = [α/2; α[. To avoid ambiguity, we restrict β to be in r 1 for the training images used in the optimization problem of Eq. (4). The drawback is of course that, without doing anything else, we would not be able to estimate the poses when β is in r 2 .</p><p>We therefore introduce a CNN classifier k(·) to predict at run-time if β is in r 1 or r 2 : If β is in r 1 , we can estimate the pose as before; If β is in r 2 , one option would be to apply another g Θ (·) network trained for this range. However, it is actually possible to use the same network g Θ (·) for both r 1 and r 2 , as follows. If the classifier predicts that β in in r 2 , we mirror the input image W : As illustrated in <ref type="figure" target="#fig_2">Fig. 3(e)</ref>, the object appears in the mirror image with a rotation angle equal to α − β, which is in r 1 . Therefore we can apply g Θ (·) to the mirrored W . To obtain the correct pose, we finally mirror back the projections of the corners predicted by g Θ (·). We currently consider the case where the axis of symmetry is more or less vertical in the image, and mirror the image from left to right. When the axis is closer to be horizontal, we should mirror the image from top to bottom.</p><p>Objects of revolution are a special and simpler case: since their angle of symmetry is 0 • , we predict their poses under the same angle of rotation. For training the pose predictor g Θ (·), we use the original training images with angles of rotation in r 1 , and mirror the training images with angles of rotation in r 2 .</p><p>Handling Objects that are 'Not Exactly Symmetrical' As mentioned in the introduction, some objects of the T-LESS dataset are only approximately symmetrical, such as Object #5 in <ref type="figure">Fig. 1(d)</ref>. The small details that make the object not perfectly symmetrical, however, do not help the optimization problem of Eq. (4), but we would still like to predict the pose of this object.</p><p>In the case of Object #5, we consider 4 regions instead of 2: r 1 = [0; π/2[, r 1 = [π/2; π[, r 3 = [π; 3π/2[, and r 4 = [3π/2; 2π[, and we train the classifier k(·) to predict in which of these four regions the angle of rotation β is. If β ∈ r 2 or β ∈ r 4 , we mirror the image before computing the pose as before. Then, if β ∈ r 3 or β ∈ r 4 , we still have to add π to the angle of rotation of the recovered pose to get an angle between 0 and 2π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Refining the Pose</head><p>We also introduce an optional additional stage to improve the accuracy of the pose estimates inspired by <ref type="bibr" target="#b16">[17]</ref>. As illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>, we train another CNN that predicts an update to improve the pose. Because this CNN takes 4 or 6 channels as input, it is not clear how we can use VGG, as we did for the previously introduced networks, and we use here one CNN per object. However, this stage is optional, and without it, we already outperform the-state-of-the-art. The first image is the image window W as for g Θ (·). The second image depends on the current estimate of the pose: While <ref type="bibr" target="#b16">[17]</ref> generates a depth map with a deep network, we render (using OpenGL) either a binary mask or a color rendering of the target object as seen from this current estimate. More formally we train this CNN by minimizing:</p><formula xml:id="formula_6">(W,e,t)∈T (ê,t)∈N (e,t) i Proj e,t (M o i ) − Projê ,t (M o i )− m i (h µ (W, Render(ê,t))) 2 ,<label>(5)</label></formula><p>where h µ denotes the CNN, µ its parameters; N (e, t) is a set of poses sampled around pose (e, t), and Render(e, t) a function that returns a binary mask, or a color rendering, of the target object seen from pose (e, t).</p><p>At run-time, given a current estimate of the object pose represented by the projections of the cornersv = [. . .m i . . .] , and the corresponding parameterisation (ê,t), we can update this estimate by invoking h µ (·):</p><p>v ←v + h µ (W, Render(ê,t)) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Generating Training Images</head><p>In Section 4, we will compare our method to the stateof-the art for 3D object detection in color images <ref type="bibr" target="#b1">[2]</ref>, and like them, for each of 15 objects of the LINEMOD dataset, we use 15% of the images for training and use the rest for testing. The training images are selected as in <ref type="bibr" target="#b1">[2]</ref>, such that relative orientation between them should be larger than a threshold. We also tried a random selection, and there was only a slight drop in performance, for some objects only. The selection method thus does not seem critical. The T-LESS dataset provides regularly sampled training images.</p><p>As shown in <ref type="figure">Fig. 5</ref>, we also use a similar method as [2] to augment the training set: We extract the objects' silhouettes from these images, which can be done as the ground <ref type="figure">Figure 5</ref>. Two generated training images for different objects from the LINEMOD dataset <ref type="bibr" target="#b6">[7]</ref>. The object is shifted from the center to handle the inaccuracy of the detection method, and the background is random to make sure that the network gΘ cannot exploit the context specific to the dataset. truth poses and the objects' 3D models are available. Note that this means the results are not influenced by the scene context, which makes the pose estimation more difficult.</p><p>To be robust to clutter and scale changes, we scale the segmented objects by a factor of s ∈ [0.8, 1.2], and change the background by a patch extracted from a randomly picked image from the ImageNet dataset <ref type="bibr" target="#b17">[18]</ref>. Moreover, the object is shifted by some pixels from the center of the image window in both x and y directions. This helps us to handle small object localization errors made during detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present and discuss the results of our evaluation. We first describe the three evaluation metrics used in the literature and in this paper. We evaluate our method on all the possible datasets with color images for instance 3D detection and pose estimation we are aware of: the LINEMOD <ref type="bibr" target="#b6">[7]</ref>, Occlusion <ref type="bibr" target="#b0">[1]</ref>, and T-LESS <ref type="bibr" target="#b9">[10]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Metrics</head><p>As in <ref type="bibr" target="#b1">[2]</ref>, we use the percentage of correctly predicted poses for each sequence and each object, where a pose is considered correct if it passes the tests presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Projections [2]</head><p>This is a metric suited for applications such as augmented reality. A pose is considered correct if the average of the 2D distances between the projections of the object's vertices from the estimated pose and the ground truth pose is less than 5 pixels.</p><p>6D Pose <ref type="bibr" target="#b7">[8]</ref> With this metric, a pose is considered correct if the average of the 3D distances between the transformed of the object's vertices  <ref type="table">Table 1</ref>. Evaluation using the 2D Projections metric of using the 2D projections of the bounding box ('BB'), compared to the direct prediction of the pose ('Direct'), and of the refinement methods. For this evaluation, we used the ground truth 2D object center to avoid the influence of the detection. For the objects marked with a (*), we optimize the value of the weight balancing the rotation and translation terms on the test set, giving an advantage to the 'Direct' pose method. For the other objects, we used the value that is optimal for both the Ape and the Driller.</p><formula xml:id="formula_8">1 |V| M∈V Trê ,t (M) − Trē ,t (M) 2<label>(7)</label></formula><p>is less than 10% of the object's diameter. V is the set of the object's vertices, (ê,t) the estimated pose and (ē,t) the ground truth pose, and Tr e,t (·) a rigid transformation by rotation e, translation t. For the objects with ambigious poses due to symmetries, <ref type="bibr" target="#b7">[8]</ref> replaces this measure by:</p><formula xml:id="formula_9">1 |V| M1∈V min M2∈V Trê ,t (M 1 ) − Trē ,t (M 2 ) 2 . (8)</formula><p>5cm 5 • Metric <ref type="bibr" target="#b18">[19]</ref> With this metric, a pose is considered correct if the translation and rotation errors are below 5cm and 5 • respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Contributions of the Different Steps</head><p>The columns 'BB', 'Mask Ref.', and 'RGB Ref.' of Table 1 compare the results of our method before and after two iterations of refinement, using either a binary mask or a color rendering. For this evaluation, we used the ground truth 2D object center to avoid the influence of the detection. Using refinement improves the results on average by 4.5% and 6.3% for the mask and color rendering respectively. Using a color rendering systematically yields the best results, but using the binary mask yields already a significant improvement, showing that an untextured model can be used.  <ref type="bibr" target="#b1">[2]</ref> and our method without and with RGB Refinement using our segmentation-based method to obtain the 2D object centers on the LINEMOD dataset. <ref type="bibr" target="#b1">[2]</ref> does not provide results for the Bowl and the Cup, hence for the sake of comparison the average is taken over the first 13 objects. <ref type="table">Table 2</ref> compares our BB8 method with and without RGB refinement against the one presented in <ref type="bibr" target="#b1">[2]</ref> on the LINEMOD dataset. Because of lack of space, we provide the results without refinement only for the 2D Projection metric, however, the results for the other metrics are comparable. For this evaluation, we used the results of our detection method presented in Section 3.1, not the ground truth 2D object center. Our method outperforms <ref type="bibr" target="#b1">[2]</ref> by a large margin: 15.6% for 2D Projection, 12.6% for 6D Pose and 28.4% for the 5cm 5 • metric. <ref type="figure" target="#fig_5">Fig. 7</ref> shows qualitative results for our method on this dataset. For most of the images, the two bounding boxes, for the ground truth pose and for the pose we estimate, overlap almost perfectly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The LINEMOD Dataset: Comparison with [2]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The Occlusion Dataset: Robustness to Partial Occlusions</head><p>The Occlusion dataset was created by <ref type="bibr" target="#b0">[1]</ref> from the LINEMOD dataset. The partial occlusions make it significantly more difficult, and to the best of our knowledge, the only published results use both color and depth data. <ref type="bibr" target="#b1">[2]</ref> provide results using only color images, but limited to 2D detection, not 3D pose estimation.</p><p>We only use images from the LINEMOD dataset to generate our training images by using the approach explained in Section 3.5, except that we also randomly superimpose objects extracted from the other sequences to the target ob- ject to be robust to occlusions. We do not use any image of the test sequence to avoid having occlusions similar to the ones presented in the test sequence. Although all the poses in the test sets are not visible in the training sequences, we can estimate accurate poses with a 2D Projection error lower than 15px for about 80% of the frames for these seven objects. We do not report the performance of our method for the Eggbox, as more than 70% of close poses are not seen in the training sequence. Some qualitative results are shown in the second row of <ref type="figure" target="#fig_5">Fig. 7</ref>. To the best of our knowledge, we are the first to present results on this dataset using color images only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">The T-LESS Dataset: Handling Objects with an Axis of Symmetry</head><p>The test sequences of the T-LESS dataset are very challenging, with sometimes multiple instances of the same objects and a high amount of clutter and occlusion. We considered only Scenes #1, #2, #4, #5, and #7 in our experiments. It is also difficult to compare against the only published work on T-LESS <ref type="bibr" target="#b9">[10]</ref>, as it provides the 6D pose metric averaged per object or per scene, computed using RGB-D data, while, to the best of our knowledge, we are the first to report results on the T-LESS dataset using RGB images   <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> ...39.7, 45.7, 50.2, 83.7 53.5 <ref type="table">Table 3</ref>. Our quantitative results on T-LESS <ref type="bibr" target="#b9">[10]</ref>. Most of the errors are along the z axis of the camera, as we rely on color images.</p><p>only. Similarly to <ref type="bibr" target="#b9">[10]</ref>, we evaluate the poses with more than 10% of the object surface visible in the ground truth poses. As shown in <ref type="table">Table 3</ref>, the 6D Pose average per scene with our method is 54%. The object 3D orientation and translation along the x and y axes of the camera are typically very well estimated, and most of the error is along the z axis, which should not be surprising for a method using color images only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Computation Times</head><p>Our implementation takes 140 ms for the segmentation, 130 ms for the pose prediction, and 21 ms for each refinement iteration, on an Intel Core i7-5820K 3.30 GHz desktop with a GeForce TITAN X. If there is only one object of interest, we can replace VGG by a specific network with a simpler architecture, the computation times then become 20 ms for the segmentation and 12 ms for the pose prediction, with similar accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Our "holistic" approach, made possible by the remarkable abilities of Deep Networks for regression, allowed us to significantly advance the state-of-the-art on 3D pose estimation from color images, even on challenging objects from the T-LESS dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Object localization using our segmentation approach: (a) The input image is resized to 512 × 384 and split into regions of size 128 × 128. (b) Each region is first segmented into a binary mask of 8 × 8 for each possible object o. (c) Only the largest component is kept if several components are present, the active locations are segmented more finely. (d) The centroid of the final segmentation is used as the 2D object center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>M o i are the 3D coordinates of the corners of the bounding box of object o in the object coordinate system. Proj e,t (M) projects the 3D point M on the image from the pose defined by e and t. m i ((g Θ (W ))[o]) returns the two components of the output of g Θ corresponding to the predicted 2D coordinates of the i-th corner for object o.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Handling Objects with a symmetry of rotation: Object #5 of T-LESS has an angle of symmetry α of 180 • , if we ignore the small screw and electrical contact. If we restrict the range of poses in the training set between 0 • (a) and 180 • (b), pose estimation still fails for test samples with an angle of rotation close to 0 • modulo 180 • (c). Our solution is to restrict the range during training to be between 0 • and 90 • . We use a classifier to detect if the pose in an input image is between 90 • and 180 • . If this is the case (d), we mirror the input image (e), and mirror back the predicted projections for the corners (f).At run-time, the segmentation gives the identity and the 2D locations of the visible object(s) o. The 3D pose can then be estimated for the correspondences between the 3D points M o i and the predicted m i ((g Θ (W ))[o]) using a PnP algorithm. Other 3D points could be used here, however, the corners of the bounding box are a natural choice as they frame the object and are well spread in space 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Refining the pose. Given a first pose estimate, shown by the blue bounding box (a), we generate a binary mask (b) or a color rendering (c) of the object. Given the input image and this mask or rendering, we can predict an update that improves the object pose, shown by the red bounding box (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Performance on the Occlusion dataset<ref type="bibr" target="#b0">[1]</ref>. Top: Percentages of correctly estimated poses as a function of the distance threshold of the 2D projections metric for 'BB8' on the Occlusion dataset. For a 15px threshold, about 80% of the frames are correctly registered, and about 90% for a 20px threshold. Bottom: Two registered frames for the Driller with a 15px and 20px error respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Some qualitative results. First row: LINEMOD dataset; Second row: Occlusion dataset; Third row: T-LESS dataset (for objects of revolution, we represent the pose with a cylinder rather than a box); Last row: Some failure cases. From left to right: An example of a pose rejected by the 2D Projections metric, a failure due to the lack of corresponding poses in the training set, two examples from T-LESS rejected by the 6D pose metric, and one failure due to the fact that some objects are made of several instances of another object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where s 1,o is a 8 × 8 binary segmentation of J for object o, and τ 1 is a threshold used to binarize the network's output. To obtain a binary segmentation for the full input image, we split this image into regions and compute the s 1,o for each region.This gives us one binary segmentation S 1,o for the full input image, and each possible object. This usually results in a single connected component per visible object; if several components are present, we keep only the largest one for each object. If the largest component in a segmentation S 1,o is small, object o is likely not visible. For the remaining object(s), we refine the shape of the largest component by applying a second network to each 16 × 16 image patch P that corresponds to an active location in S 1 :</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Bench Vi. 67.9 80.0 90.1 64.8 91.8</figDesc><table><row><cell>Metric</cell><cell cols="2">2D Projection</cell><cell cols="2">6D Pose</cell><cell cols="2">5cm 5 •</cell></row><row><cell>Sequence</cell><cell>[2]</cell><cell cols="3">w/o w/Ref. [2] w/Ref.</cell><cell cols="2">[2] w/Ref.</cell></row><row><cell>Ape Camera Can Cat Driller Duck Egg Box Glue Hole P. Iron Lamp Phone</cell><cell cols="4">85.2 95.3 96.6 33.2 40.4 58.7 80.9 86.0 38.4 55.7 70.8 84.1 91.2 62.9 64.1 84.2 97.0 98.8 42.7 62.6 73.9 74.1 80.9 61.9 74.4 73.1 81.2 92.2 30.2 44.3 83.1 87.9 91.0 49.9 57.8 74.2 89.0 92.3 31.2 41.2 78.9 90.5 95.3 52.8 67.2 83.6 78.9 84.8 80.0 84.7 64.0 74.4 75.8 67.0 76.5 60.6 77.6 85.3 38.1 54.0</cell><cell>34.4 40.6 30.5 48.4 34.6 54.5 22.0 57.1 23.6 47.3 58.7 49.3 26.8</cell><cell>80.2 81.5 60.0 76.8 79.9 69.6 53.2 81.3 54.0 73.1 61.1 67.5 58.6</cell></row><row><cell>average</cell><cell cols="4">73.7 83.9 89.3 50.2 62.7</cell><cell>40.6</cell><cell>69.0</cell></row><row><cell>Bowl Cup</cell><cell>--</cell><cell>97.0 98.9 93.4 94.8</cell><cell>--</cell><cell>60.0 45.6</cell><cell>--</cell><cell>90.9 58.4</cell></row><row><cell cols="3">Table 2. Comparison between</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, 13, 14, ... 42.0, 61.7, 64.5, 40.7, ... 7: ...</figDesc><table><row><cell>Scene ID: [Obj. IDs]</cell><cell>6D Pose</cell><cell>Average</cell></row><row><cell>1: [2, 30]</cell><cell>50.8, 55.4</cell><cell>53.1</cell></row><row><cell>2: [5, 6]</cell><cell>56.5, 55.6</cell><cell>56.1</cell></row><row><cell>4: [5, 26, 28]</cell><cell>68.7, 53.3, 40.6</cell><cell>54.3</cell></row><row><cell>5: [1, 10, 27]</cell><cell>39.6, 69.9, 50.1</cell><cell>53.2</cell></row><row><cell>7: [1, 3</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The bounding boxes shown in the figures of this paper were obtained by projecting the 3D bounding box given the recovered poses, not directly from the output of g Θ .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment: This work was funded by the Christian Doppler Laboratory for Semantic 3D Computer Vision.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning 6D Object Pose Estimation Using 3D Object Coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Uncertainty-Driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Novel Representation of Parts for Accurate 3D Object Detection and Tracking in Monocular Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crivellaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">6D Object Detection and Next-Best-View Prediction in the Crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model Globally, Match Locally: Efficient and Robust 3D Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aligning 3D Models to RGB-D Images of Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gradient Response Maps for Real-Time Detection of Textureless Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model Based Training, Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Going Further with Point Pair Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haluza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Obdrzalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hashmod: A Hashing Method for Scalable 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Analysis-By-Synthesis for 6D Pose Estimation in RGB-D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Scalable Tree-Based Approach for Joint Object and Pose Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training a Feedback Loop for Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imagenet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Latent-Class Hough Forests for 3D Object Detection and Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Auto-Context and Its Applications to High-Level Vision Tasks and 3D Brain Image Segmentation. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pose Tracking from Natural Features on Mobile Phones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reitmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mulloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Descriptors for Object Recognition and 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
