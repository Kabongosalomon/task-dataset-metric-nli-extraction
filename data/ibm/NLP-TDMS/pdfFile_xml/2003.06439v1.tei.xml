<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mutual Information Maximization for Effective Lip Reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University of Technology</orgName>
								<address>
									<postCode>310014</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mutual Information Maximization for Effective Lip Reading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lip reading has received an increasing research interest in recent years due to the rapid development of deep learning and its widespread potential applications. One key point to obtain good performance for the lip reading task depends heavily on how effective the representation can be to capture the lip movement information and meanwhile to resist the noises resulted from the change of pose, lighting conditions, speaker's appearance, speaking speed and so on. Towards this target, we propose to introduce the mutual information constraints on both the local feature's level and the global sequence's level to enhance the relations of the features with the speech content. On the one hand, we constraint the features generated at each time step to enable them carry a strong relation with the speech content by imposing the local mutual information maximization constraint (LMIM), leading to improvements over the model's ability to discover fine-grained lip movements and the fine-grained differences among words with similar pronunciation, such as "spend" and "spending". On the other hand, we introduce the mutual information maximization constraint on the global sequence's level (GMIM), to make the model be able to pay more attention to discriminate key frames related with the speech content, and less to various noises appeared in the speaking process. By combining these two advantages together, the proposed method is expected to be both discriminative and robust for effective lip reading. To verify this method, we evaluate on two large-scale benchmarks whose videos are all collected from different TV shows, covering a wide range of various speaking conditions. We perform a detailed analysis and comparison on several aspects, including the comparison of the LMIM and GMIM with the baseline, the visualization of the learned representation and so on. The results not only prove the effectiveness of the proposed method but also report new state-of-the-art performance on both the two benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Lip reading has received an increasing research interest in recent years due to the rapid development of deep learning and its widespread potential applications. One key point to obtain good performance for the lip reading task depends heavily on how effective the representation can be to capture the lip movement information and meanwhile to resist the noises resulted from the change of pose, lighting conditions, speaker's appearance, speaking speed and so on. Towards this target, we propose to introduce the mutual information constraints on both the local feature's level and the global sequence's level to enhance the relations of the features with the speech content. On the one hand, we constraint the features generated at each time step to enable them carry a strong relation with the speech content by imposing the local mutual information maximization constraint (LMIM), leading to improvements over the model's ability to discover fine-grained lip movements and the fine-grained differences among words with similar pronunciation, such as "spend" and "spending". On the other hand, we introduce the mutual information maximization constraint on the global sequence's level (GMIM), to make the model be able to pay more attention to discriminate key frames related with the speech content, and less to various noises appeared in the speaking process. By combining these two advantages together, the proposed method is expected to be both discriminative and robust for effective lip reading. To verify this method, we evaluate on two large-scale benchmarks whose videos are all collected from different TV shows, covering a wide range of various speaking conditions. We perform a detailed analysis and comparison on several aspects, including the comparison of the LMIM and GMIM with the baseline, the visualization of the learned representation and so on. The results not only prove the effectiveness of the proposed method but also report new state-of-the-art performance on both the two benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Lip reading is a task to infer the speech content in a video by using only the visual information, especially the lip movements. It has many crucial applications in practice, such as assisting audio-based speech recognition <ref type="bibr" target="#b3">[4]</ref>, biometric authentication <ref type="bibr" target="#b1">[2]</ref>, aiding hearing-impaired people <ref type="bibr" target="#b23">[24]</ref>, and so on. With the huge success of deep learning based models for several related tasks in the computer vision domain, some works began to introduce the powerful deep models for effective lip reading in these years <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b15">[16]</ref>. For example, <ref type="bibr" target="#b19">[20]</ref> proposed an end-to-end deep learning architecture for word level visual speech recognition, which is a combination of a convolutional network with a bidirectional Long Short-Term Memory network, yielding an improvement of 6.8% on the accuracy than before. Besides the great impetus of deep learning technologies, several large-scale lip reading datasets, were released in recent years, such as LRW <ref type="bibr" target="#b5">[6]</ref>, LRW-1000 <ref type="bibr" target="#b24">[25]</ref>, LRS2 <ref type="bibr" target="#b4">[5]</ref>, LRS3 <ref type="bibr" target="#b0">[1]</ref>, and so on. These datasets have also contributed significantly to the recent progress of lip reading. In this paper, we focus on the word-level lip reading, which is a basic but important branch in the lip reading domain. For this task, each input video is annotated with a single word label even when there are other words in the same video, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. For example, the video sample in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, including 29 frames in total, is annotated as "ABOUT", but the actual frames of the word "ABOUT" include only frames at time step T = 12∼19, shown in the red boxes. The frames before and after this interval are corresponding to the word "JUST" and "TEN" respectively, not "ABOUT". This is consistent with the actual case where the exact boundary of a single word is always hard to get. This property requires a good lip reading model to be able to learn the latent but consistent patterns reflected in different videos with the same word label, and so able to pay more attention to valid key frames, but less to other unrelated frames. Besides the challenges of inaccurate word boundaries, the video samples corresponding to the same word label always have greatly diversified appearance changes, as shown in <ref type="figure" target="#fig_0">Fig. 1(b</ref>). All these properties require the lip reading model to be able to resist the noises in the sequence to capture the consistent latent patterns in various speech conditions.</p><p>In the meanwhile, due to the limited effective area of lip movements, different words probably show similar appearance in the speaking process. Especially, the existence of homophones where different words may look the same or quite similar increases many extra difficulties to this task. These properties require the model being able to discover the fine-grained differences related to different words in the frame-level to distinguish each word from the other.</p><p>To solve the above issues, we introduce the mutual information maximization (MIM) on different levels to help the model learn both robust and discriminative representations for effective lip reading. On the one hand, the representations at the global sequence level would be required to have a maximized mutual information with the speech content, to force the model learning the latent consistent global patterns of the same word label in different samples, while being robust to the variations of pose, light and other labelunrelated conditions. On the other hand, the features at the local frame level would be required to maximize their mutual information with the speech content to enhance the word-related fine-grained movements at each time step to further enhance the differences between different words. By combining these two types of constraints together, the model could automatically find and distinguish the valid important frames for the target word, and ignore other unrelated frames. Finally, we evaluate the proposed approach on two largescale benchmarks LRW and LRW-1000, whose samples are all collected from various TV shows with a wide variation of the speaking conditions. The results show a new state-of-theart performance on both the two challenging datasets when compared with other related work in the same condition of using no extra data or extra pre-trained models.</p><p>The proposed method could also be easily modified to other existing models for other tasks, which may bring some meaningful insights to the community for other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we provide an overview of the related literature on two closely related aspects, lip reading and mutual information based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Lip Reading</head><p>When deep learning technologies are not so popular, many methods have achieved several encouraging results by using specifically-designed and hand-engineered features, such as optical flow <ref type="bibr" target="#b17">[18]</ref>, lip landmarks tracking <ref type="bibr" target="#b7">[8]</ref>, and so on. The classification is often done by Support Vector Machine <ref type="bibr" target="#b17">[18]</ref> together with the Hidden Markov Models (HMMs) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>. We refer to <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b16">[17]</ref> for a detailed review on these non-deep methods for lip reading. These previous work have provided an important impetus to the advancement of lip reading at the early stage.</p><p>With the rapid development of deep learning in recent years, more and more researchers gradually tend to perform the lip reading task by deep neural networks.</p><p>2D-CNN is the first type of network applied to lip reading to extract features for each frame. <ref type="bibr" target="#b12">[13]</ref> proposed a system including a CNN and a hidden Markov model with Gaussian mixture observation model (GMM-HMM). The outputs of the CNN are regarded as visual feature sequences, and the GMM-HMM is applied on this sequence for word classification. In the later works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b4">[5]</ref>, long short-term memory (LSTM) or gated recurrent unit (GRU) is used to model the patterns in the temporal dimension. The CNN-LSTM based models, which can be trained in an end-to-end manner, has gradually become a processing pipeline for lip reading.</p><p>However, the mouth regions in different frames are not always aligned at exactly the same position. So the context shown in nearby frames always plays an important role for effective lip reading. Several methods introduce the 3D convolution operation to tackle this problem <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>. For example, LipNet [2] employed a 3D-CNN at the front-end on the visual frames and obtained remarkable performance for lip reading. Stafylakis et al. <ref type="bibr" target="#b19">[20]</ref> combined a 3D-CNN and a 2D-CNN based network to obtain robust features, which got a much higher accuracy on LRW dataset than before.</p><p>Besides directly applying different types of deep networks to lip reading, some recent impressive works begun to design particular modules to solve the shortcomings of some existing networks for more effective lip reading. For example, Stafylakis et al. <ref type="bibr" target="#b18">[19]</ref> introduced additional word boundary information to improve the performance on the word-level LRW dataset. <ref type="bibr" target="#b4">[5]</ref> employed the attention mechanism to select key frames in a sequence-to-sequence model. Wand et al. <ref type="bibr" target="#b21">[22]</ref> improved the accuracy of lip reading by domain-adversarial training, which is expected to get speaker-independent features, beneficial to the final word classification. However, their method is hard to apply when coming to a large scale dataset with large number of speakers. Recently, Wang <ref type="bibr" target="#b22">[23]</ref> extracted both frame-level fine-grained features and short-term medium-grained features by a 2D-CNN network and 3D-CNN network respectively. In this paper, we propose a new way for effective lip reading. Specifically, we introduce the constraints on both the local feature level and the global representation level to make the model both be able to learn fine-grained features and pay attention to key frames respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mutual Information Mechanism</head><p>Mutual information (MI) is a fundamental quantity for measuring the relationship between two random variables. It is always used to evaluate the "amount of information" owned by one random variable when given the other random variable. Based on this property, the mutual information of two random variables is always used as a measure of the mutual dependence between two variables. Moreover, unlike the Pearson correlation coefficient which only captures the information in the degree of linear relationship, mutual information also captures nonlinear statistical dependencies <ref type="bibr" target="#b9">[10]</ref>, and therefore has a wide range of applications.</p><p>For example, Ranjay et al. <ref type="bibr" target="#b10">[11]</ref> solve the visual question answer problem by maximizing the MI between the image, the expected answer and the generated question, leading to the models ability to select corresponding powerful features. Li et al. <ref type="bibr" target="#b11">[12]</ref> tried to maximize the MI between the source and target sentences in the neural machine translation task to improve the diversity of translation results.</p><p>One work which has a bit relation with our work is Zhu et al. <ref type="bibr" target="#b26">[27]</ref>, who performed talking face generation by maximizing the MI between the words distribution and the facial/audio distribution. But in our work, we try to maximize the MI between the words distribution and the representation at different levels, to guide the model towards learning both robust and discriminative features for the lip reading task, which is totally different with <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED MUTUAL INFORMATION MAXIMIZATION FOR LIP READING</head><p>In this section, we would first give an overview to the overall architecture. Then the particular manner to impose mutual information mechanisms on different levels is presented. Finally, the optimization process to learn the model is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Overall Architecture</head><p>Let X = (x 1 , x 2 , ..., x T ) denotes the input sequence with T frames in total, where x i is the feature vector of the i-th frame. The task of the model is to classify the input sequence X into one of the C classes, where C is the number of classes. Let Y = (0, 0, 1, ..., 0) denotes the annotated word label of the sequence, where Y is a C−dimensional one-hot vector with only a single 1 at the position corresponding to its word label index. We construct our base architecture with two principal components, named as front-end and back-end respectively, which enable the total network to be trained end-to-end.</p><p>The Front-end includes a 3D-CNN layer, a spatial pooling layer, a ResNet18 network, and a GAP layer, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Specifically, given the input image sequence X, a 3D-CNN layer is firstly applied on the raw frames, in order to perform an initial spatial temporal alignment in the sequence for effective recognition. A spatial max-pooling layer is then followed to compact the features in the spatial domain. It should be noted that we keep the temporal dimension unchanged in this procedure to avoid a further shortage of the movement information in the sequence because the duration of each word is always very short. In the next step, we divide the features into T parts and employ a ResNet18 module at each time step t = 1, 2, ..., T to separately extract discriminative features. To improve the ability to capture fine-grained movements related to the spoken word, we impose the mutual information constraint on the pairs of outputs of ResNet18 and the annotated label. Having been maximized the relations with the annotated label, all these features obtained from the ResNet18 module would be fed into a global average pooling(GAP) layer to compress into T × D-dimensional outputs, where D is the channel of the last layer and 512 in this paper.</p><p>With the initial representation from the Front-end, the Back-end, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, include a 3-layer Bi-GRU network and a linear layer, to capture and classify the latent patterns of the sequence. A Bi-GRU contains two independent single directional GRUs. The input sequence is fed into one GRU in the normal order, and into another GRU in the reverse order. The outputs of the two GRUs would be concatenated together at each time step to represent the whole sequence. The output of the Bi-GRU is expected to be a global representation of the whole input sequence with dimension T × 2N, where N is the number of hidden neurons in each GRU. The representation will be finally sent to a linear layer for classification. To improve its ability to resist noises and select key frames in the sequence, we impose the second mutual information constraint on this global representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Local Mutual Information Maximization (LMIM)</head><p>As stated in the previous section, the performance of lip reading is heavily affected by the models ability to capture the local fine-grained lip movements, so as to generate discriminative features to distinguish different words from each other. The MI-based constraint is a promising tool for learning good features in an unsupervised way, because we never need any extra data to train it. As stated above, we would introduce Local Mutual Information Maximization (LMIM) on ResNet18 to help the model focus more on related spatial regions at each time step and produce more discriminative features. For lip reading, the local features nearby the mouth regions are significant for the final accurate recognition. Therefore, unlike most existing work <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b26">[27]</ref>, we perform maximization of the MI on each patch of the feature maps rather than the whole feature maps. Because mutual information is notoriously hard to compute for unknown distribution, we estimate it with the help of deep network here. Following the representation of Jensen-Shannon(JS) MI estimator <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_0">I (JSD) θ (A, B) = E p(A,B) [−ϕ (−T θ (a, b))] − E p(A)p(B) [ϕ (T θ (a, b))] ,<label>(1)</label></formula><p>where ϕ(k) = log(1 + e k ), A and B are the two variables that we want to estimate the MI between them, T θ is a continuous function that we directly use a network to approximate it. The p <ref type="figure">(A, B)</ref> is the joint distribution of paired samples {a, b}, and the p(A)p(B) is the marginal distribution of the unpaired samples {a, b} by randomly sampling A and B.</p><p>In the optimization process, because ϕ(k) = log(1 + e k ) is a monotone increasing function, so maximizing the JS MI estimator is equivalent to optimize (1) with ϕ(k) = log(1+k) when the formula is equal to the binary cross-entropy loss. With the estimation above, the process of the LMIM is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We assume the feature map in the last layer of ResNet18 (which will be sent to the GAP layer) as F with a shape of H × W × D, where H,W and D are the height, width and the channels respectively. Then we divide the feature F into H × W local patches ( f 1 , f 2 , ..., f H×W ) which looks like we separate the original frame to H × W patches when the receptive field of the features are mapped to the original frame. The label of each sample is expanded by repetition from one-hot vector of dimension C × 1 to the same height and width as C × H ×W . Then we concatenate the labels and features together to obtain a representation of dimension (C +D)×H ×W , which would be used as the input to estimate the Local Mutual Information Maximization network (LMIM). To obtain the local mutual information at each position of the H × W locations, we employ two convolutional layers with kernel size 1×1 on the concatenated representation. Then a sigmoid activation is applied to the last layer to simulate the value of the mutual information. Please note that the architecture of the network in this step can be any other form, because it is just applied to approximate a continuous function T θ . But the output layer should always be based on a sigmoid activation function to employ the binary cross-entropy based estimation. The dimension of the outputs of LMIM is H ×W , with each number illustrating the degree of how much the corresponding patch is related with the given word label. In the learning process, we expect the mutual information of every patch close to 1 (Real) if the features and the labels are of the same sample (paired samples), and 0 if the label is different with the annotated label of the input sequence (unpaired sample). To collect unpaired samples, we randomly concatenated the features with other labels in the same batch in the implementation process.</p><p>Therefore, the optimization for LMIM can be denoted as a binary cross-entropy loss as:</p><formula xml:id="formula_1">L (LMIM) = E p(F,Y ) [log (LMIM ( f , y))] + E p(F)p(Y ) [log (1 − LMIM ( f , y))] .<label>(2)</label></formula><p>Noting that in this stage, we have not any special process in the temporal dimension. The features of T time steps in an input video will be sent to LMIM successively. In the end, the mean of the loss at all time steps is computed to obtain the gradients for subsequent update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Global Mutual Information Maximization (GMIM)</head><p>In each sequence, the amount of valuable information provided by different frames is not equal for robust lip reading. In several practical cases, there are many frames corresponding with other words than the given target word </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1×1 1×1</head><p>Label-1 Label-2 <ref type="figure">Fig. 4</ref>. The process of training the network with the proposed GMIM, noted that when we apply the GMIM, a single layer LSTM and a linear layer are also added to the Back-end for computing the weight of each frame, it will be retained after training while the GMIM will be dropped.</p><p>in a given sequence. One popular way in current related methods is to average over all the time steps to get the final representation, which would suffer superior performance when coming to practice. In this paper, we introduce global mutual information maximization on the global representation obtained by the Bi-GRU. Specifically, we introduce an additional LSTM together with a linear layer over the outputs of the Frontend. This additional LSTM would assign different weights β (T × 1-dimensional) for different frames according to the target word. The total architecture is shown in <ref type="figure">Fig. 4</ref>.</p><p>Based on the outputs Z with dimension T × 2N of the 3-layer Bi-GRU layers and the weighted value β , the final global representation is obtained as the weighted average of the outputs Z as:</p><formula xml:id="formula_2">O = ∑ T t=1 β t · Z t T .<label>(3)</label></formula><p>The output O of dimension 2N is then sent to a linear layer to transform its shape from 2N to C, where C is the number of classes. Specifically, the final representation of the whole sequence O of dimension C is applied to get the classification score aŝ</p><formula xml:id="formula_3">Y i = exp (O i ) ∑ C j=1 exp (O j ) .<label>(4)</label></formula><p>For related valuable key-frames, the weight β should be positive and can be of any value in our method. While for unrelated frames, we just want its weight close to zero, not a negative number for the optimization problem. Therefore we use ReLU to obtain the weight β as</p><formula xml:id="formula_4">β t = ReLU (W linear × LSTM(G) t + b linear ) ,<label>(5)</label></formula><p>where G is the outputs of the GAP layer, W linear and b linear are the parameters of the linear layer and LSTM(G) t denotes the hidden state at time step t of the extra LSTM layer.</p><p>To guide the learning of the weights, we constrain the weighted average vector to contain most of the information about the target word. Specifically, we maximize the MI between the above weighted average representation O and the annotated label Y, both of which will be fed into the global mutual information maximization module (GMIM), which consists of two linear layers and outputs a scalar after a sigmoid activation. Similarly to LMIM, If O and Y come from paired samples, we expect the outputs of GMIM as large as possible and even close to 1 (Real). In other cases, the output is expected to be close to 0 (Fake). So the objective function can be written as:</p><formula xml:id="formula_5">L (GMIM) = E p(O,Y ) [log (GMIM (o, y))] + E p(O)p(Y ) [log (1 − GMIM (o, y))] .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>Combining the cross-entropy loss with the LMIM and GMIM optimization function, the final objective loss function for the whole model is:</p><formula xml:id="formula_6">L total = − C ∑ i=1 Y i logŶ i − L (LMIM) − L (GMIM) ,<label>(7)</label></formula><p>where the first term is the cross-entropy loss and Y i is the label. Because the three items in the above equation have the similar numbers in our experiments, we did not allocate different weights to each loss item in our implementation.</p><p>IV. EXPERIMENTS In this section, we first evaluate the performance of our base architecture (baseline) which can be trained easier than previous methods. Then we conduct a thorough ablation study to the proposed LMIM and GMIM (GLMIM) and figure out how they help the model get better results respectively. we also compare with other state-of-the-art lip reading methods on two large word-level benchmarkss. Finally, we visualize the discriminative representations leaned with the GLMIM. Codes will be available at https://github. com/xing96/MIM-lipreading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluate our method on two large-scale word-level lip reading benchmarks, LRW and LRW-1000. The samples in both of these two datasets are collected from TV shows, with a wide coverage of the speaking conditions including the lighting conditions, resolution, pose, gender, make-up etc.</p><p>LRW <ref type="bibr" target="#b5">[6]</ref>: It is released in 2016, including 500 word classes with more than a thousand speakers. It displays substantial diversities in the speaking conditions. The number of instances in the training set reaches 488766, and the number in validation and test set contains 25000 instances for each. LRW remains a challenging dataset and has been widely used by most existing lip reading methods.</p><p>LRW-1000 <ref type="bibr" target="#b24">[25]</ref>: The dataset is a large-scale naturally distributed word-level benchmark, which has 1000 word classes in total. There are more than 70,000 sample instances in total, with a duration of about 57 hours. This dataset aims at covering a natural variability over different speech modes and imaging conditions to incorporate challenges encountered in practical applications. So the samples of the same word are not limited to a pre-specified length range, to allow the existence of various speech rates, which is consistent with the practical case and also brings more challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The input frames in our implementation are all cropped or resized to 88 ×88 (Each video in LRW contains full face and the resolution is larger than 88 × 88, we cropped the mouth region by 88 × 88 directly; LRW1000 only contains the mouth region but the resolution is not fixed, we resized them to 88 × 88). The kernel size, stride and padding of the first 3D-CNN are <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b6">7)</ref>, (1, 2, 2) and (2, 3, 3) respectively. Each GRU or LSTM layer has 1024 hidden units (which means each Bi-GRU contains 2048 neurons). The Adam optimizer is applied for fast convergence. In the training process, the learning rate would decay from 0.0001 to 0.00001 when the accuracy doesn't increase. Dropout is utilized at each Bi-GRU layer to mitigate the overfitting problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy Petridis <ref type="bibr" target="#b15">[16]</ref> 82.00% Petridis <ref type="bibr" target="#b15">[16]</ref>(our re-implement) 81.70% The Modified Baseline Architecture 82.14% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline</head><p>We adopt <ref type="bibr" target="#b15">[16]</ref> as the base architecture. The accuracy of our re-implementation on LRW is a little lower than the value in the original paper. So we use the modified network as described in III-A and take it as our baseline when using no MI constraint. Unlike <ref type="bibr" target="#b15">[16]</ref>, we introduce the GAP layer to the modified network in order to get rid of training the front-end and the back-end separately. As shown in <ref type="table" target="#tab_1">Table I</ref>, our modified architecture is superior to the base architecture, which achieves an accuracy of 82.14% on the LRW dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effect of the LMIM</head><p>In order to evaluate the effectiveness of the proposed LMIM, we train the baseline network with and without the LMIM separately. In both the two cases, the LMIM will be dropped when coming to test, which means that these two networks are totally the same in the test process. When we compare the accuracy between these two networks, we find that the network trained with the LMIM performs better. Besides the total accuracy, we conduct a further statistics analysis of the accuracy over each class. As shown in <ref type="table" target="#tab_1">Table II</ref>, most classes with the LMIM show a higher accuracy and a clear improvement over the words with similar spellings or pronunciations, such as MAKES/MAKING and POLITICAL/POLITICIANS. This result shows that the proposed LMIM enable to extract the local fine-grained features indeed, which is significant to improve the ability to distinguish the words with similar pronunciations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effect of the GMIM</head><p>The ability to Select key frames is essential for lip reading because a video is always hard to cut to exactly containing only one word. This is why we apply GMIM to make the model pay different attention to all frames to select valid key frames. We directly based the experiments in this part on the model trained with LMIM in IV-D because of its excellent ability to extract fine-grained features. For the sake of fairness, the Front-end is fixed and only the Backend is trained with GMIM. Without sending any additional word boundary information, we observed that the model has learned the key frames precisely and the accuracy has  <ref type="bibr" target="#b6">[7]</ref> 71.50% Chung[2017] <ref type="bibr" target="#b4">[5]</ref> 76.20% Petridis[2018] <ref type="bibr" target="#b15">[16]</ref> 82.00% <ref type="bibr" target="#b19">Stafylakis[2017]</ref> <ref type="bibr" target="#b19">[20]</ref> 83.00% Wang[2019] <ref type="bibr" target="#b22">[23]</ref> 83.34% Baseline 82.14% Baseline+LMIM 83.33% The Proposed GLMIM 84.41% The result of the weights β learned with the proposed GMIM, is shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. The horizontal axis represents the temporal dimension of the video, corresponding to 29 frames in the video. The vertical axis represents the numeric of the learned weights. The blue line shows the curve composed by the learned weights for each frame. The red dashed line with value 1 denotes the range divided by the annotated word boundary for the target word. Our model trained with GMIM not only learns the key frames successfully and pays more attention to the frames which are included in the word boundary, but also allocates small amount of weights to the frames close to the word boundary for capturing the context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Compare with state-of-the-art methods</head><p>In this part, we compare the proposed GLMIM with the current state-of-the-art methods on both the two challenging benchmarks, LRW and LRW-1000. On the LRW dataset, although our baseline is not the best, the accuracy is improved for about 1.21% after introducing the LMIM, which is expected to capture more discriminative and fine-grained features for the main task. Meanwhile, the GMIM improves the accuracy to 84.41% furthermore, mainly beneficial from its advantage to pay different attention to different frames. Comparing with other lip reading methods which also have no extra inputs except the visual information, as shown in <ref type="table" target="#tab_1">Table III</ref>, we get the best result and provide a new state-ofthe-art result on the LRW dataset.</p><p>LRW1000 is another challenging large-scale benchmark, with a large variation of speech conditions including lighting conditions, resolution, speakers age, pose, gender, and makeup, etc. The best result is only 38.19% up to now. It is challenging to obtain a good performance on this dataset while we achieve a high accuracy of 38.79% which outperforms the existing state-of-the-art results. <ref type="table" target="#tab_1">Table IV</ref> gives the accuracy of our models. The improvement of the GMIM is smaller when comparing with the improvement on LRW, this interesting phenomenon may be due to that the number of useless frames in each word sample in LRW-1000 is smaller than LRW, which reduces the role of selecting key frames for each word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Visualization</head><p>In this section, we explore further the effect of the proposed GLMIM by visualization. Specifically, we randomly choose 6 classes and each of them contains 20 samples. We send them to thee model of our original baseline architecture with and without the proposed GLMIM respectively. Then we extract the final representations O which will be sent to the linear layer for classification. We apply PCA to reduce its dimension form higher dimensions to 2 dimensions for better visualization. As is shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, the variance among these classes before applying GLMIM ranges only from −20 to 20; While the variance has been enlarged to the interval between −40 and 60 after applying GLMIM, which means the variance among the classes have been greatly increased due to the introduction of the proposed GLMIM, which makes it easier to distinguish different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a mutual information maximization based method for both the local fine-grained feature extraction and global key frames selection. We also modify the existing model for lip reading that make it can be trained easier. We performed a detailed ablation study and obtain the best results on both the two largest word-level lip reading datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENTS</head><p>This work is partially supported by National Key R&amp;D Program of China (No. 2017YFA0700804) and National Natural Science Foundation of China (No. 61702486, 61876171). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An example of a video sample with the annotated label "ABOUT" … … (b) Another two samples of "ABOUT" The word-level lip reading is a challenge task. (a) The actual frames of the annotated word "ABOUT" include only frames at the time step T = 12∼19. (b) The same word label always have a greatly diversified appearances changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The base architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The process of training the base network with the proposed LMIM. The total loss is computed by averaging over all the time steps and patches. The gradients from the LMIM will be back-propagated to the Front-end through the features sampled from the ResNet18. The LMIM will be dropped after training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>We randomly sample three words and show the weights of each frame learned with GMIM. The blue line shows the learned weight for each frame. The red dashed line denotes the word boundary for the target word when its value is 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>(a) Results before applying the GLMIM (b) Results after applying the GLMIM An example of the visualization for the final representations form the Bi-GRU. With the help of the GLMIM, the architecture gets more discriminative results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF THE MODIFIED BASELINE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="4">EXAMPLES OF THE IMPROVEMENT OVER WORDS WITH SIMILAR</cell></row><row><cell></cell><cell></cell><cell>PRONUNCIATIONS.</cell><cell></cell></row><row><cell>Class</cell><cell cols="3">Baseline Baseline with LMIM Improvement</cell></row><row><cell>MAKES</cell><cell>62%</cell><cell>74%</cell><cell>12%</cell></row><row><cell>MAKING</cell><cell>80%</cell><cell>92%</cell><cell>12%</cell></row><row><cell>POLITICAL</cell><cell>82%</cell><cell>90%</cell><cell>8%</cell></row><row><cell>POLITICS</cell><cell>84%</cell><cell>92%</cell><cell>8%</cell></row><row><cell>STAND</cell><cell>48%</cell><cell>60%</cell><cell>12%</cell></row><row><cell>STAGE</cell><cell>70%</cell><cell>80%</cell><cell>10%</cell></row><row><cell>NORTH</cell><cell>78%</cell><cell>90%</cell><cell>12%</cell></row><row><cell>NOTHING</cell><cell>78%</cell><cell>86%</cell><cell>8%</cell></row><row><cell>SPEND</cell><cell>36%</cell><cell>46%</cell><cell>10%</cell></row><row><cell>SPENDING</cell><cell>78%</cell><cell>82%</cell><cell>4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III COMAPRISON</head><label>III</label><figDesc>WITH OTHER RELATED WORK ON LRW.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Chung[2018]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMAPRISON</head><label>IV</label><figDesc>WITH OTHER RELATED WORK ON LRW1000.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>LSTM-5</cell><cell>25.76%</cell></row><row><cell>D3D[2018][25]</cell><cell>34.76%</cell></row><row><cell>3D+2D</cell><cell>38.19%</cell></row><row><cell>Wang[2019][23]</cell><cell>36.91%</cell></row><row><cell>Baseline</cell><cell>38.35%</cell></row><row><cell>Baseline+LMIM</cell><cell>38.69%</cell></row><row><cell>The Proposed GLMIM</cell><cell>38.79%</cell></row><row><cell cols="2">increased further. When the Front-end is trained together</cell></row><row><cell cols="2">with the Back-end, we get a new state-of-the-art result.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Lrs3-ted: a largescale dataset for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00496</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<title level="m">Lipnet: End-to-end sentence-level lipreading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The natural statistics of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stillittano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ghazanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1000436</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio-visual integration in multimodal communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="837" to="852" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to lip read words by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="76" to="85" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward movement-invariant automatic lip-reading and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duchnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Busching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1995 International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="109" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Equitability, mutual information, and the maximal information coefficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3354" to="3359" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Information maximizing visual question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mutual information and diverse decoding improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00372</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lipreading using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pitsikalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="435" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6548" to="6552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Audio-visual automatic speech recognition: An overview. Issues in visual and audiovisual speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lip reading using optical flow and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Azemin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gubbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 3rd International Congress on Image and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="327" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of audiovisual word recognition using residual networks and lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="22" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04105</idno>
		<title level="m">Combining residual networks with lstms for lipreading</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lipreading with long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6115" to="6119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving speaker-independent lipreading with domain-adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01565</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multi-grained spatio-temporal modeling for lip-reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11618</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lcanet: End-to-end lipreading with cascaded attention-ctc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cassimatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="548" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06990</idno>
		<title level="m">Lrw-1000: A naturally-distributed largescale benchmark for lip reading in the wild</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="590" to="605" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">High-resolution talking face generation via mutual information approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06589</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
