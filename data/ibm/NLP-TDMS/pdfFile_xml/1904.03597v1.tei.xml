<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Spatio-temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Linchao Bao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Spatio-temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of video representation learning without human-annotated labels. While previous efforts address the problem by designing novel self-supervised tasks using video data, the learned features are merely on a frame-by-frame basis, which are not applicable to many video analytic tasks where spatio-temporal features are prevailing. In this paper we propose a novel self-supervised approach to learn spatio-temporal features for video representation. Inspired by the success of two-stream approaches in video classification, we propose to learn visual features by regressing both motion and appearance statistics along spatial and temporal dimensions, given only the input video data. Specifically, we extract statistical concepts (fastmotion region and the corresponding dominant direction, spatio-temporal color diversity, dominant color, etc.) from simple patterns in both spatial and temporal domains. Unlike prior puzzles that are even hard for humans to solve, the proposed approach is consistent with human inherent visual habits and therefore easy to answer. We conduct extensive experiments with C3D to validate the effectiveness of our proposed approach. The experiments show that our approach can significantly improve the performance of C3D when applied to video classification tasks. Code is available at https://github.com/laura-wang/video repres mas.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning powerful spatio-temporal representations is the most fundamental deep learning problem for many video understanding tasks such as action recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>, action proposal and localization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, video captioning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42]</ref>, etc. Great progresses have been made by training expressive networks with massive human-annotated video data <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. However, annotating video data is very laborious and expensive, which makes the learning from un- † Work done during an internship at Tencent AI Lab. * Corresponding authors. (5, ) (2, Blue) (4, Green) <ref type="figure">Figure 1</ref>. The main idea of the proposed approach. Given a video sequence, we design a novel task to predict several numerical labels derived from motion and appearance statistics for spatio-temporal representation learning, in a self-supervised manner. Each video frame is first divided into several spatial regions using different partitioning patterns like the grid shown above. Then the derived statistical labels, such as the region with the largest motion and its direction (the red patch), the most diverged region in appearance and its dominant color (the yellow patch), and the most stable region in appearance and its dominant color (the blue patch), are employed as supervision during the learning.</p><p>labeled video data important and interesting.</p><p>Recently, several approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12]</ref> have emerged to learn transferable representations for video recognition tasks with unlabeled video data. In these approaches, a CNN is first pre-trained on unlabeled video data using novel self-supervised tasks, where supervision signals can be easily derived from input data without human labors, such as solving puzzles with perturbed video frame orders <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref> or predicting flow fields or disparity maps obtained with other computational approaches <ref type="bibr" target="#b11">[12]</ref>. Then the learned representations can be directly applied to other video tasks as features, or be employed as initialization during succeeding supervised learning. Unfortunately, although these work demonstrated the effectiveness of selfsupervised representation learning with unlabeled videos, their approaches are only applicable to a CNN that accepts one or two frames as inputs, which is not a recommended way for tackling video tasks. In most video understanding tasks, spatio-temporal features that can capture information of both appearances and motions are proved to be vital in many recent studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>In order to extract spatio-temporal features, a network ar-chitecture that can accept multiple frames as inputs and perform operations along both spatial and temporal dimensions is needed. For example, the popular C3D network <ref type="bibr" target="#b36">[37]</ref>, which accepts 16 frames as inputs and employs 3D convolutions along both spatial and temporal dimensions to extract features, is becoming more and more popular for many video tasks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref>. Vondrick et al. <ref type="bibr" target="#b38">[39]</ref> proposed to address the representation learning by C3D-based networks, while motion and appearance are not explicitly incorporated thus the performance is not satisfactory when transferring the learned features to other video tasks.</p><p>In this paper, we propose a novel self-supervised learning approach to learn spatio-temporal video representations by predicting motion and appearance statistics in unlabeled videos. The idea is inspired by Giese and Poggio's work on human visual system <ref type="bibr" target="#b13">[14]</ref>, in which the representation of motion is found to be based on a set of learned patterns. These patterns are encoded as sequences of snapshots of body shapes by neurons in the form pathway, and by sequences of complex optic flow patterns in the motion pathway. In our work, the two pathways are the appearance branch and motion branch respectively. Besides, the abstract statistical concepts are also inspired by the biological hierarchical perception mechanism. The main idea of our approach is shown in <ref type="figure">Figure 1</ref>. We design several spatial partitioning patterns to encode each spatial location and its motion and appearance statistics over multiple frames, and use the encoded vectors as supervision signals to train the spatio-temporal representation network. The novel objectives are simple to learn and informative for the motion and appearance distributions in video, e.g., the spatial locations of the most dominant motions and their directions, the most consistent and the most diverse colors over a certain temporal cube, etc. We conduct extensive experiments with C3D network to validate the effectiveness of the proposed approach. We show that, compared with training from scratch, pre-training C3D without labels using our proposed approach gives a large boost to the performance of the action recognition task (e.g., 45.4% v.s. 61.2% on UCF101). By transferring the learned representations to other video tasks on smaller datasets, we demonstrate significant performance gains on various tasks like dynamic scene recognition, action similarity labeling, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Self-supervised representation learning is proposed to leverage the huge amounts of unlabeled data to learn useful representations for various problems, for example, image classification, object detection, video recognition, etc. It has been proved that lots of deep learning methods can benefit from pre-trained models on large labeled datasets, e.g., ImageNet <ref type="bibr" target="#b6">[7]</ref> for image tasks and Kinetics <ref type="bibr" target="#b18">[19]</ref> or Sports-1M <ref type="bibr" target="#b17">[18]</ref> for video tasks. The basic motivation behind self-supervised representation learning is to replace the expensive labeled data with "free" unlabeled data.</p><p>A common way to achieve self-supervised learning is to derive easy-to-obtain supervision signals without human annotations, to encourage the learning of useful features for regular tasks. Various novel tasks are proposed to learn image representations from unlabeled image data, e.g., re-ordering perturbed image patches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref>, colorizing grayscale images <ref type="bibr" target="#b44">[45]</ref>, inpainting missing regions <ref type="bibr" target="#b31">[32]</ref>, counting virtual primitives <ref type="bibr" target="#b29">[30]</ref>, classifying image rotations <ref type="bibr" target="#b12">[13]</ref>, predicting image labels obtained using a clustering algorithm <ref type="bibr" target="#b2">[3]</ref>, etc. There are also studies that try to learn image representations from unlabeled video data. Wang and Gupta <ref type="bibr" target="#b42">[43]</ref> proposed to derive supervision labels from unlabeled videos using traditional tracking algorithms. Pathak et al. <ref type="bibr" target="#b30">[31]</ref> instead obtained labels from videos using conventional motion segmentation algorithms.</p><p>Recent studies leveraging video data try to learn transferable representations for video tasks. Misra et al. <ref type="bibr" target="#b26">[27]</ref> designed a binary classification task and asked the CNN to predict whether the video input is in right order or not. Fernando et al. <ref type="bibr" target="#b10">[11]</ref> and Lee et al. <ref type="bibr" target="#b23">[24]</ref> also designed tasks based on video frame orders. Gan et al. <ref type="bibr" target="#b11">[12]</ref> proposed a geometry-guided network that force the CNN to predict flow fields or disparity maps between two input frames. Although these work demonstrated the effectiveness of selfsupervised representation learning with unlabeled videos and showed impressive performances when transferring the learned features to video recognition tasks, their approaches are only applicable to a CNN that accepts one or two frames as inputs and cannot be applied to network architectures that are suitable for spatio-temporal representations. The most related work to ours are Vondrick et al. <ref type="bibr" target="#b38">[39]</ref> and Kim et al. <ref type="bibr" target="#b19">[20]</ref>. Vondrick et al. <ref type="bibr" target="#b38">[39]</ref> proposed a GAN model for videos with a spatio-temporal 3D convolutional architecture, which can be employed as a self-supervised approach for video representation learning. Kim et al. <ref type="bibr" target="#b19">[20]</ref> proposed to learn spatio-temporal representations with unlabeled video data, by solving space-time cubic puzzles, which is a straightforward extension of the 2D puzzles <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>We design a novel task for self-supervised video representation learning by predicting the motion and appearance statistics in a video sequence. The task is bio-inspired and consistent with human visual habits <ref type="bibr" target="#b13">[14]</ref> to capture highlevel concepts of videos. In this section, we first illustrate the statistical concepts and motivations to design the task (Sec. 3.1). Next, we formally define the proposed statistical labels (Sec. 3.2 and 3.3). Finally, we present the whole learning framework when applying the self-supervised task to the C3D <ref type="bibr" target="#b36">[37]</ref> network (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Statistical Concepts</head><p>Given a video clip, humans usually first notice the moving proportion of the visual field <ref type="bibr" target="#b13">[14]</ref>. By observing the foreground motion and the background appearance, we can easily tell the motion class based on prior knowledge. Inspired by human visual system, we break the process of understanding videos into several questions and encourage a CNN to answer them accordingly: (1) Where is the largest motion in the video? (2) What is the dominant direction of the largest motion? (3) Where is the largest color diversity and what is its dominant color? (4) Where is the smallest color diversity, i.e., the potential background of a scene and what is its dominant color? The approach to quantify these questions into annotation-free training labels will be described in details in the following sections. Here, we introduce the statistical concepts for motion and appearance. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates an example of a three-frame video clip with two moving objects (blue circle and yellow triangle). A typical video clip normally consists of much more frames. We here instead use the three-frame clip for better understanding of the key ideas. To accurately represent the location and quantify "where", each frame is divided into 4-by-4 blocks and each block is assigned to a number in an ascending order. The blue circle moves from block four to block seven, and the yellow triangle moves from block 12 to block 11. Comparing the moving distance, we can easily tell that the motion of the blue circle is larger than the motion of the yellow triangle. And the largest motion lies in block seven since it contains moving-in motion between frame one and two, and moving-out motion between frame two and three. As for the question "what is the dominant direction of the largest motion?", it can be easily observed from <ref type="figure" target="#fig_1">Figure 2</ref> that the blue circle is moving towards lowerleft. To quantify the directions, the full angle 360 • is divided into eight angle pieces, with each piece covering a 45 • motion direction range. And similar to location quantification, each angle piece is assigned to a number in an ascending order counterclockwise. The corresponding angle piece number of "lower-left" is five.</p><p>For the appearance statistics, the largest spatio-temporal color diversity area is also block seven, as it changes from the background color to the circle color. The dominant color is the same as the moving circle color, i.e., blue. As for the smallest color diversity location, most of the blocks stay the same and the background color is white.</p><p>Keeping the above concepts and motivation in mind, we next present the proposed novel self-supervised approach. We assume that by training a spatio-temporal CNN to predict the motion and appearance statistics mentioned above, better spatio-temporal representations can be learned, by which the video understanding tasks could be benefited consequentely. Specifically, we design a novel regression task to predict a group of numbers related to motion and appearance statistics, such that by correctly predicting them, the following queries could be roughly derived: the largest motion location and the dominant motion direction in the video, the most consistent colors over the frames and their spatial locations, and the most diverse colors over the frames and their spatial locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motion Statistics</head><p>We use optical flow computed by classic coarse-to-fine algorithms <ref type="bibr" target="#b0">[1]</ref> to derive the motion statistical labels to be predicted in our task. Optical flow is a motion representation feature that is commonly used in many video recognition methods. For example, the classic two-stream network <ref type="bibr" target="#b34">[35]</ref> and the recent I3D network <ref type="bibr" target="#b3">[4]</ref>, both of which use stack of optical flow as their inputs for action recognition tasks. However, optical flow based methods are sensitive to camera motion, since they represent the absolute motion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>. To suppress the influence of camera motion, we instead seek a more robust feature, motion boundary <ref type="bibr" target="#b5">[6]</ref>, to capture the video motion information.</p><p>Motion Boundary. Denote optical flow horizontal component and vertical component as u and v, respectively. Motion boundaries are calculated by computing x-and yderivatives of u and v, i.e., u x = ∂u ∂x , u y = ∂u ∂y , v x = ∂v ∂x , v y = ∂v ∂y . As motion boundaries capture changes in the flow field, constant or smoothly varied motion, such as motion caused by camera view change, will be cancelled out. Only motion boundaries information is kept, as shown in <ref type="figure" target="#fig_2">Figure  3</ref>. Specifically, for an N -frame video clip, (N − 1) * 2 motion boundaries are computed. Diverse video motion information can be encoded into two summarized motion boundaries by summing up all these (N − 1) sparse motion boundaries of each component as follows:</p><formula xml:id="formula_0">M u = ( N −1 i=1 u i x , N −1 i=1 u i y ), M v = ( N −1 i=1 v i x , N −1 i=1 v i y ),<label>(1)</label></formula><p>where M u denotes the motion boundaries on horizontal optical flow u, and M v denotes the motion boundaries on vertical optical flow v. <ref type="figure" target="#fig_2">Figure 3</ref> shows the visualization of the two sum-up motion boundaries images.  Spatial-aware Motion Statistical Labels. In this section, we describe how to design the spatial-aware motion statistical labels to be predicted by our self-supervised task: 1) where is the largest motion; 2) what is the dominant orientation of the largest motion, based on motion boundaries. Given a video clip, we first divide it into several blocks using simple patterns. Although the pattern design is an interesting problem to be investigated, here, we introduce three simple yet effective patterns as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. For each video block, we assign a number to it for representing its location. Then we compute M u and M v as described above. The motion magnitude and orientation of each pixel can be obtained by casting motion boundaries M u and M v from the Cartesian coordinates to the Polar coordinates. As for the largest motion statistics, we compute the average magnitude of each block and use the number of the block with the largest average magnitude as the largest motion location. Note that the largest block number computed from M u and M v can be different. Therefore, we use two labels to represent the largest motion locations of M u and M v separately. While for the dominant orientation statistics, an orientation histogram is computed based on the largest motion block, similar to the computation motion boundary histogram (MBH) <ref type="bibr" target="#b5">[6]</ref>. Note that we do not have the normalization step since we are not computing a descriptor. Instead, we divide 360 • into 8 bins, with each bin containing 45 • angle range and again assign each bin to a number to represent its orientation. For each pixel in the largest motion block, we first use its orientation angle to determine which angle bin it belongs to and then add the corresponding magnitude number into the angle bin. The dominant orientation is the number of the angle bin with the largest magnitude sum.</p><p>Global Motion Statistical Labels. We also propose a set of global motion statistical labels to provide complementary information to the local motion statistics described above. Instead of focusing on the local patch of video clips, a CNN is asked to predict the largest motion frame. That is given an N -frame video clip, the CNN is encouraged to understand the video evolution from a global perspective and find out between which two frames, contains the largest motion. The largest motion is quantified by M u and M v separately and two labels are used to represent the global motion statistics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Appearance Statistics</head><p>Spatio-temporal Color Diversity Labels. Given an Nframe video clip, same as motion statistics, we divide it into several video blocks by patterns described above. For an N -frame video block, we first compute the 3D distribution V i in 3D color space of each frame i. We then use the Intersection over Union (IoU) along temporal axis to quantify the spatio-temporal color diversity as follows:</p><formula xml:id="formula_1">IoU score = V 1 ∩ V 2 ∩ ... ∩ V i ... ∩ V N V 1 ∪ V 2 ∪ ... ∪ V i ... ∪ V N .<label>(2)</label></formula><p>The largest color diversity location is the block with the smallest IoU score , while the smallest color diversity location is the block with the largest IoU score . In practice, we calculate the IoU score on R,G,B channels separately and compute the final IoU score by averaging them.</p><p>Dominant Color Labels. After we compute the largest and smallest color diversity locations, the corresponding dominant color is represented by another two labels. In the 3-D RGB color space, we evenly divide it into 8 bins. For the two representative video blocks, we assign each pixel a corresponding bin number by its RGB value, and the bin with the largest number of pixels is the dominant color.</p><p>Global Appearance Statistical Labels. We also design a global appearance statistics to provide supplementary information. Particularly, we use the dominant color of the whole video as the global statistics. The computation method is the same as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning with Spatio-temporal CNNs</head><p>We adopt the popular C3D network <ref type="bibr" target="#b36">[37]</ref> as the backbone for video spatio-temporal representation learning. Instead of using 2D convolution kernel k × k, C3D proposed to use 3D convolution kernel k × k × k to learn spatial and temporal information together. To have a fair comparison with other self-supervised learning methods, we use the smaller version of C3D as described in <ref type="bibr" target="#b36">[37]</ref>. It contains 5 convolutional layers, 5 max-pooling layers, 2 fully-connected layers and a soft-max loss layer in the end to predict the action class, which is similar to CaffeNet <ref type="bibr" target="#b15">[16]</ref>. We followed the same video pre-processing procedure as C3D. Input video samples are first split into non-overlapped 16-frame video clips. And for each input video clip, it is first reshaped into 128 × 171 and then randomly cropped into 112 × 112 for spatial jittering. Thus, the input size of C3D is 16 × 112 × 112 × 3. Temporal jittering is also adopted by randomly flipping the whole video clip horizontally.</p><p>We model our self-supervised task as a regression problem. The whole framework of our proposed method is shown in <ref type="figure" target="#fig_4">Figure 5</ref>. When pre-training the C3D network with the self-supervised labels introduced in the previous section, after the final convolutional layer, we use two branches to regress motion statistical labels and appearance statistical labels separately. For each branch, two fully connected layers are used similarly to the original C3D model design. And we replace the final soft-max loss layer with a fully connected layer, with 14 outputs for the motion branch and 13 outputs for the appearance branch. Mean squared error is used to compute the differences between the target statistics labels and the predicted labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the effectiveness of our proposed approach. We first conduct several ablation studies on the local and global, motion and appearance statistics design. Specifically, we use motion statistics as our auxiliary task and appearance statistics acts the similar way. The activation based attention map of different video samples is visualized to validate our proposed methodology. Second, we compare our method with other self-supervised learning auxiliary tasks on action recognition problem based on two popular dataset UCF101 <ref type="bibr" target="#b35">[36]</ref> and HMDB51 <ref type="bibr" target="#b22">[23]</ref>. Our method achieves the state-of-the-art result. Finally, we conduct two more experiments on action similarity <ref type="bibr" target="#b20">[21]</ref> and dynamic scene recognition <ref type="bibr" target="#b7">[8]</ref> to validate the transferability of our self-supervised spatio-temporal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluations</head><p>In our experiment, we incorporate five datasets: the UCF101 <ref type="bibr" target="#b35">[36]</ref>, the Kinetics <ref type="bibr" target="#b18">[19]</ref>, the HMDB51 <ref type="bibr" target="#b22">[23]</ref>, the ASLAN <ref type="bibr" target="#b20">[21]</ref>, and the YUPENN <ref type="bibr" target="#b7">[8]</ref>. Unless specifically state, we use UCF101 dataset for our model pre-training.</p><p>UCF101 dataset <ref type="bibr" target="#b35">[36]</ref> consists of 13,320 video samples, which fall into 101 action classes. Actions in it are all naturally performed as they are collected from YouTube. Videos in it are quite challenging due to the large variation in human pose and appearance, object scale, light condition, camera view and etc. It contains three train/test splits and in our experiment, we use the first train split to pre-train C3D.</p><p>Kinetics-400 dataset is a very large human action dataset <ref type="bibr" target="#b18">[19]</ref> proposed recently. It includes 400 human action classes, with 400 or more video clips for each class. Each sample is collected from YouTube and is trimmed into a 10-seconds video clip. This dataset is very challenging as it contains considerable camera motion/shake, illumination variations, shadows, etc. We use the training split for pretraining, which contains around 240k videos.</p><p>HMDB51 dataset <ref type="bibr" target="#b22">[23]</ref> is a smaller dataset which contains 6766 videos and 51 action classes. It also consists of three train/test splits. In our experiment, to have fair comparison with others, we use HMDB51 train split 1 to finetune the pre-trained C3D network and test the action recognition accuracy on HMDB51 test split 1.</p><p>When pre-training on UCF101 train split 1 video data, we set the batch size to 30 and use the SGD optimizer with learning rate 0.001. We divide the leaning rate every 5 epochs by 10. The training process is stopped at 20 epochs. When pre-training on the Kinetics-400 train split, the batch size is 30 and we use the SGD optimizer with learning rate 0.0005. The learning rate is divided by 10 for every 7 epochs and the model is also trained for 20 epochs. When finetuning the C3D, we retain the conv layers weights from the pre-trained network and initialize three fully-connected layers. The entire network is finetuned with SGD on 0.001 learning rate. The learning schedule is the same as the pretraining procedure. When testing, average accuracy for action classification is computed on all videos to obtain the video-level accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Analysis</head><p>In this section, we analyze the performance of our local and global statistics, motion and appearance statistics on extensive experiments. Particularly, we first pre-train the C3D using different statistics design on UCF101 train split 1. For local and global statistics ablation studies, we finetune the pre-train model on UCF101 train split 1 data with human annotated labels. For the high-level appearance and motion statistics studies, we also finetune the C3D with HMDB51 train split 1 to get more understanding of the design.</p><p>Pattern. The objective of this section is to investigate the performance of different pattern design. Specifically, we use the motion statistics and appearance statistics follow the same trend. As shown in <ref type="table" target="#tab_1">Table 1</ref>, all the three patterns outperform the random initialization, i.e., train from scratch setting, by around 8%, which strongly proves that our motion statistics is a very useful task. The performance of the three patterns are quite similar, indicating that we have balanced pattern design.</p><p>Local v.s. Global. In this section, we compare the performance of local statistics, where is the largest motion video block?, global statistics, where is the largest motion frame? and their combination. As can be seen in <ref type="table" target="#tab_2">Table 2</ref>, only global statistics serves as a useful auxiliary task for action recognition problem, with a improvement of 3%. And when all the three motion patterns are combined together, we can further get around 1.5% improvement, compared with single pattern. Finally, all motion statistics labels can achieve 57.8% accuracy, which is a significant improvement compared with train from scratch. more powerful as the temporal information is more important for video understanding. It is also interesting to note that although UCF101 only improves 1% when combined motion and appearance, the HMDB51 dataset benefits a lot from the combination, with a 3% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Action Recognition</head><p>In this section, we compare our method with other selfsupervised learning methods on the action recognition problem. Particularly, we compare the results with RGB video input and directly quote the number from <ref type="bibr" target="#b11">[12]</ref>. As shown in <ref type="table" target="#tab_3">Table 4</ref>, our method can achieve significantly improvement compared with the state-of-the-art both on UCF101 and HMDB51. Compared with methods that are pretrained on UCF101 dataset, we improve 9.3% accuracy on HMDB51 than <ref type="bibr" target="#b11">[12]</ref> and 2.5% accuracy on UCF101 than <ref type="bibr" target="#b23">[24]</ref>. Compared with the method proposed recently <ref type="bibr" target="#b19">[20]</ref> that are pre-trained on Kinetics dataset using 3D CNN models, we can also achieve 0.6% improvement on UCF101 and 5.1% improvement on HMDB51. And please note that <ref type="bibr" target="#b19">[20]</ref> used various regularization techniques during pre-training, such as channel replication, rotation with classification and spatio-temporal jittering while we do not use these techniques. The results strongly support that our proposed predicting motion and appearance statistics task can really drive the CNN to learn powerful spatio-temporal features. And our method can generate multi-frame spatio-temporal features transferable to many other video tasks.</p><p>Visualization. To further validate that our proposed method really helps the C3D to learn video related features, we visualize the attention map <ref type="bibr" target="#b43">[44]</ref> on several video frames as shown in <ref type="figure" target="#fig_5">Figure 6</ref>. It is interesting to note that for similar actions: Apply eye makeup and Apply lipstick, C3D is just sensitive to the location that is exactly the largest motion location as quantified by the motion boundaries as shown in the right. For different scale motion, for example, the bal-  ance beam action, the pre-trained C3D is also able to focus on the discriminative location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Action Similarity Labeling</head><p>We validate our learned spatio-temporal features on ASLAN dataset <ref type="bibr" target="#b20">[21]</ref>. This dataset contains 3,631 video samples of 432 classes. The task is to predict whether the given two videos are of the same class or not. We use C3D as a feature extractor, followed by a linear SVM to do the classification. Each video sample is split into several 16 frames video clips with 8 frames overlapped and then go through a feed-forward pass on C3D to extract features from the last conv layer. The video-level spatio-temporal feature is obtained by averaging the clip feature, followed by l2normalization. When testing on the ASLAN dataset, we follow the same 10-fold cross validation with leave-one-out evaluation protocol in each fold. Given a pair of videos, we first extract C3D feature from each video and then compute the 12 different distances described in <ref type="bibr" target="#b20">[21]</ref>. The 12 (dis-)similarity are finally concatenated together to obtain a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Accuracy (%) HOF <ref type="bibr" target="#b20">[21]</ref> 56.68 HOG <ref type="bibr" target="#b20">[21]</ref> 59.78 STIP <ref type="bibr" target="#b20">[21]</ref> 60.9 C3D, random initialization 51.7 C3D, train from scratch with label 58.3 C3D, self-supervised training 59.4 C3D, finetune on self-supervised 62.3</p><p>video-pair descriptor which is then fed into a linear SVM classifier. Since the scales of each distance are different, we normalize the distances separately into zero-mean and unit-variance as described in <ref type="bibr" target="#b36">[37]</ref>. As no previous self-supervised learning methods have done experiment on this dataset, to validate that our selfsupervised task can drive C3D to learn powerful spatiotemporal features, we design 4 scenarios to extract features from ASLAN dataset: (1) Use the random initialization C3D as feature extractor. (2) Use the C3D pre-trained on UCF101 with labels as feature extractor. (3) Use the C3D pre-trained on UCF101 with our self-supervised task as feature extractor. (4) Use the C3D finetuned on UCF101 on our self-supervised model as feature extractor. <ref type="table" target="#tab_4">Table 5</ref> shows the performance of different feature extractors. The random initialization model can achieve 51.4% accuracy as the problem is a binary classification problem. What surprises us is that although our self-supervised pre-trained C3D has never seen the ASLAN dataset before, it can still do well in this problem and outperforms the C3D trained with humanannotated labels by 1.1%. Such results strongly support that our proposed self-supervised task is able to learn powerful and transferable spatio-temporal features. This can be explained by the internal characteristics of the action similarity labeling problem. Different from the previous action recognition problem, the goal of ASLAN dataset is to predict video similarity instead of predicting the actual label. To achieve good performance, C3D must understand the video context, which is just what we try to drive the C3D to do with our self-supervised method. When finetuned our self-supervised pre-trained model with labels on UCF101, we can further get around 3% improvement. It outperforms the handcrafted features STIP <ref type="bibr" target="#b20">[21]</ref>, which is the combination of three popular features: HOG, HOF, and HNF (a composition of HOG and HOF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Dynamic Scene Recognition</head><p>The performance on UCF101, HMDB51 and ASLAN dataset shows that our proposed self-supervised learning task can drive the C3D to learn powerful spatio-temporal features for action recognition problem. One may wonder that can action-related features be generalized to other  <ref type="bibr" target="#b7">[8]</ref>, which contains 420 video samples of 14 dynamic scenes. For each video in the dataset, first split it into 16 frames clips with 8 frames overlapped. The spatio-temporal features are then extracted based on our self-supervised C3D pre-trained model from the last conv layer. The video-label representations are obtained by averaging each video-clip features, followed with l2 normalization. A linear SVM is finally used to classify each video scene. We follow the same leave-one-out evaluation protocol as described in <ref type="bibr" target="#b7">[8]</ref>.</p><p>We compared our methods with both hand-crafted features and other self-supervised learning tasks as shown in <ref type="table" target="#tab_5">Table 6</ref>. Our self-supervised C3D outperforms both the traditional features and self-supervised learning methods. It shows that although our self-supervised C3D is trained on a action dataset, the learned weights has impressive transferability to other video-related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we presented a novel approach for selfsupervised spatio-temporal video representation learning by predicting a set of statistical labels derived from motion and appearance statistics. Our approach is bio-inspired and consistent with human visual systems. We demonstrated that by pre-training on unlabeled videos with our method, the performance of C3D network is improved significantly over random initialization on the action recognition problem. Compared with other self-supervised representation learning approaches, our method achieves state-of-the-art performances on UCF101 and HMDB51 datasets. This strongly supports that our method can drive C3D network to capture more crucial spatio-temporal information. We also showed that our pre-trained C3D network can be used as a powerful feature extractor for other tasks, such as action similarity labeling and dynamic scene recognition, where we also achieve state-of-the-art performances on public datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A simple illustration of statistical concepts in a threeframe video clip. See explanations in Sec. 3.1 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Motion boundaries computation. For a given input video clip, we first extract optical flow across each frame. For each optical flow, two motion boundaries are obtained by computing gradients separately on the horizontal and vertical components of the optical flow. The final sum-up motion boundaries are obtained by aggregating the motion boundaries on u flow and v flow of each frame separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Three different partitioning patterns (from left to right: 1 to 3) used to divide video frames into different types of spatial regions. Pattern 1 divides each frame into 4×4 blocks. Pattern 2 divides each frame into 4 different non-overlapped areas with the same gap between each block. Pattern 3 divides each frame by the two center lines and the two diagonal lines. The indexing strategies of the labels are shown in the bottom row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>ul , uo, vl , vo) Global (ui , vi ) Pattern 2 (ul , uo, vl , vo) Pattern 3 (ul , uo, vl , vo) Pattern 1 (pd, cd, ps, cs) Global (C) Pattern 2 (pd, cd, ps, cs) Pattern 3 (pd, cd, ps, cs) The network architecture of the proposed method. Given a 16-frame video, we regress 14 outputs for the motion branch and 13 outputs for the appearance branch. For each motion pattern, 4 labels are generated by aggregating motion boundaries Mu and Mv: (1) u l -the largest magnitude location of Mu. (2) uo -the corresponding orientation of u l . (3) v l -the largest magnitude location of Mv. (4) vo -the corresponding orientation of v l . For each appearance pattern, 4 labels are predicted: (1) p d -the position of largest color diversity. (2) c d -the corresponding dominant color. (3) ps -the position of smallest color diversity. (4) cs -the corresponding dominant color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Attention visualization. From left to right: A frame from a video clip, activation based attention map of conv5 layer on the frame by using<ref type="bibr" target="#b43">[44]</ref>, motion boundaries Mu of the whole video clip, and motion boundaries Mv of the whole video clip.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison the performance of different patterns of motion statistics for action recognition on UCF101.</figDesc><table><row><cell>Initialization</cell><cell>Accuracy (%)</cell></row><row><cell>Random</cell><cell>45.4</cell></row><row><cell>Motion pattern 1</cell><cell>53.8</cell></row><row><cell>Motion pattern 2</cell><cell>53.2</cell></row><row><cell>Moiton pattern 3</cell><cell>54.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of local and global motion statistics for action recognition on the UCF101 dataset.</figDesc><table><row><cell cols="2">Initialization</cell><cell>Accuracy (%)</cell></row><row><cell>Random</cell><cell></cell><cell>45.4</cell></row><row><cell cols="2">Motion global</cell><cell>48.3</cell></row><row><cell cols="2">Motion pattern all</cell><cell>55.4</cell></row><row><cell cols="2">Motion pattern all + global</cell><cell>57.8</cell></row><row><cell cols="3">Table 3. Comparison of different supervision signals on the</cell></row><row><cell cols="2">UCF101 and the HMDB51 datasets.</cell><cell></cell></row><row><cell>Domain</cell><cell cols="2">UCF101 acc.(%) HMDB51 acc. (%)</cell></row><row><cell>From scratch</cell><cell>45.4</cell><cell>19.7</cell></row><row><cell>Appearance</cell><cell>48.6</cell><cell>20.3</cell></row><row><cell>Motion</cell><cell>57.8</cell><cell>29.95</cell></row><row><cell>Joint</cell><cell>58.8</cell><cell>32.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison with the state-of-the-art self-supervised video representation learning methods on UCF101 and HMDB51.</figDesc><table><row><cell>Method</cell><cell cols="2">UCF101 acc.(%) HMDB51 acc.(%)</cell></row><row><cell>DrLim [15]</cell><cell>38.4</cell><cell>13.4</cell></row><row><cell>TempCoh [28]</cell><cell>45.4</cell><cell>15.9</cell></row><row><cell>Object Patch [43]</cell><cell>42.7</cell><cell>15.6</cell></row><row><cell>Seq Ver.[27]</cell><cell>50.9</cell><cell>19.8</cell></row><row><cell>VGAN [39]</cell><cell>52.1</cell><cell>-</cell></row><row><cell>OPN [24]</cell><cell>56.3</cell><cell>22.1</cell></row><row><cell>Geometry [12]</cell><cell>55.1</cell><cell>23.3</cell></row><row><cell>Ours (UCF101)</cell><cell>58.8</cell><cell>32.6</cell></row><row><cell>ST-puzzle (Kinetics) [20]</cell><cell>60.6</cell><cell>28.3</cell></row><row><cell>Ours (Kinetics)</cell><cell>61.2</cell><cell>33.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison with different handcrafted features and our proposed four scenarios performance on the ASLAN dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison with hand-crafted features and other selfsupervised representation learning methods for dynamic scene recognition problem on the YUPENN dataset. Ours Accuracy (%) 86.0 80.7 70.47 76.67 86.9 90.2 problems? We investigate this question by transferring the learned features to the dynamic scene recognition problem based on the YUPENN dataset</figDesc><table><row><cell>Method</cell><cell>[10]</cell><cell>[8]</cell><cell>[43]</cell><cell>[27]</cell><cell>[12]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Motion, RGB, and Joint Statistics. We finally compare all motion statistics, all RGB statistics, and their combination on UCF101 and HMDB51 dataset as shown inTable 3. From the table, we can find that both the appearance and motion statistics serve as a useful self-supervised signals for UCF101 and HMDB51 dataset. The motion statistics is</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining spatiotemporal video patterns towards robust action retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="69" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic scene understanding: The role of orientation features in space and time in scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spacetime forests with complementary features for dynamic scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cognitive neuroscience: neural mechanisms for the recognition of biological movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Giese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human action recognition in unconstrained videos by explicit motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3781" to="3795" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The action similarity labeling challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orit</forename><surname>Kliper-Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="615" to="621" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Jointly localizing and describing events for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Video classification via weakly supervised sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="79" to="87" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeruIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reconstruction network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
