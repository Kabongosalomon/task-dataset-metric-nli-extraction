<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Forwarding for Efficient Single Image Dehazing NYU Depth O/I-HAZE Dense-Haze</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Morales</surname></persName>
							<email>peter.morales@ll.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory Lexington</orgName>
								<address>
									<postCode>02421</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzofi</forename><surname>Klinghoffer</surname></persName>
							<email>tzofi.klinghoffer@ll.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory Lexington</orgName>
								<address>
									<postCode>02421</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
							<email>seungjae.lee@ll.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory Lexington</orgName>
								<address>
									<postCode>02421</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Forwarding for Efficient Single Image Dehazing NYU Depth O/I-HAZE Dense-Haze</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Equal contribution.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: (top) Hazy images. (bottom) Images after haze is removed using our fully convolutional neural network (CNN) approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Haze degrades content and obscures information of images, which can negatively impact vision-based decisionmaking in real-time systems. In this paper, we propose an efficient fully convolutional neural network (CNN) image dehazing method designed to run on edge graphical processing units (GPUs). We utilize three variants of our architecture to explore the dependency of dehazed image quality on parameter count and model design. The first two variants presented, a small and big version, make use of a single efficient encoder-decoder convolutional feature extractor. The final variant utilizes a pair of encoder-decoders for atmospheric light and transmission map estimation. Each variant ends with an image refinement pyramid pooling network to form the final dehazed image. For the big variant of the single-encoder network, we demonstrate state-of-theart performance on the NYU Depth dataset. For the small variant, we maintain competitive performance on the superresolution O/I-HAZE datasets without the need for image cropping. Finally, we examine some challenges presented by the Dense-Haze dataset when leveraging CNN architectures for dehazing of dense haze imagery and examine the impact of loss function selection on image quality. Benchmarks are included to show the feasibility of introducing this approach into real-time systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many computer vision applications, such as those for image classification and object detection, are trained on datasets comprised of mostly pristine imagery. However, to ensure dependability in real-world environments, computer vision algorithms must be able to perform consistently in various levels of visual degradation. One primary source of image degradation is haze, which introduces challenging, nonlinear noise to a scene. Haze is caused by particulates in the atmosphere, such as dust, fumes, and mist, that absorb and scatter light. Image degradation from haze can adversely affect computer vision algorithms, making it a principle concern for future systems that incorporate visual information into their decision-making processes. Previous works <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref> have established the negative impact of haze on object detection and recognition tasks and have furthermore shown the benefit of introducing image dehazing as a prepossessing step to computer vision tasks. Introducing image enhancement algorithms such as image dehazing may prove to be an important step in creating reliable vision-based systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Single Image Dehazing</head><p>The presence of haze in images is often described by the atmospheric scattering model <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>, which is classically formulated:</p><formula xml:id="formula_0">I(x) = J(x)t(x) + A(1 − t(x))<label>(1)</label></formula><p>where I(x) is the captured hazy image, J(x) is the hazefree image, A is the global atmospheric light, and t(x) is the transmission map. Consequently, by estimating the global atmospheric light and transmission map for a captured hazy image, the haze-free image can be recovered. This approach has been the basis of several successful approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b34">34]</ref>. More recently, neural network approaches have also been proposed to estimate these scene properties <ref type="bibr" target="#b32">[32]</ref>.</p><p>To evaluate the performance of these algorithms, peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) are commonly used to quantify dehazed image restoration quality. PSNR (measured in decibels) is an absolute error, calculated using the mean square error (MSE) of a pixel relative to its maximum possible value. Alternatively, SSIM attempts to improve upon absolute error metrics by more closely aligning with human perception under the assumption that humans' visual systems are highly attuned to extracting structural information <ref type="bibr" target="#b31">[31]</ref>. Nevertheless, these metrics do not always agree with human assessment of the similarity of images <ref type="bibr" target="#b22">[22]</ref> and qualitative assessment remains an important component in evaluating performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>In this paper, we present a family of fully convolutional neural network architectures for single image dehazing capable of being deployed on edge GPUs. First, we present two network variants, dubbed Small and Big FastNet, where Small and Big refer to the widths of the networks. Second, we present a neural network based on the atmospheric scattering model that estimates the transmission map and atmospheric light of a scene. We utilize these networks to study change in accuracy as a function of total network parameters, as well as to assess the benefits of estimating a scene's transmission map and atmospheric light. For this paper, we loosely define efficiency based on model performance versus parameter count. All of the proposed networks utilize both an encoder-decoder structure adapted from efficient image segmentation networks <ref type="bibr" target="#b7">[8]</ref> and a fully connected pyramid pooling network <ref type="bibr" target="#b11">[12]</ref> for output image refinement. Finally, we show benchmarks on reference hardware for varying pixel counts to examine the feasibility of incorporating these algorithms in real-time systems.</p><p>The paper makes the following contributions:</p><p>• A novel neural network architecture that efficiently achieves state-of-the-art performance in single image dehazing on the NYU Depth dataset.</p><p>• A scaled-down architecture capable of running on super-resolution imagery without the need for cropping, which is a common requirement for previous approaches.</p><p>• An empirical evaluation of the impact of loss function on restoration quality.</p><p>• A discussion of the value of utilizing the atmospheric scattering model when designing neural network image dehazing models.</p><p>• A discussion on the challenges of using deep learning methods for haze removal, such as the effects from overfitting.</p><p>• Timing benchmarks for running our architectures on desktop and edge GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Although there has and continues to be a tremendous amount of success in single image dehazing without the use of neural networks, many recent state-of-the-art techniques utilize deep learning frameworks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b32">32]</ref>. These approaches generally incorporate neural network building blocks originally proposed for image segmentation, style transfer, object detection, and other computer vision tasks. For example, U-Nets <ref type="bibr" target="#b28">[28]</ref>, feature pyramid networks <ref type="bibr" target="#b21">[21]</ref>, and residual networks <ref type="bibr" target="#b12">[13]</ref> were all utilized as part of the 2018 NTIRE Image Dehazing Challenge [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Atmospheric Model Learning</head><p>Several successful techniques leverage hand-engineered features to estimate the transmission map for image dehazing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">30]</ref>. In contrast to these approaches, Cai et al. <ref type="bibr" target="#b6">[7]</ref> proposed an end-to-end network that learns features useful for estimating a transmission map. However, this method and similar transmission estimation methods <ref type="bibr" target="#b27">[27]</ref> do not address estimating the atmospheric light within a scene. Zhang and Patel <ref type="bibr" target="#b32">[32]</ref> addressed this issue by estimating both the atmospheric light and transmission map within a generative adversarial learning framework. In this approach, the unknown variables from the atmospheric scattering model are estimated using independent neural network architectures; U-Net is used to learn atmospheric light and a densely connected network is used to learn a transmission map estimation. Additionally, Li et al. <ref type="bibr" target="#b20">[20]</ref> showed that the atmospheric scattering model, described in Equation 1, could be reformulated via a linear transform to a single variable and bias.</p><formula xml:id="formula_1">J(x) = K(x)I(x) − K(x) + b (2) K(x) = 1 t(x) (I(x) − A) + (A − b) I(x) − 1<label>(3)</label></formula><p>This formulation fits naturally within a deep learning framework and hints at the effectiveness of purely convolutional approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Style Transfer and Segmentation Networks</head><p>Generative adversarial networks (GANs) for image style transfer have become increasingly popular in recent years with algorithms such as Pix2Pix <ref type="bibr" target="#b13">[14]</ref> and CycleGAN <ref type="bibr" target="#b33">[33]</ref>. Haze removal can also be thought of from a style transfer perspective: transferring images from the hazy domain to the haze-free domain. This approach was attempted by Engin et al. <ref type="bibr" target="#b8">[9]</ref>, in which cycle consistency and perceptual losses were combined in a CycleGAN framework.</p><p>Additionally, approaches from semantic image segmentation, such as feature pyramid networks, have proven to be effective in image dehazing applications. Image segmentation networks often utilize encoder-decoder pairs to learn embedded representations of inputs that take into account multi-scale features. Chaurasia and Culurciello <ref type="bibr" target="#b7">[8]</ref> proposed an efficient semantic segmentation architecture based on a fully convolutional encoder-decoder framework. Their encoder uses a ResNet18 model <ref type="bibr" target="#b12">[13]</ref> for feature encoding and avoids a loss of spatial information by reintroducing residuals from each encoder to the output of its corresponding decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Super-Resolution Imagery</head><p>One challenge in using neural networks for single image dehazing is processing high-resolution input. Several techniques in the 2018 NTIRE Image Dehazing Challenge handled the relatively high-input resolution of the I-HAZE <ref type="bibr" target="#b3">[4]</ref> and O-HAZE <ref type="bibr" target="#b4">[5]</ref> datasets by cropping input imagery into many smaller frames or downsampling the input imagery and resizing the final outputs <ref type="bibr" target="#b0">[1]</ref>. These approaches are limited by total GPU memory and not GPU processing power; therefore, models with fewer parameters are capable of accepting higher-resolution input imagery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Our proposed fully convolutional neural networks (CNNs) build upon past work in efficient image segmentation and deep learning-based image dehazing. For our Small FastNet model, we adapted the LinkNet architecture <ref type="bibr" target="#b7">[8]</ref> by removing the final softmax and prediction layers in order to pass features directly into a pyramid pooling network at the full input spatial resolution. LinkNet uses layers from a pretrained ResNet18 model for its encoder modules. For our Big FastNet model, we modified the original architecture's encoder to utilize ResNet50 as its encoder module; we observe that the increased model width (achieved with the deeper ResNet encoder) leads to improved restoration quality at a small speed trade off. Both these models use a single encoder-decoder to learn features of the image, followed by an image refinement pyramid pooling network. The pyramid pooling network helps preserve multi-scale features when forming the final output image by progressively embedding inputs at multiple scales and then resizing all scaled embeddings to the output resolution.</p><p>In addition to the two single-encoder models, we introduce DualFastNet, which is inspired by past work in atmospheric model networks, notably by Zhang and Patel <ref type="bibr" target="#b32">[32]</ref>. Rather than using a single encoder-decoder, our DualFast-Net approach uses two separate encoder-decoder models to learn atmospheric light and transmission map estimations. These estimations are then used as input to calculate a dehazed image using the formulation described in Equation 1. This approach was used in our submission to the 2019 NTIRE Image Dehazing Challenge; however, as described in later sections, further studies indicate that Big Fast-Net yields better performance on larger datasets. Our single encoder-decoder FastNet variant and double encoderdecoder DualFastNet variant are both shown in <ref type="figure" target="#fig_0">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation and Training Details</head><p>We utilized several loss functions and data augmentation techniques described further in subsequent sections. Our implementation was developed in PyTorch and all results can be generated using our provided code * . We utilized ADAM <ref type="bibr" target="#b18">[18]</ref> as an optimizer for training with an initial learning rate of 1 × 10 −3 . During training, validation was done per epoch and models with improved vali- * https://github.com/pmm09c/ntire-dehazing dation loss were saved. Early stopping was used and training ended upon reaching convergence in validation loss to prevent overfitting. Each model was initially trained using MSE as the loss function. However, as described in later sections, some models were fine tuned using a secondary loss function. When validating models based on SSIM and PSNR, we chose to report the model with the highest SSIM, even if the corresponding PSNR was not the highest of all models trained. This means that for all results presented in this paper, models with higher PSNR may be achievable, but with degradation to SSIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We trained and evaluated our proposed methods on four datasets. First, we leveraged the NYU Depth dataset V2 as prepared by <ref type="bibr">Zhang</ref>  Additionally, we evaluated our approach on the more challenging Dense-Haze dataset <ref type="bibr" target="#b2">[3]</ref> and the high-resolution O-and I-HAZE datasets. These datasets provide real-world imagery that can be used to evaluate the generalizability of our models, the usability of our models on high-resolution data, and the overall performance of our models in various conditions. For the NTIRE 2019 Image Dehazing Challenge, our models were trained exclusively on the Dense-Haze dataset with randomly initialized weights. Models used to evaluate the O/I-HAZE datasets were trained on O/I-HAZE data using weights generated from training the NYU Depth dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Architecture Comparison</head><p>We studied three variants of our fully convolutional neural network: Small FastNet, Big FastNet, and DualFastNet. As a result of studying these models, we present empirical evidence of the benefits of increasing model width and show the capability of fully convolutional methods to generalize image dehazing mechanisms without the need for an explicit atmospheric model. <ref type="bibr">In</ref>   through the atmospheric scattering model, as done in the DualFastNet architecture, can benefit training when limited training samples are available.</p><p>Each model was trained and tested on the NYU Depth dataset with MSE loss only and we report the resulting PSNR and SSIM. MSE loss was enforced on the output image of both Small and Big FastNet. For DualFastNet, we examined three ways to train our model. Originally proposed by Zhang and Patel <ref type="bibr" target="#b32">[32]</ref>, we employed a stage-wise learning technique to train atmospheric light, transmission map, and image formation networks separately to quicken convergence; training was completed with the entire model being fine tuned. Although this approach was found to be effective, it burdens the training process. We denote this step-wise learning technique as DualFastNet step . We also explored whether our DualFastNet model can be trained wholly from scratch -both with MSE loss enforced on atmospheric light, the transmission map, the dehazed image, and refined output image (DualFastNet M SE×4 ), and with MSE loss enforced only on the refined output image (DualFastNet M SE×1 ). Results are summarized in <ref type="table" target="#tab_2">Table  1</ref>. Model performance and parameter count appear to be related; models with higher parameter count yield higher performance. In addition, the step-wise learning technique is the most effective for training the atmospheric scattering model-based DualFastNet. The widest architecture, Big FastNet, performs the best of our proposed architectures in both PSNR and SSIM, indicating that using a wider network is a viable alternative to incorporating an atmospheric model prior into the neural network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Loss Functions and Fine Tuning</head><p>We investigated the impact of loss function selection when optimizing our model on the NYU Depth dataset. Specifically, we fitted our models using a least absolute deviations (L1) loss and MSE loss baseline, and then further trained with a second refinement loss function. Refinement functions considered were: content loss <ref type="bibr" target="#b15">[16]</ref>, L1 loss, MSE loss, and SSIM loss. For the purpose of training time, we trained with our smallest model, Small FastNet. Results from this study are summarized in <ref type="table" target="#tab_4">Table 2</ref>. Results indi-cate that training with L1 loss followed by MSE refinement generates images with the highest PSNR, whereas training with MSE loss followed by SSIM refinement generates images with the highest SSIM. Images generated with any of the loss functions studied are qualitatively similar, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Timing Benchmarks</head><p>We performed timing benchmarks to help asses the feasibility of introducing our method as a pre-processing step for computer vision algorithms in real-time systems. The average timing over 20 runs is presented on both the Titan RTX desktop GPU and Tegra Xavier edge GPU. We progressively increased input resolution until we could no longer process a given input batch size due to GPU memory limitations. Timing results are given in frames per second for both floating point 32 and floating point 16. Full timing results are presented in <ref type="table" target="#tab_9">Table 4</ref>. Unsurprisingly, the biggest timing gains come from utilizing a batch size greater than 1 and operating at floating point 16. For real-time applications, this introduces latency in exchange for throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-Art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Results on NYU Depth Dataset</head><p>For the NYU Depth dataset, we show state-of-the-art performance using our Big FastNet model trained with MSE loss and SSIM loss as refinement. Additionally, the model performs efficiently relative to its parameter count. Our model width can also be scaled down in exchange for SSIM and PSNR. For instance, Small FastNet has 11 million parameters, 6x smaller than Zhang and Patel's <ref type="bibr" target="#b32">[32]</ref> approach, and still performs competitively. Results for our method and other approaches are summarized in <ref type="table" target="#tab_6">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Results on High-Resolution O/I-HAZE Dataset</head><p>We evaluated our method on the benchmark high-resolution O/I-HAZE datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Because of limitations in GPU memory, we used our Small FastNet model in this evaluation. Because this model has fewer parameters than other   <ref type="bibr" target="#b32">[32]</ref> while PSNR values and parameter counts were reproduced with the original public implementations. Parameter count is measured in millions. While smaller models exist, our method has fewer parameters and higher SSIM and PSNR than current state-of-the-art methods. models studied, we were able to perform inference on the native full-resolution test imagery with a Titan RTX GPU.</p><p>Past approaches typically use one of two methods: (1) forward pass patches of the test image and stitch the final output, or (2) operate on lower-input resolution imagery and rescale the output <ref type="bibr" target="#b0">[1]</ref>. Two models were trained separately using MSE loss, one on the O-HAZE dataset and one on the I-HAZE dataset. Each model was trained using the NYU Depth dataset pretrained model generated in earlier experiments rather than from randomly initialized weights. Training loss converged after only a few epochs, indicating that features learned from the NYU Depth dataset transfer well to other datasets, such as the O/I-HAZE datasets. To train our models, we augmented the dataset by extracting multiscale patches reshaped to our training input size. Our Small FastNet results are competitive with results from the 2018 NTIRE Image Dehazing Challenge, Indoor and Outdoor tracks <ref type="bibr" target="#b0">[1]</ref>. We achieve SSIM of 0.8089 and PSNR of 18.56 for the I-HAZE test dataset and SSIM of 0.7459 and PSNR of 22.07 for the O-HAZE test dataset. Each metric is ranked within the top 10 for its category with respect to the 2018 NTIRE Image Dehazing Challenge <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure" target="#fig_3">Figure 4</ref> shows several images generated with our approach, including results from early training and results from the end of training when top SSIM has been reached. Although SSIM and PSNR continue to improve in later epochs of training, artifacts in imagery commonly seen in neural network approaches for image generation become noticeable. This indicates that it is important to not only maximize SSIM and PSNR, but also to conduct thorough qualitative analysis when evaluating top models for image dehazing. Because pixel values within areas of continuous dense haze are likely unrecoverable, the neural network learns to minimize its loss by using the average pixel value learned from similar areas in training data when it encoun- ters dense haze. This causes the artifacts observed in <ref type="figure" target="#fig_3">Figure  4</ref>. In short, areas with unrecoverable pixel values are substituted with random training artifacts, which are likely to be strong indicators of overfitting.</p><formula xml:id="formula_2">Input L1 L1 − → MSE L1 − → SSIM MSE MSE − → L1 MSE − → SSIM Truth</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Results on Dense-Haze Dataset</head><p>The Training started with randomly initialized weights and data were randomly cropped and rotated throughout. Our model produced results that were competitive with other models in the challenge, achieving a PSNR score of 16.37 and an SSIM score of 0.569 on test images. Examples of the images generated are shown in <ref type="figure">Figure 1</ref>.</p><p>Although Big FastNet outperforms our other models in earlier experiments, these experiments did not use models trained on sparse datasets with randomly initialized weights as was done in this challenge. We have observed that when limited to fewer training samples, DualFastNet can generate superior results, indicating that the atmospheric scattering model is a helpful prior in certain conditions. Specifically, on the Dense-Haze dataset, Big FastNet achieved the highest SSIM and DualFastNet achieved the highest PSNR. A qualitative study of output images was done that informed the decision to use DualFastNet in our challenge submission.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a family of novel neural network architectures for single image dehazing, as well as present both quantitative and a qualitative evaluation of these architectures and their loss functions. On the NYU Depth dataset, Big FastNet, our largest model, outperforms its smaller variant and our architecture based on the atmospheric scattering model. Additionally, this approach outperforms other state-of-the-art neural networks on the NYU Depth dataset in both performance and efficiency. However, our experimental results indicate that the atmospheric scattering model is a useful prior for a neural network architecture when training data is limited. Our architectures can be run as part of real-time systems on edge GPUs and have been benchmarked on multiple input imagery sizes. Finally, we discuss our results and challenges working with the O-HAZE, I-HAZE, and Dense-Haze datasets. In the 2019 NTIRE Image Dehazing Challenge, our efficient, atmospheric scattering model-based neural network architec-ture, DualFastNet, achieved competitive results, obtaining a PSNR score of 16.37 and an SSIM score of 0.569 on test images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Proposed models: (left) Our FastNet single encoderdecoder architecture, which forwards features directly into a pyramid refinement network. (right) Our DualFastNet architecture, which estimates both atmospheric light and transmission maps to form dehazed images via Equation 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples of images produced using different loss functions and our Small FastNet architecture. Each column shows results for a different loss function. Images are from the NYU Depth dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Example of images produced by our Small FastNet architecture for the I-HAZE dataset (top) and O-HAZE dataset (bottom). Overfitting leads to improved SSIM and PSNR, but causes qualitative defects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Patel [32] † and demonstrate an improvement over previous state-of-the-art approaches. This dataset contains 1,000 unique training examples from the NYU Depth dataset V2 [29] and 4,000 total training samples (each sample has four variations with varying levels of haze). These images are synthesized with the following parameters: A ∈ [0.5, 1.0] and β ∈ [1.4, 1.6], where A is atmospheric light and β is the scattering coefficient. Each training sample consists of a hazy image, an atmospheric light image, a transmission map image, and a dehazed ground truth image. Four hundred test examples are generated in a similar fashion from the NYU Depth dataset V2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>later sections, we use the Dense-Haze dataset to show that introducing model priors † https://github.com/hezhangsprinter/DCPDN PSNR SSIM Parameters Small FastNet 24.08 0.9034 11,554,167 DualFastNet MSE×1 24.69 0.9081 23,072,725 DualFastNet MSE×4 22.30 0.8650 23,072,725 DualFastNet step 28.13 0.9483 23,072,725</figDesc><table><row><cell>Big FastNet</cell><cell>29.69 0.9563 28,782,647</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>We compare the three variations of the proposed model architecture. Each model is trained until convergence with MSE loss only. DualFastNet is trained with three methods. Models were trained and evaluated on the NYU Depth dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of loss functions used to train Small FastNet.</figDesc><table /><note>Models were trained and evaluated on the NYU Depth dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison of our Big FastNet method with other state-of-the-art approaches tested on the NYU Depth test dataset. SSIM of other methods are drawn from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Timing benchmarks generated with TensorRT to assess the feasibility of introducing our model to a real-time system. In general, increasing batch size allows for higher frames per second (FPS) processing in exchange for latency. Small or Big indicates the FastNet model used. The floating point precision is indicated by 16 or 32. Results are described in average FPS over 20 iterations.</figDesc><table><row><cell>Input</cell><cell>Early Training Output</cell><cell>Top SSIM Output</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2018 challenge on image dehazing: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Codruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="891" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on image dehazing: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Codruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dense haze: A benchmark for image dehazing with dense-haze and haze-free images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Codruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02904</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">I-haze: a dehazing benchmark with real hazy and haze-free indoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Codruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vleeschouwer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05091v1</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">O-haze: a dehazing benchmark with real hazy and haze-free outdoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Codruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vleeschouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, NTIRE Workshop, NTIRE CVPR&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunmei</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno>abs/1601.07661</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linknet: Exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Visual Communications and Image Processing (VCIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cycledehaze: Enhanced cyclegan for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Engin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anıl</forename><surname>Genç</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazım Kemal</forename><surname>Ekenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
		<idno>13:1-13:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1406.4729</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image dehazing using adaptive bi-channel priors on superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="17" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1603.08155</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single image haze removal based on the improved atmospheric scattering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenfei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="page" from="180" to="191" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Theorie der horizontalen sichtweite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Koschmieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Beitrge zur Physik der Freien Atmosphre</title>
		<imprint>
			<date type="published" when="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of single image dehazing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3600" to="3604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Vision through the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Edgar Knowles</forename><surname>Middleton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952" />
			<publisher>University of Toronto Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chromatic framework for vision in bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vision and the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single image dehazing via multiscale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
