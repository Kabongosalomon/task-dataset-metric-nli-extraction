<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Alexandru</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosu</forename><forename type="middle">Peer</forename><surname>Schütt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Quenzel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
						</author>
						<title level="a" type="main">LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. Applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes raw point clouds as input. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on multiple datasets where our method achieves state-of-the-art performance.</p><p>• a hybrid architecture which leverages the strength of PointNet to obtain low-level features and sparse 3D convolutions to aggregate global context,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Environment understanding is a crucial ability for autonomous agents. Perceiving not only the geometrical structure of the scene but also distinguishing between different classes of objects therein enables tasks like manipulation and interaction that were previously not possible. Within this field, semantic segmentation of 2D images is a mature research area, showing outstanding success in dense per pixel categorization on images <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>. The task of semantically labelling 3D data is still an open area of research however as it poses several challenges that need to be addressed.</p><p>First, 3D data is often represented in an unstructured manner -unlike the grid-like structure of images. This raises difficulties for current approaches which assume a regular structure upon which convolutions are defined.</p><p>Second, the performance of current 3D networks is limited by their memory requirements. Storing 3D information in a dense structure is prohibitive for even high-end GPUs, clearly indicating the need for a sparse structure.</p><p>Third, discretization issues caused by imposing a regular grid onto point clouds can negatively affect the network's performance and interpolation is necessary to cope with quantization artifacts <ref type="bibr" target="#b33">[34]</ref>.</p><p>In this work, we propose LatticeNet, a novel approach for point cloud segmentation which alleviates the previously mentioned problems. Hence, our contributions are: • a framework suitable for sparse data onto which all common CNN operators are defined, and • a novel slicing operator that is end-to-end trainable for mapping features of a regular lattice grid back onto an unstructured point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Semantic segmentation approaches applied to 3D data can be categorized depending on data representation upon which they operate. Point cloud networks: The first category of networks operates directly on the raw point cloud.</p><p>From this area, PointNet <ref type="bibr" target="#b24">[25]</ref> is one of the pioneering works. The approach processes raw point clouds by individually embedding the points into a higher-dimensional space and applying max-pooling for permutation-invariance to obtain a global scene descriptor. The descriptor can be used for both classification and semantic segmentation. However, PointNet does not take local information into account which is essential for the segmentation of highly-detailed objects. This has been partially solved in the subsequent work of PointNet++ <ref type="bibr" target="#b25">[26]</ref> which applies PointNet hierarchically, capturing both local and global contextual information.</p><p>Chen et al. <ref type="bibr" target="#b4">[5]</ref> use a similar approach but they input the point responses w.r.t. a sparse set of radial basis functions (RBF) scattered in 3D space. Optimizing jointly for the extent and center of the RBF kernels allows to obtain a more explicit modelling of the spatial distribution.</p><p>Instead, PointCNN <ref type="bibr" target="#b16">[17]</ref> deals with the permutation invariance not by using a symmetric aggregation function, but by learning a K × K matrix for the K input points that permutes the cloud into a canonical form. Voxel networks: Voxel-based approaches discretize the space in cubic or tetrahedral volume elements which are used for 3D convolutions.</p><p>SEGCloud <ref type="bibr" target="#b33">[34]</ref> voxelizes the point cloud into a uniform 3D grid and applies 3D convolutions to obtain per-voxel class probabilities. A Conditional Random Field (CRF) is used to smooth the labels and enforce global consistency. The class scores are transferred back to the points using trilinear interpolation. The usage of a dense grid results in high memory consumption while our approach uses a permutoherdral lattice stored sparsely. Additionally, their voxelization results in a loss of information due to the discretization of the space. Our approach avoids quantization issues by using a PointNet architecture to summarize the local neighborhood.</p><p>Rethage et al. <ref type="bibr" target="#b27">[28]</ref> perform semantic segmentation on a voxelized point cloud and employ a PointNet architecture as a low-level feature extractor. The usage of a dense grid, however, leads to high memory usage and slow inference, requiring various seconds for medium-sized point clouds.</p><p>SplatNet <ref type="bibr" target="#b30">[31]</ref> is the work most closely related to ours. It alleviates the computational burden of 3D convolutions by using a sparse permutohedral lattice, performing convolutions only around the surfaces. It discretizes the space in uniform simplices and accumulates the features of the raw point cloud onto the vertices of the lattice using a splatting operation. Convolutions are applied on the lattice vertices and a slicing operation barycentrically interpolates the features of the vertices back onto the point cloud. A series of splat-convslice operations are applied to obtain contextual information. The main disadvantage is that splat and slice operations are not learned and repeated application slowly degrades the point clouds features as they act as Gaussian filters <ref type="bibr" target="#b0">[1]</ref>. Furthermore, storing high-dimensional features for each point in the cloud is memory intensive which limits the maximum number of points that can be processed. In contrast, our approach has learned operations for splatting and slicing which brings more representational power to the network. We also restrict their usage to only the beginning and the end of the network, leaving the rest of the architecture fully convolutional. Mesh networks: Mesh-based approaches operate on triangular or quadrilateral meshes. The connectivity information provided by the faces of the mesh allows to easily compute normal vectors and to establish local tangent planes.</p><p>GCNN <ref type="bibr" target="#b19">[20]</ref> operates on small local patches which are convolved using a series of rotated filters, followed by max pooling to deal with the ambiguity in the patch orientation. However, the max pooling disregards the orientation. MoNet <ref type="bibr" target="#b20">[21]</ref> deals with the orientation ambiguity by aligning the kernels to the principal curvature of the surface. Yet, this does not solve cases in which the local curvature is not informative, e.g. for walls or ceilings. TextureNet <ref type="bibr" target="#b15">[16]</ref> further improves on the idea by using a global 4-RoSy orientations field. This provides a smooth orientation field at any point on the surface which is aligned to the edges of the mesh and has only a 4-direction ambiguity. Defining convolution on patches oriented according to the 4-RoSy field yields significantly improved results. Graph networks: Graph-based approaches operate on vertices of a graph connected in an arbitrary topology, without the restrictions of triangular or quadrilateral meshes.</p><p>Wang et al. <ref type="bibr" target="#b34">[35]</ref> and Wu et al. <ref type="bibr" target="#b36">[37]</ref> define a convolution operator over non-grid structured data by having continuous values over the full vector space. The weights of these continuous filters are parametrized by an multi-layer perceptron (MLP).</p><p>Defferrard et al. <ref type="bibr" target="#b9">[10]</ref> formulate CNNs in the context of spectral graph theory. They define the convolution in the Fourier domain with Chebychev polynomials to obtain fast localized filters. However, spectral approaches are not directly transferable to a new graph as the Fourier basis changes. Additionally, the learned filters are rotation invariant which can be seen as a limitation to the representational power of the network.</p><p>Multi-view networks: The convolution operation is well defined in 2D and hence, there is an interest in casting 3D segmentation as a series of single-view segmentations which are fused together.</p><p>Pham et al. <ref type="bibr" target="#b23">[24]</ref> simultaneously reconstruct the scene geometry and recover the semantics by segmenting sequences of RGB-D frames. The segmentation is transferred from 2D images to the 3D world and fused with previous segmentations. A CRF finally resolves noisy predictions.</p><p>TangentConv <ref type="bibr" target="#b32">[33]</ref> assumes that the data is sampled from locally Euclidean surfaces and project the local surface geometry onto a tangent plane to which 2D convolutions can be applied. A heavy preprocessing step for normal calculation is required. In contrast, our approach can deal with raw point clouds without requiring normals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NOTATION</head><p>Throughout this paper, we use bold upper-case characters to denote matrices and bold lower-case characters to denote vectors.</p><p>The vertices of the d-dimensional permutohedral lattice are defined as a tuple v = (c v , x v ), with c v ∈ Z (d+1) denoting the coordinates of the vertex and x v ∈ R v d representing the values stored at vertex v. The full lattice containing n vertices is denoted with V = (C, X), with C ∈ Z n×(d+1) representing the coordinate matrix and X ∈ R n×v d the value matrix.</p><p>The points in a cloud are defined as a tuple p = (g p , f p ), with g p ∈ R d denoting the coordinates of the point and f p ∈ R f d representing the features stored at point p (color, normals, etc.). The full point cloud containing m points is denoted by P = (G, F) with G ∈ R m×d being the positions matrix and F ∈ R m×f d the feature matrix. The feature matrix F can also be empty in which case f d is set to zero.</p><p>We denote with I p the set of lattice vertices of the simplex that contains point p. The set I p always contains d+1 vertices as the lattice tessellates the space in uniform simplices with d + 1 vertices each. Furthermore, we denote with J v the set of points p for which vertex v is one of the vertices of the containing simplices. Hence, these are the points that contribute to vertex v through the splat operation.</p><p>We denote with S the splatting operation, with Y the slicing operation, withỸ the deformable slicing, with P the PointNet module, with D G and D F the distribution of the point positions and the points features, respectively, and with G the gathering operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PERMUTOHEDRAL LATTICE</head><p>The d-dimensional permutohedral lattice is formed by projecting the scaled regular grid (d + 1)Z d+1 along the vector</p><formula xml:id="formula_0">1 = [1, . . . , 1] onto the hyperplane H d : p · 1 = 0.</formula><p>The lattice tessellates the space into uniform d-dimensional simplices. Hence, for d = 2 the space is tessellated with triangles and for d = 3 into tetrahedra. The enclosing simplex of any point can be found by a simple rounding algorithm <ref type="bibr" target="#b0">[1]</ref>.</p><p>Due to the scaling and projection of the regular grid, the coordinates c v of each lattice vertex sum up to zero. Each vertex has 2(d + 1) immediate neighboring vertices. The coordinates of these neighbors are separated by a vector of</p><formula xml:id="formula_1">form ± [−1, . . . , −1, d, −1, . . . , −1] ∈ Z d+1 .</formula><p>The vertices of the permutohedral lattice are stored in a sparse manner using a hash map in which the key is the coordinate c v and the value is x v . Hence, we only allocate the simplices that contain the 3D surface of interest. This sparse allocation allows for efficient implementation of all typical operations in CNNs (convolution, pooling, transposed convolution, etc.).</p><p>The permutohedral lattice has several advantages w.r.t. standard cubic voxels. The number of vertices for each simplex is given by d+1 which scales linearly with increasing dimension, in contrast to the 2 d for standard voxels. This small number of vertices per simplex allows for fast splatting and slicing operations. Furthermore, splatting and slicing create piecewise linear outputs as they use barycentric interpolation. In contrast, standard quantization in cubic voxels create piecewise constant outputs, leading to discretization artefacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. METHOD</head><p>The input to our method is a point cloud P = (G, F) containing coordinates and per-point features.</p><p>We define the scale of the lattice by scaling the positions G as G s = G/σ σ σ, where σ σ σ ∈ R d is the scaling factor. The higher the sigma the less number of vertices will be needed to cover the point cloud and the coarser the lattice will be. For ease of notation, unless otherwise specified, we refer to G s as G as we usually only need the scaled version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Common Operations on Permutohedral Lattice</head><p>In this section we will explain in detail the standard operations on a permutohedral lattice that are used in previous works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref>. Splatting refers to the interpolation of point features onto the values of the lattice V using barycentric weighting ( <ref type="figure" target="#fig_2">Fig. 3a)</ref>. Each point splats onto d + 1 lattice vertices and their weighted features are summed onto the vertices.</p><p>Convolving operates analogously to standard spatial convolutions in 2D or 3D, i.e. a weighted sum of the vertex values together with its neighbors is computed. We use convolutions that span over the 1-hop ring around a vertex and hence convolve the values of 2(d + 1) + 1 vertices <ref type="figure" target="#fig_1">(Fig. 2)</ref>. Slicing is the inverse operation to splatting. The vertex values of the lattice are interpolated back for each position with the same weights used during splatting. The weighted contributions from the simplexes d + 1 vertices are summed up ( <ref type="figure" target="#fig_3">Fig. 5a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed Operations on Permutohedral Lattice</head><p>The operations defined in section Sec. V-A are typically used in a cascade of splat-conv-slice to obtain dense predictions <ref type="bibr" target="#b30">[31]</ref>. However, splatting and slicing act as Gaussian kernel low-pass filtering encoded information <ref type="bibr" target="#b0">[1]</ref>. Their repeated usage at every layer is detrimental to the accuracy of the network. Additionally, splatting acts as a weighted average on the feature vectors where the weights are only determined through barycentric interpolation. Including the weights as trainable parameter allows the network to decide on a better interpolation scheme. Furthermore, as the network grows deeper and feature vectors become higher-dimensional, slicing consumes increasingly more memory, as it assigns the features to the points. Since in most cases |P | |V |, it is more efficient to store the features only in the lattices vertices.</p><p>To address these limitations, we propose four new operators on the permutohedral lattice which are more suitable for CNNs and dense prediction tasks. Distribute is defined as the list of features that each lattice vertex receives. However, they are not summed as done by splatting:</p><formula xml:id="formula_2">x v = S(P, V ) = p∈Jv b pv f p ,<label>(1)</label></formula><p>where x v is the value of lattice vertex v and b pv is the barycentric weight between point p and lattice vertex v. Instead, our distribute operators D G and D F concatenate coordinates and features of the contributing points: where D vg ∈ R |Jv|×d and D v f ∈ R |Jv|×f d are matrices containing the distributed coordinates and features, respectively, for the contributing points into a vertex v. The matrices are concatenated and processed by a PointNet P to obtain the final vertex value x v . <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the difference between splatting and distributing. Note that we use a different distribute function for coordinates then for point features. For coordinates, we subtract the mean of the contributing coordinates. The intuition behind this is that coordinates by themselves are not very informative w.r.t. the potential semantic class. However, the local distribution is more informative as it gives a notion of the geometry. Downsampling refers to a coarsening of the lattice, by reducing the number of vertices. This allows the network to capture more contextual information. Downsampling consists of two steps: creation of a coarse lattice and obtaining its values. Coarse lattices are created by repeatedly dividing the point cloud positions by 2 and using them to create new lattice vertices <ref type="bibr" target="#b1">[2]</ref>. The values of the coarse lattice are obtained by convolving over the finer lattice from the previous level ( <ref type="figure">Fig. 4</ref>). Hence, we must embed the coarse lattice inside the finer one by scaling the coarse vertices by 2. Afterwards, the neighbors vertices over which we convolve are separated by a vector of form ± [−1, . . . , −1, d, −1, . . . , −1] ∈ Z d+1 . The downsampling operation effectively performs a strided convolution.</p><formula xml:id="formula_3">x v = P(D vg ; D v f ),<label>(2)</label></formula><formula xml:id="formula_4">D vg = D G (P, V ) = { g p − µ v | p ∈ J v },<label>(3)</label></formula><formula xml:id="formula_5">D v f = D F (P, V ) = { f p | p ∈ J v },<label>(4)</label></formula><formula xml:id="formula_6">µ v = 1 |J v | p∈Jv g p ,<label>(5)</label></formula><p>Upsampling follows a similar reasoning. The fine vertices need first to be embedded in the coarse lattice using a division by 2. Afterwards, the neighboring vertices over which we convolve are separated by a vector of form ± [−0.5, . . . , −0.5, d/2, −0.5, . . . , −0.5]. The careful reader will notice that in this case, the coordinates of the neighboring vertices may not be integer anymore; they may have a fractional part and will therefore lie in the middle of a coarser simplex. In this case we ignore the contribution of this neighboring vertices and only take the contribution of the center vertex. The upsampling operation effectively performs a transposed <ref type="figure">Fig. 4</ref>: Coarsen: Downsampling of the lattice is performed by embedding the coarse lattice in the finer one and convolving over the neighbors. This effectively performs a strided convolution. Transposed convolution is performed in an analogous manner by embedding a fine lattice into a coarse one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>convolution.</head><p>DeformSlicing: While the slicing operation Y barycentrically interpolates the values back to the points by using barycentric coordinates:</p><formula xml:id="formula_7">f p = Y(P, V ) = v∈Ip b pv x v ,<label>(6)</label></formula><p>we propose the DeformSlicingỸ which allows the network to directly modify the barycentric coordinates and shift the position within the simplex for data-dependent interpolation:</p><formula xml:id="formula_8">f p =Ỹ(P, V ) = v∈Ip (b pv + ∆b pv )x v .<label>(7)</label></formula><p>Here, ∆b pv are offsets that are applied to the original barycentric coordinates. A parallel branch within our network first gathers the values from all the vertices in a simplex and regresses the ∆b pv :</p><formula xml:id="formula_9">q p = G(P, V ) = { b pv x v | v ∈ I p }, (8) ∆b p = F(q p ),<label>(9)</label></formula><p>where q p is a set containing the weighted values of all the vertices of the simplex containing p and the prediction ∆b p = { ∆b pv | v ∈ I p } is a set of offsets to the barycentric coordinates towards the d + 1 vertices. With a slight abuse of notation -due to the fact that the vertices of a simplex are always enumerated in a consistent manner, we can regard b p and q p as vectors in R (d+1) and R (d+1)v d , respectively, and cast the prediction of offsets as a fully connected layer followed by a non-linearity:</p><formula xml:id="formula_10">∆b p = F(q p ) = σ(q p · W + b).<label>(10)</label></formula><p>However, this prediction has the disadvantage of not being permutation equivariant; therefore permutation of the vertices would not imply the same permutation in the barycentric offsets:</p><formula xml:id="formula_11">F(πq p ) = πF(q p ),<label>(11)</label></formula><p>where π is the set of all permutations of the d + 1 vertices. It is important for our prediction to be permutation equivariant because the vertices may be arranged in any order and the barycentric offsets need to keep a consistent preference In order for the prediction of the offsets to be consistent with permutations of the vertices, we take inspiration from the work of <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b39">[40]</ref> of equivariant layers and design F as:</p><formula xml:id="formula_12">∆b pv = σ(b + (b pv x v − max d∈Ip {b pd x d }) · W),<label>(12)</label></formula><formula xml:id="formula_13">∆b p = F(q p ) = { ∆b pv | v ∈ I p },<label>(13)</label></formula><p>where W ∈ R v d ×1 is a weight matrix and b ∈ R corresponds to a scalar bias. In other words, we subtract from each weighted vertex the maximum of the weighted values of all the other vertices in the simplex. Since the max operation is invariant to permutations of the input, the regression of the offsets is equivariant to permutations of the vertices.</p><p>The difference between the slicing and our DeformSlicing is visualized in <ref type="figure" target="#fig_3">Fig. 5</ref> VI. NETWORK ARCHITECTURE Input to our network is a point cloud P which may contain per-point features stored in F. The output is class probabilities for each point p.</p><p>Our network architecture has a U-Net structure <ref type="bibr" target="#b28">[29]</ref> and is visualized in <ref type="figure" target="#fig_4">Fig. 6</ref> together with the used individual blocks.</p><p>The first layers distribute the point features onto the lattice and use a PointNet to obtain local features. Afterwards, a series of ResNet blocks <ref type="bibr" target="#b12">[13]</ref>, followed by repeated downsampling, aggregates global context. The decoder branch mirrors the encoder architecture and upsamples through transposed convolutions. Finally, a DeformSlicing propagates lattice features onto the original point cloud. Skip connections are added by concatenating the encoder feature maps with matching decoder features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. IMPLEMENTATION</head><p>Our lattice is stored sparsely on a hash map structure, which allows for fast access of neighboring vertices. Unlike <ref type="bibr" target="#b30">[31]</ref>, we construct the hash map directly on the GPU, saving us from incurring an expensive CPU to GPU memory copy.</p><p>For memory savings, we implemented the DeformSlice and the last linear classification layer in one fused operation,  avoiding the storage of high-dimensional feature vectors for each point in the point cloud.</p><p>All of the lattice operators containing forwards and backwards passes are implemented on the GPU and exposed to PyTorch <ref type="bibr" target="#b22">[23]</ref>.</p><p>Following recent works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, all convolutions are preactivated using Group Normalization <ref type="bibr" target="#b37">[38]</ref> and a ReLU unit. We chose Group Normalization instead of the standard batch normalization because it is more stable when the batch size is small. We use the default of 32 groups.</p><p>The models were trained using the Adam optimizer, using a learning rate of 0.001 and a weight decay of 10 −4 . The learning rate was reduced by a factor of 10 when the loss plateaued.</p><p>We share the PyTorch implementation of LatticeNet at https://github.com/AIS-Bonn/lattice net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. EXPERIMENTS</head><p>We evaluate our proposed lattice network on three different datasets: ShapeNet <ref type="bibr" target="#b38">[39]</ref>, ScanNet <ref type="bibr" target="#b8">[9]</ref> and SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>. For the task of semantic segmentation we report the mean Intersection over Union (mIoU). We use a shallow model for ShapeNet and a deeper model for ScanNet and SemanticKITTI as the datasets are larger. We augment all data using random mirroring and translations in space. For ScanNet, we also apply random color jitter. A video with additional footage of the experiments is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation of Segmentation Accuracy</head><p>ShapeNet part segmentation is a subset of the ShapeNet dataset <ref type="bibr" target="#b38">[39]</ref> which contains objects from 16 different categories each segmented into 2 -6 parts. The dataset consists of points sampled from the surface of the objects, together with the ground truth label of the corresponding object part. The objects have an average of 2613 points. We train and evaluate our network on each object individually. The results for our method and five competing methods are gathered in Tab. I and visualized in <ref type="figure">Fig. 8</ref>.</p><p>We observe that for some classes, we obtain state-of-theart performance and for other objects, the IoU is slightly lower than for other approaches. We ascribe this to the fact that training one fixed architecture size for each individual object is suboptimal as some objects like the "cap" have as few as 55 examples while others like the table have more than 5K. This causes the network to be prone for overfitting on the easy object or underfitting on the difficult ones. A fair evaluation would require finding an architecture that performs well for all objects on average. However due to various issues with mislabeled ground truths <ref type="bibr" target="#b30">[31]</ref> we deem that experimentation with more architectures or with different regularization strengths for individual objects would overfit the dataset. ScanNet 3D segmentation <ref type="bibr" target="#b8">[9]</ref> consists of 3D reconstructions of real rooms. It contains ≈ 1500 rooms segmented into 20 classes (bed, furniture, wall, etc.). The rooms have between 9K and 537K points -on average 145K. We segment an entire room at once without cropping.</p><p>We obtain an IoU of 64.0 which is significantly higher than the most similar related work of SplatNet. It is to be noted that MinkowskiNet achieves a higher IoU but at the expense of an extremely high spatial resolution of 2 cm per voxel. In contrast, our approach allocates lattice vertices so that each vertex covers approximately 30 points. On this dataset, this corresponds to a spatial extent of approximately 10 cm. SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> contains semantically annotated scans from the KITTI dataset which consists of laser scans from real urban environments. The scans are annotated with a total of 19 classes and each scan contains between 82K and 129K points. We process each scan entirely without any cropping. The results are provided in Tab. II. Our LatticeNet outperforms all other methods -in case of the most similar SplatNet by more than a factor of two. It is to be noted that DarkNet53Seg <ref type="bibr" target="#b2">[3]</ref>, DarkNet21Seg <ref type="bibr" target="#b2">[3]</ref> and SqueezeSegV2 <ref type="bibr" target="#b35">[36]</ref> are methods that operate on a 2D image by wrapping the laser scans to 2D using spherical coordinates. In contrast, our method can operate on general point clouds, directly in 3D. Bonn Activity Maps <ref type="bibr" target="#b31">[32]</ref> is a dataset for human tracking, activity recognition and anticipation of multiple persons. It contains annotations of persons, their trajectories and activities. The 3D reconstruction of the four kitchen scenarios is however 1 http://www.ais.uni-bonn.de/videos/RSS 2020 Rosu/ <ref type="figure">Fig. 7</ref>: Bonn Activity Maps segmentations. Colored meshes are reconstructed from KinectV2 data using volumetric integration <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref> and semantically segmented using LatticeNet. Color coding of semantic labels corresponds to the ScanNet dataset <ref type="bibr" target="#b8">[9]</ref>.</p><p>Ground-truth Prediction <ref type="figure">Fig. 8</ref>: ShapeNet <ref type="bibr" target="#b38">[39]</ref> results of our method.</p><p>of more interest to us. The environments are reconstructed as 3D colored meshes and have no ground truth semantic annotations. We trained our LatticeNet on the ScanNet dataset and evaluate it on the 4 kitchens in order to provide an annotation for each vertex of the mesh. The results are shown in <ref type="figure">Fig. 7</ref>. We can observe that our network generalizes well to unseen datasets, recorded with different sensors and with different noise properties as the semantic segmentations look plausible and exhibit sharp borders between classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies</head><p>We perform various ablations regarding our contribution to judge how much they affect the network's performance. DeformSlice. We assess the impact that DeformSlice has on the network by comparing it with the Slice operator which does not use learned barycentric interpolation. We evaluate it on the SemanticKITTI, the largest dataset that we are using.</p><p>We also evaluate a version of DeformSlice which ensures that the new barycentric coordinates still sum up to one by adding an additional loss term:  <ref type="figure">Fig. 9</ref>: SemanticKITTI results. We compare the prediction from our LatticeNet with the results from TangentConv <ref type="bibr" target="#b32">[33]</ref> and SplatNet <ref type="bibr" target="#b30">[31]</ref>. We can observe that our approach can better learn small objects like tree trunks, despite their relatively small number of points. Additionally, the network also effectively makes use of contextual information in order to correctly predict the parking place due to the existence of nearby cars.    However, we observe little change after adding this regularization term and hence use the default version of DeformSlice for the rest of the experiments.</p><formula xml:id="formula_14">L = 1 |P | p∈P   v∈Ip ∆b pv   2 .<label>(14)</label></formula><p>The results are gathered in Tab. V. Distribute and PointNet. Another contribution of our work is the usage of a Distribute operator to provide values to the lattice vertices which are later embedded and in a higherdimensional space by a PointNet-like architecture. The positions and features of the point cloud are treated separately where the features (normals, color) are distributed directly. From the positions, we substract the locally averaged position as we assume that the local point distribution is more important than the coordinates in the global reference frame. We evaluate the impact of elevating the point features to a higherdimensional space and subtracting the local mean against a simple splatting operator which just averages the features of the points around each corresponding vertex.</p><p>We observe that not subtracting the local mean, and just using the xyz coordinates as features, heavily degrades the performance, causing the IoU to drop from 52.9 to 43.0. This further reinforces the idea that the local point distribution is a good local feature to use in the first layers of the network.</p><p>Not elevating the point cloud features to a higherdimensional space before applying the max-pool operation also hurts performance but not as severely. In our experiments, we elevate the features to 64 dimensions by using a series of fully connected layers.</p><p>Finally, naively performing a splat operation performs worst with a mere 37.8 IoU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance</head><p>We report the time taken for a forward pass and the maximum memory used in our shallow and deep network on the three evaluated datasets. The performance was measured on a NVIDIA Titan X Pascal and the results are gathered in Tab. IV.</p><p>Despite the reduced memory usage compared to SplatNet and increased speed of execution, there are still memory savings possible by fusing the Distribute and PointNet operators into one GPU operation. This is similar to fusing our Deform-Slice and the classification layer. Additionally, we expect the network to become even faster as further advances on highly optimized kernels for convolution on sparse lattices become available. At the moment, the convolutions are performed by our custom CUDA kernels. Tighter integration however with highly optimized libraries like cuDNN <ref type="bibr" target="#b5">[6]</ref> could be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>We presented LatticeNet, a novel method for semantic segmentation of point clouds. A sparse permutohedral lattice allows us to efficiently process large point clouds. The usage of PointNet together with a data-dependent interpolation alleviates the quantization issues of other methods. Experiments on three datasets show state-of-the-art results, at a reduced time and memory budget. In the future, we would like to incorporate temporal information into our model in order to process sequential data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Semantic segmentation: LatticeNet takes raw point clouds as input and embeds them into a sparse lattice where convolutions are applied. Features on the lattice are projected back onto the point cloud to yield a final segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Convolution: The neighboring vertices of a lattice are convolved similarly to standard 2D convolutions. If a neighbor is not allocated in the sparse structure, we assume that it has a value of zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Splat and Distribute operations: Splatting uses barycentric weighting to add the features of points onto neighboring vertices. The naïve summation can be detrimental to the network as splatting acts as a Gaussian filter. Distributing stores all the features of the contributing points, causing no loss of information and allows further processing by the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Slice and DeformSlice: Slicing barycentrically interpolates the vertex values back onto a point. DeformSlice allows for the network to directly affect the interpolated value by learning offsets of the barycentric coordinates. towards a certain vertexes features, regardless of its position within a simplex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Architecture: Our model follows a U-Net structure. For ease of representation, blocks which are repeated one after another are indicated with a multiplier on the right side of the operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>ScanNet results. The left image shows the ground truth and the right one our prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Results on ShapeNet part segmentation<ref type="bibr" target="#b38">[39]</ref>.</figDesc><table><row><cell>#instances</cell><cell></cell><cell>2690 76 55 898 3758</cell><cell>69</cell><cell>787</cell><cell>392 1547 451</cell><cell>202</cell><cell>184 283</cell><cell>66</cell><cell>152 5271</cell></row><row><cell></cell><cell cols="9">instance air-bag cap car chair ear-guitar knife lamp laptop motor-mug pistol rocket skate-table</cell></row><row><cell></cell><cell cols="2">avg. plane</cell><cell>phone</cell><cell></cell><cell></cell><cell>bike</cell><cell></cell><cell></cell><cell>board</cell></row><row><cell>PointNet [25]</cell><cell>83.7</cell><cell cols="2">83.4 78.7 82.5 74.9 89.6 73.0</cell><cell cols="2">91.5 85.9 80.8 95.3</cell><cell cols="3">65.2 93.0 81.2 57.9</cell><cell>72.8 80.6</cell></row><row><cell>PointNet++ [26]</cell><cell>85.1</cell><cell cols="2">82.4 79.0 87.7 77.3 90.8 71.8</cell><cell cols="2">91.0 85.9 83.7 95.3</cell><cell cols="3">71.6 94.1 81.3 58.7</cell><cell>76.4 82.6</cell></row><row><cell>SplatNet 3D [31]</cell><cell>84.6</cell><cell cols="2">81.9 83.9 88.6 79.5 90.1 73.5</cell><cell cols="2">91.3 84.7 84.5 96.3</cell><cell cols="3">69.7 95.0 81.7 59.2</cell><cell>70.4 81.3</cell></row><row><cell>SplatNet 2D-3D [31]</cell><cell>85.4</cell><cell cols="2">83.2 84.3 89.1 80.3 90.7 75.5</cell><cell cols="2">92.1 87.1 83.9 96.3</cell><cell cols="3">75.6 95.8 83.8 64.0</cell><cell>75.5 81.8</cell></row><row><cell>FCPN [28]</cell><cell>84.0</cell><cell cols="2">84.0 82.8 86.4 88.3 83.3 73.6</cell><cell cols="2">93.4 87.4 77.4 97.7</cell><cell cols="2">81.4 95.8 87.7</cell><cell>68.4</cell><cell>83.6 73.4</cell></row><row><cell>Ours</cell><cell>83.9</cell><cell cols="2">82.3 84.8 79.1 81.0 86.9 71.0</cell><cell cols="2">91.9 89.4 84.7 96.6</cell><cell cols="2">77.2 95.8 86.0</cell><cell>70.5</cell><cell>79.3 87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Results on SemanticKITTI [3]. 84.1 86.4 25.5 24.5 32.7 22.6 78.3 50.1 64.0 36.2 33.6 4.7 55.0 38.9 52.2 Ours 52.9 90.0 74.1 59.4 22.0 88.2 92.9 26.6 16.6 22.2 21.4 81.7 63.6 63.1 35.6 43.0 46.0 58.8 51.9 48.4</figDesc><table><row><cell>Approach</cell><cell>mIoU</cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other-ground</cell><cell>building</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicle</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>fence</cell><cell>pole</cell><cell>traffic sign</cell></row><row><cell>PointNet [25]</cell><cell>14.6</cell><cell cols="19">61.6 35.7 15.8 1.4 41.4 46.3 0.1 1.3 0.3 0.8 31.0 4.6 17.6 0.2 0.2 0.0 12.9 2.4 3.7</cell></row><row><cell>SplatNet [31]</cell><cell>18.4</cell><cell cols="19">64.6 39.1 0.4 0.0 58.3 58.2 0.0 0.0 0.0 0.0 71.1 9.9 19.3 0.0 0.0 0.0 23.1 5.6 0.0</cell></row><row><cell>PointNet++ [26]</cell><cell>20.1</cell><cell cols="19">72.0 41.8 18.7 5.6 62.3 53.7 0.9 1.9 0.2 0.2 46.5 13.8 30.0 0.9 1.0 0.0 16.9 6.0 8.9</cell></row><row><cell cols="2">Minkowski34(25cm) [7] 33.0</cell><cell cols="19">80.8 43.0 36.9 0.5 73.5 83.0 42.9 2.0 2.9 7.8 74.4 42.9 36.7 11.2 22.8 4.4 37.2 35.4 28.6</cell></row><row><cell>SqueezeSegV2 [36]</cell><cell>39.7</cell><cell cols="19">88.6 67.6 45.8 17.7 73.7 81.8 13.4 18.5 17.9 14.0 71.8 35.8 60.2 20.1 25.1 3.9 41.1 20.2 36.3</cell></row><row><cell>TangentConv [33]</cell><cell>40.9</cell><cell cols="19">83.9 63.9 33.4 15.4 83.4 90.8 15.2 2.7 16.5 12.1 79.5 49.3 58.1 23.0 28.4 8.1 49.0 35.8 28.5</cell></row><row><cell>DarkNet21Seg [3]</cell><cell>47.4</cell><cell cols="19">91.4 74.0 57.0 26.4 81.9 85.4 18.6 26.2 26.5 15.6 77.6 48.4 63.6 31.8 33.6 4.0 52.3 36.0 50.0</cell></row><row><cell>DarkNet53Seg [3]</cell><cell>49.9</cell><cell cols="4">91.8 74.6 64.8 27.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III</head><label>III</label><figDesc>Ours 64.0 † : post-CVPR submissions.</figDesc><table><row><cell>: Results on ScanNet [9]</cell></row><row><cell>Method mIOU</cell></row><row><cell>PointNet++ [26] 33.9</cell></row><row><cell>SplatNet [31] 39.3</cell></row><row><cell>TangetConv [33] 43.8</cell></row><row><cell>3DMV  ‡ [8] 48.4</cell></row><row><cell>MinkowskiNet42 (5cm) [7] 67.9</cell></row><row><cell>SparseConvNet [11]  † 72.5</cell></row><row><cell>MinkowskiNet42 (2cm) [7]  † 73.4</cell></row></table><note>‡ : uses 2D images additionally.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Average time used by the forward pass and the maximum memory used during training. An X indicates a method that failed to process the whole cloud due to memory limitations.</figDesc><table><row><cell></cell><cell cols="2">ShapeNet</cell><cell cols="2">ScanNet</cell><cell cols="2">SemanticKITTI</cell></row><row><cell></cell><cell cols="2">[ms] [GB]</cell><cell cols="2">[ms] [GB]</cell><cell cols="2">[ms] [GB]</cell></row><row><cell>SplatNet</cell><cell>129</cell><cell>0.6</cell><cell>X</cell><cell>X</cell><cell>2931</cell><cell>8.9</cell></row><row><cell>Ours</cell><cell>49</cell><cell>0.5</cell><cell>180</cell><cell>6.5</cell><cell>143</cell><cell>3.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Ablation study of the various components of LatticeNet. Various features are disabled (indicated in red) and the impact to the IoU is evaluated.</figDesc><table><row><cell></cell><cell>mIoU</cell><cell>LocalAvg</cell><cell>PointNet elevate</cell><cell>DeformSlice</cell><cell>Offsets zero sum</cell></row><row><cell>LN splat</cell><cell>37.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LN no local avg</cell><cell>43.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LN no elevate</cell><cell>46.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LN slice</cell><cell>50.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LN reg</cell><cell>52.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LatticeNet</cell><cell>52.9</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some useful properties of the permutohedral lattice for Gaussian filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">other words</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast bilateral-space stereo for synthetic defocus Supplemental material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Yichang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conference on Computer Vision (ICCV)</title>
		<meeting>of the IEEE Int. Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Sven Behnke, Cyrill Stachniss, and Juergen Gall</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04302</idno>
		<title level="m">Deep RBFNet: Point cloud feature learning using radial basis functions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08755</idno>
		<title level="m">4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3DMV: Joint 3D-multi-view prediction for 3D semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="452" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>of the Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Largescale Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3254" to="3263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TextureNet: Consistent local parametrizations for learning from high-resolution signals on meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4440" to="4449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on xtransformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>of the Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Proc. of the IEEE Int. Conference on Computer Vision (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time 3D reconstruction at scale using voxel hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic Differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-time progressive 3D semantic segmentation for indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Workshop on Applications of Computer Vision</title>
		<meeting>of the IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1089" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>of the Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04500</idno>
		<title level="m">Deep Learning with Sets and Point Clouds</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully-convolutional point networks for large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="596" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient 3D Reconstruction and Streaming for Group-Scale Multi-Client Live Telepresence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Stotko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Krumpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Symposium on Mixed and Augmented Reality (ISMAR)</title>
		<meeting>of the IEEE Int. Symposium on Mixed and Augmented Reality (ISMAR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SplatNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Tanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Oh-Hun Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stotko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Errami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maren</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Bennewitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06354</idno>
		<title level="m">Bonn Activity Maps: Dataset Description</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SEGCloud: Semantic segmentation of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">SqueezeSegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08495</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PointConv: Deep convolutional networks on 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>of the Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
