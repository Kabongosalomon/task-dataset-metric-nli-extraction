<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Graph Convolutional Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-27">27 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Ruiz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Gama</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
						</author>
						<title level="a" type="main">Gated Graph Convolutional Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-27">27 Jun 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-graph neural networks</term>
					<term>recurrent neural net- works</term>
					<term>gating</term>
					<term>graph processes</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph processes model a number of important problems such as identifying the epicenter of an earthquake or predicting weather. In this paper, we propose a Graph Convolutional Recurrent Neural Network (GCRNN) architecture specifically tailored to deal with these problems. GCRNNs use convolutional filter banks to keep the number of trainable parameters independent of the size of the graph and of the time sequences considered. We also put forward Gated GCRNNs, a time-gated variation of GCRNNs akin to LSTMs. When compared with GNNs and another graph recurrent architecture in experiments using both synthetic and real-word data, GCRNNs significantly improve performance while using considerably less parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To deal with these scenarios, we introduce a Graph Convolutional Recurrent Neural Network (GCRNN) architecture where the hidden state is a graph signal computed from the input and the previous state using banks of graph convolutional filters and, as such, stored individually at each node. In addition to being local, in GCRNNs the number of parameters to learn is independent of time because the graph filters that process the input and the state are the same at every time instant. GCRNNs can take in graph processes of any duration, which gives control over how frequently gradient updates occur. They can also learn many different representations: a signal (whether supported on a graph or not) or a sequence of signals; a class label or a sequence of labels. While other graph-based recurrent architectures have been proposed in <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>, they are limited to representing sequences of graph signals and, in general, problem specific (most commonly to traffic forecasting). A fourth graph recurrent formulation has been introduced in <ref type="bibr" target="#b16">[16]</ref>, but it uses recurrence as a way of re-introducing the input data at each layer to capture multiple types of diffusion, and as such does not operate on graph processes. In this architecture, the number of learnable parameters also depends on the number of recurrent layers.</p><p>Our GCRNN architecture is further extended to include time gating variables analogous to the input and forget gates of Long Short Term Memory units (LSTMs) <ref type="bibr" target="#b8">[9]</ref>, which are also implemented using GCRNNs. The objective of gating is twofold: on the input side, to control the importance given to new information, and on the state side, how much of the stored past information the model should "forget".</p><p>GCRNNs' ability to learn both graph and time dependencies and the importance of the gating mechanism for long input sequences are demonstrated in experiments on synthetic and real-world data. GCRNNs are compared with basic GNNs and with the DCRNN, a gated graph recurrent architecture from the existing literature <ref type="bibr" target="#b12">[13]</ref>. Numerical results show that: (i) GCRNNs largely improve upon GNNs when processing sequential graph data, and (ii) our model uses considerable less parameters and achieves better performance than the DCRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. GRAPH PROCESSES</head><p>Let G = (V, E, W) be a graph where V is a set of N nodes, E ⊆ V × V is the set of edges and W : E → R is a function that assigns weights to each edge. The topology of the graph G can be described by a matrix S ∈ R N ×N that captures the sparsity pattern of its structure, i.e., [S] ij = s ij can be nonzero only if (j, i) ∈ E or j = i. Typical choices for this matrix in the literature are the adjacency <ref type="bibr" target="#b17">[17]</ref>, the Laplacian <ref type="bibr" target="#b18">[18]</ref>, the random walk <ref type="bibr" target="#b19">[19]</ref> and their normalized counterparts <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>. We define the neighborhood of node i as N i = {j ∈ V : (j, i) ∈ E}, and use the expression local operations to refer to operations that can be computed by successive interactions of a node i with its neighborhood N i .</p><p>We model data as graph signals: a graph signal x : V → R is such that each element [x] i = x i corresponds to the value of the graph signal at node i ∈ V. The most basic interaction between the graph signal x and the graph G is given by the operation Sx, where, for each i = 1, . . . , N , we have</p><formula xml:id="formula_0">[Sx] i = N j=1 [S] ij [x] j = j∈Ni s ij x j .<label>(1)</label></formula><p>The operation described by (1) is local, because its output can be computed by interacting only with N i due to the sparsity pattern of S. The matrix S has the effect of shifting around the graph the information contained in its nodes and is henceforth called the graph shift operator (GSO). We are interested in graph signals that change with time over the same graph support, typically known as graph processes <ref type="bibr" target="#b9">[10]</ref>. A graph process {x t } t∈N0 is a sequence of graph signals x t ∈ R N , which are all defined over the same graph support, x t : V → R. More often than not, there exists some dependency relationship between graph signals at different time instants. This (causal) dependency can be described, generically, by</p><formula xml:id="formula_1">x t = f (x t−1 , x t−2 , . . .)</formula><p>for some function f that in practice is usually unknown.</p><p>The graph process is typically accompanied by some target representation Y relevant to the task at hand, which gives rise to pairs ({x t }, Y). In regression problems, the target representation is usually another sequence {y t }, while in classification problems it is a single element y characterizing the sequence over some time interval. The general objective of learning over sequences is to obtain a meaningful estimateŶ of the target representation Y using {x t }. To do this, we propose a novel architecture that we call the Graph Convolutional Recurrent Neural Network (GCRNN).</p><p>To enhance the descriptive capabilities of our data model, in what follows we consider sequences comprised of F different features x f t for f = 1, . . . , F , where each x f t ∈ R N is a graph signal. A more compact representation is given by the matrix X t ∈ R N ×F , where each column x f t ∈ R N is a graph signal (f = 1, . . . , F ) and each rowx i t ∈ R F gathers the feature values collected at a single node (i = 1, . . . , N )</p><formula xml:id="formula_2">X t = x 1 t , . . . , x F t =    (x 1 t ) T . . . (x N t ) T    .<label>(2)</label></formula><p>While we have defined a local operation on x f t as one that respects the sparsity of the graph [cf. (1)], we note that any operation onx i t can be called local as well, since it involves values that are already available at that node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRAPH CONVOLUTIONAL RECURRENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NEURAL NETWORKS</head><p>A recurrent neural network (RNN) approximates the temporal dependencies of a sequence {x t } using a hidden Markov model, i.e. x t+1 ≈ g(x t , h t ) for some function g and some hidden state sequence {h t }. The hidden state sequence is</p><formula xml:id="formula_3">h t = σ (Ax t + Bh t−1 ) ,<label>(3)</label></formula><p>where A and B are linear transforms and σ is a nonlinear function, so as to endow the RNN (3) with higher descriptive power. The representation estimateŶ can then be obtained from these hidden states. For instance, in a regression problem, we would haveŷ t = ρ(Ch t ) with ρ a nonlinear function and C a linear transform, and in a classification problem,ŷ = ρ(Ch T ) for the state h T computed after some interval T . The parameters of the linear transforms A, B and C can be learned by minimizing some loss function L(Y,Ŷ) over a training set T = {({x t }, Y)}. We note that the learned linear transforms are the same for all t, giving the RNN (3) enough flexibility to adapt to sequences of different length. Likewise, the number of parameters is independent of the length of the sequence. The hidden state h t is expected to store all the past information that is relevant for estimating the target representation.</p><p>The knowledge that the sequence {x t } is comprised of graph signals defined over the same graph G with GSO S can be exploited by forcing the linear transforms A and B to account for this structure,</p><formula xml:id="formula_4">h t = σ A(S)x t + B(S)h t−1 .<label>(4)</label></formula><p>We name model (4) a Graph Recurrent Neural Network (GRNN). A particularly compelling parametrization of the linear transforms A(S) and B(S) is given by banks of linear shift-invariant graph filters (LSI-GFs). LSI-GFs don the GRNN model (4) with convolutional characteristics, since they are permutation-invariant local operations and make the number of learnable parameters independent of the size of the graph. More precisely, let the hidden state be described by D features, where each feature h d t ∈ R N is a graph signal. We define the Graph Convolutional Recurrent Neural Network (GCRNN) as</p><formula xml:id="formula_5">h d t = σ   F f =1 A df (S)x f t + D d ′ =1 B dd ′ (S)h d ′ t−1   ,<label>(5)</label></formula><p>where A df (S) and B dd ′ (S) are the LSI-GFs</p><formula xml:id="formula_6">A df (S) = K−1 k=0 a df k S k , B dd ′ (S) = K−1 k=0 b dd ′ k S k ,<label>(6)</label></formula><p>for d, d ′ = 1, . . . , D and f = 1, . . . , F . The filter taps</p><formula xml:id="formula_7">{a df k } K−1 k=0 and {b dd ′ k } K−1 k=0</formula><p>are the learnable parameters of the linear transform. Note that there are DF K + D 2 K such parameters and that their number is independent of the sequence length and of the size of the graph N . Another primary feature of GCRNNs is that the LSI-GFs (6) are local operations, since they can be computed by K − 1 successive interactions with the neighbors of each node. The capacity of GCRNNs can be further increased (while maintaining the convolutional characteristics) by using graph convolutional neural networks <ref type="bibr" target="#b4">[5]</ref> with several layers in place of A(S) and B(S) in <ref type="bibr" target="#b3">(4)</ref>.</p><p>Describing the hidden states h d t as a collection of D graph signals lets us again exploit the graph structure in the computation of the target representationŶ. Let H t ∈ R N ×D be a matrix where the hidden states h d t ∈ R N are its columns,</p><formula xml:id="formula_8">H t = h 1 t , . . . , h D t =    (h 1 t ) T . . . (h N t ) T    ,<label>(7)</label></formula><p>and where the rows of H t collect the D features at node i,</p><formula xml:id="formula_9">h i t ∈ R D for i = 1, . . . , N .</formula><p>The estimated representation is computed asŷ t = ρ(C(S)H t ) for the regression problem andŷ = ρ(C(S)H T ) for the classification problem. Operation C(S) can be replaced by a graph convolutional neural network to exploit locality, followed by a fully connected layer to adapt dimensions when mapping H t toŷ t orŷ.</p><p>The regression problem where the target representation sequence {y t } is a sequence of graph signals y g t ∈ R N , with g = 1, . . . , G denoting different features, is of particular interest, as it allows for two possible local models to computê y g t . The first possibility is to apply a LSI-GF to h d t ,</p><formula xml:id="formula_10">y g t = ρ D d=1 C gd (S)h d t , C gd (S) = K−1 k=0 c gd k S k ,<label>(8)</label></formula><p>which demands K − 1 successive interactions with the neighbors of each node. This requires DGK learnable parameters. Alternatively, the second possibility is to estimate the target features at each node by applying a linear transformation to its own features,</p><formula xml:id="formula_11">ỹ i t = Ch i t<label>(9)</label></formula><p>where C ∈ R G×D , for each i = 1, . . . , N . This alternative entails no neighbor interactions sinceh i t is stored at node i, and requires only DG parameters if the same linear transform C is learned for all nodes.</p><p>IV. GATING Deeper RNNs allow taking long term dependencies into account, but this often comes with the challenge of vanishing gradients as long term interactions get exponentially smaller weights at each training step <ref type="bibr" target="#b8">[9]</ref>. This is addressed by time gating mechanisms such as the input and forget gates of Long Short-Term Memory units (LSTMs) and the reset and update gates of Gated Recurrent Units (GRUs). These gates are essentially variables taking values between 0 and 1 -each one estimated by their own neural network model -that multiply either or both the input and the state to control the amount of information passed through with time.</p><p>Time gating can be readily extended to the context of graph processes. Based on the GCRNN model (5), we define a timegated architecture as follows,</p><formula xml:id="formula_12">h d t = σ α t F f =1 A f d (S)x f t + β t D d ′ =1 B d ′ d (S)h d ′ t−1 ,<label>(10)</label></formula><p>where the parameter α t ∈ [0, 1] is the external input gate, and β t ∈ [0, 1] is the forget gate. We call model (10) a Gated Graph Convolutional Recurrent Neural Network (GGCRNN). Note that α t adjusts the importance given to the external inputs {x f t } F f =1 at time t and β t controls how much the GGCRNN (10) will forget (or, equivalently, remember) from the hidden states {h d t } D d=1 . Each gate is calculated as the output of a new GCRNN (5) followed by a fully connected layer. To ensure that α t and β t are well within the unit interval, a sigmoid activation function follows the fully connected layer computations. More precisely, let µ u t ∈ R N be the U graph signals that describe the state of the input gate (alternatively, letμ i t ∈ R U be the U internal values stored at node i that determine the state of the input gate). These states are updated as</p><formula xml:id="formula_13">µ u t = ξ   F f =1 Γ uf (S)x f t + U u=1 ∆ uu ′ (S)µ u ′ t−1   ,<label>(11)</label></formula><p>where ξ is a nonlinear function, and Γ uf (S) and ∆ uu ′ (S) are LSI-GFs, cf. <ref type="bibr" target="#b5">(6)</ref>. The value α t ∈ [0, 1] of the input gate is then calculated by projecting the states {µ u t } U u=1 of the input gate onto a learned vector ω ∈ R N U and applying a sigmoid</p><formula xml:id="formula_14">α t = sigmoid(ω T [(µ 1 t ) T , . . . , (µ U t ) T ] T ) .<label>(12)</label></formula><p>Analogously, let ν v t ∈ R N be V graph signals that make up the state of the forget gate. These states are updated as</p><formula xml:id="formula_15">ν v t = η   F f =1 Φ vf (S)x f t + V v ′ =1 Ψ vv ′ (S)ν v ′ t−1   .<label>(13)</label></formula><p>with η a nonlinear function, and Φ vf (S) and Ψ vv ′ (S) collections of LSI-GFs, cf. <ref type="bibr" target="#b5">(6)</ref>. Then, the forget gate β t ∈ [0, 1] is computed as</p><formula xml:id="formula_16">β t = sigmoid(τ T [(ν 1 t ) T , . . . , (ν V t ) T ] T )<label>(14)</label></formula><p>for some learned τ ∈ R N V . Notice that α t and β t are the same for every node and every feature, but vary with time. This allows exploiting local operations through the graph filters Γ, ∆, Φ and Ψ, and keeps the number of learnable parameters under control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. NUMERICAL EXPERIMENTS</head><p>In this section, we present numerical results obtained using multiple variations of our GCRNN architecture in a synthetic experiment -ten-step prediction -and a classification problem involving real seismic data. All simulated architectures consist of a 1-layer GCRNN <ref type="bibr" target="#b4">(5)</ref> or GGCRNN (10) and an output neural network mapping the state H to the target representation Y, which is either a GNN or a localized multi-layer perceptron that mixes each node's local features individually, cf. <ref type="bibr" target="#b8">(9)</ref>. In all graph filters, the GSO is the adjacency matrix. The activation function in the GCRNNs is always the hyperbolic tangent, and in the intermediate layers of the output neural network it is the ReLU. In all experiments, the LSI-GFs (6) have K = 4 filter (a) (b) <ref type="figure">Figure 1</ref>: Test loss (mean absolute error) in 10-step prediction. Average loss and standard deviation on 10 different graphs and diffusion datasets. (a) Test loss for a GNN with 2 convolutional layers (right), a GCRNN followed by a GNN (center) and a Gated GCRNN followed by a GNN (left). (b) Test loss for a GCRNN followed by nodewise MLPs with shared parameters (right), the DCRNN from <ref type="bibr" target="#b12">[13]</ref> (center) and the gated GCRNN with localized MLPs (left).</p><p>taps. If a GCRNN is gated, the GCRNNs used to compute its input and forget gates have state variables with U = V = D features and K filter taps as well, and are always followed by output neural networks that are full MLPs with a total of N D parameters. All architectures were optimized using ADAM <ref type="bibr" target="#b20">[20]</ref> with decaying factors β 1 = 0.9 and β 2 = 0.999.</p><p>Ten-step prediction. In this experiment, we consider a stochastic block model (SBM) graph G with N = 20 nodes, 4 communities and intra and inter-community edge probabilities of 0.8 and 0.2, respectively. Let S be the GSO of G. Given an initial graph signal x 0 ∈ R N , 0 ≤ [x 0 ] i ≤ 1, the diffused signals x t , t = 1, 2, . . ., are generated as</p><formula xml:id="formula_17">x t = Sx t−1 + w t ,<label>(15)</label></formula><p>where w t is a zero mean gaussian noise that can be correlated both in time and in between nodes. In particular, we chose σ 2 time = 0.01 for the variance across time and σ 2 nodes = 0.01 for the variance across nodes. Likewise, we set the crosscorrelation factors across time and nodes to ρ time = ρ nodes = 0.1. Fixing the input sequence length to T = 10, the 10-step prediction problem consists of estimating x 10 , x 11 , . . . , x 19 from x 0 , x 1 , . . . , x 9 .</p><p>We consider 4 GCRNN architectures. The first two are a GCRNN and a Gated GCRNN with D = 10 state features whose output neural network is a 1-layer GNN with K = 4 filter taps and without fully connected layers. The total number of parameters in these architectures are 480 and 1, 760 (480 for the main GCRNN architecture, and 640 for those of each gate) respectively. The baseline for comparison is a GNN with 2 convolutional layers and no fully connected layer containing 480 parameters, the same as the non-gated GCRNN.</p><p>The other two architectures are a GCRNN and a Gated GCRNN where the output neural network is a localized MLP, consisting of the same D-parameter MLP per node. These were compared with the Diffusion Convolutional Recurrent Neural Network (DCRNN) from <ref type="bibr" target="#b12">[13]</ref> with 1 recurrent layer, 10 recurrent units and diffusion length 4. The number of parameters of the simple GCRNN, the Gated GCRNN and the DCRNN were 450, 1, 730 and 3, 370 respectively.</p><p>All of these architectures were used to simulate the 10step prediction problem in 10 rounds, with 10 different graphs and dataset realizations. In all rounds, the training, validation and test sets comprised 10, 000, 2, 400 and 200 samples, respectively. All models were trained to optimize the L1 loss (mean absolute error) in 5 training epochs, with batch size 100 and learning rate 0.001 for the GCRNNs followed by GNNs and 0.005 for those followed by localized perceptrons. The average test losses and corresponding standard deviations for each architecture are shown in <ref type="figure">Figure 1</ref>. The main takeaway from <ref type="figure">Figure 1a</ref> is that the GCRNN, even when not gated and containing the same number of parameters as the GNN, achieves a considerably smaller loss than the non-recurrent GNN, which attests to the importance of the recurrence mechanism in processing time sequences. In <ref type="figure">Figure 1b</ref>, although the difference among the average test losses achieved by each architecture is minimal, both GCRNN architectures have about a half or less of the 3, 370 parameters of the DCRNN (with the simple GCRNN counting as little as 450 parameters), and present a much smaller variance as well.</p><p>Earthquake epicenter placement on a seismograph network. The second numerical experiment is a classification problem based on data from GeoNet <ref type="bibr" target="#b21">[21]</ref>, a geological hazard database from New Zealand, and the Incorporated Research Institutions for Seismology (IRIS) database <ref type="bibr" target="#b22">[22]</ref>, which contains data for 8 active seismographs in the country in the timeframe of analysis. By gathering the origin times of all earthquakes registered by GeoNet between 12/25/2018 and 02/25/2019, we obtain seismic wave readings at these seismographs 30s and 60s before each earthquake, sampled at 2Hz. We then construct a 3 nearest-neighbor seismograph network from the seismographs' coordinates, and from the earthquakes' epicenter coordinates we generate labels corresponding to the nearest sensor to the earthquake's epicenter. In this setting, the objective is to accurately predict the closest node to the epicenter of an earthquake from seismic waves collected at the network's nodes immediately before the shock.</p><p>The experiment is conducted twice, once for the 30s and once for 60s duration wave. This results in input sequences  with length T = 60 and T = 120 respectively. We consider 2 GCRNNs. They are: (i) a GCRNN followed by a GNN with 1 convolutional layer mapping the state features to a single feature, and (ii) its gated version. The number of state features are 60 and 120 for the 30s and 60s experiments respectively. In the GCRNNs, the input sequences are used to process a state variable of same length, but only the last state is fed into the output GNN for label prediction. The baseline is a GNN with 1 convolutional layer. Because GNNs cannot process sequences, each element of the sequence is interpreted as an input feature. The convolutional layer of this GNN maps T input features to T + 2 features, which comes down to 4T (T + 2) parameters because the number of filter taps is always 4. This GNN was chosen to make for a reasonable comparison with the non-gated GCRNN, which has 4T 2 + 8T parameters.</p><p>Out of the 2, 503 earthquakes that happened in the two months to which we restrict our analysis, around 80% were used for training and 20% for testing each model. We optimize a cross-entropy loss with learning rate 0.001 over 10 training epochs and in batches of 100. Test accuracy for each model and each experiment are reported in <ref type="table" target="#tab_1">Table I</ref>, as well as the number of parameters in the convolutional layers of each architecture.</p><p>Even though all models achieve higher accuracy than random placement, the GCRNN architectures outperform a GNN with same number of parameters as the non-gated GCRNN. Table I also illustrates the importance of the gating mechanism as input sequences grow longer: the percentage difference in the test accuracy achieved by the gated GCRNN and the non-gated GCRNN is 23.8% bigger in the case of the 60-second wave.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We introduced Graph Convolutional Recurrent Neural Networks (GCRNNs) as NN architectures specifically tailored to deal with problems involving graph processes. Their primary feature is the use of banks of graph convolutional filters to implement the recurrence relationship. Thus, the number of parameters is independent of time and of the size of the graph. We have further extended this architecture to Gated GCRNNs (GGCRNNs) with input and forget gates that are akin to those of LSTMs. Numerical results obtained in a synthetic regression problem show that GCRNNs largely improve performance with respect to GNNs when the graph signals are part of a graph process. As for Gated GCRNNs, their ability to take long term dependencies into account was demonstrated in a real world experiment with different input sequence lengths.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>Test accuracy and # of parameters for each model in the epicenter placement problem for 30s and 60s waves.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to diagnose with LSTM recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wetzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03677v7</idno>
		<imprint>
			<date type="published" when="2017-03-21" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Business analytics in the context of big data: A roadmap for research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Phillips-Wren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ariyachandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Assoc. Inform. Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The CNN as a guided multilayer RECOS transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="81" to="89" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Int. Conf. Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for signals supported on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1034" to="1049" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annu. Conf. Neural Inform. Process. Syst</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-10" />
		</imprint>
	</monogr>
	<note>NIPS Foundation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Median activation functions for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th IEEE Int. Conf. Acoust., Speech and Signal Process</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimal graph-filter design and applications to distributed linear network operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="4117" to="4131" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep Learning, The Adaptive Computation and Machine Learning Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ergodicity in stationary graph processes: A weak law of large numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04550v1</idno>
		<imprint>
			<date type="published" when="2018-03" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>eess.SP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stationary signal processing on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3462" to="3477" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A time-vertex signal processing framework: Scalable processing and meaningful representations for time-series on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ricaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="817" to="829" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GaAN: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Uncertainty Artificial Intell</title>
		<meeting><address><addrLine>Monterey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-10" />
			<biblScope unit="volume">139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Int. Joint Conf. Artificial Intell</title>
		<imprint>
			<biblScope unit="page" from="3634" to="3640" />
			<date type="published" when="2018-07-19" />
		</imprint>
	</monogr>
	<note>Eur. Assoc. Artificial Intell</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A recurrent graph neural network for multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th IEEE Int. Conf. Acoust., Speech and Signal Process</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1644" to="1656" />
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending highdimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A unified view of diffusion maps and signal processing on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heimowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Int. Conf. Sampling Theory and Appl</title>
		<meeting><address><addrLine>Tallin, Estonia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Int. Conf. Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Earthquake Commission</title>
		<ptr target="https://www.geonet.org.nz/" />
	</analytic>
	<monogr>
		<title level="m">GNS Science, and Land Information New Zealand</title>
		<imprint>
			<date type="published" when="2019-02-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Incorporated Research Institutions for Seismology</title>
		<ptr target="https://www.iris.edu/hq/" />
		<imprint>
			<date type="published" when="2019-02-20" />
		</imprint>
	</monogr>
	<note>IRIS eartqueake browser</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
