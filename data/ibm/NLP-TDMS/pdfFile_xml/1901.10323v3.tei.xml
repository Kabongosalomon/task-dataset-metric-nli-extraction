<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time Hand Gesture Detection and Classification Using Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Köpüklü</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Human-Machine Communication, TU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Gunduz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Human-Machine Communication, TU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neslihan</forename><surname>Kose</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dependability Research Lab</orgName>
								<orgName type="institution" key="instit1">Intel Labs Europe</orgName>
								<orgName type="institution" key="instit2">Intel Deutschland GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Human-Machine Communication, TU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-time Hand Gesture Detection and Classification Using Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget. In this work, we address these challenges by proposing a hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach. The proposed architecture consists of two models:</p><p>(1) A detector which is a lightweight CNN architecture to detect gestures and (2) a classifier which is a deep CNN to classify the detected gestures. In order to evaluate the single-time activations of the detected gestures, we propose to use Levenshtein distance as an evaluation metric since it can measure misclassifications, multiple detections, and missing detections at the same time. We evaluate our architecture on two publicly available datasets -EgoGesture and NVIDIA Dynamic Hand Gesture Datasets -which require temporal detection and classification of the performed hand gestures. ResNeXt-101 model, which is used as a classifier, achieves the state-of-the-art offline classification accuracy of 94.04% and 83.82% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In real-time detection and classification, we obtain considerable early detections while achieving performances close to offline operation. The codes and pretrained models used in this work are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget. In this work, we address these challenges by proposing a hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach. The proposed architecture consists of two models:</p><p>(1) A detector which is a lightweight CNN architecture to detect gestures and (2) a classifier which is a deep CNN to classify the detected gestures. In order to evaluate the single-time activations of the detected gestures, we propose to use Levenshtein distance as an evaluation metric since it can measure misclassifications, multiple detections, and missing detections at the same time. We evaluate our architecture on two publicly available datasets -EgoGesture and NVIDIA Dynamic Hand Gesture Datasets -which require temporal detection and classification of the performed hand gestures. ResNeXt-101 model, which is used as a classifier, achieves the state-of-the-art offline classification accuracy of 94.04% and 83.82% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In real-time detection and classification, we obtain considerable early detections while achieving performances close to offline operation. The codes and pretrained models used in this work are publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Computers and computing devices are becoming an essential part of our lives day by day. The increasing demand for such computing devices increased the necessity of easy and practical computer interfaces. For this reason, systems using vision-based interaction and control are becoming more common, and as a result of this, gesture recognition is getting more and more popular in research community due to various application possibilities in human machine interaction. Compared to mouse and keyboard, any visionbased interface is more convenient, practical and natural because of the intuitiveness of gestures.</p><p>Gesture recognition can be practiced with mainly three methods: Using (i) glove-based wearable devices <ref type="bibr" target="#b0">[2]</ref>, (ii) 3dimensional locations of hand keypoints <ref type="bibr" target="#b21">[23]</ref> and (iii) raw visual data. The first method comes with the obligation of wearing an additional device with which lots of cables come even though it provides good results in terms of both accuracy and speed. The second, on the other hand, requires an extra step of hand-keypoints extraction, which brings 1 https://github.com/ahmetgunduz/Real-time-GesRec <ref type="figure" target="#fig_4">Fig. 1</ref>: Illustration of the proposed pipeline for real-time gesture recognition. The video stream is processed using a sliding window approach with stride of one. Top graph shows the detector probability scores which is activated when a gesture starts and kept active till it ends. The second graph shows classification score for each class with a different color. The third graph applies weightedaverage filtering on raw classification scores which eliminates the ambiguity between possible gesture candidates. The bottom graph illustrates the single-time activations such that red arrows represent early detections and black ones represent detections after gestures finalize.</p><p>additional time and computational cost. Lastly, for (iii), only an image capturing sensor is required such as camera, infrared sensor or depth sensor, which are independent of the user. Since the user does not require to wear a burdensome device to achieve an acceptable accuracy in recognition and sufficient speed in computation, this option stands out as the most practical one. It is important for the infrastructure of any gesture recognition system to be practical. After all, we aim to use it in real life scenarios.</p><p>In this work, in order to provide a practical solution, we have developed a vision based gesture recognition approach using deep convolutional neural networks (CNNs) on raw video data. Currently, CNNs provide the state-ofthe-art results for not only image based tasks such as object detection, image segmentation and classification, but also for video based tasks such as activity recognition and action localization as well as gesture recognition <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr" target="#b23">[25]</ref>.</p><p>In real-time gesture recognition applications, there are several characteristics that the system needs to satisfy: (i) An acceptable classification accuracy, (ii) fast reaction time, (iii) resource efficiency and (iv) single-time activation per each performed gesture. All these items contain utmost importance for a successful real-time vision based gesture recognition application. However, most of the previous research only considers (i) and tries to increase the offline classification accuracy in gesture recognition disregarding the remaining items. Some proposed approaches are even impossible to run in real-time since they consist of several deep CNNs on multiple input modalities, which is forcing the limits of memory and power budget <ref type="bibr" target="#b12">[14]</ref>.</p><p>In this paper, we propose a hierarchical architecture for the task of real-time hand gesture detection and classification that allows us to integrate offline working models and still satisfy all the above-mentioned attributes. Our system consists of an offline-trained deep 3D CNN for gesture classification (classifier) and a light weight, shallow 3D CNN for gesture detection (detector). <ref type="figure" target="#fig_4">Fig. 1</ref> illustrates the pipeline of the proposed approach. A sliding window is used over the incoming video stream feeding the input frames to the detector via detector queue. Top graph in <ref type="figure" target="#fig_4">Fig. 1</ref> shows the detector probability scores which become active when the gestures are being performed, and remain inactive for the rest of the time. The classifier becomes active only when the detector detects a gesture. This is very critical since most of the time, no gesture is performed in realtime gesture recognition applications. Therefore, there is no need to keep the high-performance classifier always active, which increases the memory and power consumption of the system considerably. The second graph shows the raw classification scores of each class with a different color. As it can be seen from the graph, scores of similar classes become simultaneously high especially at the beginning of the gestures. In order to resolve these ambiguities, we have weighted the class scores to avoid making a decision at the beginning of the gestures (third graph in <ref type="figure" target="#fig_4">Fig. 1</ref>). Lastly, the bottom graph illustrates the single-time activations, where red arrows represent the early detections and black ones represent the detections after gestures end. Our system can detect gestures earlier in their nucleus part, which is the part distinguishing the gesture from the rest. We propose to use the Levenshtein distance as an evaluation metric to compare the captured single-time activations with ground-truth labels. This metric is more suitable and evaluative since it can measure misclassifications, multiple detections and missing detections at the same time.</p><p>We evaluated our approach on two publicly available datasets, which are EgoGesture Dataset <ref type="bibr" target="#b22">[24]</ref> and NVIDIA Dynamic Hand Gesture Dataset <ref type="bibr" target="#b11">[13]</ref> (nvGesture) 2 . For the classifier of the proposed approach, any offline working CNN architecture can be used. For our experiments, we have used well-known C3D <ref type="bibr" target="#b18">[20]</ref> and ResNeXt-101 <ref type="bibr" target="#b4">[6]</ref>. We have achieved the state-of-the-art offline classification accuracies of 94.03% and 83.82% on depth modality with ResNeXt-101 architecture on EgoGesture and nvGesture datasets, respectively. For real-time detection and classification, we achieve considerable early detections by relinquishing little amount of recognition performance.</p><p>The rest of the paper is organized as follows. In Section II, the related work in the area of offline and real-time gesture recognition is presented. Section III introduces our real-time gesture recognition approach, and elaborates training and evaluation processes. Section IV presents experiments and results. Lastly, Section V concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The success of CNNs in object detection and classification tasks <ref type="bibr" target="#b8">[10]</ref>, <ref type="bibr" target="#b3">[5]</ref> has created a growing trend to apply them also in the other areas of computer vision. For video analysis tasks, CNNs have been initially extended to be applied for video action and activity recognition and they have achieved state-of-the-art performances <ref type="bibr" target="#b16">[18]</ref>, <ref type="bibr" target="#b2">[4]</ref>.</p><p>There have been various approaches using CNNs to extract spatio-temporal information from video data. Due to the success of 2D CNNs in static images, video analysis based approaches initially applied 2D CNNs. In <ref type="bibr" target="#b16">[18]</ref>, <ref type="bibr" target="#b6">[8]</ref>, video frames are treated as multi-channel inputs to 2D CNNs. Temporal Segment Network (TSN) <ref type="bibr" target="#b20">[22]</ref> divides video into several segments, extracts information from color and optical flow modalities for each segment using 2D CNNs, and then applies spatio-temporal modeling for action recognition. A convolutional long short-term memory (LSTM) architecture is proposed in <ref type="bibr" target="#b2">[4]</ref>, where the authors extract first the features from video frames by a 2D CNN and then apply LSTM for global temporal modeling. The strength of all these approaches comes from the fact that there are plenty of very successful 2D CNN architectures, and these architectures can be pretrained using the very large-scale ImageNet dataset <ref type="bibr" target="#b1">[3]</ref>.</p><p>Although 2D CNNs perform pretty well on video analysis tasks, they are limited to model temporal information and motion patterns. Therefore 3D CNNs have been proposed in <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b19">[21]</ref>, <ref type="bibr" target="#b4">[6]</ref>, which use 3D convolutions and 3D pooling to capture discriminative features along both spatial and temporal dimensions. Different from 2D CNNs, 3D CNNs take a sequence of video frames as inputs. In this work, we also use the variants of 3D CNNs.</p><p>The real-time systems for hand gesture recognition requires to apply detection and classification simultaneously on continuous stream of video. There are several works addressing detection and classification separately. In <ref type="bibr" target="#b13">[15]</ref>, authors apply histogram of oriented gradient (HOG) algorithm together with an SVM classifier. The authors in <ref type="bibr" target="#b10">[12]</ref> use a special radar system to detect and segment gestures. In our work, we have trained a light weight 3D CNN for gesture detection. Moreover, in human computer interfaces, performed gestures must be recognized only once (i.e. singletime activations) by the computers. This is very critical and this problem has not been addressed well yet. In <ref type="bibr" target="#b11">[13]</ref>, the authors apply connectionless temporal classification (CTC) loss to detect consecutive similar gestures. However, CTC does not provide single time activations. To the best of our knowledge, in this study, it is the first time singletime activations are performed for deep learning based hand gesture recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we elaborate on our two-model hierarchical architecture that enables the-state-of-the-art CNN models to be used in real-time gesture recognition applications as efficiently as possible. After introducing the architecture, training details are described. Finally, we give a detailed explanation for the used post processing strategies that allow us to have single-time activation per gesture in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture</head><p>Recently, with the availability of large datasets, CNN based models have proven their ability in action/gesture recognition tasks. 3D CNN architectures especially stand out for video analysis since they make use of the temporal relations between frames together with their spatial content. However, there is no clear description of how to use these models in a real-time dynamic system. With our work, we aim to fill this research gap. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates the used workflow for an efficient realtime recognition system using a sliding window approach. In contrary to offline testing, we do not know when a gesture starts or ends. Because of this, our workflow starts with a detector which is used as a switch to activate classifier if a gesture gets detected. Our detector and classifier models are fed by a sequence of frames with size n and m, respectively, such as n m with an overlapping factor as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. The stride value used for the sliding window is represented by s in <ref type="figure" target="#fig_0">Fig. 2</ref>, and it is same for both the detector and the classifier. Although higher stride provides less resource usage, we have chosen s as 1 since it is small enough not to miss any gestures and allows us to achieve better performance. In addition to the detector and classifier models, one post-processing and one single-time activation service is introduced to the workflow. In the following parts, we are going to explain these blocks in detail.</p><p>1) Detector: The purpose of the detector is to distinguish between gesture and no gesture classes by running on a sequence of images, which detector queue masks. Its main and only role is to act as a switch for the classifier model, meaning that if it detects a gesture, then the classifier is activated and fed by the frames in the classifier queue.</p><p>Since the overall accuracy of this system highly depends on the performance of detector, we require the detector to be (i) robust, (ii) accurate in detection of true positives (gestures), and (iii) lightweight as it runs continuously. For the sake of (i), the detector runs on smaller number of frames than the classifier to which we refer as detector and classifier queues. For (ii), detector queue is placed on the very beginning of classifier queue as shown in <ref type="figure" target="#fig_0">Fig.  2</ref>, and this enables the detector to activate the classifier whenever a gesture starts regardless of the gesture duration. Moreover, the detector model is trained with a weightedcross entropy loss in order to decrease the likelihood of false positives (i.e., achieve higher recall rate). The class weights for no gesture and gesture classes are selected as 1 and 3, respectively as our experiments showed that this proportion is sufficient to have 98+% and 97+% recall rates in EgoGesture and nvGesture datasets, respectively. Besides that, we postprocess the output probabilities, and set a counter for the consecutive number of no gesture predictions in decision of deactivating classifier. For (iii), ResNet-10 architecture is constructed using the ResNet block in <ref type="figure" target="#fig_1">Fig. 3</ref> with very small feature sizes in each layer as given in <ref type="table" target="#tab_1">Table I</ref>, which results in less than 1M (≈ 862K) parameters. F and N correspond to the number of feature channels and the number blocks in corresponding layers, respectively. BN, ReLU and group in <ref type="figure" target="#fig_1">Fig. 3</ref> refers to batch normalization, rectified linear unit nonlinearities and the number of group convolutions, respectively.</p><p>2) Classifier: Since we do not have any limitation regarding the size or complexity of the model, any architecture providing a good classification performance can be selected as classifier. This leads us to use two recent 3D CNN architectures (C3D <ref type="bibr" target="#b17">[19]</ref> and ResNext-101 <ref type="bibr" target="#b5">[7]</ref>) as our classifier model. However, it is important to note that our architecture is independent of the model type.</p><p>For C3D model, we have used the exact same model as in <ref type="bibr" target="#b17">[19]</ref>, but only changed the number of nodes in the last two fully connected layers from 4096 to 2048. For ResNeXt-101, we have followed the guidelines of <ref type="bibr" target="#b4">[6]</ref> and chosen the model parameters as given in <ref type="table" target="#tab_1">Table I</ref> with ResNeXt block as given in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>Since the number of parameters for 3D CNNs are much more than 2D CNNs, they require more training data in order to prevent overfitting. Because of this reason, we pretrained our classifier architectures first on Jester dataset [1], which is the largest publicly available hand gesture dataset, and then fine tune our model on EgoGesture and nvGesture datasets. This approach increased the accuracy and shortened the training duration drastically.</p><p>Training Details: We use stochastic gradient descent (SGD) with Nesterov momentum = 0.9, damping factor = 0.9, and weight decay = 0.001 as optimizer. After pretraining on Jester dataset, the learning rate is started with 0.01, and divided by 10 at 10 th and 25 th epochs, and training is completed after 5 more epochs.</p><p>For regularization, we used a weight decay (γ = 1 × 10 −3 ), which is applied on all the parameters of the network. We also used dropout layers in C3D and several data augmentation techniques throughout training.</p><p>For data augmentation, three methods were used: (1) Each image is randomly cropped with size 112 × 112 and scaled randomly with one of {1,  elastic displacement <ref type="bibr" target="#b15">[17]</ref> with α = 1 and σ = 2 is applied on the cropped and scaled images. For temporal augmentation, (3) we randomly select consecutive frames according to the size of input sample duration from the entire gesture videos. If the sample duration is more than the number of frames in target gesture, we append frames starting from the very first frame in a cyclic fashion. We also normalized the images into 0-1 scale using mean and standard deviation of the whole training sets in order to force models to learn faster. The same training details are used for the detector and classifier models. During offline and online testing, we scale images and apply center cropping to get 112 × 112 images. Then only normalization is performed for the sake of consistency between training and testing.</p><p>3) Post-processing: In dynamic hand gestures, it is possible that the hand gets out of the camera view while performing gestures. Even though the previous predictions of the detector are correct, any misclassification reduces the overall performance of the proposed architecture. In order to make use of previous predictions, we add the raw softmax probabilities of the previous detector predictions into a queue (q k ) with size k, and apply filtering on these raw values and obtain final detector decisions. With this approach, detector increases its confidence in decision making, and clears out most of the misclassifications in consecutive predictions. The size of the queue (k) is selected as 4, which achieved the best results for stride s of 1 in our experiments.</p><p>We have applied (i) average, (ii) exponentially-weighted average and (iii) median filtering separately on the values in q k . While average filtering simply takes the mean value of q k , median filtering takes the median. Exponentiallyweighted average filtering, on the other hand, takes the weighted average of the samples using the weight function of w i = exp −(1−(k−i))/k where i stands for the index of the i th previous sample and satisfies 0 ≤ i &lt; k, and w i is the weight for the i th previous sample. Out of these three filtering strategies, we have used median filtering since it achieves slightly better results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Single-time Activation:</head><p>In real-time gesture recognition systems, it is extremely important to have smaller reaction time and single-time activation for each gesture. Pavlovic et al. states that dynamic gestures have preparation, nucleus (peak or stroke <ref type="bibr" target="#b9">[11]</ref>) and retraction parts <ref type="bibr" target="#b14">[16]</ref>. Out of all parts, nucleus is the most discriminative one, since we can decide which gesture is performed in nucleus part even before it ends.</p><p>Single-time activation is achieved through two level control mechanism. Either a gesture is detected when a confidence measure reaches a threshold level before the gesture actually ends (early-detection), or the gesture is predicted when the detector deactivates the classifier (late-detection). In late-detection, we assume that the detector should not miss any gesture since we assure that the detector has a very high recall rate.</p><p>The most critical part of the early-detection is that, the gestures should be detected after their nucleus parts for a better recognition performance. Because several gestures can contain a similar preparation part which creates an ambiguity at the beginning of the gestures, as can be seen on the top graph of <ref type="figure" target="#fig_3">Fig. 5</ref>. Therefore, we have applied weightedaveraging on class scores with a weight function as in <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>, and its formula is given as:</p><formula xml:id="formula_0">w j = 1 (1 + exp −0.2×( j−t) ) (1)</formula><p>where j is the iteration index of an active state, at which a gesture is detected, and t is calculated by using the following formula:</p><formula xml:id="formula_1">t = µ 4 × s<label>(2)</label></formula><p>where µ corresponds to the mean duration of the gestures (in number of frames) in the dataset and s is the stride length. For EgoGesture dataset, µ is equal to 38,4 and for stride of s = 1, t is calculated as 9, which is similar for also nvGesture dataset. When a gesture starts, we start to multiply raw class scores with weights w j and apply averaging. These parameters allow us to have weights equal to or higher than 0.5 in the nucleus part of the gestures on average. <ref type="figure" target="#fig_3">Fig.  5</ref> shows the probability scores of five gestures over each iteration and their corresponding weighted-averages. It can easily be observed that the ambiguity of the classifier at the preparation part of the gestures is successfully resolved with this approach. With this weighted-averaging strategy, we force our singletime activator to make decision at mid-late part of the gestures after capturing their nucleus parts. On the other hand, we need a confidence measure for early-detections in real-time since the duration of gestures varies. Hence, we decided to use the difference between weighted average scores of each classes as our confidence measure for earlydetection. When the detector switches the classifier on, weighted average probabilities for each class is calculated at each iteration. If the difference between two highest average probabilities is more than a threshold τ early , then earlydetection is triggered; otherwise, we wait for the detector if a gesture is detected then <ref type="bibr">3:</ref> state ← "Active" <ref type="bibr">4:</ref> α ← probs j−1 × ( j − 1) <ref type="bibr">5:</ref> mean probs = (α + w j × probs j )/ j 6:</p><formula xml:id="formula_2">(max 1 , max 2 ) = max gesture [mean probs] 2 7:</formula><p>if (max1 − max2) ≥ τ early then 8:</p><p>early-detection ← "True" <ref type="bibr">9:</ref> return gesture with max 1 10:</p><p>j ← j + 1 <ref type="bibr">11:</ref> if the gesture ends then <ref type="bibr">12:</ref> state ← "Passive" <ref type="bibr">13:</ref> if early-detection = "True" &amp; max 1 ≥ τ late then <ref type="bibr">14:</ref> return gesture with max 1 15:   to switch off the classifier and the class with the highest score above τ late (fixed to 0.15 as it showed the best results in our experiments) is predicted as late-detection. Details for this strategy can be found in Algorithm 1. 5) Evaluation of the Activations: As opposed to offline testing which usually considers only about class accuracies, we must also consider the following scenarios for our realtime evaluation:</p><formula xml:id="formula_3">i ← i + 1 Model</formula><p>• Misclassification of the gesture due to the classifier, • Not detecting the gesture due to the detector, • Multiple detections in a single gesture. Considering these scenarios, we propose to use the Levenshtein distance as our evaluation metric for online experiments. The Levenshtein distance is a metric that measures distance between sequences by counting the number of item-level changes (insertion, deletion, or substitutions) to transform one sequence into the other. For our case, one video and the gestures in this video correspond to a sequence and the items in this sequence, respectively. For example, lets consider the following ground truth and predicted gestures of a video:</p><p>GroundTruth [1, 2, 3, 4, 5, 6, 7, <ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b7">9]</ref> Predicted [1, <ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b7">9]</ref> For this example, the Levenshtein distance is 2: The deletion of one of "6" which is detected two times, and the substitution of "7" with "3". We average this distance over the number of true target classes. For this case, the average distance is 2/9 = 0.2222 and we subtract this value from 1 since we want to measure closeness (in this work it is referred as the Levenshtein accuracy) of our results, which is equal to (1 − 0.2222) × 100 = 77.78%.   IV. EXPERIMENTS The performance of the proposed approach is tested on two publicly available datasets: EgoGesture and NVIDIA Dynamic Hand Gestures dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Offline Results Using EgoGesture Dataset</head><p>EgoGesture dataset is a recent multimodal large scale dataset for egocentric hand gesture recognition <ref type="bibr" target="#b22">[24]</ref>. This dataset is created not only for segmented gesture classification, but also for gesture detection in continuous data. There are 83 classes of static and dynamic gestures collected from 6 diverse indoor and outdoor scenes. The dataset splits are created by distinct subjects with ratio 3:1:1 resulting in 1239 training, 411 validation and 431 testing videos, having 14416, 4768 and 4977 gesture samples, respectively. All models are first pretrained on Jester dataset <ref type="bibr">[1]</ref>. For test set evaluations, we have used both training and validation set for training.</p><p>We initially investigated the performance of C3D and ResNeXt architectures on the offline classification task. <ref type="table" target="#tab_1">Table  II</ref> shows the comparison of used architectures with the stateof-the-art approaches. ResNeXt-101 architecture with 32frames input achieves the best performance.</p><p>Secondly, we investigated the effect of the number of input frames on the gesture detection and classification performance. Results in <ref type="table" target="#tab_1">Table III and Table IV</ref> show that we achieve a better performance as we increase the input size for all the modalities. This depends highly on the characteristics of the used datasets, especially on the average duration of the gestures.</p><p>Thirdly, the RGB and depth modalities are investigated for different input sizes. We always observed that the models with depth modality show better performance than the models with RGB. Depth sensor filters out the background motion, and allows the models to focus more on the hand motion, hence more discriminative features can be obtained from depth modality. For real-time evaluation, ResNet-10 with depth modality and input size of 8-frames is chosen as the detector, since smaller window size allows the detector to discover the start and end of the gestures more robustly. The detailed results of this model are shown in <ref type="table" target="#tab_7">Table V</ref>.   We again initially investigated the performance of C3D and ResNeXt architectures on the offline classification task, by comparing them with the state-of-the-art models. As shown in <ref type="table" target="#tab_1">Table VI</ref>, ResNeXt-101 architecture achieves the best performance. Similar to EgoGesture dataset, we achieve a better classification and detection performance as we increase the input size, for all the modalities, as shown in <ref type="table" target="#tab_1">Table VII and Table VIII</ref>. Depth modality again achieves better performance than RGB modality for all input sizes. Moreover, ResNet-10 with depth modality and input size of 8-frames is chosen as the detector in the online testing, whose detailed results are given in <ref type="table" target="#tab_1">Table IX.</ref> For real-time evaluation, we have selected 8-frames ResNet-10 detectors with depth modality and best performing classifiers in both dataset, which have * sign in corresponding tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Real-Time Classification Results</head><p>EgoGesture and nvGesture datasets have 431 and 482 videos, respectively in their test sets. We evaluated our proposed architecture on each video separately and calculated an average Levenshtein accuracy at the end. We achieve 91.04% and 77.39% Levenshtein accuracies in EgoGesture and nvGesture datasets, respectively.</p><p>Moreover, the early detection times are investigated by simulating different early-detection threshold levels (τ early ) varying from 0.2 to 1.0 with 0.1 steps. <ref type="figure" target="#fig_5">Fig. 6</ref> compares early detection times of weighted averaging and uniform averaging   approaches for both EgoGesture and nvGesture datasets. The <ref type="figure" target="#fig_5">Fig. 6</ref> shows the importance of weighted averaging, which performs considerably better than uniform averaging. As we increase the threshold, we force the architecture to make decision towards the end of gestures, hence achieving better performance. However, we can gain considerable early detection performance by relinquishing little amount of performance. For example, if we set detection threshold τ early to 0.4 for EgoGesture dataset, we can make our single time activations 9 frames earlier on average by relinquishing only 1.71% Levenshtein accuracy. We also observe that mean early detection times are longer for nvGesture dataset since it contains weakly-segmented videos. Lastly, we investigated the execution performance of our two-model approach. Our system runs on average at 460 fps when there is no gesture (i.e. only detector is active) and 62 (41) fps in the presence of gesture (i.e. both detector and classifier are active) for ResNeXt-101 (C3D) as the classifier on a single NVIDIA Titan Xp GPU with batch size of 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper presents a novel two-model hierarchical architecture for real-time hand gesture recognition systems. The proposed architecture provides resource efficiency, early detections and single time activations, which are critical for real-time gesture recognition applications.</p><p>The proposed approach is evaluated on two dynamic hand gesture datasets, and achieves similar results for both of them. For real-time evaluation, we have proposed to use a new metric, Levenshtein accuracy, which we believe is a suitable evaluation metric since it can measure misclassifications, multiple detections and missing detections at the same time. Moreover, we have applied weighted-averaging on the class probabilites over time, which improves the overall performance and allows early detection of the gestures at the same time.</p><p>We acquired single-time activation per gesture by using difference between highest two average class probabilities as a confidence measure. However, we would like to investigate more on the statistical hypothesis testing for the confidence measure of the single-time activations as a future work. Also, Early detection times are calculated only for correctly predicted gestures. Blue color refers to the "weighted" approach in single-time activation, and green color refers to "not weighted" approach. For both datasets, as early detection threshold increases, average early detection times reduce, but we achieve better Levenshtein accuracies.</p><p>we intend to utilize different weighting approaches in order to increase the performance even further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The general workflow of the proposed two-model hierarchical architecture. Sliding windows with stride s run through incoming video frames where detector queue placed at the very beginning of classifier queue. If the detector recognizes an action/gesture, then the classifier is activated. The detector's output is post-processed for a more robust performance, and the final decision is made using single-time activation block where only one activation occurs per performed gesture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>ResNet and ResNeXt blocks used in the detector and classifier architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Histogram of the gesture durations for the EgoGesture dataset, (b) Sigmoid-like weight function used for single-time activations according to the Equation (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Raw (top) and weighted (bottom) classification scores. At top graph, we observe a lot of noise at the beginning of all gestures; however, near to the end of each gesture the classifier gets more confident. The bottom graph shows that we can remove this noise part by assigning smaller weights to the beginning part of the gestures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>Single-time activation in real-time gesture recognition Input: Incoming frames from video data. Output: Single-time activations. 1: for each "frame-window" w i of length m do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Comparison of early detection time, early detection threshold and acquired Levenshtein accuracies for (a) EgoGesture and (b) nvGesture datasets. Numerals on each data point represent the Levenshtein accuracies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell>: Detector (ResNet-10) and Classifier (ResNeXt-101)</cell></row><row><cell>architectures. For ResNet-10, max pooling is not applied when input</cell></row><row><cell>of 8-frames is used.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Comparison with state-of-the-art on the test set of EgoGesture dataset.</figDesc><table><row><cell>Model</cell><cell>Input</cell><cell cols="2">Modality</cell></row><row><cell></cell><cell></cell><cell>RGB</cell><cell>Depth</cell></row><row><cell>C3D</cell><cell cols="2">16-frames 86.88</cell><cell>88.45</cell></row><row><cell>C3D</cell><cell cols="2">24-frames 89.20</cell><cell>89.07</cell></row><row><cell>C3D</cell><cell cols="2">32-frames 90.57</cell><cell>91.44</cell></row><row><cell cols="3">ResNeXt-101 16-frames 90.94</cell><cell>91.80</cell></row><row><cell cols="3">ResNeXt-101 24-frames 92.89</cell><cell>93.47</cell></row><row><cell cols="3">ResNeXt-101 32-frames 93.75</cell><cell>94.03*</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc></figDesc><table /><note>Classifier's classification accuracy scores on the test set of EgoGesture dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Detector's binary classification accuracy scores on the test set of EgoGesture dataset.</figDesc><table><row><cell cols="4">Modality Recall Precision f1-score</cell></row><row><cell>RGB</cell><cell>96.64</cell><cell>97.10</cell><cell>96.87</cell></row><row><cell>Depth</cell><cell>99.37</cell><cell>99.43</cell><cell>99.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>Detection results of 8-frames ResNet-10 architecture on the test set of EgoGesture dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison with state-of-the-art on the test set of nvGesture dataset.</figDesc><table><row><cell>Model</cell><cell>Input</cell><cell cols="2">Modality</cell></row><row><cell></cell><cell></cell><cell>RGB</cell><cell>Depth</cell></row><row><cell>C3D</cell><cell cols="2">16-frames 62.67</cell><cell>70.33</cell></row><row><cell>C3D</cell><cell cols="2">24-frames 65.35</cell><cell>70.33</cell></row><row><cell>C3D</cell><cell cols="2">32-frames 73.86</cell><cell>77.18</cell></row><row><cell cols="3">ResNeXt-101 16-frames 66.40</cell><cell>72.82</cell></row><row><cell cols="3">ResNeXt-101 24-frames 72.40</cell><cell>79.25</cell></row><row><cell cols="3">ResNeXt-101 32-frames 78.63</cell><cell>83.82*</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>Classifier's classification accuracy scores on the test set of nvGesture dataset.B. Offline Results Using nvGesture DatasetnvGesture dataset contains 25 gesture classes, each intended for human-computer interfaces. The dataset is recorded with multiple sensors and viewpoints at an indoor car simulator. There are in total 1532 weakly-segmented videos (i.e., there are no-gesture parts in the videos), which are split with ratio 7:3 resulting in 1050 training and 482 test videos each containing only one gesture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VIII :</head><label>VIII</label><figDesc>Detector's binary classification accuracy scores on the test set of nvGesture dataset.</figDesc><table><row><cell cols="4">Modality Recall Precision f1-score</cell></row><row><cell>RGB</cell><cell>70.22</cell><cell>80.31</cell><cell>74.93</cell></row><row><cell>Depth</cell><cell>97.30</cell><cell>97.41</cell><cell>97.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE IX :</head><label>IX</label><figDesc>Detection results of 8-frames ResNet-10 architecture on the test set of nvGesture dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">NVIDIA Dynamic Hand Gesture Dataset is referred as 'nvGesture' in this work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Glove-based hand gesture recognition sign language translator using capacitive touch sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C F</forename><surname>Qubeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Electron Devices and Solid-State Circuits (EDSSC)</title>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="page" from="334" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<idno>arXiv: 1512.03385</idno>
		<title level="m">Deep Residual Learning for Image Recognition</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Motion fused frames: Data level fusion strategy for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Köpüklü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Köse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcneill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levy</surname></persName>
		</author>
		<title level="m">Conceptual representations in language activity and gesture. ERIC Clearinghouse Columbus</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-sensor system for driver&apos;s hand-gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online detection and classification of dynamic hand gestures with recurrent 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4207" to="4215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gesture recognition: Focus on the hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5235" to="5244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hand gesture recognition in real time for automotive interfaces: A multimodal vision-based approach and evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2368" to="2377" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual interpretation of hand gestures for human-computer interaction: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="677" to="695" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0767</idno>
		<idno>arXiv: 1412.0767</idno>
		<title level="m">Learning Spatiotemporal Features with 3d Convolutional Networks</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<title level="m">Convnet architecture search for spatiotemporal feature learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Intraoperative Visual Guidance and Control Interface for Augmented Reality Robotic Surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th IEEE International Conference on Control and Automation, ICCA 2010</title>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="947" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">EgoGesture: A New Dataset and Benchmark for Egocentric Hand Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1038" to="1050" />
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal gesture recognition using 3-d convolution and convolutional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4517" to="4524" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
