<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Multi-class Object Detection in Unconstrained Remote Sensing Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><surname>Majid</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">German Aerospace Center</orgName>
								<orgName type="institution" key="instit2">Remote Sensing Technology Institute</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Chair of Remote Sensing</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azimi</forename></persName>
						</author>
						<title level="a" type="main">Towards Multi-class Object Detection in Unconstrained Remote Sensing Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Object detection · Remote sensing · CNN</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0000−0002−6084−2272] , Eleonora Vig 1[0000−0002−7015−6874] , Reza Bahmanyar 1[0000−0002−6999−714X] , Marco Körner 2[0000−0002−9186−4175] , and Peter Reinartz 1[0000−0002−8122−1475]</p><p>Abstract. Automatic multi-class object detection in remote sensing images in unconstrained scenarios is of high interest for several applications including traffic monitoring and disaster management. The huge variation in object scale, orientation, category, and complex backgrounds, as well as the different camera sensors pose great challenges for current algorithms. In this work, we propose a new method consisting of a novel joint image cascade and feature pyramid network with multi-size convolution kernels to extract multi-scale strong and weak semantic features. These features are fed into rotation-based region proposal and region of interest networks to produce object detections. Finally, rotational non-maximum suppression is applied to remove redundant detections. During training, we minimize joint horizontal and oriented bounding box loss functions, as well as a novel loss that enforces oriented boxes to be rectangular. Our method achieves 68.16% mAP on horizontal and 72.45% mAP on oriented bounding box detection tasks on the challenging DOTA dataset, outperforming all published methods by a large margin (+6% and +12% absolute improvement, respectively). Furthermore, it generalizes to two other datasets, NWPU VHR-10 and UCAS-AOD, and achieves competitive results with the baselines even when trained on DOTA. Our method can be deployed in multi-class object detection applications, regardless of the image and object scales and orientations, making it a great choice for unconstrained aerial and satellite imagery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent advances in remote sensing (RS) technologies have eased the acquisition of very high-resolution multi-spectral satellite and aerial images. Automatic RS data analysis can provide an insightful understanding over large areas in a short time. In this analysis, multi-class object detection (e.g., vehicles, ships, airplanes, etc.) plays a major role. It is a key component of many applications such as traffic monitoring, parking lot utilization, disaster management, urban management, search and rescue missions, maritime traffic monitoring and so on.</p><p>Object detection in RS images is a big challenge as the images can be acquired with different modalities (e.g., panchromatic, multi-and hyper-spectral, and Radar) with a wide range of ground sampling distance (GSD) e.g., from 10 cm to 30 m. Furthermore, the objects can largely vary in scale, size, and orientation.</p><p>In recent years, deep learning methods have achieved promising object detection results for ground imagery and outperformed traditional methods. Among them, deep convolutional neural networks (DCNNs) have been widely used <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. In the RS domain, newly introduced large-scale multi-class image datasets such as DOTA <ref type="bibr" target="#b29">[30]</ref> have provided the opportunity to leverage the applications of deep learning methods. The majority of current deep learningbased methods detect objects based on horizontal bounding boxes (HBBs), which are appropriate for ground-level images. However, in the RS scenarios, objects can be arbitrarily oriented. Therefore, utilizing oriented bounding boxes (OBBs) is highly recommended, especially when multiple objects are located tightly close to each other (e.g., cars in parking lots).</p><p>Region-based convolutional neural networks (RCNNs) such as (Fast(er))-RCNN <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> and Mask-RCNN <ref type="bibr" target="#b8">[9]</ref> have achieved state-of-the-art object detection results in large-scale ground imagery datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>. Fast-RCNN <ref type="bibr" target="#b23">[24]</ref> improves the detection accuracy of RCNN <ref type="bibr" target="#b7">[8]</ref> by using a multi-task loss function for the simultaneous region proposal regression and classification tasks. As an improvement, Faster-RCNN integrates an end-to-end trainable network, called region proposal network (RPN), to learn the region proposals for increasing the localization accuracy of Fast-RCNN. To further improve Faster-RCNN, one could perform multi-scale training and testing to learn feature maps in multiple levels; however, this will increase the memory usage and inference time.</p><p>Another alternative is image or feature pyramids <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>. Recently, Lin et al. <ref type="bibr" target="#b13">[14]</ref> proposed the feature pyramid network (FPN) which extracts feature maps through a feature pyramid, thus facilitating object detection in different scales, at a marginal extra cost. Although joint image and feature pyramids may further improve results, this is avoided due to its computation cost.</p><p>Object detection in RS images has been investigated by a number of works in the recent years. The majority of the proposed algorithms focus on object detection with a small number of classes and a limited range of GSDs. Liu and Mattyus <ref type="bibr" target="#b15">[16]</ref> proposed histogram of oriented gradients (HOG) features and the AdaBoost method for feature classification to detect multi-class oriented vehicles. Although this approach achieves a fast inference time, it does not have high detection accuracy as it lacks high-level feature extraction. Sommer et al. <ref type="bibr" target="#b26">[27]</ref> and Tang et al. <ref type="bibr" target="#b28">[29]</ref> proposed RCNN-based methods using hard-negative mining together with concatenated and deconvolutional feature maps. They showed that these methods achieve high accuracies in single-class vehicle detection in aerial images for HBBs task. Liu et al. <ref type="bibr" target="#b16">[17]</ref> proposed rotated region proposals to predict object orientation using single shot detector (SSD) <ref type="bibr" target="#b17">[18]</ref> improving the localization of the OBBs task. Yang et al. <ref type="bibr" target="#b31">[32]</ref> improved <ref type="bibr" target="#b16">[17]</ref> by integrating FPNs.</p><p>In this paper, we focus on improving the object localization of region-based methods applied to aerial and satellite images. We propose a new end-to-end CNN to address the aforementioned challenges of multi-class object detection in RS images. The proposed method is able to handle images with a wide range of scales, aspect ratios, GSDs, and complex backgrounds. In addition, our proposed method achieves accurate object localization by using OBBs. More specifically, the method is composed of the following consecutive modules: image cascade network (ICN), deformable inception network (DIN), FPN, multi-scale rotational region-proposal network (R-RPN), multi-scale rotational region of interest network (R-ROI), and rotational non-maximum suppression (R-NMS). The main contributions of our work are the following:</p><p>-We propose a new joint image cascade and feature pyramid network (ICN and FPN) which allows extracting information on a wide range of scales and significantly improves the detection results. -We design a DIN module as a domain adaptation module for adapting the pre-trained networks to the RS domain using deformable convolutions and multi-size convolution kernels. -We propose a new loss function to enforce the detection coordinates, forming quadrilaterals, to shape rectangles by constraining the angles between the edges to be 90 degrees. This augments object localization. -We achieve significant improvements on three challenging datasets in comparison with the state of the art.</p><p>In addition, we employ rotational region proposals to capture object locations more accurately in RS images. Finally, in order to select the best localized regions and to remove redundant detections, we apply R-NMS which is the rotational variant of the conventional NMS. Furthermore, we initialize anchor sizes in R-RPNs with clustered data from rotated ground truth bounding boxes proposed by Redmon and Farhadi <ref type="bibr" target="#b21">[22]</ref> rather than manual initialization used in Faster-RCNN. In order to evaluate the proposed method, we applied it to the DOTA <ref type="bibr" target="#b29">[30]</ref> dataset, a recent large-scale satellite and aerial image dataset, as well as the UCAS-AOD and NWPU VHR-10 datasets. Results show that the proposed method achieves a significantly higher accuracy in comparison with state-of-the-art object detection methods. <ref type="figure" target="#fig_0">Figure 1</ref> gives a high-level overview of our joint horizontal and oriented bounding box prediction pipeline for multi-class object detection. Given an input image, combined image cascade and feature pyramid networks (ICN and FPN) extract rich semantic feature maps tuned for objects of substantially varying sizes. Following the feature extraction, a R-RPN returns category-agnostic rotated regions, which are then classified and regressed to bounding-box locations with a R-ROI. During training, we minimize a multi-task loss both for R-RPN and R-ROI. To obtain rectangular predictions, we further refine the output quadrilaterals by computing their minimum bounding rectangles. Finally, R-NMS is applied as a post-processing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Cascade, Feature Pyramid, and Deformable Inception Subnetworks</head><p>In order to extract strong semantic information from different scales, this work aims at leveraging the pyramidal feature hierarchy of convolutional neural networks (CNNs). Until recently, feature extraction was typically performed on a single scale <ref type="bibr" target="#b22">[23]</ref>. Lately, however, multi-scale approaches became feasible through FPN <ref type="bibr" target="#b13">[14]</ref>. As argued in <ref type="bibr" target="#b13">[14]</ref>, the use of pyramids both at the image and the feature level is computationally prohibitive. Nevertheless, here we show that by an appropriate weight sharing, the combination of ICN ( <ref type="figure" target="#fig_1">Figure 2</ref>) and FPN ( <ref type="figure" target="#fig_2">Figure 3</ref>) becomes feasible and outputs proportionally-sized features at different levels/scales in a fully-convolutional manner. This pipeline is independent of the backbone CNN (e.g., AlexNet <ref type="bibr" target="#b12">[13]</ref>, VGG <ref type="bibr" target="#b25">[26]</ref>, or ResNet <ref type="bibr" target="#b9">[10]</ref>). Here, we use ResNet <ref type="bibr" target="#b9">[10]</ref>. In the ICN, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, we use ResNet to compute a feature hierarchy C 1 , C 2 , C 3 , C 4 , C 5 , which correspond to the outputs of the residual blocks: conv1, conv2, conv3, conv4, and conv5 (blue boxes in <ref type="figure" target="#fig_1">Figure 2</ref>). The pixel strides for different residual boxes are 2, 4, 8, 16, and 32 pixels with respect to the input image.</p><p>To build our image cascade network, we resize the input image by bilinear interpolation to obtain four scaled versions (1.5×, 1×, 0.75×, 0.5×) and extract the feature hierarchy using ResNet subnetworks. For example, while all five residual blocks are used for the upsampled input (1.5×), for the half-resolution version (0.5×), only C 4 and C 5 are used. The cascade network is thus composed of different subnetworks of the ResNet sharing their weights with each other. Therefore, apart from resizing the input image, this step does not add further computation costs with respect to the single resolution baseline. ICN allows combining the low-level semantic features form higher resolutions (used for detecting small objects) with the high-level semantic features from low resolutions (used for detecting large objects). This helps the network to handle RS images with a wide range of GSD. A similar definition of ICN was proposed for real-time seman- tic segmentation in <ref type="bibr" target="#b32">[33]</ref>, but without taking into account different scales in the feature domain and using a cascaded label for each level to compensate for the sub-sampling. Such a cascaded label is more suitable for semantic segmentation.</p><p>FPNs <ref type="bibr" target="#b13">[14]</ref> allow extracting features at different scales by combining the semantically strong features (from the top of the pyramid) with the semantically weaker ones (from the bottom) via a top-down pathway and lateral connections (cf. <ref type="figure" target="#fig_2">Figure 3</ref>). The original bottom-up pathway of FPN (i.e., the feed-forward computation of the backbone CNN) is here replaced with the feature hierarchy extraction of ICN, more specifically with the output of their residual blocks C i , i ∈ {1, 2, 3, 4, 5}. The top-down pathway upsamples coarse-resolution feature maps (M i ) by a factor of 2 and merges them with the corresponding bottomup maps C i−1 (i.e., the lateral connections). The final set of feature maps P i , i ∈ {1, 2, 3, 4, 5}, is obtained by appending 3×3 convolutions to M i to reduce the aliasing effect of upsampling. We refer the reader to the work of Lin et al. <ref type="bibr" target="#b13">[14]</ref> for more details on FPNs. In the original FPN, the output of each C i goes through a 1×1 convolution to reduce the number of feature maps in M i . Here, we replace the 1×1 convolution with a DIN (Deformable Inception Network, cf. <ref type="figure" target="#fig_2">Figure 3</ref>) to enhance the localization properties of CNNs, especially for small objects which are ubiquitous in RS datasets. Although Inception modules <ref type="bibr" target="#b27">[28]</ref> have shown promising results in various tasks such as object recognition, their effectiveness for detection has not been extensively studied. While most current state-of-theart methods, such as Faster-RCNN, R-FCN <ref type="bibr" target="#b2">[3]</ref>, YOLOv3 <ref type="bibr" target="#b21">[22]</ref>, and SSD <ref type="bibr" target="#b17">[18]</ref>, focus on increasing the network depth, the benefit of Inception blocks lies in capturing details at varied scales which is highly desirable for RS imagery.</p><p>Deformable networks aim at overcoming the limitations of CNNs in modeling geometric transformations due to their fixed-size convolution kernels. When applying the models pretrained on ground imagery (such as our ResNet backbone) to RS images, the parameters of traditional convolution layers cannot adapt effectively to the new views of objects leading to degradations in localization performance. Using deformable convolutions in DIN helps accommodating such geometric transformations <ref type="bibr" target="#b3">[4]</ref>. Furthermore, the offset regression property of deformable convolution layers helps localizing the objects even outside the kernel range. Here, we train the added offset layer from scratch to let the network adjust to the new domain. 1 × 1 convolution layers reduce dimensions by half for the next deformable convolution (def-conv) layers. The channel input to DIN is divided equally among the four DIN branches. In our experiments, we did not observe an improvement by using 5 × 5 def-conv layers, hence the use of 3 × 3 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Rotation Region Proposal Network (R-RPN)</head><p>The output of each P i block in the FPN module is processed by multi-scale rotated region proposal networks (R-RPN) in order to provide rotated proposals, inspired by <ref type="bibr" target="#b18">[19]</ref>. More precisely, we modify RPN to propose rotated regions with 0, 45, 90, and 135 degrees rotation, not differentiating between the front and back of objects. For initializing the anchors, we cluster the scales and aspect ratios using K-means++ with the intersection over union (IoU) distance metric <ref type="bibr" target="#b21">[22]</ref>. We assign anchors with four different orientations to each level, P 2 through P <ref type="bibr">6 3</ref> . As in the original RPN, the output feature maps of FPN go through a 3 × 3 convolutional layer, followed by two parallel 1 × 1 fully-connected layers: an objectness classification layer (obj) and a box-regression layer (reg) (cf. <ref type="figure" target="#fig_0">Figure 1</ref>). For training, we assign labels to the anchors based on their IoUs with the ground-truth bounding boxes. In contrast to the traditional RPN, we use the smooth l 1 loss to regress the four corners (x i , y i ), i ∈ {1, 2, 3, 4}, of the OBB instead of the center point (x, y), and size (w and h) of the HBB. In this case, (x 1 , y 1 ) indicates the front of objects which allows to infer their orientations. As in Faster-RCNN, we minimize the multi-task loss</p><formula xml:id="formula_0">L ({p i }, {t i }) = 1 N obj i L obj (p i , p * i ) + λ 1 N reg i p * i L reg (t i , t * i ) ,<label>(1)</label></formula><p>where, for an anchor i in a mini-batch, p i is its predicted probability of being an object and p * i is its ground-truth binary label. For classification (object/notobject), the log-loss L obj (p i , p * i ) = −p * i log p i is used, while we employ the smooth l 1 loss</p><formula xml:id="formula_1">L reg (t i , t * i ) = l smooth 1 (t i − t * i ) with l smooth 1 (x) = 0.5x 2 if |x| &lt; 1 |x| − 0.5 otherwise<label>(2)</label></formula><p>for the bounding box regression. Here,</p><formula xml:id="formula_2">t xi = (x i − x i,a )/w a , t yi = (y i − y i,a )/h a (3) t * xi = (x * i − x i,a )/w a , t * yi = (y * i − y i,a )/h a<label>(4)</label></formula><p>are the four parameterized coordinates of the predicted and ground-truth anchors with x i , x i,a , and x * i denoting the predicted, anchor, and ground-truth, respectively (the same goes for y); and w a and h a are width and height of the anchor. N obj and N reg are normalizing hyper-parameters (the mini-batch size and number of anchor locations); and λ is the balancing hyper-parameter between the two losses which is set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Rotated Region of Interest Network (R-ROI)</head><p>Similar to <ref type="bibr" target="#b13">[14]</ref>, we use a multi-scale ROI pooling layer to process the regions proposed by R-RPN. Because the generated proposals are rotated, we rotate them to be axis-aligned. The resulting fixed-length feature vectors are fed into sequential fully-connected (fc) layers, and are finally sent through four sibling fc layers, which -for each object proposal -output the class prediction, refined HBB and OBB positions, as well as the angles of OBBs.</p><p>As seen for R-RPNs, OBBs are not restricted to be rectangular: R-RPN predicts the four corners of quadrilaterals without any constraint on the corners or edges. However, we observed that annotators tend to label rotated objects in RS images with quadrilaterals that are close to rotated rectangles. In order to enforce a rectangular shape of OBBs, we propose a new loss that considers the angles between adjacent edges, i.e., we penalize angles that are not 90 • .</p><p>Let us consider P ij a quadrilateral side connecting the corners i to j, where i, j ∈ {1, 2, 3, 4} and i = j. Then, using the cosine rule, we calculate the angle between adjacent sides (e.g., θ 1 between P 12 and P 13 ) as:</p><formula xml:id="formula_3">θ 1 = arccos((|P 12 | 2 + |P 13 | 2 − |P 23 | 2 )/(2 * |P 12 | * |P 13 |)) ,<label>(5)</label></formula><p>where |P ij | is the length of the side P ij . There are multiple ways to constrain θ l , l ∈ {1, 2, 3} to be right angles. (Note that θ 4 can be computed from the other three angles). We experimented with the following three angle-losses:</p><formula xml:id="formula_4">Tangent L1 : L angle−OBB (θ) = 3 l=1 (|tan(θ l − 90)|) Smooth L1 : L angle−OBB (θ) = 3 l=1 smooth L1 (|θ l − 90|) L2 : L angle−OBB (θ) = 3 l=1 (θ l − 90) 2 .<label>(6)</label></formula><p>Our final loss function is a multi-task loss composed of four losses that simultaneously predict the object category (L cls ), regress both HBB and OBB coordinates (L loc−HBB and L loc−OBB ), and enforce OBBs to be rectangular (L angle−OBB ):</p><formula xml:id="formula_5">L(p, u, t u , v) = L cls (p, u) + λ[u ≥ 1]L loc−HBB (t u , v)+ λ[u ≥ 1]L loc−OBB (t u , v) + λ[u ≥ 1]L angle−OBB (θ) ,<label>(7)</label></formula><p>where L cls (p, u) = −u log p and L loc−OBB (t u , v) is defined similar to L reg as in R-RPN above. u is the true class and p is the discrete probability distribution for the predicted classes, defined over K + 1 categories as p = (p 0 , ...., p K ) in which "1" is for the background category. t u = (t u xi , t u yi ) is the predicted OBB regression offset for class u and v = (v xi , v yi ) is the true OBB (i ∈ {1, 2, 3, 4}). L loc−HBB (t u , v) is defined similar to L reg in Faster-RCNN in which instead of OBB coordinates, {xmin, ymin, w, h} (the upper-left coordinates, width and height) of t u and v for the corresponding HBB coordinates are utilized. In case the object is classified as background, [u ≥ 1] ignores the offset regression. The balancing hyper-parameter λ is set to 1. To obtain the final detections, we compute the minimum bounding rectangles of the predicted quadrilaterals. As the final post-processing, we apply R-NMS in which the overlap between rotated detections is computed to select the best localized regions and to remove redundant regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Discussion</head><p>In this section, we present and discuss the evaluation results of the proposed method on three RS image datasets. All experiments were conducted using NVIDIA Titan X GPUs. The backbone network's weights were initialized using the ResNet-50/101 and ResNeXt-101 models pretrained on ImageNet <ref type="bibr" target="#b4">[5]</ref>. Images were preprocessed as described in baseline <ref type="bibr" target="#b29">[30]</ref>. Furthermore, the learning rate was 0.0005 for 60 epochs with the batch size of 1 using flipped images as the data augmentation. Additionally, during training, we applied online hard example mining (OHEM) <ref type="bibr" target="#b24">[25]</ref> to reduce false positives and we use Soft-NMS <ref type="bibr" target="#b0">[1]</ref> as a more accurate non-maximum suppression approach only for the HBB benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>The experiments were conducted on the DOTA <ref type="bibr" target="#b29">[30]</ref>, UCAS-AOD <ref type="bibr" target="#b33">[34]</ref>, and NWPU VHR-10 <ref type="bibr" target="#b1">[2]</ref> datasets which all have multi-class object annotations.</p><p>DOTA is the largest and most diverse published dataset for multi-class object detection in aerial and satellite images. It contains 2,806 images from different camera sensors, GSDs (10 cm to 1 m), and sizes to reflect real-world scenarios and decrease the dataset bias. The images are mainly acquired from Google Earth, and the rest from the JL-1 and GF-2 satellites of the China Center for Resources Satellite Data and Application. Image sizes vary from 288 to 8,115 pixels in width, and from 211 to 13,383 pixels in height. There are 15 object categories: plane, baseball diamond (BD), bridge, ground field track (GTF), small vehicle (SV), large vehicle (LV), tennis court (TC), basketball court (BC), storage tank (SC), soccer ball field (SBF), roundabout (RA), swimming pool (SP), helicopter (HC), and harbor. DOTA is split into training (1/2), validation (1/6), and test (1/3) sets.</p><p>UCAS-AOD contains 1,510 satellite images (≈ 700 × 1300 px) with 14,595 objects annotated by OBBs for two categories: vehicles and planes. The dataset was randomly split into 1,110 training and 400 testing images.</p><p>NWPU VHR-10 contains 800 satellite images (≈ 500 × 1000 px) with 3,651 objects were annotated with HBBs. There are 10 object categories: plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, and small vehicle. For training, we used non-rotated RPN and region of interest (ROI) networks only for the HBBs detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>In order to assess the accuracy of our detection and the quality of region proposals, we adapted the same mean average precision (mAP) and average recall (AR) calculations as for DOTA <ref type="bibr" target="#b29">[30]</ref>. We conducted ablation experiments on the validation set of DOTA. Furthermore, we compare our method to the ones in <ref type="bibr" target="#b29">[30]</ref> for HBB and OBB prediction tasks as well as Yang et al. <ref type="bibr" target="#b31">[32]</ref> for OBB task based on the test set whose ground-truth labels are undisclosed. The results reported here were obtained by submitting our predictions to the official DOTA evaluation server 4 . We used 0.1 threshold for R-NMS and 0.3 for Soft-NMS.</p><p>The impact of ICN: From <ref type="table">Table 1</ref> shows the evaluation results of ICN. According to the table, adding OHEM to ResNet-50 improved the accuracy by a narrow margin. Using a deeper network such as ResNet-101 further improved the accuracy. As a next step, adding a 1.5× cascade level increased mAP by around 2% indicating that the up-sampled input can have a significant impact. Based on this, we added smaller cascade levels such as 0.75× and 0.5×, which however, increased the accuracy to a lesser extent. This could be due to the fact that the majority of objects within this dataset are small, so reducing resolution is not always optimal. Further increasing the cascade levels (e.g., 1.75× and 2×) <ref type="table">Table 1</ref>: Evaluation of (1) the impact of ICN with different cascade levels, <ref type="bibr" target="#b1">(2)</ref> the effect of the backbone network (ResNet50/101, ResNeXt101), and (3) the influence of the number of proposals for the OBB prediction task. The models were trained on the DOTA training set and results are on the validation set. degraded the accuracy, which is due to the lack of annotations for very small objects such as small vehicles. We argue that extracting ResNet features on upsampled images (1.5×) is beneficial for the small objects in the DOTA dataset, whereas doing this on the downsampled input (0.75×, 0.5×) brings smaller improvements because of the lower number of large objects in the dataset. We observed that replacing ResNet-101 with ResNeXt-101 causes a small drop in accuracy which could be due to the shallower architecture of ResNeXt-101. Results indicated that using a higher number of proposals (2000) increases the accuracy to a small degree, which however came with an increased computation cost; thus, we considered 300 proposals for the rest of our experiments. The impact of DIN: From <ref type="table" target="#tab_1">Table 2</ref> we see that replacing the 1 × 1 convolution after the residual blocks C i by DIN can augment mAP by more than 2%. More specifically, using DIN after lower level C i s resulted in slightly higher accuracy than using it after higher levels (e.g., mAP for C4 &gt; mAP for C5). In addition, employing DIN after multiple C i s can further improve model performance (e.g., mAP for C4 &lt; mAP for C4-C5 &lt; mAP for C3-C5). Kernel size strongly affects the high resolution (semantically weak) features. Thus, applying DIN to the low-level C i s enriched the features and adapts them to the new data domain. Comparing the last two rows of <ref type="table" target="#tab_1">Table 2</ref>, we see that deformable convolutions also have a positive impact; however, the improvement is smaller.</p><p>Rotated RPN and ROI modules: Using clustered initialized anchors with rotation, we obtained an additional 0.7% mAP. To initialize anchors, we selected 18 anchors compared to 15 in Faster-RCNN in clustering ground-truth OBBs. We observed no significant increase in IoU with higher number for anchors. Furthermore, we considered each anchor at four different angles (0, 45, 90, 135 degrees rotation). The total number of anchors is thus 18 × 4. <ref type="table" target="#tab_2">Table 3</ref> shows that using rotated proposals in the R-RPN/ R-ROI layers improves mAP by 1.4%, indicating that these proposals are more appropriate for RS images. In addition, we see that using a joint loss function (for HBB and OBB prediction) can increase the prediction of OBBs by 0.81% mAP. We believe that HBBs provide useful "hints" on the position of the object for regressing OBBs more accurately. This is not the case for HBB prediction: here, using only the HBB regression loss achieves 3.98% higher mAP as compared to the joint loss. This could be due to the complexity that OBB imposes on the optimization problem. Thus, we apply our algorithm on the HBB benchmark without the OBB loss.</p><p>Enforcing rectangular bounding boxes: We investigated three different loss functions to enforce the rectangularity of the quadrilateral bounding boxes. Results in <ref type="table" target="#tab_2">Table 3</ref> show that all three angle losses improve the output accuracy and angle L2 performs the best. The reason behind the lower performance of angle tangent L1 could be the property of the tangent function: it leads to very high loss values when the deviation from the right angle is large. Angle smooth  L1 performs marginally worse than angle L2 which could be due to its equal penalization for deviations larger than 1 degree from the right angle. By studying the recall-IoU curve, we noticed that very small and very large objects (e.g., small vehicles and very large bridges) have the lowest localization recall and medium-size objects have the highest recall. Overall AR for the proposals on DOTA is 61.25%. A similar trend is observed for prec-recall curves.</p><p>On False Positives: To investigate false positives, we used the object detection analysis tool from <ref type="bibr" target="#b10">[11]</ref>. For the sake of brevity, we merge the bridge and harbor as the long objects class, and the LV, SV, and ship classes as the vehicles class. Similar observations were made for the rest of the classes. The large blue area in <ref type="figure">Figure 7</ref> indicates that our method detects object categories with a high accuracy. Moreover, recall is around 80% (the red line) and is even higher with "weak" (10% overlap with the ground truth) localization criteria (dashed red line). The majority of confusions are with the background (the green area) while the confusion with similar object classes is much smaller (yellow area). This issue is more severe for long objects. Although using only down-sampled levels in the image cascade alleviates this issue, it lowers the performance for small objects. Since the proposals are not able to capture long objects effectively, they cause a large localization error. Additionally, the false positives for similar-classes often occur for vehicles: small and large vehicles are mistaken for each other.</p><p>Comparison with the state of the art: <ref type="table" target="#tab_3">Tables 4 and 5</ref> show the performance of our algorithm on the HBB and OBB prediction tasks DOTA, based on the official evaluation of the methods on the test set with non-disclosed ground- <ref type="figure">Fig. 6</ref>: Sample outputs of our algorithm on the NWPU VHR-10 (three right columns -different camera sensors) and UCAS-AOD (two left columns -different weather conditions, camera angles, and GSDs) datasets. truth. We evaluate our method in two scenarios: training only on the 'train' subset, and training on the training and validation sets ('trainval'). Our method significantly outperforms all the published methods evaluated on this benchmark, and training on 'trainval' brings an additional 2-4% in mAP over training only on 'train'. Looking at individual class predictions, only the mAPs of the helicopter, bridge, and SBF classes are lower than the baseline, possibly due to their large (and unique) size, complex features, and low occurrence in the dataset. Generalization on the NWPU VHR-10 and UCAS-AOD datasets: As shown in <ref type="table" target="#tab_5">Table 6</ref>, our algorithm significantly improves upon the baseline also on these two additional datasets. This demonstrates the good generalization capability of our approach. Results are competitive even when we trained our algorithm only on DOTA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we presented a new algorithm for multi-class object detection in unconstrained RS imagery evaluated on three challenging datasets. Our algorithm uses a combination of image cascade and feature pyramids together with rotation proposals. We enhance our model by applying a novel loss function for <ref type="figure">Fig. 7</ref>: False positive trends. Stacked area plots show the fraction of each type of false positive by increasing the number of detections; line plots show recall for the weak localization with more 10% overlap with ground truth (dashed line) and the strong one with more than 50% overlap (solid line). Cor: correct, Loc: localization, Sim:similar classes, Oth: other reasons, BG: background. geometric shape enforcement using quadrilateral coordinates. Our method outperforms other published algorithms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref> on the DOTA dataset by a large margin. Our approach is also robust to differences in spatial resolution of the image data acquired by various platforms (airborne and space-borne). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of our algorithm for (non-)rotated multi-class object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of the Image Cascade Network (ICN). Input images are first up-and down-sampled. Then they are fed into different CNN cascade levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of the ICN and FPN subnetworks with deformable inception network (DIN). DIN is the modified Inception block to learn features of objects including geometrical features in flexible kernel sizes with stride 1. "defconv" stands for deformable convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Outputs of HBB (left) and OBB (right) prediction on an image of DOTA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of employing DIN after certain residual blocks C i with and without deformable convolutions on the validation set of DOTA.</figDesc><table><row><cell cols="3">DIN Def. conv. mAP (%)</cell></row><row><cell>-</cell><cell>-</cell><cell>65.97</cell></row><row><cell>C4</cell><cell>-</cell><cell>66.24</cell></row><row><cell>C5</cell><cell>-</cell><cell>66.28</cell></row><row><cell>C4-C5</cell><cell>-</cell><cell>66.41</cell></row><row><cell>C3-C5</cell><cell>-</cell><cell>66.75</cell></row><row><cell>C2-C5</cell><cell>-</cell><cell>67.47</cell></row><row><cell>C2-C5</cell><cell></cell><cell>68.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of (1) the impact of rotated RPN and RoI and (2) the effect of the loss functions enforcing the rectangularity of the bounding boxes.</figDesc><table><row><cell cols="3">Angle Loss functions Rotated BBs in RPN &amp; RoI mAP (%)</cell></row><row><cell>-</cell><cell>-</cell><cell>64.27</cell></row><row><cell>-</cell><cell></cell><cell>65.67</cell></row><row><cell>Tangent L1</cell><cell></cell><cell>66.91</cell></row><row><cell>Smooth L1</cell><cell></cell><cell>67.41</cell></row><row><cell>L2</cell><cell></cell><cell>68.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison of the baseline and our method on the HBB task in test set of DOTA dataset. FR-H stands for Faster R-CNN[23] trained on HBB. TV stands for 'trainval' and T for 'train' subsets. TV 39.20 76.90 33.87 22.73 34.88 38.73 32.02 52.37 61.65 48.54 33.91 29.27 36.83 36.44 38.26 11.61 R-FCN[3] TV 52.58 81.01 58.96 31.64 58.97 49.77 45.04 49.29 68.99 52.07 67.42 41.83 51.44 45.15 53.3 33.89 SSD[18] TV 29.86 57.85 32.79 16.14 18.67 0.05 36.93 24.74 81.16 25.1 47.47 11.22 31.53 14.12 9.09 0.0 FR-H[23] TV 60.64 80.32 77.55 32.86 68.13 53.66 52.49 50.04 90.41 75.05 59.59 57.00 49.81 61.69 56.46 41.85 ours T 70.54 89.54 73.48 51.96 70.33 73.39 67.91 78.15 90.39 78.73 78.48 51.02 59.41 73.81 69.00 52.59 ours TV 72.45 89.97 77.71 53.38 73.26 73.46 65.02 78.22 90.79 79.05 84.81 57.20 62.11 73.45 70.22 58.08</figDesc><table><row><cell>method data mAP plane BD bridge GTF SV</cell><cell>LV ship TC</cell><cell>BC</cell><cell>ST SBF RA harbor SP</cell><cell>HC</cell></row><row><cell>Yolov2-[22]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Quantitative comparison of the baselines and our method on the OBB prediction task in test set of DOTA dataset. Abbreviations are the same as inTable 4. Note that only FR-O[23] is trained with OBB. TV 25.49 52.75 24.24 10.6 35.5 14.36 2.41 7.37 51.79 43.98 31.35 22.3 36.68 14.61 22.55 11.89 R-FCN[3] TV 30.84 39.57 46.13 3.03 38.46 9.1 3.66 7.45 41.97 50.43 66.98 40.34 51.28 11.14 35.59 17.45 SSD[18] TV 17.84 41.06 24.31 4.55 17.1 15.93 7.72 13.21 39.96 12.05 46.88 9.09 30.82 1.36 TV 39.95 49.74 64.22 9.38 56.66 19.18 14.17 9.51 61.61 65.47 57.52 51.36 49.41 20.8 45.84 24.38 FR-O[23] TV 54.13 79.42 77.13 17.7 64.05 35.3 38.02 37.16 89.41 69.64 59.28 50.3 52.91 47.89 47.4 46.3 R-DFPN[31] TV 57.94 80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88 Yang et al.[32] TV 62.29 81.25 71.41 36.53 67.44 61.16 50.91 56.60 90.67 68.09 72.39 55.06 55.60 62.44 53.35 51.47 ours T 64.98 81.24 68.74 43.36 61.07 65.25 67.72 69.20 90.66 71.47 70.21 55.41 57.28 66.49 61.3 45.27 ours TV 68.16 81.36 74.30 47.70 70.32 64.89 67.82 69.98 90.76 79.06 78.20 53.64 62.90 67.02 64.17 50.23</figDesc><table><row><cell>method</cell><cell>data mAP plane BD bridge GTF SV</cell><cell>LV ship TC</cell><cell>BC</cell><cell>ST SBF RA harbor SP</cell><cell>HC</cell></row><row><cell cols="5">Yolov2-[22] 3.5</cell><cell>0.0</cell></row><row><cell>FR-H[23]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of results on NWUH VHR-10 and UCAS-AOD datasets.</figDesc><table><row><cell>method</cell><cell>train data</cell><cell>test data</cell><cell>mAP</cell></row><row><cell cols="4">Cheng et al.[2] NWUH VHR-10 NWUH VHR-10 72.63</cell></row><row><cell>ours</cell><cell cols="3">NWUH VHR-10 NWUH VHR-10 95.01</cell></row><row><cell>ours</cell><cell>DOTA</cell><cell cols="2">NWUH VHR-10 82.23</cell></row><row><cell>Xia et al.[30]</cell><cell>UCAS-AOD</cell><cell>UCAS-AOD</cell><cell>89.41</cell></row><row><cell>ours</cell><cell>UCAS-AOD</cell><cell>UCAS-AOD</cell><cell>95.67</cell></row><row><cell>ours</cell><cell>DOTA</cell><cell>UCAS-AOD</cell><cell>86.13</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">P6 is a stride 2 sub-sampling of P5 used to propose regions for large objects. P1 is not computed due to its large memory footprint.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://captain.whu.edu.cn/DOTAweb/evaluation.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TGRS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">Deformable convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN. ICCV</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<title level="m">Recombinator networks: Learning coarse-to-fine feature aggregation. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft coco: Common objects in context. ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast multiclass vehicle detection on aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mattyus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS Letters</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning a rotation invariant detector with rotatable bounding box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09405</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">SSD: single shot multibox detector. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolo9000: Better, faster, stronger. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ross</surname></persName>
		</author>
		<editor>Fast R-CNN. CVPR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks For Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning based multicategory object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schuchert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Defense and Security</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Position detection and direction prediction for arbitrary-oriented ships via multiscale rotation region convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04828</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08545</idno>
		<title level="m">Icnet for real-time semantic segmentation on high-resolution images</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Orientation robust object detection in aerial images using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
