<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic-structured Semantic Propagation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xiaodan1@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Zhou</surname></persName>
							<email>hfzhou@wumii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wumii</forename><surname>Tech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Limited</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
							<email>eric.xing@petuum.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Petuum Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic-structured Semantic Propagation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic concept hierarchy is still under-explored for semantic segmentation due to the inefficiency and complicated optimization of incorporating structural inference into dense prediction. This lack of modeling semantic correlations also makes prior works must tune highly-specified models for each task due to the label discrepancy across datasets. It severely limits the generalization capability of segmentation models for open set concept vocabulary and annotation utilization. In this paper, we propose a Dynamic-Structured Semantic Propagation Network (DSSPN) that builds a semantic neuron graph by explicitly incorporating the semantic concept hierarchy into network construction. Each neuron represents the instantiated module for recognizing a specific type of entity such as a super-class (e.g. food) or a specific concept (e.g. pizza). During training, DSSPN performs the dynamic-structured neuron computation graph by only activating a sub-graph of neurons for each image in a principled way. A dense semantic-enhanced neural block is proposed to propagate the learned knowledge of all ancestor neurons into each fine-grained child neuron for feature evolving. Another merit of such semantic explainable structure is the ability of learning a unified model concurrently on diverse datasets by selectively activating different neuron sub-graphs for each annotation at each step. Extensive experiments on four public semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape and Mapillary) demonstrate the superiority of our DSSPN over state-of-the-art segmentation models. Moreoever, we demonstrate a universal segmentation model that is jointly trained on diverse datasets can surpass the performance of the common fine-tuning scheme for exploiting multiple domain knowledge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing and segmenting arbitrary objects, posed as a primary research direction in computer vision, has achieved great success driven by the advance of convolutional neural networks (CNN). However, current segmentation models using generic deeper and wider network lay-ers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b21">22]</ref> still show unsatisfactory results of recognizing objects in a large concept vocabulary with limited segmentation annotations. The reason is that they ignore the intrinsic taxonomy and semantic hierarchy of all concepts. For example, giraffe, zebra and horse categories share one super-class ungulate that depicts their common visual characteristics, which makes them be easily distinguished from cat/dog. In addition, due to diverse level of expertise and application purposes, the target concept set of semantic segmentation can be inherently open-ended and highly structured for each specific task/dataset. However, some few techniques also explored the semantic hierarchy for visual recognition by resorting to complex graphical inference <ref type="bibr" target="#b6">[7]</ref>, hierarchical loss <ref type="bibr" target="#b30">[31]</ref> or word embedding priors <ref type="bibr" target="#b39">[39]</ref> on final prediction scores. Their loss constraints can only indirectly guide visual features to be hierarchy-aware, which is hard to be guaranteed and often leads to inferior results compared to generic CNN models.</p><p>Furthermore, this lack of modeling semantic hierarchy also prohibits the research towards a universal segmentation model that can address the segmentation of all concepts at once. Existing works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b37">37]</ref> often strive to train a task-specific model due to the label discrepancy across dataset with limited annotations. That way largely limits the model generation capability and deviates from human perception that can recognize and associate all concepts by considering the concept hierarchy. If one wants to improve one task by fully utilizing other annotations with different label set, prior works must remove the classification layer and only share intermediate representations. Our target of learning a universal segmentation model also has some connections to very recent researches in combining different visual tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">36]</ref> or multi-modal tasks <ref type="bibr" target="#b16">[17]</ref> in one model, which often use several fixed network branches with specialized losses to integrate all tasks.</p><p>In this work, we aim at explicitly integrating a semantic concept hierarchy into the dynamic network optimization, called as Dynamic-Structured Semantic Propagation Network (DSSPN). In the spirit of curriculum learning <ref type="bibr" target="#b1">[2]</ref> that gradually increases the target difficulty levels and exploits previously learned knowledge for learning new fine-grained  concepts, DSSPN first progressively builds a semantic neuron graph following the semantic concept hierarchy in which each neuron is responsible for segmenting out regions of one concept in the word hierarchy. The learned features of each neuron are further propagated into its child neurons for evolving features in order to recognize more fine-grained concepts. For each image or dataset, DSSPN performs the dynamic-structured semantic propagation over an activated semantic neuron sub-graph where only the present concepts and their ancestors are selected. Benefiting from the merits of semantically ordered network modules and the dynamic optimization strategy, our DSSPN would enable the learned visual representation to naturally embed rich semantic correlations between diverse concepts. Such explicit neuron definition mechanism makes the proposed DSSPN be a semantically explainable dynamic network architecture with good memory and computation efficiency. Rather than only taking into account features of the parent neuron for each neuron, we introduce a new dense semanticenhanced neural block which densely integrates the features of all ancestor neurons to evolve feature representation of each neuron, inspired by DenseNets <ref type="bibr" target="#b15">[16]</ref>. By broadcasting the learned knowledge of all ancestor neurons into each neuron, our DSSPN can fully exploit the semantic correlation and inheritance into the feature learning in a more efficient way. As explained in very recent information bottleneck theory <ref type="bibr" target="#b35">[35]</ref>, the deep networks often tend to squeeze the information through a bottleneck and retain only the features most relevant to targets. Such dense semantic connection thus alleviates the information loss along deeper layers by explicitly enforcing ancestor neurons to preserve discriminate features for recognizing more fine-grained concepts.</p><p>Note that our DSSPN activates dynamic computation graphs for each sample during training. For scalability, a dynamic batching optimization scheme is proposed to enable optimize multiple computation graphs within one batch by configuring a dynamic number of samples for learning distinct neural modules at each step. A memory efficient implementation of our DSSPN is also described.</p><p>Extensive experiments on four popular semantic segmentation datasets (i.e. Coco-Stuff <ref type="bibr" target="#b3">[4]</ref>, ADE20k <ref type="bibr" target="#b41">[41]</ref>, Cityscape <ref type="bibr" target="#b5">[6]</ref> and Mapillary <ref type="bibr" target="#b26">[27]</ref>) demonstrate the effectiveness of incorporating our DSSPN into the state-of-the-art basic segmentation networks. We thus demonstrate that our dynamic-structure propagation mechanism is an effective way to implement a semantic explaining way that is needed for segmenting massive intrinsically structured concepts. Moreover, we show that learning a unified DSSPN model over diverse models can also bring the performance over the commonly used fine-tuned scheme for utilizing annotations in multiple domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic segmentation. Semantic segmentation has recently attracted a hug amount of interests and achieved great progress with the advance of deep convolutional neural networks. Most of prior works focus on developing new structures and filter designs to improve general feature representation, such as deconvolutional neural network <ref type="bibr" target="#b28">[29]</ref>, encoder-decoder architecture <ref type="bibr" target="#b0">[1]</ref>, dilated convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">38]</ref>, pyramid filters <ref type="bibr" target="#b40">[40]</ref> and wider nets <ref type="bibr" target="#b37">[37]</ref>. Although these methods show promising results on datasets with a small label set, e.g. 21 of PASCAL VOC <ref type="bibr" target="#b8">[9]</ref>, they obtain relatively low performance on recently released benchmarks with large concept vocabularies (e.g. 150 of ADE20k <ref type="bibr" target="#b41">[41]</ref> and 182 of COCO-stuff <ref type="bibr" target="#b3">[4]</ref>). These models directly use one flat prediction layer to classify all concepts and disregard their intrinsic semantic hierarchy and correlations. Such prediction strategy largely limited the model capability and also makes the network parameters hardly adapt to other recognition tasks or new objects. In this paper, our DSSPN builds the dynamic network structure according the semantic concept hierarchy, where each neural module takes care of recognizing one concept in the taxonomy, and modules are connected following the structure to enforce semantic feature propagation.</p><p>Dynamic and graph network structure. Exploring dynamic networks has recently received increasing attentions due to their good model flexibility and huge model capacity. Prior works proposed a family of graph-based CNNs <ref type="bibr" target="#b27">[28]</ref>, RNNs <ref type="bibr" target="#b20">[21]</ref> and reinforcement learning structures <ref type="bibr" target="#b18">[19]</ref> to accommodate networks into different graph-structured data, such as superpixels, social networks and object relationships. There exists some few works that investigated dynamic networks. For example, Liang et al. <ref type="bibr" target="#b19">[20]</ref> evolved the network structures by learning how to merge the graph nodes automatically. Shi et al. <ref type="bibr" target="#b31">[32]</ref> aims at learning the local correlation structure for spatio-temporal data. Different from them, our DSSPN introduces a general dynamic network for recognizing and segmenting out objects in the large-scale and highly-structured concept vocabulary. The neural modules are dynamically activated following the present concept tree for each image.</p><p>Hierarchical recognition. There is a line of researches that exploit the structure of WordNet to achieve hierarchical recognization. For example, Deng et al. <ref type="bibr" target="#b7">[8]</ref> used an accuracy-specificity trade-off algorithm to explore the Word-Net hierarchy while Ordonez et al. <ref type="bibr" target="#b29">[30]</ref> learns the mapping of common concepts to entry-level concepts. Deng et al. <ref type="bibr" target="#b6">[7]</ref> further employed a label relation graph to guide the neural network learning. Most recently, Zhao et al. <ref type="bibr" target="#b39">[39]</ref> addressed the open-vocabulary scene parsing by constructing asymmetric word-embedding space. Rather than implicitly enforcing semantic relations into network representations as previous works did, the proposed DSSPN explicitly constructs the network modules guided by their semantic hierarchy. The dynamic neural activation strategy makes the model scalable and applicable for a universal segmentation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dynamic Tree-Structured Propagation Network</head><p>In order to fully exploit concept correlations for recognizing and segmenting out a large-scale concept vocabulary, we aim at explicitly incorporating the semantic concept hierarchy into the dynamic network structure for semantic ℎ 0 (256)</p><formula xml:id="formula_0">ℎ 1 (48) ℎ 3 (48) ℎ 4 (48)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Propagation Graph</head><p>Dense semantic-enhanced block segmentation. <ref type="figure">Figure 1</ref> gives an overview of our proposed DSSPN. After feeding the images into basic convolutional networks for extracting intermediate features x, the DSSPN is appended to perform dense pixel-wise recognition with a dynamically induced neural propagation scheme. We first build a large semantic neuron graph that each neuron corresponds to one parent concept in the semantic concept hierarchy and aims at recognizing between its child concepts. During training, given the concepts appeared in each image, only a small neuron graph that would derive the target concepts are activated, leading to the dynamic semantic propagation graph for effective and efficient computation. A new dense semantic-enhanced neural block is proposed to evolve features for fine-grained concepts by incorporating features of their ancestor concepts. We describe in more details in the following sections.</p><formula xml:id="formula_1">0 (2) 2 (27) 3 (15) 4 (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Semantic Neuron Graph</head><p>We first denote the semantic concept graph as G c =&lt; C v , C e &gt;, where C v consists of all concepts {C v i } in a predefined knowledge graph (described in Section 4.5) and C e = (C i , C j ) indicates C i (e.g. chair) is the parent concept of C j (e.g. armchair). Our DSSPN thus is constructed with the whole semantic neuron graph G n =&lt; N v , N e &gt; with M neurons in total. Each semantic neuron n i ∈ N v corresponds to one parent concept (e.g chair) that has at least two child concepts within C v and N e corresponds to C e . As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, each neuron n i first employs one dense semantic-enhanced block to generate fine-grained features h i using inherited features from its ancestors . The prediction layer L i with 1 × 1 convolutional filters takes h i as input and produces T i prediction maps to distinguish between its  </p><formula xml:id="formula_2">T i child concepts {C j }, &lt; C v i , C v j &gt;∈ C e .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dense Semantic-enhanced Block</head><p>Inspired from the successful practice of dense connectivity <ref type="bibr" target="#b15">[16]</ref> for image classification, we design a tree-structured dense semantic-enhanced block for improving information flow from the highly abstracted concepts to fine-grained concepts , following the inheritance path. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the scheme of dense semantic-enhanced block for the desk concept with the inheritance path: entity-furniture-thingstable-desk. Let Ω i denote the ancestor indexes of the concept C i and x as the convolutional features x from basic ConvNet. Consequently, each semantic neuron n i receives the feature maps {h k } k∈Ωi of all inherited preceding neurons {n k } k∈Ωi starting from the root entity concept as input:</p><formula xml:id="formula_3">h 0 = Q(x), h i = H i ([{h k } k∈Ωi ])<label>(1)</label></formula><p>where (Q)(·) indicates the transition layer from basic convolutional features x to features h 0 of the root neuron.</p><p>[{h k } k∈Ωi ] refers to the concatenation of the feature maps produced in ancestral neurons {n k } k∈Ωi . H i indicates the non-linear transformation function, composed of operations: rectified linear units (ReLU) <ref type="bibr" target="#b11">[12]</ref> and Convolution (Conv). h i is the resulting m hidden feature maps of the neuron n i . Each neuron n i thus has m 0 + m × (d − 1) input feature maps, where m 0 = 256 is the channel number of hidden features h 0 of the root neuron after transitioning from x and d i is the depth of concept C i in the semantic concept hierarchy. m is set as 48 which is sufficient from our experiments in Section 4.5. This information suppression with a relatively small number of hidden features can be regarded as retaining only details that are enough to distinguish between a small set of child concepts.</p><p>Different from traditional semantic segmentation that learns one final prediction layer with a large number of feature maps to directly recognize all concepts, our DSSPN decomposes the pixel-wise predictions into a set of easier sub-tasks, which only needs a small feature map size for each sub-task and also improves the feature discriminative capability. An important difference between DSSPN and DenseNet <ref type="bibr" target="#b15">[16]</ref> is that DSSPN dynamically specifies different feature concatenation routes and depths for each concept following the concept knowledge graph.</p><p>We design Q as as a Atrous Spatial Pyramid Pooling (ASSP) module <ref type="bibr" target="#b4">[5]</ref> with three branches of 3 × 3 convolution layers and three rates as 6, 12, 18, respectively. The output feature size of Q is 256. The input feature size for H i depends on the concept depth d, that is, the degree of fine-grained specification. In our case, the maximal depth is 5, which effectively constrains the memory footprint growth. To improve computation efficiency, H i first employs a bottleneck layer with a 1 × 1 convolution and a 3 × 3 convolution layer to reduce the number of input feature-maps, i.e., to the ReLU-Conv(1 × 1)-ReLU-Conv(3 × 3). Each semantic neuron and transition layer are followed by a ReLu function. The output feature size of 1 × 1 conv. layer is set as 4m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic-structured Semantic Propagation</head><p>During training, our DSSPN performs the dynamicstructured semantic propagation, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Given a set of groundtruth concepts {C t } appearing in each image I, the activated semantic neuron structureḠ n =&lt; N v ,N e &gt; can be obtained by only awakening semantic</p><formula xml:id="formula_4">neuronsN v = {n i }, C i ∈ β({C t })</formula><p>and their edges that can derive the target labels. β(·) indicates the ancestor set of all groundtruth concepts. For example, for the second image in <ref type="figure" target="#fig_2">Figure 3</ref>, only the neurons for the concepts entity, structure-stuff, plant-stuff are activated in order to hierarchically segmenting out the targets tree, grass, plant-other, fence and animal-things. Note that the neuron of animalthings is deactivated since the image is only annotated with a coarse animal class instead of more precise dog. Formally, the dynamic-structured neural computation graph can be constituted by recurrently propagating hidden features along the activate structure as:</p><formula xml:id="formula_5">h i = H i ([x, {h k } k∈Ωi ]), P i = L i (h i ), h j = H j ([x, {h k } k∈Ωi , h i ]), (n i , n j ) ∈N e ,<label>(2)</label></formula><p>where the output hidden features h i are only propagated to the activated child neurons inN v for each training image. Starting from the root neuron, our DSSPN recurrently traverses the whole activated semantic neuron sub-graph for hierarchical pixel-wise prediction. It thus leads to the dynamicstructured neural module back-propagation for each image.</p><p>For training each neuron, we use the pixel-wise binary cross-entropy loss to supervise the dense prediction of each child concept, which focuses more on recognizing each child concept instead of learning any competition between them. This good characteristic leads a better flexibility for adding and pruning child concepts of each parent neuron, especially for joint training multiple datasets and extending the semantic concept hierarchy.</p><p>During testing phase, we use the hierarchical pixel-wise prediction over the semantic neuron graph. Starting from the root neuron, each neuron predicts the per-pixel predictions for classifying its child neurons and then only activates child ones with available predictions for further parsing regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Universal Semantic Segmentation</head><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, our DSSPN can be naturally used to train a universal semantic segmentation for combining diverse segmentation dataset. The distinct concept sets from different dataset can be simply projected into a unified knowledge graph, and each image is then trained using the same strategy described in Section 3.3. However, different datasets may be annotated with diverse granularities. For example, the road region on Cityscape dataset is further annotated into several fine-grained concepts on Mapillary dataset, e.g. curb, crosswalk, curb cut and lane. In order to alleviate the label discrepancy issues and stabilize the parameter optimization during joint training, we propose a concept-masking scheme.</p><p>For training each image from the dataset D t , we mask out the undefined concepts that share the same parent with defined concepts in D t during training. As a toy example, to train the third image in <ref type="figure" target="#fig_2">Figure 3</ref>, the way neuron only outputs the pixel-wise predictions for road and sidewalk and ignores the predictions for undefined concepts in Cityscape, e.g. lane. That way would thus improve the labeling consistency during joint training.</p><p>Another merit of our DSSPN is the ability of updating and extending the model capacity in an online way. Benefiting the usage of dynamic-structured propagation scheme and joint training strategies, we can dynamically add and prune semantic neurons and concept labels for different purposes (e.g. adding more dataset) while keeping the previously learned parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Dynamic Batching Optimization</head><p>Instead of using single instance training in most of treestructured <ref type="bibr" target="#b34">[34]</ref> and graph-structured <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19]</ref> networks, our DSSPN uses dynamic graph batching strategy to make good use of efficient data-parallel algorithms and hardware, inspired by very recent attempts <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26]</ref>. We implement our DSSPN on PyTorch, which is one of dynamic neural network toolkit that offers more flexibility for coping with data of varying structures, compared to those that operate on statically declared computations (e.g., TensorFlow). Note that the neurons for high-level concepts (e.g. animal-things) are executed more often than those for fine-grained concepts (e.g. dog). For each batch, our DSSPN automatically batches those semantic neurons that are shared over all images for parallelism and then forwards execution to rest few isolated neurons following the activated neuron graph. DSSPN can thus speedups dynamically declared computation graphs for all images within one batch since most of shared semantic neurons are in place in the first few depths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Memory-Efficient Implementation</head><p>Despite of the large whole semantic neruon graph of DSSN, it only activates a relative small computation graph for each image during training, which effectively constraints the memory and computation consumption. Although each semantic neuron only produces m feature maps (where m is small-set as 48), but uses all previous feature maps from its ancestors as input. This would cause the number of parameters to grow quadratically with semantic hierarchy depth, which could be solved by a proper memory allocation strategy. To further reduce the memory consumption, we share memory allocations of neurons for parent concepts in practice. It effectively reduces the memory cost for storing feature maps from quadratic to linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We empirically demonstrate DSSPN's effectiveness on four benchmark semantic segmentation datasets and compare with state-of-the-art architectures</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Details</head><p>Network Training. We implement our DSSPN on Pytorch, with 2 GTX TITAN X 12GB cards on a single server. We use the Imagenet-pretrained ResNet-101 <ref type="bibr" target="#b13">[14]</ref> networks as our basic ConvNet following the procedure of <ref type="bibr" target="#b4">[5]</ref> and employ output stride = 8, and replace the 1000-way Imagenet classifier in the last layer with our DSSPN structure. The network parameters in each neuron are presented in Section 3, and the padding is set to keep the feature resolutions in DSSPN. We fix the moving means and variations in batch normalization layers of Resnet-101 during finetuning. The sum of binary cross-entropy loss for each position is employed on each semantic neuron to train hierarchical dense prediction. The predictions are thus compared with the ground truth labels (subsampled by 8), and the unlabeled pixels are ignored. We optimize the objective function with respect to the weights at all layers by the standard SGD procedure.Inspired by <ref type="bibr" target="#b4">[5]</ref>, we use the "poly" learning rate policy and set base learning rate to 0.003 for newly initialized DSSPN parameters and power to 0.9. We set the learning rate as 0.00003 for pretrained layers. For each dataset, we train 90 epochs for the good convergence. Momentum and weight decay are set to 0.9 and 0.0001 respectively. For data augmentation, we adopt random flipping, random cropping and random resize between 0.5 and 2 for all datasets. Due to the GPU memory limitation, the batch size is 6 for Cityscape and Mapillary dataset, and 4 for Cocostuff and ADE20k dataset due to their larger label numbers (e.g. 182). The input crop size is set as 513 × 513 for all datasets.</p><p>We first evaluate the proposed DSSPN on four challenging datasets: Coco-Stuff <ref type="bibr" target="#b3">[4]</ref>, ADE20k <ref type="bibr" target="#b41">[41]</ref>, Cityscape <ref type="bibr" target="#b5">[6]</ref> and Mapillary dataset <ref type="bibr" target="#b26">[27]</ref>. Note that we use the same DSSPN structure for all dataset during training. During testing, we only perform hierarhical pixe-wise prediction by only selecting a semantic neuron sub-graph that can reach out the defined concepts on each dataset. The mean IoU metrics are used for all datasets. We then evaluate its effectiveness on the universal semantic segmentation task by training a single model using all datasets.</p><p>Semantic Concept Hierarchy Construction. We build the semantic concept hierarchy by combining labels from all four popular dataset. Starting from the label hierarchical tree of COCO-Stuff <ref type="bibr" target="#b3">[4]</ref> that includes 182 concepts and 27 super-classes, we manually merge concepts from the rest three dataset together by using WordTree. Note that we only add minimal number of intermediate super-classes during merging. It results in 359 concepts in the final concept hierarchical tree, as included in the supplementary materials. <ref type="table">Table 1</ref>. Comparison with existing semantic segmentation models (%) on the ADE20K val set <ref type="bibr" target="#b41">[41]</ref>. PSPNet (101)+DA+AL <ref type="bibr" target="#b40">[40]</ref> used other data augmentation scheme and auxiliary loss. "Conditional Softmax (VGG) <ref type="bibr" target="#b30">[31]</ref>", "Word2Vec(VGG) <ref type="bibr" target="#b9">[10]</ref>" and "Joint-Cosine (VGG) <ref type="bibr" target="#b39">[39]</ref>" indicate existing approaches that also attempted the hierarchical classification, obtained from <ref type="bibr" target="#b39">[39]</ref>. The maximal depth of resulting concept hierarchy is five. On average, six semantic neurons of DSSPN within each batch are activated for the images in COCO-Stuff, and 5 in ADE20k, 10 in Cityscape and 8 in Mapillary during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with state-of-the-arts</head><p>We directly apply the same hyper-parameters described in Section 4.1 for clearly demonstrating the effectiveness of our dynamic-structure propagation network in general cases. Due to space limitation, we refer the readers to their dataset papers <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref> for different evaluation metrics.</p><p>ADE20k dataset <ref type="bibr" target="#b41">[41]</ref> consists of 20,210 images for training and 2,000 for validation. Images from both indoor and outdoor are annotated with 150 semantic concepts, including painting, lamp, sky, land, etc. We first compare DSSPN with state-of-the-art methods that also use Resnet-101 as basic network in <ref type="table">Table 1</ref>. Our DSSPN performs better than the previous methods based on ResNet-101. Our DSSPN obtains 2.63% higher mean IoU than the baseline model "ResNet-101, 2 conv <ref type="bibr" target="#b37">[37]</ref>" that does multi-class recognition. We cannot fairly compare the state-of-the-arts [37, 40] since they used wider or deeper Imagenet pretrained networks. This clearly shows that incorporating dynamic-structured neurons can improve the model capacity for recognizing over a large concept vocabulary.</p><p>We further compare our DSSPN with prior works that also tried the hierarchical classification <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">39]</ref> based on pretrained VGG net, as reported in <ref type="bibr" target="#b39">[39]</ref>. Benefiting from learning distinct features for differentiating the child concepts of each super-class, "DSSPN (VGG)-Softmax" that also uses Softmax loss on each semantic neuron significantly outperforms <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">39]</ref> that simply utilized identical features for super-class categorizations at different levels.   Coco-Stuff dataset <ref type="bibr" target="#b3">[4]</ref> contains 10,000 complex images from COCO with dense annotations of 91 thing and 91 stuff classes, including 9,000 for training and 1,000 for testing. We compare DSSPN with the state-of-the-art methods in <ref type="table" target="#tab_3">Table 2</ref>. We can observer DSSPN significantly outperforms existing methods. It further demonstrates that modeling semantic label hierarchy into network feature learning benefits for recognizing over a large vocabulary (e.g. 182) that can be hierarchically grouped into diverse super-classes .</p><p>Cityscape dataset <ref type="bibr" target="#b5">[6]</ref> contains 5,000 urban scene images collected from 50 cities, which are splited into 2,975, 500, and 1,525 for training, validation and testing. The pixel-wise annotations of 19 concepts (e.g. road, fence) are provided. We reports results on Cityscape test set in <ref type="table" target="#tab_4">Table 3</ref>. Our DSSPN is also based on ResNet101 using single scale inputs for testing and does not employ post-processing like CRF as in our fair baseline "DeepLabv2 (ResNet-101) <ref type="bibr" target="#b4">[5]</ref>". Compared to our fair baseline, "DSSPN (ResNet-101)" brings significant improvement, i.e. 3.6% in IoU class. Note that we cannot fairly compare with recent best performances <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b37">37]</ref> on Cityscape benchmark since they often combined results from several scaled inputs or used different base models.</p><p>Mapillary dataset <ref type="bibr" target="#b26">[27]</ref> includes 20,000 street-level images annotated into 66 object categories (e.g. rider, streetlight, traffic sign back), in which 18,000 are used for training and 2,000 for validation. We report the result comparisons in <ref type="table" target="#tab_5">Table 4</ref>. We mainly compare our DSSPN with the baseline "ResNet-101" instead of previous methods <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b40">40]</ref> since they used different basic networks. The large improvement by our DSSPN can be again observed, i.e. 4.81% on mean IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Universal Semantic Segmentation Model</head><p>Another interesting advantage of our DSSPN is its ability of training all dataset annotations within a unified segmentation model as "DSSPN (ResNet-101) Universal". To train a unified model, we combine all training samples from four datasets and select images from the same dataset to construct one batch at each step. Since images on each dataset are collected from different scenarios and domains, we first train a unified model using all images for 60 epoch, and then decrease the learning rate by 1/10 to further finetune models for 20 epochs on each dataset. We reports results on each dataset in <ref type="table">Table 1</ref>, 2, 3, 4, respectively.</p><p>The commonly used strategy for utilizing other dataset annotations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">40]</ref> is to remove the final classification layer and retrain the newly initialized layers for new label sets due to the label discrepancy. Following such strategy, we also report the results of "DSSPN (ResNet-101) finetune" that first train models on one dataset, and then finetune on the new dataset by retaining only network parameters of basic ConvNet. Since such strategy cannot support training on more than one dataset at once, we thus use the COCO-Stuff and ADE20k pair due to their similar image resources, and Cityscape and Mapillary training pair.</p><p>By comparing results of our "DSSPN (ResNet-101) Universal" with "DSSPN (ResNet-101) finetune" in all Tables, it can be demonstrated that jointly training all semantic neu- <ref type="table">Table 5</ref>. Ablation studies on the ADE20K val set <ref type="bibr" target="#b41">[41]</ref>.  <ref type="table">Table 5</ref> shows the ablation studies of our DSSPN to validate the effectiveness of its main components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>Dynamic-structure propagation. The semantic propagation network can also be trained using a fixed structure where images must be passed through all semantic neurons needed for each dataset during training. No noticeable improvement can be seen by comparing "DSSPN fixed" with our dynamic version while "DSSPN fixed" needs more computation and memory cost. Since we use the hierarchical prediction scheme during testing, our DSSPN can be efficiently learned by only focusing on differentiating confusing concepts at each level of semantic neuron graph.</p><p>Dense semantic-enhanced block. An alternative basic block for each semantic neuron can be the directly feature propagation without dense connection, as "DSSPN w/o dense block". Our experiment shows that it would sacrifice the performance but reduce the parameter numbers. We further demonstrate the feature concatenation used in dense block outperforms the feature summation version by comparing "DSSPN w/ dense block, summation" with ours. For the hyper-parameter m of hidden feature map size, we also evaluate the results of setting m as 32 and 64. It can be seen that using moderately small feature map size (m = 48) is sufficient for capturing key feature characteristics, which are used in all experiments on different datasets.</p><p>Binary cross-entropy vs Softmax. In <ref type="table">Table 1</ref>, we also show that using per-pixel sigmoid with binary cross-entropy loss (our model) significantly outperforms the Softmax loss that is common practice in other hierarchical classification model <ref type="bibr" target="#b30">[31]</ref>. The similar conclusion has been shown in Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> for instance-level segmentation that predicts a binary mask for each class independently, without competition among classes. The class competition by Softmax loss also hinders the model's capability of learning a unified model using diverse label annotations, where only some parts of concepts belonging to one super-class are visible. The affect of different concept hierarchies. Another interesting point that may be raised is how different concept graphs influence the final performance. We thus try the synset provided in original ADE20k dataset <ref type="bibr" target="#b41">[41]</ref> as the whole concept hierarchy and the results are reported as "(synset in <ref type="bibr" target="#b41">[41]</ref>)". We can observe that there is only slight performance changes by using the original synset tree in <ref type="bibr" target="#b41">[41]</ref> that includes more hypernyms for grouping the object affordance.</p><p>Model and computation complexity.</p><p>In <ref type="table" target="#tab_7">Table 6</ref>, we report experiments with the baseline model "Deeplabv2 (ResNet-101)" and our DSSPN variants on Cityscape validatation set for comparing their model sizes and time efficiency. Both our DSSPN variants using ResNet-50 and ResNet-101 yield much better performance than the baseline model. Moreover, "DSSPN (Resnet-50)" reduces both computation consumption and model size compared to the baseline model. It should be noted that although DSSPN has more parameters by taking into account all semantic neurons within the graph, it only activates a small sub-set of neurons for each image during training and testing, benefiting from the dynamic-structured semantic propagation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Conclusion and Future work</head><p>In this paper, we proposed a novel dynamic-structured semantic propagation network for the general semantic segmentation tasks. Our DSSPN explicitly constructs a semantic neuron graph network by incorporating the semantic concept hierarchy. A dynamic-structured network optimization is performed to dynamically activate semantic neuron sub-graphs for each image during training. Extensive experiments on four public benchmarks demonstrate the superiority of our DSSPN. We further show our DSSPN can be naturally used to train a unified segmentation model over all available segmentation annotations, leading to its better generalization capability. In future, we plan to generalize DSSPN to other vision tasks and investigate how to embed more complex semantic relationships naturally into the network design. The proposed DSSPN is general enough to handle more complex semantic concept graphs that contain categories with multiple ancestors in the hierarchy. In that case, each semantic neuron can simply combine features passed from multiple ancestor neurons via summation, and then performs the dynamic pixel-wise prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Dense semantic-enhanced block. For each activated semantic neuron vi, it concatenates features evolved from the neurons in the whole path (dashed orange arrows) to obtain enhanced representation hi, which is further passed through dynamic pixel-wise prediction Pi for distinguishing between its children nodes. The output dimensions (e.g. 48 ) of each block and those (e.g. 27) of pixel-wise prediction layer are shown in the parentheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>DSSPN can learn a unified segmentation model for accommodating diverse annotation policies. For training with diverse annotations with discrepant label granularity, DSSPN activates dynamic-structured semantic propagation graphs for each image. For example, Ade20k only annotates a single "animal-thing" label for all animal categories while Cocostuff elaborately categories each fine-grained concept, e.g. cat or elephant. The semantic neurons that correspond to target labels are deactivated (grey colored solid circles). It thus fully exploits the shared concept patterns and concept relationship in a semantic hierarchy for a more general segmentation model. For simplicity, only target labels and their ancestor concepts are shown. a large-scale concept vocabulary in the spirit of curriculum learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparison on Coco-Stuff. For each image, we sequentially show its groudtruth labeling, prediction of "DSSPN (ResNet-101)", prediction of "DSSPN (ResNet-101) Universal".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table -</head><label>-</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">outdoor -things</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>table dining table</cell><cell>Semantic neuron</cell></row><row><cell></cell><cell>things</cell><cell></cell><cell>furniture-things motorcyclist</cell><cell cols="2">chair</cell><cell>things</cell><cell>coffee table stool armchair</cell><cell>Activated semantic neuron</cell><cell>block dense semantic-enhanced neural</cell></row><row><cell>DSSPN</cell><cell>entity</cell><cell>-stuff indoor indoor -things</cell><cell cols="2">electronic-things screen-things floor-stuff carpet kitchen-things bowl cup plate food-things living-stuff textile-stuff</cell><cell cols="2">potted plant couch tv crt screen edible fruit curtain pillow hot-dog</cell><cell>swivel chair … banana orange apple</cell><cell>Deactivated semantic neuron (no computation)</cell><cell>Dynamic pixel-wise prediction layer</cell></row><row><cell></cell><cell>stuff</cell><cell></cell><cell>furniture-stuff</cell><cell>floor-wood light-stuff</cell><cell></cell><cell cols="2">light chandelier</cell><cell>Groundtruth concepts</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ceiling-stuff</cell><cell>ceiling-tile</cell><cell></cell><cell>lamp</cell><cell></cell><cell>Negative concepts</cell></row><row><cell></cell><cell></cell><cell></cell><cell>window-stuff</cell><cell>ceiling-other</cell><cell></cell><cell cols="2">windowpane window-other</cell><cell>Deactivated semantic neuron With groundtruth concept</cell></row><row><cell></cell><cell></cell><cell>outdoor -stuff</cell><cell>wall-stuff</cell><cell>wall-brick wall-other</cell><cell></cell><cell></cell><cell></cell></row></table><note>Figure 1. An overview of the proposed DSSPN that explicitly models dynamic network structures according to a semantic concept hierarchy. The basic convolutional features are propagated into a dynamic-structured semantic neuron graph for hierarchical pixel-wise recognition. During training, DSSPN only activates a sub-graph of semantic neurons that reach out the target labels for each image, leading to dynamic- structured feed-forward propagation and back-propagation. It means DSSPN only needs to focus on hierarchically classifying confusing concepts with the same parent during training. For clarity, we only show a portion of semantic neurons.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Thus, each neuron is only responsible for recognizing a small set of confusing concepts by producting a distinct number of pixel-wise predictions. This hierarchical semantic propagation scheme significantly improves the model capacity for</figDesc><table><row><cell></cell><cell cols="2">Coco-Stuff (182)</cell><cell cols="2">ADE20k (150)</cell><cell></cell><cell cols="3">Cityscape (19)</cell><cell>Mapillary (65)</cell></row><row><cell></cell><cell>stuff</cell><cell>things</cell><cell>stuff</cell><cell>things</cell><cell></cell><cell>stuff outdoor-stuff</cell><cell cols="2">things outdoor-things</cell><cell>stuff</cell><cell>things</cell><cell>outdoor-things</cell></row><row><cell cols="3">person-things food-things ground-stuff vehicle-things building-stuff outdoor-stuff outdoor-things indoor-things</cell><cell cols="2">outdoor-stuff outdoor-things</cell><cell cols="2">ground-stuff building-stuff structural-stuff plant-stuff sky-stuff</cell><cell cols="2">outdoor-signal vehicle-things</cell><cell>ground-stuff building plant-stuff sky-stuff outdoor-stuff</cell><cell>person-things outdoor-signals streetlight structural -stuff furniture</cell></row><row><cell>way</cell><cell cols="2">motor vehicle person food-other building-other</cell><cell cols="2">plant-stuff tree grass plant-other structure-stuff fence animal-things</cell><cell>road terrain</cell><cell cols="2">way sidewalk fence pole person rider person-things traffic</cell><cell>signal motor vehicle wheeled vehicle traffic car bicycle</cell><cell>-stuff terrain curb cut road curb way sidewalk</cell><cell>pole lane</cell><cell>-stuff junction person signboard fire hydrant traffic light box traffic sign</cell></row><row><cell>road</cell><cell>pavement</cell><cell>motorcycle</cell><cell></cell><cell></cell><cell></cell><cell cols="2">light</cell><cell>sign</cell><cell>crosswalk</cell><cell>traffic sign back traffic sign front</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison on Coco-Stuff testing set (%). All previous results are collected from<ref type="bibr" target="#b3">[4]</ref> </figDesc><table><row><cell>Method</cell><cell cols="3">Class-average acc. acc. mean IoU</cell></row><row><cell>FCN [24]</cell><cell>38.5</cell><cell>60.4</cell><cell>27.2</cell></row><row><cell>DeepLabv2 (ResNet-101) [5]</cell><cell>45.5</cell><cell>65.1</cell><cell>34.4</cell></row><row><cell>DAG-RNN + CRF [33]</cell><cell>42.8</cell><cell>63.0</cell><cell>31.2</cell></row><row><cell>OHE + DC + FCN [15]</cell><cell>45.8</cell><cell>66.6</cell><cell>34.3</cell></row><row><cell>DSSPN (ResNet-101)</cell><cell>47.0</cell><cell>68.5</cell><cell>36.2</cell></row><row><cell>DSSPN (ResNet-101) finetune</cell><cell>48.1</cell><cell>69.4</cell><cell>37.3</cell></row><row><cell>DSSPN (ResNet-101) Universal</cell><cell>50.3</cell><cell>70.7</cell><cell>38.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison on Cityscapes testing set.</figDesc><table><row><cell>Method</cell><cell cols="4">IoU cla. iIoU cla. IoU cat. iIoU cat.</cell></row><row><cell>FCN [24]</cell><cell>65.3</cell><cell>41.7</cell><cell>85.7</cell><cell>70.1</cell></row><row><cell>LRR [11]</cell><cell>69.7</cell><cell>48.0</cell><cell>88.2</cell><cell>74.7</cell></row><row><cell>DeepLabv2 (ResNet-101) [5]</cell><cell>70.4</cell><cell>42.6</cell><cell>86.4</cell><cell>67.7</cell></row><row><cell>Piecewise [23]</cell><cell>71.6</cell><cell>51.7</cell><cell>87.3</cell><cell>74.1</cell></row><row><cell>DSSPN (ResNet-101)</cell><cell>74.0</cell><cell>53.5</cell><cell>88.5</cell><cell>76.1</cell></row><row><cell>DSSPN (ResNet-101) finetune</cell><cell>74.6</cell><cell>53.9</cell><cell>89.1</cell><cell>77.0</cell></row><row><cell cols="2">DSSPN (ResNet-101) Universal 76.6</cell><cell>56.2</cell><cell>89.6</cell><cell>77.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">Comparison on Mapillary validation set (%). The results</cell></row><row><cell>of [37, 40] are reported in [27].</cell><cell></cell></row><row><cell>Method</cell><cell>mean IoU</cell></row><row><cell>Wider Network [37]</cell><cell>41.12</cell></row><row><cell>PSPNet [40]</cell><cell>49.76</cell></row><row><cell>Baseline ResNet-101</cell><cell>37.58</cell></row><row><cell>DSSPN (ResNet-101)</cell><cell>42.39</cell></row><row><cell>DSSPN (ResNet-101) finetune</cell><cell>42.57</cell></row><row><cell cols="2">DSSPN (ResNet-101) Universal 45.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>We show number of model parameters, average testing speed (img/sec) on the full image resolution, and mean IOU (%) on Cityscape validation set. All results are evaluated using Pytorch under the same setting.</figDesc><table><row><cell>Model</cell><cell cols="3">Params Test-speed mean IoU</cell></row><row><cell cols="2">Deeplabv2 (ResNet-101) [5] 176.6M</cell><cell>1.78</cell><cell>71.0</cell></row><row><cell>DSSPN (ResNet-50)</cell><cell>141.0M</cell><cell>2.26</cell><cell>73.2</cell></row><row><cell>DSSPN (ResNet-101)</cell><cell>217.3M</cell><cell>1.45</cell><cell>75.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06021</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03716</idno>
		<title level="m">Coco-stuff: Thing and stuff classes in context</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scharwächter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3450" to="3457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask r-cnn. ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Labelbank: Revisiting global perspectives for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09891</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">One model to learn them all</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05137</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ubernet: Training auniversal&apos;convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep variation-structured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpretable structure-evolving lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Refinenet: Multipath refinement networks with identity mappings for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep learning with dynamic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Looks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herreshoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02181</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On-the-fly operation batching in dynamic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From large scale image categorization to entry-level categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2768" to="2775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning for precipitation nowcasting: A benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Scene segmentation with dag-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory Workshop (ITW)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Open vocabulary scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05442</idno>
		<title level="m">Semantic understanding of scenes through the ade20k dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
