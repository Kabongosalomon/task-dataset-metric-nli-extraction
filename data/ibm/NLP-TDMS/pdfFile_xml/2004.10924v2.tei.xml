<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PolyLaneNet: Lane Estimation via Deep Polynomial Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Tabelini</surname></persName>
							<email>tabelini@lcad.inf.ufes.br</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Berriel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><forename type="middle">M</forename><surname>Paixo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Instituto Federal do Esprito Santo (IFES)</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Badue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">F</forename><surname>De Souza</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Oliveira-Santos</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Universidade Federal do Esprito Santo (UFES)</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PolyLaneNet: Lane Estimation via Deep Polynomial Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the main factors that contributed to the large advances in autonomous driving is the advent of deep learning. For safer self-driving vehicles, one of the problems that has yet to be solved completely is lane detection. Since methods for this task have to work in real-time (+30 FPS), they not only have to be effective (i.e., have high accuracy) but they also have to be efficient (i.e., fast). In this work, we present a novel method for lane detection that uses as input an image from a forwardlooking camera mounted in the vehicle and outputs polynomials representing each lane marking in the image, via deep polynomial regression. The proposed method is shown to be competitive with existing state-of-the-art methods in the TuSimple dataset while maintaining its efficiency (115 FPS). Additionally, extensive qualitative results on two additional public datasets are presented, alongside with limitations in the evaluation metrics used by recent works for lane detection. Finally, we provide source code and trained models that allow others to replicate all the results shown in this paper, which is surprisingly rare in state-of-the-art lane detection methods. The full source code and pretrained models are available at https://github.com/lucastabelini/PolyLaneNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Autonomous driving <ref type="bibr" target="#b0">[1]</ref> is a challenging field of research that has received a lot of attention in recent years. The perceptual problems related to this field have been immensely impacted by the advances in deep learning <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[4]</ref>. In particular, autonomous vehicles should be capable of estimating traffic lanes because, besides working as a spatial limit, each lane provides specific visual cues ruling the travel. In this context, the two most important traffic lines (i.e., lane markings) are those defining the lane of the vehicle, i.e., the ego-lane. These lines set the limits for the driver's actions and their types define whether or not maneuvers (e.g., lane changes) are allowed. Also, it might be useful to detect the adjacent lanes so that the systems' decisions might be based on a better understanding of the traffic scene.</p><p>Lane estimation (or detection) may seem trivial at first, but it can be very challenging. Although fairly standardized, lane markings vary in shape and colour. Estimating a lane when dashed or partially occluded lane markers are presented requires a semantic understanding of the scene. Moreover, the environment itself is inherently diverse: there may be a lot of traffic, people passing by, or it could be just a free highway. In addition, these environments are subject to several weather (e.g., rain, snow, sunny, etc.) and illumination (e.g., day, night, dawn, tunnels, etc.) conditions, which might just change while driving.</p><p>The traditional approach for the lane estimation (or detection) task consists in the extraction of hand-crafted features <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref> followed by a curve-fitting process. Although this approach tends to work well under normal and limited circumstances, it is not usually as robust as needed in adverse conditions (as the aforementioned ones). Therefore, following the trend in many computer vision problems, deep learning has recently started to be used to learn robust features and improve the lane marking estimation process <ref type="bibr" target="#b7">[7]</ref>- <ref type="bibr" target="#b10">[9]</ref>. Once the lane markings are estimated, further processing can be performed to determine the actual lanes. Still, there are limitations to be tackled. First, many of these deep learning-based models tackle the lane marking estimation as a two-step process: feature extraction and curve fitting. Most works extract features via segmentation-based models, which usually are inefficient and have trouble running in real-time, as required for autonomous driving. Additionally, the segmentation step alone is not enough for providing a lane marking estimate since the segmentation maps have to be post-processed in order to output traffic lines. Further, these two-step processes might ignore global information <ref type="bibr" target="#b9">[8]</ref>, which are specially important when there are missing visual cues (as in strong shadows and occlusions). Second, some of these works are performed by private companies that often (i) do not provide means to replicate their results and (ii) develop their methods on private datasets, which hinders research progress. Lastly, there is room for improvement in the evaluation protocol. The methods are usually tested on datasets from the USA only (roads in developing countries are usually not as well maintained) and the evaluation metrics are too permissive (they allow error in such a way that it hinders proper comparisons), as discussed in Section IV.</p><p>In this context, methods focusing on removing the need for a two-step process further reducing the processing cost could benefit advanced driver assistance systems (ADAS) that often rely on low-energy and embedding hardware. In addition, a method that has been tested on roads other than the USA's is also of benefit to the broader community. Moreover, less permissive metrics would allow to better differentiate methods and provide a clearer overview of the methods and their usefulness.  This work proposes PolyLaneNet, a convolutional neural network (CNN) for end-to-end lane markings estimation. Poly-LaneNet takes as input images from a forward-looking camera mounted in the vehicle and outputs polynomials that represent each lane marking in the image, along with the domains for these polynomials and confidence scores for each lane. This approach is shown to be competitive with existing state-ofthe-art methods while being faster and not requiring postprocessing to have the lane estimates. In addition, we provide a deeper analysis using metrics suggested by the literature. Finally, we publicly released the source-code (for both training and inference) and the trained models, allowing the replication of all the results presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully-Connected</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Lane Detection. Before the rise of deep learning, methods on lane detection were mostly model-or learning-based, i.e., they used to exploit hand-crafted and specialized features. Shape and color were the most commonly used features <ref type="bibr" target="#b11">[10]</ref>, <ref type="bibr" target="#b12">[11]</ref>, and lanes were normally represented both by straight and curved lines <ref type="bibr" target="#b13">[12]</ref>, <ref type="bibr" target="#b14">[13]</ref>. These methods, however, were not robust to sudden illumination changes, weather conditions, differences in appearance between cameras, and many other things that can be found in driving scenes. The interested reader is referred to <ref type="bibr" target="#b5">[5]</ref> for a more complete survey on earlier lane detection methods.</p><p>With the success of deep learning, researchers have also investigated its use to tackle lane detection. Huval et al. <ref type="bibr" target="#b15">[14]</ref> were one of the first to use deep learning in lane detection. Their model is based on the OverFeat and produces as output a sort of segmentation map that is later post-processed using DBSCAN clustering. They collected a private dataset on San Francisco (USA) that was used to train and evaluate their system. Because of the success of their application, companies were also interested in investigating this problem. Later, Ford released DeepLanes <ref type="bibr" target="#b16">[15]</ref>, which unlike most of the literature, detects lanes based on laterally-mounted cameras. Despite the good results, the way they modeled the problem made it less widely applicable, and they also used a private US-based dataset.</p><p>More recently, a lane detection challenge was held on CVPR'17 in which the TuSimple <ref type="bibr" target="#b17">[16]</ref> dataset was released. The winner of the challenge was SCNN <ref type="bibr" target="#b7">[7]</ref>, a method proposed for traffic scene understanding that exploits the propagation of spatial information via specially designed CNN structure. Their model outputs a probability map for the lanes that are post-processed in order to provide the lane estimates. To evaluate their system, they used an evaluation metric that is based on the IoU between the prediction and the groundtruth. After that, in <ref type="bibr" target="#b9">[8]</ref>, the authors proposed Line-CNN, a model in which the key component is the line proposal unit (LPU) adapted from the region proposal network (RPN) of Faster R-CNN. They also submitted their results to the TuSimple benchmark (after the challenge was finished) with marginally better results compared to SCNN. Their main experiments, though, were with a much larger dataset that was not publicly released. In addition to this private dataset, the source code is proprietary and the authors will not release it. Another approach is FastDraw <ref type="bibr" target="#b18">[17]</ref> in which the common post-processing of segmentation-based methods is substituted by "drawing" the lanes according to the likelihood of polylines that are maximized at training time. In addition to evaluating on the TuSimple and CULane <ref type="bibr" target="#b7">[7]</ref> datasets, the authors provide qualitative results on yet another private US-based dataset. Moreover, they did not release their implementation, which hinders further comparisons. Some of the segmentationbased methods focus on improving the inference speed, as in <ref type="bibr" target="#b10">[9]</ref> (ENet-SAD) which focuses on learning lightweight CNNs by exploiting self-attention distillation. The authors evaluated their method on three well-known datasets. Although the source code was publicly released, some of the results are not reproducible 1 . Closer to our work, <ref type="bibr" target="#b19">[18]</ref> proposes a differentiable least-squares fitting module to fit a curve on points predicted by a deep neural network. In our work, we bypass the need for this module by directly predicting the polynomial coefficients, which simplifies the method while also making it faster.</p><p>In summary, one of the main problems with existing stateof-the methods is reproducibility, since most either do not publish the datasets used or the source code. In this work, we present results that are competitive with state-of-the-art methods on public datasets and fully reproducible, since we provide the source code and use only publicly available datasets (including one from outside the US).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. POLYLANENET</head><p>Model Definition. PolyLaneNet expects as input images taken from a forward-looking vehicle camera, and outputs, for each image, M max lane marking candidates (represented as polynomials), as well as the vertical position h of the horizon line, which helps to define the upper limit of the lane markings. The architecture of PolyLaneNet consists of a backbone network (for feature extraction) appended with a fully connected layer with M max + 1 outputs, being the outputs 1, . . . , M max for lane marking prediction and the output M max + 1 for h. PolyLaneNet adopts a polynomial representation for the lane markings instead of a set of points. Therefore, for each output j, j = 1, . . . , M max , the model estimates the coefficients</p><formula xml:id="formula_0">P j = {a k,j } K k=0 representing the polynomial p j (y) = K k=0 a k,j y k ,<label>(1)</label></formula><p>where K is a parameter that defines the order of the polynomial. As illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, the polynomials have restricted domain: the height of the image. Besides the coefficients, the model estimates, for each lane marking j, the vertical offset s j , and the prediction confidence score c j ∈ [0, 1]. In summary, the PolyLaneNet model can be expressed as</p><formula xml:id="formula_1">f (I; θ) = ({P j , s j , c j )} Mmax j=1 , h),<label>(2)</label></formula><p>where I is the input image and θ is the model parameters. At inference time, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, only the lane marking candidates whose confidence score is greater or equal than a threshold are considered as detected. </p><formula xml:id="formula_2">= (x * i,j , y * i,j ) N i=1 , where y * i+1,j &gt; y * i,j , for every i = 1, . . . , N − 1.</formula><p>As a rule of thumb, the higher is N , the more it allows to capture richer structures. We assume that the lane markings {L * j } M j=1 are ordered according to the x-coordinate of the point closest to the bottom of the image, i.e., x * 0,j &lt; x * 0,j+1 for every j = 1, . . . , M − 1. For each lane marking j, the vertical offset s * j was set as min {y * i,j } N i=1 ; the confidence score is defined as</p><formula xml:id="formula_3">c * j = 1, if j ≤ M 0, otherwise.<label>(3)</label></formula><p>The model is trained using the multi-task loss function defined as (for a single image) </p><formula xml:id="formula_4">L({P j }, h, {s j }, {c j }) =W p L p ({P j }, {L * j }) +W s 1 M j L reg (s j , s * j ) +W c 1 M j L cls (c j , c * j ) +W h L reg (h, h * ),<label>(4)</label></formula><formula xml:id="formula_5">= [x * 1,j , . . . , x * N,j ] T and x j = [x 1,j , . . . , x N,j ] where x i,j = p j (y * i,j ), if |p j (y * i,j ) − x * i,j | &gt; τ loss 0, otherwise.<label>(5)</label></formula><p>where τ loss is an empirically defined threshold that tries to reduce the focus of the loss on points that are already well aligned. Such effect appears because the lane markings comprise several points with different sampling differences (i.e., points closer to the camera are denser than points further away). Finally, L p is defined as</p><formula xml:id="formula_6">L p ({P j }, {L * j }) = L reg (x j , x * j ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL METHODOLOGY</head><p>PolyLaneNet was evaluated on publicly available which are introduced in this section. Following, the section describes the implementation details, the metrics, and the experiments performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Three datasets were used to evaluate PolyLaneNet: TuSimple <ref type="bibr" target="#b17">[16]</ref>, LLAMAS <ref type="bibr" target="#b20">[19]</ref> and ELAS <ref type="bibr" target="#b6">[6]</ref>. For quantitative results, the widely-used TuSimple <ref type="bibr" target="#b17">[16]</ref> was employed. The dataset has a total of 6,408 annotated images with a resolution of 1280×720 pixels, and it is originally split in 3,268 for training, 358 for validation, and 2,782 for testing. For qualitative results, two other datasets were used: LLAMAS <ref type="bibr" target="#b20">[19]</ref> and ELAS <ref type="bibr" target="#b6">[6]</ref>. The first is a large dataset, split into 58,269 images for training, 20,844 for validation, and 20,929 for test, with a resolution of 1280×717 pixels. Both TuSimple and LLAMAS are datasets from the USA. Since neither the benchmark nor the test set annotations for LLAMAS are available yet, only qualitative results are presented. ELAS is a dataset with 16,993 images from various cities in Brazil, with a resolution of 640×480 pixels. Since the dataset was originally proposed for a non-learning based method, it does not provide training/testing splits. Thus, we created those splits by separating 11,036 images for training and 5,957 for testing. The main difference between ELAS and the other two datasets is that in ELAS only the ego-lane is annotated. Nonetheless, the dataset also provides other types of useful information for the lane detection task, such as lane types (e.g., solid or dashed, white or yellow), but they are not used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>The hyperparameters for every experiment in this work were the same, except for the ablation study, where in each training one parameter was modified. For the backbone network, the EfficientNet-b0 <ref type="bibr" target="#b21">[20]</ref> was used. For the TuSimple training, data augmentation was applied with a probability of <ref type="bibr">10 11</ref> . The transformations used were: rotation with an angle in degrees θ ∼ U(−10, 10), horizontal flip with a probability of 0.5, and a random crop with size 1152×648 pixels. After the data augmentation, the following transformations were applied: a resize to 640×360 pixels and then a normalization with ImageNet's <ref type="bibr" target="#b22">[21]</ref> mean and standard deviation. The Adam optimizer was used, along with the Cosine Annealing learning rate scheduler with an initial learning rate of 3e-4 and a period of 770 epochs. The training session ran for 2695 epochs, taking approximately 35 hours on a Titan V, with a batch size of <ref type="bibr" target="#b17">16</ref> images, from a model pretrained on ImageNet <ref type="bibr" target="#b22">[21]</ref>. A thirdorder polynomial degree was chosen to be the default. For the loss function, the parameters W s = W c = W h = 1 and W p = 300 were used. The threshold τ loss (Equation 5) was set to 20 pixels. In the testing phase, lane markings predicted with a confidence score c j &lt; 0.5 were ignored. For more details, the source code and trained models are publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>The metrics used to measure the proposed method's performance come from TuSimple's benchmark <ref type="bibr" target="#b17">[16]</ref>. The three metrics are: accuracy (Acc), false positive (F P ) and false negative (F N ) rates. For a predicted lane marking to be considered a true positive (i.e., a correct one), its accuracy, defined as</p><formula xml:id="formula_7">Acc(P j , L * j ) = 1 |L * j | (x * i,j ,y * i,j )∈L * j 1[|p j (y * i,j ) − x * i,j | &lt; τ acc ]<label>(7)</label></formula><p>has to be equal to or greater than . The values used for τ acc and were 20 pixels and 0.85, respectively, the same ones used in TuSimple's benchmark. All the three reported metrics (Acc, F P and F N ) are reported as the average across all images of the average of each image.</p><p>Although TuSimple's metric has been widely used in the literature, it is too permissive w.r.t. local errors. To avoid relying on only such metric, we also used a metric proposed in <ref type="bibr" target="#b23">[22]</ref>, which discusses several evaluation metrics of interest to the lane estimation process. The Lane Position Deviation 2 https://github.com/lucastabelini/PolyLaneNet (LPD) was proposed to better capture the accuracy of the model on both the near and far depths of view of the egovehicle. It is the error between the prediction and the groundtruth for the ego-lane. To define what are the ego-lanes (given that the dataset labels and our model are agnostic to this definition), we use a simple definition: the lane markings that are closer to the middle of the bottom part of the image are the ones that compose the ego-lane, i.e., one lane marking to the left and another one to the right.</p><p>In addition to metrics w.r.t. the quality of the predictions, we also report two speed-related metrics: frames-per-second (FPS) and MACs 3 . The frames-per-second provide a concrete assessment of how fast an implementation can run on a modern computer with a recent GPU, whereas MACs provide a more reliable way to compare different methods that might be running on different frameworks and setups. As discussed in <ref type="bibr" target="#b23">[22]</ref>, analyzing the trade-off between computation efficiency and accuracy is also important. In this paper, we provide such an analysis by reporting the MACs of PolyLaneNet variants with different computational requirements in an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quantitative Evaluation</head><p>State-of-the-art Comparison. The main quantitative experiment for the proposed method is the comparison against state-of-the-art methods using the same evaluation conditions. For that, the proposed method was used to train a model using a union of TuSimple's training and validation sets and then evaluated in its testing set. Four state-of-the-art methods were compared: SCNN <ref type="bibr" target="#b7">[7]</ref>, Line-CNN <ref type="bibr" target="#b9">[8]</ref>, ENet-SAD <ref type="bibr" target="#b10">[9]</ref>, and FastDraw <ref type="bibr" target="#b18">[17]</ref>. Besides prediction quality metrics, model speed w.r.t. FPS is also presented. For our model, we also reported the MACs.</p><p>Polynomial Degree. In most lane marking detection datasets, it is clear that lane markings with a more accentuated curvature are rarer, while straight ones represent the majority of the cases. With this in mind, one might enquire: what would be the impact of modeling lane markings with polynomials of lower orders? To help answer this question, our method was evaluated using first-and second-order polynomials, instead of the default of third-order polynomials. Furthermore, we also show the permissiveness of the standard TuSimple's metric used by the literature by computing upper bounds for polynomials of different orders.</p><p>Ablation Study. To investigate the impact of some of the decisions made for the proposed method, an ablation study was carried out, using only TuSimple's training set for training and the validation set for testing. For the model backbone f (·, θ), ResNet <ref type="bibr" target="#b24">[23]</ref> was evaluated, on two of its variants: ResNet-34 and 50. Another variant of EfficientNet was also evaluated, the EfficientNet-b1. Moreover, when training CNNs, in addition to the impact of the backbone, there is a trade-off when using different image input sizes. For example, if a smaller input size is used, the network forward will be faster, but information may be lost. To measure this trade-off <ref type="bibr" target="#b23">[22]</ref> in the proposed method, two additional models were trained, one using an input size of 480 × 270 pixels, and the other using an input size of 320 × 180 pixels. Additionally, three other practical decisions were evaluated: (i) the impact of not sharing h (i.e., having the end of each lane predicted individually), (ii) the use of a pre-trained model, by training from scratch instead of a model pretrained on ImageNet; and (iii) the impact of using data augmentation, by removing the online data augmentation, which reduces the variability seen by the model at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Evaluation</head><p>For qualitative results, an extensive evaluation was carried out. Using the model trained on TuSimple as a pretraining, three models were trained: two on ELAS, one with and one without lane marking type classification, and another on LLAMAS. On ELAS, the model was trained for 385 additional epochs (half of a period of the chosen learning rate scheduler, where the learning rate will be at a minimum). On LLAMAS, the model was trained for 75 additional epochs, an approximation to the number of iterations used on ELAS, as the training set of LLAMAS is around five times larger than the one of ELAS. The experiment with lane marking type classification is a straightforward extension of PolyLaneNet, in which a category is predicted for each lane showcasing how trivial it is to extend our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>First, we present the results of the comparison with the stateof-the-art. Then, the results of the ablation study are detailed and discussed. Finally, qualitative results are shown.</p><p>State-of-the-art Comparison. The state-of-the-art results on the TuSimple dataset are presented in <ref type="table" target="#tab_2">Table I</ref>. As evidenced, PolyLaneNet results are competitive. Since none of the compared methods provide source codes that replicate their respective published results, it is very difficult to investigate situations where the other methods succeed that ours fail. On <ref type="figure" target="#fig_2">Figure 2</ref>, some qualitative results of PolyLaneNet on TuSimple are shown. It is noticeable that PolyLaneNet's predictions on parts of the lane marking closer to the camera (where more details can be seen) are very accurate. Nonetheless, on parts of the lane marking closer to the horizon, the predictions are less accurate. We conjecture that this might be a result of a local minimum, caused by the dataset's imbalance. Since most lane markings in the dataset can be represented fairly well with 1st order polynomials (i.e., lines), the neural network has a bias towards predicting lines, thus the poor performance on lane markings with accentuated curvature.</p><p>Polynomial Degree. In terms of the polynomial degree used to represent the lane marking, the small difference in accuracy when using lower-order polynomials shows how unbalanced the dataset is. Using 1st order polynomials (i.e., lines) decreased the accuracy by only 0.35 p.p. Although the dataset's imbalance certainly has an impact on this, another important factor is the metric used by the benchmark to evaluate a  model's performance. The LPD metric <ref type="bibr" target="#b23">[22]</ref>, however, is able to better capture the difference between the models trained using 1st order polynomials and the others. This can be further seen in <ref type="table" target="#tab_2">Table III</ref>  Ablation Study. The ablation study results are shown in <ref type="table" target="#tab_2">Table IV</ref>. EfficientNet-b1 achieved the highest accuracy, followed by EfficientNet-b0 and ResNet-34. Those results suggest that larger networks, such as ResNet-50, may overfit the data. Although EfficientNet-b1 achieved the highest accuracy, we chose not to use it in other experiments, as the accuracy  As to the other ablation studies we carried out, one can see that sharing the top-y (h) is slightly better than not sharing. Moreover, training from a model pretrained on ImageNet seems to have a significant impact on the final result, as shown by the difference of 4.26 p.p. The same happens with data augmentation, as the model trained with more data has a significantly higher accuracy. Qualitative Evaluation. A sample of the qualitative results on ELAS and LLAMAS are shown in <ref type="figure">Figure 3</ref>. For more extensive results, videos are available <ref type="bibr" target="#b4">4</ref> . The results show that transfer learning works well on PolyLaneNet, since a smaller number of epochs was enough to obtain reasonable results in different datasets. However, in ELAS, there are many lane changes. In those situations, the model's accuracy decreased significantly. Since the images of those situations have a very different structure (e.g., the car is not heading towards the road direction), the low amount of samples in this situation may have not been enough for the model to learn it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, a novel method for lane detection based on deep polynomial regression was proposed. The proposed method is simple and efficient while maintaining competitive accuracy when compared to state-of-the-art methods. Although works with state-of-the-art methods with slightly higher accuracy exist, most do not provide source code to replicate their results, therefore deeper investigations on differences between methods are difficult. Our method, besides being computationally efficient, will be publicly available so that future works on lane markings detection have a baseline to start work and for comparison. Furthermore, we've shown problems on the metrics used to evaluate lane markings detection methods. For future works, metrics that can be used across different approaches to lane detection (e.g., segmentation) and that better highlights flaws in lane detection methods can be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the proposal method. From left to right: the model receives as input an image from a forward-looking camera and outputs information about each lane marking in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative results of PolyLaneNet on TuSimple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Model Training. For an input image, let M be the number of annotated lane markings given an input image. In general, traffic scenes contain few lanes, being M ≤ 4 for most images in the available datasets. For training (and metric evaluation), each annotated lane marking j, j = 1, . . . , M , is associated to the neuron unit j of the output. Therefore, predictions related to the outputs M + 1, . . . , M max should be disregarded in the loss function. An annotated lane marking j is represented by a set of points L * j</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="6">STATE-OF-THE-ART RESULTS ON TUSIMPLE.</cell><cell></cell></row><row><cell></cell><cell cols="4">PP = REQUIRES POST-PROCESSING.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Acc</cell><cell>FP</cell><cell>FN</cell><cell>FPS</cell><cell>MACs</cell><cell>PP</cell></row><row><cell>Line-CNN [8]</cell><cell>96.87%</cell><cell cols="2">0.0442 0.0197</cell><cell>30</cell><cell></cell><cell></cell></row><row><cell cols="4">ENet-SAD [9] 96.64% 0.0602 0.0205</cell><cell>75</cell><cell></cell><cell></cell></row><row><cell>SCNN [7]</cell><cell cols="3">96.53% 0.0617 0.0180</cell><cell>7</cell><cell></cell><cell></cell></row><row><cell>FastDraw [17]</cell><cell cols="3">95.20% 0.0760 0.0450</cell><cell>90</cell><cell></cell><cell></cell></row><row><cell>PolyLaneNet</cell><cell cols="3">93.36% 0.0942 0.0933</cell><cell>115</cell><cell>1.748G</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, which shows the maximum performance (i.e., the upper bound) of methods that represent lane markings as polynomials, measured by fitting polynomials on the test data itself. As it can be seen, TuSimple's metric does not punish predictions that are accurate only in parts of the lane marking closer to the car, wherein the image it will look almost straight (i.e., can be represented well by 1st order polynomials), since the thresholds may hide those mistakes. Meanwhile, the LDP metric clearly distinguishes the upper bounds, showing a clear difference even between the 4th and 5th degrees, in which TuSimple's metrics are almost identical.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY RESULTS ON TUSIMPLE VALIDATION SET W.R.T. POLYNOMIAL DEGREE</figDesc><table><row><cell>Modification</cell><cell></cell><cell>Acc</cell><cell>FP</cell><cell>FN</cell><cell>LPD</cell></row><row><cell></cell><cell>1st</cell><cell>88.63%</cell><cell cols="3">0.2231 0.1865 2.532</cell></row><row><cell>Polynomial Degree</cell><cell>2nd</cell><cell cols="2">88.89% 0.2223</cell><cell>0.1890</cell><cell>2.316</cell></row><row><cell></cell><cell>3rd</cell><cell>88.62%</cell><cell>0.2237</cell><cell>0.1844</cell><cell>2.314</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III TUSIMPLE</head><label>III</label><figDesc>PERFORMANCE UPPERBOUND OF POLYNOMIALS In addition, it is more computationally expensive (i.e., lower FPS, higher MACs, and longer training times). In regards to the input size, reducing it also means reducing the accuracy, as expected. In some cases, this accuracy loss may not be significant, but the speed gains may be. For example, using an input size of 480×270 decreased the accuracy by only 0.55 p.p., but the model MACs decreased by 1.82 times.</figDesc><table><row><cell>Polynomial Degree</cell><cell>Acc</cell><cell>FP</cell><cell>FN</cell><cell>LPD</cell></row><row><cell>1st</cell><cell cols="4">96.22% 0.0393 0.0367 1.512</cell></row><row><cell>2nd</cell><cell cols="3">97.25% 0.0191 0.0175</cell><cell>1.116</cell></row><row><cell>3rd</cell><cell cols="3">97.84% 0.0016 0.0014</cell><cell>0.732</cell></row><row><cell>4th</cell><cell cols="3">98.00% 0.0000 0.0000</cell><cell>0.497</cell></row><row><cell>5th</cell><cell cols="3">98.03% 0.0000 0.0000</cell><cell>0.382</cell></row><row><cell cols="5">gains are not significant nor consistent in our experiments.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY RESULTS ON TUSIMPLE VALIDATION SET W.R.T BACKBONE AND INPUT SIZE</figDesc><table><row><cell cols="2">Modification</cell><cell>Acc</cell><cell>FP</cell><cell>FN</cell><cell>MACs (G)</cell></row><row><cell></cell><cell>ResNet-34</cell><cell>88.07%</cell><cell cols="2">0.2267 0.1953</cell><cell>17.154</cell></row><row><cell>Backbone</cell><cell>ResNet-50 EfficientNet-b1</cell><cell cols="3">83.37% 89.20% 0.2170 0.1785 0.3472 0.3122</cell><cell>19.135 2.583</cell></row><row><cell></cell><cell>EfficientNet-b0</cell><cell>88.62%</cell><cell cols="2">0.2237 0.1844</cell><cell>1.748</cell></row><row><cell></cell><cell>320×180</cell><cell>85,45%</cell><cell cols="2">0.2924 0.2446</cell><cell>0.396</cell></row><row><cell>Input Size</cell><cell>480×270</cell><cell>88.39%</cell><cell cols="2">0.2398 0.1960</cell><cell>0.961</cell></row><row><cell></cell><cell>640×360</cell><cell cols="3">88.62% 0.2237 0.1844</cell><cell>1.748</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V ABLATION</head><label>V</label><figDesc>STUDY RESULTS ON TUSIMPLE VALIDATION SET</figDesc><table><row><cell cols="2">Modification</cell><cell>Acc</cell><cell>FP</cell><cell>FN</cell></row><row><cell>Top-Y Sharing</cell><cell>No Yes</cell><cell cols="3">88,43% 0.2126 0.1783 88.62% 0.2237 0.1844</cell></row><row><cell>Pretraining</cell><cell cols="4">None ImageNet 88.62% 0.2237 0.1844 84,37% 0.3317 0.2826</cell></row><row><cell></cell><cell>None</cell><cell cols="3">78.63% 0.4188 0.4048</cell></row><row><cell>Data Augmentation</cell><cell>10×</cell><cell cols="3">88.62% 0.2237 0.1844</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">According to the author of<ref type="bibr" target="#b10">[9]</ref>, the difference in performance comes from engineering tricks neither described in the paper nor included in the available code: https://web.archive.org/web/20200503114942/https://github.com/ cardwing/Codes-for-Lane-Detection/issues/208</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For reference, roughly speaking, one MAC (Multiply-Accumulate) is equivalent to 2 FLOPS. MACs were computed using the following library: https://github.com/mit-han-lab/torchprofile.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Qualitative results (videos) on ELAS/LLAMAS: https://www.youtube. com/playlist?list=PLm8amuguiXiJ2zKvcapUJI ybyOFi9yz9</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This study was financed in part by the Coordenaco de Aperfeioamento de Pessoal de Nvel Superior -Brasil (CAPES) -Finance Code 001, Conselho Nacional de Desenvolvimento Cientfico e Tecnolgico (CNPq, Brazil), PIIC UFES and Fundao de Amparo Pesquisa do Esprito Santo -Brasil (FAPES) grant 84412844. We thank NVIDIA for providing GPUs used in this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-driving Cars: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forechi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jesus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paixão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mutz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04407</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Traffic light recognition using deep learning and prior maps for autonomous cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Possatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paixão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A partaware multi-scale fully convolutional network for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haase-Schütz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hertlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Glaeser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wiesbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Driver Assistance: Survey, System, and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mccall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="37" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Video Based Lane Estimation and Tracking for</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ego-Lane Analysis System (ELAS): Dataset and Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="64" to="75" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial As Deep: Spatial CNN for Traffic Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">Fig. 3. Qualitative results of PolyLaneNet on ELAS (top row) and LLAMAS (bottom row)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Line-CNN: End-to-End Traffic Line Detection With Line Proposal Unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="248" to="258" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Lightweight Lane Detection CNNs by Self Attention Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A deformable-template approach to lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kluge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intelligent Vehicles Symposium</title>
		<meeting>the Intelligent Vehicles Symposium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lane Detection using Color-based Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Intelligent Vehicles Symposium</title>
		<meeting>Intelligent Vehicles Symposium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="706" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lane Following and Lane Departure Using a Linear-Parabolic Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Kelber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1192" to="1202" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Particle Filter-based Lane Marker Tracking Approach Using a Cubic Spline Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>De Souza Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An empirical evaluation of deep learning on highway driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pazhayampallil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Migimatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng-Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mujica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01716</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeepLanes: End-To-End Lane Position Estimation using Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gurghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Bailur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Murali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tusimple</forename><forename type="middle">Tusimple</forename><surname>Benchmark</surname></persName>
		</author>
		<ptr target="https://github.com/TuSimple/tusimple-benchmark" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FastDraw: Addressing the Long Tail of Lane Detection by Adapting a Sequential Prediction Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">End-to-end lane detection through differentiable leastsquares fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00293</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised labeled lane marker dataset generation using maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soussan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
		<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On Performance Evaluation Metrics for Lane Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2625" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
