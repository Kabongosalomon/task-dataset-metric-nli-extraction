<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Correlation-aware Adversarial Domain Adaptation and Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="201929-12-02">December 2, 2019 29 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Mahfujur</forename><surname>Rahman</surname></persName>
							<email>m27.rahman@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Technology (SAIVT) Laboratory</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<addrLine>2 George Street</addrLine>
									<postCode>4000</postCode>
									<settlement>Speech, Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Technology (SAIVT) Laboratory</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<addrLine>2 George Street</addrLine>
									<postCode>4000</postCode>
									<settlement>Speech, Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Queensland -Saint Lucia</orgName>
								<address>
									<postCode>4072</postCode>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Technology (SAIVT) Laboratory</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<addrLine>2 George Street</addrLine>
									<postCode>4000</postCode>
									<settlement>Speech, Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Mahfujur</forename><surname>Rahman</surname></persName>
						</author>
						<title level="a" type="main">Correlation-aware Adversarial Domain Adaptation and Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="201929-12-02">December 2, 2019 29 Nov 2019</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Pattern Recognition</note>
					<note>* Corresponding author:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain adaptation</term>
					<term>domain generalization</term>
					<term>correlation-alignment</term>
					<term>adversarial learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation (DA) and domain generalization (DG) have emerged as a solution to the domain shift problem where the distribution of the source and target data is different. The task of DG is more challenging than DA as the target data is totally unseen during the training phase in DG scenarios. The current state-of-the-art employs adversarial techniques, however, these are rarely considered for the DG problem. Furthermore, these approaches do not consider correlation alignment which has been proven highly beneficial for minimizing domain discrepancy. In this paper, we propose a correlation-aware adversarial DA and DG framework where the features of the source and target data are minimized using correlation alignment along with adversarial learning.</p><p>Incorporating the correlation alignment module along with adversarial learning helps to achieve a more domain agnostic model due to the improved ability to reduce domain discrepancy with unlabeled target data more effectively. Experiments on benchmark datasets serve as evidence that our proposed method yields improved state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In deep learning, it is commonly expected that the training and test data are collected from the same distribution and there will be a large set of labeled data.</p><p>With this expectation, deep neural networks (DNNs) achieved tremendous success in various applications. Unfortunately, in many realistic scenarios, the above assumptions may not valid i.e., the training and test data may not have the same distribution and labeled data may not be available.</p><p>Domain adaptation (DA) and domain generalization (DG) methods come forward to address this issue where previously labeled source data and a few or even no labeled target data is used to boost the task for a new domain. When the target data is unavailable during training, the typical approach is to utilize all the available source datasets. In the unavailability of the target data, DG techniques have been proposed which exploit all available source domain data that are less sensitive to the unknown target domain. DA methods require target data during training whereas DG methods do not require target data in the training phase. <ref type="figure" target="#fig_7">Figure 1</ref> illustrates the difference between the DA and DG methods. DNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> allow us to extract powerful features suitable for more types of input which are more domain invariant and transferable. Thus, researchers inspired to extend shallow DA techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8]</ref> to deep DA techniques <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref> and shallow DG techniques <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref> to deep DG techniques <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>shown favourable results in reducing domain shift over DNNs. Maximum mean discrepancy (MMD), Central Moment Discrepancy (CMD) and</p><p>Correlation alignment (CORAL) are the most used metrics in these domain adaptation methods <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b13">13]</ref>. These approaches use a Siamese network architecture (two streams) where one stream is used to extract features for the source data and another stream is used for the target data. The discrepancy metrics are used between the fully connected (fc) layers of the two streams of CNNs to reduce the domain discrepancy in the training stage.</p><p>In this paper, we propose a correlation-aware adversarial DA and DG framework where the correlation metric is used jointly with adversarial learning to minimize the domain disparity of the source and target data.</p><p>Using both of these strategies concurrently further enforces the features of the same classes to be mapped nearby and the features of different classes to be far apart in DA scenarios. This also enforces the discrepancies among the source domains to be further reduced in DG scenarios. This approach significantly enhances the ability of prior adversarial adaptation approaches through our additional proposed domain discrepancy module. The proposed method was evaluated on five benchmark datasets (Office-31 <ref type="bibr" target="#b25">[25]</ref>, Office-Home <ref type="bibr" target="#b26">[26]</ref>, ImageCLEF-DA 1 , Office-Caltech <ref type="bibr" target="#b27">[27]</ref> and PACS <ref type="bibr" target="#b21">[21]</ref>) on standard unsupervised DA and DG settings. Experiments prove that our proposed model for unsupervised DA and DG yields state-of-the-art results.</p><p>The contribution of this work is fourfold::</p><p>• We implement a novel deep domain adaptation and generalization framework that enables generalization on unknown target data.</p><p>• The proposed deep domain adaptation architecture jointly adapts features using correlation alignment with adversarial learning.</p><p>• The proposed framework can be used for both domain adaptation and domain generalization without changing the architecture of the model. • We report competitive accuracy compared to the state-of-the-art methods on five datasets and achieve the best average image classification accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain adaptation. Recently, DNNs have been extensively adopted in explicitly reducing domain divergence by learning more transferable features.</p><p>Deep learning based adaptation techniques can be divided into four categories:</p><p>distribution matching based approaches, adversarial learning based approaches, generative adversarial learning based approaches and batch normalization based approaches.</p><p>Maximum mean discrepancy (MMD) is one of the popular metrics for distribution matching based deep domain adaptation methods. MMD based DA methods <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12]</ref> measure the distance between the mean embeddings of the probability distributions of the source and target data in a reproducing kernel Hilbert space with a kernel trick. Another distribution matching metric is correlation alignment (CORAL) which aligns the covariances (second order statistics) between the source and target domains. CORAL based deep DA methods <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref> have also shown promising performance in reducing the domain discrepancy of two domains. Central Moment Discrepancy (CMD) <ref type="bibr" target="#b24">[24]</ref> is also used in deep domain adaptation techniques where the domain divergence is reduced by matching the higher order central moments of the probability distributions of the source and target domains.</p><p>Nowadays, adversarial learning is promisingly used in domain adaptation.</p><p>Most of the adversarial learning based domain adaptation methods <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref> adopt the idea from GAN <ref type="bibr" target="#b23">[23]</ref>. In these methods, a discriminator is trained to classify the sampled feature comes from the source domain or target domain.</p><p>On the other hand, the feature extractor is trained to fool the discriminator. All the adversarial learning based methods for domain adaptation discussed here apply adversarial losses in the embedding space.</p><p>GAN based domain adaptation methods <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b34">33]</ref> are also embraced in domain adaptation. Liu et al. proposed coupled generative adversarial networks (CoGAN) <ref type="bibr" target="#b30">[30]</ref> to learn a joint distribution of the source and target data where two classifiers are used for two domains and the classifiers are adapted so that the source classifier can classify the target samples correctly.</p><p>Another GAN based domain adaptation method is proposed in <ref type="bibr" target="#b32">[31]</ref> where the network produce source-like images from the source and target embeddings.</p><p>GAN based domain adaptation methods perform adaptation in the pixel space, by contrast, adversarial based domain adaptation techniques perform adaptation in the feature space.</p><p>Batch Normalization based domain adaptation methods <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b17">17]</ref> decrease the domain discrepancy by aligning the source and target distributions to a canonical one. Li et al. <ref type="bibr" target="#b35">[34]</ref> proposed an unsupervised domain adaptation method based on batch normalization where the statistics in all batch normalization layers are modulated across the network. In <ref type="bibr" target="#b17">[17]</ref>, the source and target distributions are matched to a reference one via domain adaptation layers.</p><p>Domain generalization. DG is a less explored area compared to the DA problem. Blanchard et al. <ref type="bibr" target="#b36">[35]</ref> proposed a DG model where all the available source domains are aggregated and they learn a support vector machine classifier. In <ref type="bibr" target="#b37">[36]</ref>, the task of DG is achieved by minimizing the discrepancy unsupervised domain adaptation. They used a common feature extractor network to extract the features of the source and target data. By contrast, we use two feature extractor networks for the corresponding two domains to achieve more domain specific features. In addition, we extend the target feature extractor network by adding a fully connected layer where the output vector of the layer is equal to the number of categories of the target data. We reduce the domain discrepancy using a domain discriminator via a GRL in the lower fully connected layer (f c B ) of a DNN along with a correlation alignment module which is used between the last fully connected layers (f c 8 ) of two streams of CNNs. We also extend our framework on DG scenarios. This approach makes our model to be unified which can be applied on both DA and DG settings. The method proposed by Tzeng et al. <ref type="bibr" target="#b16">[16]</ref> is also related to our work where two different feature extractors by unsharing the weights are used with an adversarial network. The difference between our work and <ref type="bibr" target="#b16">[16]</ref> is that they pre-train a source encoder convolutional neural network (CNN) on source data, and then perform adversarial learning. By contrast, we do not need to pre-train the source encoder CNN on the source data, and we use the correlation alignment metric in the last fully connected layer of the source and target streams of CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we present our proposed methodology for unsupervised domain adaptation and domain generalization in detail. We use the same network structure for domain adaptation and domain generalization methods.</p><p>We consider the unsupervised domain adaptation issue where only labeled source data and unlabeled target data are available. The overview of our method is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We use two feature extractor networks for obtaining more domain specific features and we extend the target feature network by integrating a fully connected layer where the output vector of this layer is equal to the number of categories of the target data. We use the correlation alignment metric between the last fully connected layers of the source and target feature extractor networks which enforce further to decrease the domain discrepancy of the source and target data. Before going into detail of our methodology for unsupervised DA and DG, we summarize the definitions of terminologies used.</p><p>It is assumed that there are N s number of source labeled data {X s i , Y s i } and N t number of target data {X t i } without their labels. It is also assumed that the data distribution of the source and target domain is different, i.e., P s ( </p><formula xml:id="formula_0">X s i , Y s i ) = P t (X t i , Y t i ) where Y t i is</formula><formula xml:id="formula_1">: X → Y which is able to classify {X t i } to the corresponding labels {Y t i } given {X s1 i , Y s1 i }, {X s2 i , Y s2 i } . . . {X sn i , Y sn i } during training as the input and {X t i } is unavailable in the training stage where {X s1 i , Y s2 i }, {X s2 i , Y s2 i } and {X sn i , Y sn i</formula><p>} are the samples from source domains 1, 2 and n respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Adversarial Domain Adaptation</head><p>A domain classifier is applied in a general feed-forward framework to form an adversarial domain adaptation model <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. The general idea of this model is to learn domain invariant and class discriminative features. These models are analogous to the GAN with min-max loss of F s and F t , the feature extractors for source and target domains respectively, and D, a binary classifier that can classify whether the features come from the source domain or target domain,</p><formula xml:id="formula_2">min Fs,Ft max D L(D, F s , F t ) = E x∼Ps [logD(F s (X))] + E x∼Pt [log(1 − D(F t (X))].<label>(1)</label></formula><p>The network for feature extraction for source and target domains can use either an identical framework <ref type="bibr" target="#b15">[15]</ref> where the weights are shared between the networks or a distinct framework <ref type="bibr" target="#b16">[16]</ref> where the weights are not shared. In this paper, we use the unshared feature extractors for the source and target domains to achieve more domain specific features. We follow a similar procedure as <ref type="bibr" target="#b16">[16]</ref> where the classification loss is used with the source feature extractor as follows,</p><formula xml:id="formula_3">min Fs,C L s = E x∼Ps L c (C(F s (X s i )), Y s i ),<label>(2)</label></formula><p>where L c is the cross entropy loss for the source domain classification task as we have the supervised data for this domain. Cross entropy loss is defined as follows with a set of input source data</p><formula xml:id="formula_4">X s i = {X s 1 , X s 2 , ..., X s n } with the corresponding labels Y s i = {Y s 1 , Y s 2 , ..., Y s n } where i = 1, 2, 3, ..., N , min X s i ,Y s i L c = − K i y i log(p i ),<label>(3)</label></formula><p>where K is the number of categories, y i is the ground truth and p i is the predicted value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Correlation-aware Adversarial Domain Adaptation (CAADA)</head><p>The objective of DA is to minimize the discrepancy of the source and target data as much as possible during the training stage without utilizing the target labels due to unavailability. <ref type="figure" target="#fig_1">Figure 2</ref> shows our proposed method for unsupervised domain adaptation based on adversarial learning along with a correlation alignment module during the training phase. The correlation alignment metric reduces the domain discrepancy using the second order statistics of the source and target data. We define the coral loss of the source and target activation features as <ref type="bibr" target="#b13">[13]</ref>,</p><formula xml:id="formula_5">min Fs,Ft L DM (F s , F t ) = 1 4d 2 C s − C t 2 F ,<label>(4)</label></formula><p>where C s and C t denote the features covariance matrices of the source and target data, d indicates the dimension of the activation features and ||.|| 2 F denotes the squared matrix Frobenius norm. The C s and C t are given by the following equations <ref type="bibr" target="#b13">[13]</ref>,</p><formula xml:id="formula_6">C s = 1 N s − 1 (F T s F s − 1 N s (1 T F s ) T (1 T F s )),<label>(5)</label></formula><formula xml:id="formula_7">C t = 1 N t − 1 (F T t F t − 1 N t (1 T F t ) T (1 T F t )).<label>(6)</label></formula><p>After incorporating the correlation alignment module between the last fully connected layers of the source and target feature extraction networks to minimize the divergence of the source and target data again, the objective</p><formula xml:id="formula_8">function of CADA is, min Fs,Ft max D L(D, F s , F t ) + L DM (F s , F t ) = E x∼Ps [logD(F s (X))]+ E x∼Pt [log(1 − D(F t (X))] + σL DM (F s , F t ),<label>(7)</label></formula><p>where σ is a hyper parameter that is fine-tuned. We discuss the procedure of fine tuning hyper parameters in Section 4.5.</p><p>The correlation alignment metric forces the model to minimize the source and target domain discrepancy further which helps to obtain a more domain agnostic model that can be applied to classify unlabeled target data during the test phase more effectively. The overall objective of our proposed model is,</p><formula xml:id="formula_9">min Xs,Ys L c + min Fs,Ft max D γL(D, F s , F t ) + σL DM (F s , F t ),<label>(8)</label></formula><p>where γ and σ are the hyper-parameters for adversarial and distribution matching loss that are needed to be fine tuned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Correlation-aware Adversarial Domain Generalization (CAADG)</head><p>The difference between DA and DG approaches is that the target data is unavailable during the training stage in the latter. We need to fully utilize all available source domains. We use the same architecture as our DA model except we discard the target data in the training phase. For domain generalization, the training data always contains more than one source domain. Most of the existing domain generalization methods <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b39">38]</ref> split the source data as 70% -30%</p><p>and aggregate these 70% data as training data and 30% data as validation data from all the source domains. Aggregating the data from the source domains and training a single deep neural network on all the data and testing on the unknown target domain provides a strong domain agnostic model that outperforms many prior approaches <ref type="bibr" target="#b41">[40]</ref>. The discrepancy is minimised between these training and validation data during training. We also adopt the same setting for fair comparison. For domain generalization, the target data is always unknown. The main objective of domain generalization is to achieve a domain agnostic model from the available source domains and apply it to the unknown target data.</p><p>Splitting and aggregating the source data also reduces the domain discrepancy.</p><p>We split all the source data 70% − 30% as training and validation sets. We feed 70% of the data data (aggregated from all source domains) in one stream of the CNN and 30% data (aggregated from all source domains) into another stream of the CNN. Splitting and aggregating data this way, and feeding the data through two streams of the CNN is a simple yet effective method to minimize the discrepancy among domains. However, this approach does still suffer from some domain mismatch which is not removed completely. In <ref type="figure" target="#fig_2">Figure 3</ref> below, we visualize the classification loss, the discrepancy loss and the discriminator loss. As the discrepancy loss is applied in the last fully connected layer and it is randomly initialized with N (0, 0.005), the discrepancy loss is very small while the classification and discriminator losses are large. From <ref type="figure" target="#fig_2">Figure 3</ref>, we can see that after splitting and aggregating the source data, the domain mismatch still exists between the data which is feed through the two streams of the CNN. The adversarial learning module forces to reduce the discrepancy among the source domains in the f c B layer whereas the correlation alignment module forces the domains to be aligned in the last fully connected layer (f c8). Introducing the correlation alignment module with adversarial learning, the nework is more capable to reduce the domain shifts among the source domains and obtains a domain agnostic model that can be applied for the target data for a certain task.</p><p>The main difference between our work and prior adversarial domain adaptation methods <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref> is that our model forces the minimization of the source and target discrepancy twice. The domain discrepancy is minimized in f c B layer using adversarial learning, and in f c8 layer using the correlation alignment metric. As a result, our model achieves the minimum discrepancy between the source and target data in domain adaptation. We also extend our  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we describe the experiments we have conducted to evaluate our method and compare the proposed method with state-of-the-art domain adaptation and generalization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The proposed approach is evaluated on five commonly used datasets in the context of image classification.  domains and each domain consists of 7 clasess. It contains total 9991 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architecture</head><p>In our method, we adopted the AlexNet <ref type="bibr" target="#b42">[41]</ref>  layers. We adopt the similar architecture as <ref type="bibr" target="#b15">[15]</ref> for discriminator for a fair comparison. Our adversarial discriminator consists of 3 fully connected layers:</p><p>two layers with 1024 hidden units followed by the final discriminator output.</p><p>Each of the 1024-unit layers uses a ReLU activation function. We finetune the conv1 − f c7 layers with the pretrained AlexNet on ImageNet datasets <ref type="bibr" target="#b43">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Setup</head><p>We use the Caffe <ref type="bibr" target="#b44">[43]</ref> framework to implement our proposed approach. We conduct all the experiments 3 times using high-performance computing (HPC) that consists of graphics processing unit (GPU), and we take the average accuracy in terms of image classification. For training the model, we set the batch size as 128 for all experiments. We set the learning rate to 0.001, momentum to 0.9 and weight decay to 5 × 10 −4 for optimizing the network.</p><p>We follow the standard protocol for unsupervised domain adaptation where labeled source data and unlabeled target data are used for all transfer tasks as <ref type="bibr" target="#b15">[15]</ref>. We also follow the standard protocol for domain generalization transfer tasks as <ref type="bibr" target="#b39">[38]</ref> where the target data is unavailable in the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Baseline Methods</head><p>For Office-31, Office-Home and ImageCLEF-DA datasets, we compare our proposed method on the DA scenario with Alexnet (without adaptation) <ref type="bibr" target="#b43">[42]</ref> and the state-of-the-art domain adaptation methods: deep domain confusion</p><formula xml:id="formula_10">Methods A → W D → W D → A W → A W → D A → D Avg.</formula><p>AlexNet <ref type="bibr" target="#b42">[41]</ref>         (DDC) <ref type="bibr" target="#b9">[9]</ref>, domain-adversarial training of neural networks (DANN) <ref type="bibr" target="#b15">[15]</ref>, deep-CORAL (D-CORAL) <ref type="bibr" target="#b13">[13]</ref>, deep adaptation networks (DAN) <ref type="bibr" target="#b10">[10]</ref>, deep reconstruction classification network (DRCN) <ref type="bibr" target="#b45">[44]</ref>, residual transfer network (RTN) <ref type="bibr" target="#b11">[11]</ref>, joint adaptation network (JAN) <ref type="bibr" target="#b12">[12]</ref>, domain adaptive hashing (DAH) <ref type="bibr" target="#b26">[26]</ref>, adversarial discriminative aomain adaptation (ADDA) <ref type="bibr" target="#b16">[16]</ref>, automatic domain alignment layers (AutoDIAL) <ref type="bibr" target="#b17">[17]</ref>, and multi-adversarial domain adaptation (MADA) <ref type="bibr" target="#b46">[45]</ref>. For the PACS dataset, Office-31 and</p><formula xml:id="formula_11">Methods A → C A → P A → R C → A C → P C → R P → A P → C P → R R → A R → C R → P Avg.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AlexNet</head><p>Office-Caltech, we compare our method on the DG scenario with the state-of-the-art DG methods: learned -support vector machine (L-SVM) <ref type="bibr" target="#b48">[47]</ref>, kernel fisher discriminant analysis (KDA) <ref type="bibr" target="#b50">[49]</ref>, domain-invariant component analysis (DICA) <ref type="bibr" target="#b37">[36]</ref>, multi-task auto-encoder (MTAE) <ref type="bibr" target="#b20">[20]</ref>, domain separation network (DSN) <ref type="bibr" target="#b49">[48]</ref>, deeper, broader and artier domain generalization (DBADG) <ref type="bibr" target="#b21">[21]</ref>, conditional invariant deep domain generalization (CIDDG) <ref type="bibr" target="#b39">[38]</ref>, undoing the damage of dataset bias (Undo-Bias) <ref type="bibr" target="#b19">[19]</ref>, unbiased metric learning (UML) <ref type="bibr" target="#b47">[46]</ref>, multi-task autoencoders (MTAE) <ref type="bibr" target="#b20">[20]</ref> and deep domain generalization with structured low-rank constraint (DGLRC) <ref type="bibr" target="#b22">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Hyper-parameter tuning</head><p>For any unsupervised domain adaptation and generalization method, we need to fine tune the hyper-parameters to achieve more efficient results. In our method, we fine tune two weight balance hyper-parameters: σ and γ. We first apply different values for these two parameters on the experiments for the transfer task of A → W and we have found that γ = σ = 0.1 provides the best results. Then we use this optimal value for γ and σ for all other transfer tasks in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results and Discussion</head><p>In this section, we discuss the results on benchmark datasets on both unsupervised domain adaptation and domain generalization settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1.">Results on domain adaptation scenarios</head><p>Office- <ref type="bibr" target="#b32">31</ref> We conducted 6 transfer tasks in the context of domain  <ref type="table" target="#tab_3">Table 1</ref>. We also calculated the average accuracy. It is worth noting that our proposed approach achieves state-of-the-art results on two transfer tasks: A → W and A → D and the average accuracy on six different tasks is 78.3%. It is also important to note that our method outperforms DANN on all transfer tasks and this improvement on transfer tasks provides the justification of integrating the correlation alignment metric in the last fully connected layer which helps to obtain a more domain agnostic model.</p><formula xml:id="formula_12">adaptation: A → W , D → W , D → A, W → A, W → D</formula><p>From the results, we make some important observations: Office-Home We conducted 12 transfer tasks of 4 domains on</p><p>Office-Home dataset in the context of domain adaptation. We report the results in <ref type="table" target="#tab_5">Table 2</ref>. We achieve the state-of-the-art performance on 6 transfer From the results of Office-Home dataset, we make two note-worthy observations: (1) All the deep domain adaptation methods obtain better performance than standard deep learning methods even when more categories are present in the dataset. It is noted that in Office-Home dataset, the number of total categories is 65; and (2) Our model is still capable to decrease the domain discrepancy even when more categories exist in the datasets.</p><p>ImageCLEF-DA We conducted 6 transfer tasks: I → P , P → I, I → C, C → I, C → P , and P → C. We report the results in <ref type="table" target="#tab_6">Table 3</ref>. In this dataset, every category has the same number of images thus the images are balanced, but each domain has only 600 images that could not be enough for training the network. Our method outperforms existing approaches in four transfer tasks:</p><p>P → I, I → C, C → I, and C → P . The prior best average accuracy on this dataset was 79.8% achieved by MADA, and our method achieved 80.2 % average accuracy which sets a new state-of-the-art performance.</p><p>From the results of ImageCLEF-DA dataset, we make two important observations: (1) Deep domain methods still achieve better performance over standard deep learning methods when few categories are shared by the source and target data; and (2) Our model is still capable to decrease the domain discrepancy even when few categories are shared by the source and target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2.">Results on domain generalization scenarios</head><p>Office-31 In the context of domain generalization, we conducted 3 transfer tasks: A, W → D; A, D → W and D, W → A where the target data is unavailable during the training phase. We report the results in <ref type="table" target="#tab_7">Table 4</ref>. From the results we cam observe that our proposed domain method achieves state-of-the performance on two transfer tasks: A, D → W and D, W → A and the average accuracy on three transfer tasks is 81.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Office-Caltech</head><p>We conducted four transfer tasks: W, D, C → A; A, W, D → C; A, C → D, W and D, W → A, C on domain generalization settings.We report the results on <ref type="table" target="#tab_8">Table 5</ref>. From the results we can observe that we achieved stateof-the-art performance on three transfer tasks and we also achieved the best average accuracy on four transfer tasks.</p><p>PACS We conducted 4 transfer tasks: P, C, S → A; P, S, A → C; S, A, C → P and P, C, A → S on DG scenario where the target data is unavailable during the training phase. We report the results in <ref type="table" target="#tab_9">Table 6</ref>. From the results we can see that our proposed approach obtains state-of-the-art performance on two transfer tasks: P, C, S → A and P, S, A → C and the average accuracy on four transfer tasks is 71.98%.</p><p>From the results <ref type="table" target="#tab_3">(Tables 1 to 6</ref>), we note that the proposed architecture performs well on both domain adaptation and domain generalization settings.   <ref type="figure">Figure 6</ref>: Convergence: (a) Test Accuracy for A → D transfer task and (b) Test Accuracy for A → W transfer task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Visualization</head><p>We use t-SNE <ref type="bibr" target="#b51">[50]</ref> to conduct an embedding visualization. We take images from Amazon and Webcam domains of Office-31 dataset to produce an embedding. In <ref type="figure" target="#fig_9">Figure 5</ref>, the representations in Amazon → Webcam transfer task is visualized. <ref type="figure" target="#fig_9">Figure 5(a)</ref> shows the representation using standard deep learning (AlexNet) without any adaptation method. For better view, we have taken 7 different categories from Office-31 dataset. <ref type="figure" target="#fig_9">Figure 5(b)</ref> shows the representation of our model without adversarial loss. <ref type="figure" target="#fig_9">Figure 5</ref>(c) depicts the representations that are learned by our model withoout discrepancy loss. On the other hand, <ref type="figure" target="#fig_9">Figure 5</ref>(d) depicts the representations that are learned by our proposed approach. Examining the embeddings, we find that the clusters created by our model separate the categories while mixing the domains much more effectively than standard DNN and using only discrepancy loss or only adversarial loss. These visualization also suggests that our proposed method is more capable to align the source and target features from the same class nearby.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Convergence Performance</head><p>Since our proposed method is based on adversarial learning, we compare the performance on convergence. The convergence is measured based on the test accuracy. <ref type="figure">Figure 6</ref>(a) and <ref type="figure">Figure 6</ref>(b) represents the test accuracy of our method and DANN <ref type="bibr" target="#b15">[15]</ref> on A → D and A → W transfer tasks. The x-axis and y-axis of these accuracy graphs represent the iterations and accuracies in percentage. We have calculated the test accuracy over 50,000 iterations. From <ref type="figure">Figure 6</ref>, we can see that our model has the analogous convergence speed as DANN with remarkably better accuracy in the entire procedure of convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Sample Size of Target Domain</head><p>In this section, we empirically show that our proposed method is data-driven. If the number of unlabeled target data is increased, the image classification performance is also boosted. We shuffle the target data on Office-31 for domain adaptation and Office-Caltech for domain generalization, and access the top 20%, 40%, 60%, 80%, 100% data of each classes of the target domain. We train and evaluate our approach for both domain adaptation and domain generalization on A → W and W, D, C → A tasks respectively. As illustrated in <ref type="figure" target="#fig_10">Figure 7</ref>(a) and 7(b) , as the target data size moderately expands, the image recognition accuracy of the corresponding task increases accordingly. It demonstrates that when more unlabeled target data are involved in the training, a more transferable classifier with regard to the target domain can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.">Sensitivity of Embedding Dimension</head><p>In this part, we investigate the sensitivity of the embedding dimension of the bottleneck layer as it plays a significant role in reducing the discrepancy among domains. We conduct the experiments on Office-31 and Office-Caltech datasets for domain adaptation and domain generalization tasks respectively. We choose A → W transfer task for Office-31 on domain adaptation and W, D, C → A transfer task for Office-Caltech on domain generalization. We report the mean accuracy with standard deviation for embedding dimensions varied in {128, 256, 512 } respectively. As illustrated in <ref type="figure" target="#fig_10">Figure 7</ref>(c) and 7(d), the accuracy almost maintains at the same level and achieves slightly higher accuracy when the embedding dimension is set to 256, implying that our approach is not sensitive to the chosen range of feature space dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.11.">Complementary with correlation alignment</head><p>In this part, we demonstrate that our approach is capable of cooperating with correlation alignment. We conduct this case study on Office-31 dataset on both domain adaptation and domain generalization tasks, and report the accuracy in <ref type="table" target="#tab_12">Table 7</ref> and <ref type="table" target="#tab_13">Table 8</ref> respectively. As illustrated, with correlation alignment to further reduce the discrepancy among the domains, we further boost the recognition performance and obtain 2.1% and 1.89% gain for domain adaptation and domain generalization tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.12.">Ablation Study</head><p>Recent studies on domain adaptation suggested that both adversarial learning <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref> and correlation alignment <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b13">13]</ref> can play a vital role to reduce the domain discrepancy due to dataset bias. It is noted that adversarial learning uses a domain classifier to reduce the discrepancy whereas a correlation alignment module calculates co-variances of the data and minimizes the distance among the data of different domains. But these methods cannot eliminate the discrepancy completely. Hence, we combined two different techniques to minimize the discrepancy so that we can estimate the minimum discrepancy of the available domains for both domain adaptation and domain generalization tasks.   contributions of components and provide more justification of adopting correlation alignment with adversarial learning. We compare the results with our method and the results of domain adaptation is reported in <ref type="table" target="#tab_12">Table 7</ref>. We report the domain generalization performance on Office-31 dataset in <ref type="table" target="#tab_13">Table 8</ref>.</p><formula xml:id="formula_13">Methods A → W D → W D → A W → A W → D A → D</formula><p>From the results of ablation studay, it can be observed that incorporating correlation alignment with adversarial learning improves the discrimination and generalization ability. We also observe that in each transfer task of the domain adaptation and generalization, we get better performance than using only correlation alignment or using only adversarial learning to minimize the discrepancy between the domains.</p><formula xml:id="formula_14">Methods A → W D → W D → A W → A W → D A → D Avg.</formula><p>CAADA <ref type="formula" target="#formula_3">(2 Discriminator</ref>     We examined the performance of incorporating another discriminator to align the features of Fc8. The results are reported in <ref type="table" target="#tab_3">Tables 9 -12. According</ref> to the results, we can observe that even using another discriminator to align features of Fc8 instead of using the correlation alignment loss still suffer from poor performance than our proposed method. In ADDA <ref type="bibr" target="#b16">[16]</ref>, the researchers use the pre-trained source model as an initialization. On the other hand, other adversarial learning methods JAN <ref type="bibr" target="#b12">[12]</ref>, DANN <ref type="bibr" target="#b15">[15]</ref>, RTN <ref type="bibr" target="#b11">[11]</ref> did not use a pre-trained source model. We adopted the approach from JAN, DANN and RTN. We examined the approach of using the pre-trained source model as an initialization. From the results (Tables 9 -12), we have seen that the improvements is about 0.4% to 0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.13.">Discussion</head><p>From the above analysis, it can be noted that all the deep domain adaptation and generalization methods outperform the standard deep learning approaches. Adversarial learning with correlation alignment improves the generalization capability of the model. When the target data size is increased, the image classification accuracy is also increased accordingly. It can be revealed that when more unlabeled target data are involved in the training, a more transferable classifier with regard to the target domain can be obtained.</p><p>The image recognition accuracy almost maintains at the same level with varying the embedding dimension and achieves a slightly higher accuracy when the embedding dimension is set to 256. Using a pre-trained source model as an initialization improves the image recognition accuracy by 0.4% to 0.5% on domain adaptation and generalization tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel correlation-aware adversarial domain adaptation and generalization method which aims to minimize the domain discrepancy as much as possible during training stages using a correlation alignment metric along with adversarial learning. We have shown that our model can be applied to address the DG issue without changing the basic network architecture. We have found that using the correlation alignment along with adversarial learning for domain adaptation and domain generalization scenarios works efficiently. Experiments on different datasets on domain adaptation and domain generalization verify the effectiveness of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 Figure 1 :</head><label>11</label><figDesc>(a) Single source domain adaptation can access the target data during training phase. (b) Multiple source domain adaptation can also access the target data during training phase. (c) Domain generalization cannot access the training data during training stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed architecture for unsupervised domain adaptation. The domain discriminator, D, is used to minimize the domain discrepancy between the source and target domains along with the distribution matching metric (correlation alignment) which further enforces the alignment of the features of the source and target data nearby from the same classes and far for the features from different classes. The same architecture is used in DG scenarios where the discrepancy is minimized among the source domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Discrepancy loss, classification loss and discriminator loss for training the P, C, S → A transfer task on PACS dataset. unseen target domain. DG considers the situation where the labeled data are collected from several source domains so that the trained model can handle new domains without the adaptation step in the future.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Office- 31 [</head><label>31</label><figDesc>25] dataset has three domains: Amazon (A), Webcam (W) and DSLR (D). The Amazon domain is formed with the downloaded images from amazon.com and this domain consists of 2817 images. The Webcam domain consists of the images that are captured by a webcam and it contains 795 images. The DSLR domain consists of images that are taken by a DSLR camera and it has 498 images. The number of categories of this dataset is 31. Office-Home [26] dataset has four domains: Clipart (C), Art (A), Real-world (R) and Product (P), and every domain consists of 65 categories. The Clipart domain is formed with clipart images. The Art domain consists of artistic images in the form of paintings, sketches, ornamentation etc. The Real-world domain's images are captured by a regular camera and the Product domain's images have no background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>presents some sample images of 7 categories of Office-Home dataset. ImageCLEF-DA is a benchmark dataset in the domain adaptation community for the ImageCLEF 2014 domain adaptation challenge. This dataset is formed with selecting 12 common categories shared by three popular datasets for object recognition: Caltech-256 (C), ImageNet ILSVRC 2012 (I) and Pascal VOC 2012 (P), and each is considered as a domain. There are 600 images in each domain and 50 images in each category. Office-Caltech [27] is another popular benchmark dataset in the domain adaptation community. It is formed by considering the 10 common categories of two datasets: Office-31 and Caltech-256. It has 4 domains: Amazon (A), Webcam (W), DSLR (D) and Caltech (C). PACS [21] is a standard dataset for DG. It is formed by taking the common classes among Sketchy, Caltech256, TU-Berlin and Google Images. It has 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>architecture where 5 convolutionFigure 4 :</head><label>54</label><figDesc>(conv1, conv2, conv3, conv4 and conv5) layers and 3 fully connected layers (f c6, f c7 and f c8) are used for extracting features for the source and target data. A bottleneck layer (f c B ) with 256 units is added after f c7 layer as domain-adversarial training of neural networks [15] so that safer transfer learning can be obtained. Due to dataset shift, the last-layer features There are some example images that are taken from Office-Home dataset. It is a newly released and most challenging dataset in the domain adaptation community which consists of 65 categories. It comprises of images of everyday objects. This dataset divided into 4 different domains; the Art domain consists of sketches, paintings, artistic images, the Clipart domain comprises clipart images, the Product domain comprises images which have no background and finally, the Real-World domain is created by taking images which are captured with a regular camera. The figure shows sample images from 7 of the 65 classes. are tailored to domain-specific structures that are not safely transferable, hence we add a bottleneck layer fcB. Gradient reversal layer ensures that the feature distributions over the two domains are made similar (as indistinguishable as possible for the domain classifier), thus resulting in the domain-invariant features. We use f c B as inputs to the discriminator and the f c8 layer for both streams of CNNs. The dimensions of the f c8 layer is set as the number of classes of the dataset (31 for Office-31 dataset). The adversarial module is used between the f c B layer of the source and target feature extraction network and the correlation alignment module is used between f c8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>and A → D and the results are reported in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 1 )</head><label>1</label><figDesc>All the deep domain adaptation methods outperform the standard deep learning methods and it can be revealed that deep neural networks without domain adaptation cannot eliminate the issue of domain shift; (2) DANN trains an extra domain classifier to enforce the minimization of discrepancy and this outperforms standard deep neural networks by about 4%. This improvement also indicates the importance of adversarial learning for minimizing the discrepancy between the source and target data; (3) The use of a distribution matching metric (RTN, D-CORAL) also brings significant improvement over a standard deep learning method; and (4) Incorporating a correlation alignment module with adversarial learning improves the average accuracy by 5% which indicates the capability of reducing domain discrepancy with our model is better than the baseline adversarial methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>tasks including A → P , C → R, P → A, P → R, R → A and R → P . It is worth noting that JAN obtains state-of-the-art performance on the other six transfer tasks as it reduces the domain shift in joint distributions of the network activations of multiple task-specific layers. In contrast, our proposed approach match the marginal distributions of features across domains. The prior best average accuracy was 48.2 % achieved by JAN. The average accuracy achieved by our proposed method is 48.19 % which outperforms the baseline method DANN by 3.25 %. Our method beats other state-of-the-art methods except JAN which also obtains the same performance in terms of average accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>The t-SNE visualization of the activations of (a) AlexNet (without adaptation) (b) without adversarial loss (c) without discrepancy loss and (d) CAADA (ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>The accuracy with regard to various unlabeled target sample size and by varying feature embedding dimension: (a) sample size on A → W domain adaptation transfer task, (b) sample size on W, D, C → A domain generalization transfer task, (c) embedding dimension on A → W domain adaptation transfer task and (d) embedding dimension on W, D, C → A domain generalization transfer task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>model to make it suitable for DG scenarios where the target data is unavailable during the training phase and the discrepancy is minimized among the source domains. With training on multiple source domains' labeled data, the model can be applied to any unseen target domain without the adaptation step in the test phase. DG approaches are more suitable than DA methods in real-world scenarios since DG methods do not focus on the generalization capability on the particular target domain, but aim to generalize to any</figDesc><table><row><cell>2.5</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell></row><row><cell>1.5</cell><cell></cell><cell></cell></row><row><cell>Loss</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell></row><row><cell cols="3">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32</cell></row><row><cell></cell><cell>Epoch</cell><cell></cell></row><row><cell>Discrepancy Loss</cell><cell>Classification loss</cell><cell>Discriminator Loss</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Image classification accuracies for deep domain adaptation on the Office-31 dataset. We use the conventional protocol for unsupervised domain adaptation where source data are labeled, but target data are unlabeled. A → W indicates A (Amazon) is the source and W (Webcam) is the target domain.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Image classification accuracies for deep domain adaptation on the Office-Home dataset. We use the conventional protocol for unsupervised domain adaptation where source data are labeled, but target data are unlabeled. A → C indicates A (Art) is the source domain and C (Clipart) is the target domain.</figDesc><table><row><cell>Methods</cell><cell>I → P</cell><cell>P → I</cell><cell>I → C</cell><cell>C → I</cell><cell>C → P</cell><cell>P → C</cell><cell>Avg.</cell></row><row><cell>AlexNet [41]</cell><cell>66.2</cell><cell>70.0</cell><cell>84.3</cell><cell>71.3</cell><cell>59.3</cell><cell>84.5</cell><cell>73.9</cell></row><row><cell>DAN [10]</cell><cell>67.3</cell><cell>80.5</cell><cell>87.7</cell><cell>76.0</cell><cell>61.6</cell><cell>88.4</cell><cell>76.9</cell></row><row><cell>DANN [15]</cell><cell>66.5</cell><cell>81.8</cell><cell>89.0</cell><cell>79.8</cell><cell>63.5</cell><cell>88.7</cell><cell>78.2</cell></row><row><cell>RTN [11]</cell><cell>67.4</cell><cell>81.3</cell><cell>89.5</cell><cell>78.0</cell><cell>62.0</cell><cell>89.1</cell><cell>77.9</cell></row><row><cell>JAN [12]</cell><cell>67.2</cell><cell>82.8</cell><cell>91.3</cell><cell>80.0</cell><cell>63.5</cell><cell>91.0</cell><cell>79.3</cell></row><row><cell>MADA [45]</cell><cell>68.3</cell><cell>83.0</cell><cell>91.0</cell><cell>80.7</cell><cell>63.8</cell><cell>92.2</cell><cell>79.8</cell></row><row><cell>CAADA (Ours)</cell><cell>67.8</cell><cell>84.5</cell><cell>91.7</cell><cell>81.3</cell><cell>63.9</cell><cell>91.8</cell><cell>80.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Image classification accuracies for deep domain adaptation on the ImageCLEF-DA</figDesc><table><row><cell>Methods</cell><cell>A, W → D</cell><cell>A, D → W</cell><cell>D, W → A</cell><cell>Avg.</cell></row><row><cell>Undo-Bias [19]</cell><cell>98.45</cell><cell>93.38</cell><cell>42.43</cell><cell>78.08</cell></row><row><cell>UML [46]</cell><cell>98.76</cell><cell>93.76</cell><cell>41.65</cell><cell>78.05</cell></row><row><cell>L-SVM [47]</cell><cell>98.84</cell><cell>93.98</cell><cell>44.14</cell><cell>78.99</cell></row><row><cell>MTAE [20]</cell><cell>98.97</cell><cell>94.21</cell><cell>43.67</cell><cell>78.95</cell></row><row><cell>DSN [48]</cell><cell>99.02</cell><cell>94.45</cell><cell>43.98</cell><cell>79.15</cell></row><row><cell>DGLRC [22]</cell><cell>99.44</cell><cell>95.28</cell><cell>45.36</cell><cell>80.02</cell></row><row><cell>CAADG (Ours)</cell><cell>99.34</cell><cell>96.17</cell><cell>47.49</cell><cell>81.0</cell></row></table><note>dataset. We use the conventional protocol for unsupervised domain adaptation where source data are labeled, but target data are unlabeled. I → P indicates I (ImageNet) is the source domain and P (Pascal VOC) is the target domain.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Image classification accuracies for domain generalization on the Office31 dataset.</figDesc><table><row><cell>Methods</cell><cell>W, D, C → A</cell><cell>A, W, D → C</cell><cell>A, C → D, W</cell><cell>D, W → A, C</cell><cell>Avg.</cell></row><row><cell>Undo-Bias [19]</cell><cell>90.98</cell><cell>85.95</cell><cell>80.49</cell><cell>69.98</cell><cell>81.85</cell></row><row><cell>UML [46]</cell><cell>91.02</cell><cell>84.59</cell><cell>82.29</cell><cell>79.54</cell><cell>84.36</cell></row><row><cell>L-SVM [47]</cell><cell>91.87</cell><cell>86.38</cell><cell>84.59</cell><cell>81.17</cell><cell>86.00</cell></row><row><cell>MTAE [20]</cell><cell>93.13</cell><cell>86.15</cell><cell>85.35</cell><cell>80.52</cell><cell>86.28</cell></row><row><cell>DSN [48]</cell><cell>93.58</cell><cell>86.71</cell><cell>85.76</cell><cell>81.22</cell><cell>86.81</cell></row><row><cell>DGLRC [22]</cell><cell>94.21</cell><cell>87.63</cell><cell>86.32</cell><cell>82.24</cell><cell>87.60</cell></row><row><cell>CAADG (Ours)</cell><cell>95.74</cell><cell>88.91</cell><cell>86.79</cell><cell>82.11</cell><cell>88.39</cell></row></table><note>We use the conventional protocol for domain generalization where the target data is totally unseen during training. A, W → D indicates A, W are the source domains and D is the target domain.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Image classification accuracies for domain generalization on the Office-Caltech dataset. We use the conventional protocol for domain generalization where the target data is totally unseen during training. W, D, C → A indicates W, D, C are the source domains and A is the target domain.</figDesc><table><row><cell>Methods</cell><cell>P, C, S → A</cell><cell>P, S, A → C</cell><cell>S, A, C → P</cell><cell>P, C, A → S</cell><cell>Avg.</cell></row><row><cell>L-SVM [47]</cell><cell>41.80</cell><cell>52.30</cell><cell>55.15</cell><cell>47.87</cell><cell>49.28</cell></row><row><cell>KDA [49]</cell><cell>47.66</cell><cell>53.29</cell><cell>59.04</cell><cell>48.21</cell><cell>52.05</cell></row><row><cell>DICA [36]</cell><cell>47.46</cell><cell>57.00</cell><cell>55.93</cell><cell>40.70</cell><cell>50.27</cell></row><row><cell>MTAE [20]</cell><cell>45.95</cell><cell>51.11</cell><cell>58.44</cell><cell>49.25</cell><cell>51.19</cell></row><row><cell>DSN [48]</cell><cell>61.13</cell><cell>66.54</cell><cell>83.25</cell><cell>58.58</cell><cell>63.38</cell></row><row><cell>DBADG [21]</cell><cell>62.86</cell><cell>66.97</cell><cell>89.50</cell><cell>57.51</cell><cell>69.21</cell></row><row><cell>CIDDG [38]</cell><cell>62.70</cell><cell>69.73</cell><cell>78.65</cell><cell>64.45</cell><cell>68.88</cell></row><row><cell>CAADG (Ours)</cell><cell>65.52</cell><cell>69.90</cell><cell>89.16</cell><cell>63.37</cell><cell>71.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Image classification accuracies for domain generalization on the PACS dataset. We use the conventional protocol for domain generalization where the target data is totally unseen</figDesc><table /><note>during training. P, C, S → A indicates P, C, S are the source domains and A is the target domain.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on Office-31 dataset in the context of domain adaptation.</figDesc><table><row><cell>Methods</cell><cell>A, W → D</cell><cell>A, D → W</cell><cell>D, W → A</cell></row><row><cell>Correlation-alignment</cell><cell>95.25</cell><cell>93.15</cell><cell>45.29</cell></row><row><cell>Adversarial-learning</cell><cell>97.34</cell><cell>94.10</cell><cell>45.35</cell></row><row><cell>CAADG (Without Correlation alignment)</cell><cell>97.51</cell><cell>94.32</cell><cell>45.60</cell></row><row><cell>CAADG (With Correlation alignment)</cell><cell>99.34</cell><cell>96.17</cell><cell>47.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on Office-31 dataset in the context of domain generalization.</figDesc><table><row><cell cols="2">In this section, we perform ablation study on Office-31 dataset in the</cell></row><row><cell cols="2">context of both domain adaptation and domain generalization with different</cell></row><row><cell cols="2">components ablation, i.e., training with only adversarial learning, training</cell></row><row><cell>with only correlation alignment.</cell><cell>Those experiments shows different</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Ablation study to show the effect of using two discriminator and pre-trained source model on Office-31 dataset in the context of domain adaptation.</figDesc><table><row><cell>Methods</cell><cell>I → P</cell><cell>P → I</cell><cell>I → C</cell><cell>C → I</cell><cell>C → P</cell><cell>P → C</cell><cell>Avg.</cell></row><row><cell>CAADA (2 Discriminator)</cell><cell>66.3</cell><cell>82.1</cell><cell>89.6</cell><cell>79.6</cell><cell>63.9</cell><cell>89.3</cell><cell>78.47</cell></row><row><cell>CAADA (without pre-trained)</cell><cell>67.8</cell><cell>84.5</cell><cell>91.7</cell><cell>81.3</cell><cell>63.9</cell><cell>91.8</cell><cell>80.2</cell></row><row><cell>CAADA (pre-trained on source data)</cell><cell>68.2</cell><cell>84.9</cell><cell>92.4</cell><cell>81.4</cell><cell>64.7</cell><cell>92.6</cell><cell>80.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Ablation study to show the effect of using two discriminator and pre-trained source model on ImageCLEF-DA dataset in the context of domain adaptation.</figDesc><table><row><cell>Methods</cell><cell>P, C, S → A</cell><cell>P, S, A → C</cell><cell>S, A, C → P</cell><cell>P, C, A → S</cell><cell>Avg.</cell></row><row><cell>CAADG (2 Discriminator)</cell><cell>63.42</cell><cell>67.28</cell><cell>86.39</cell><cell>61.45</cell><cell>69.64</cell></row><row><cell>CAADG (without pre-trained)</cell><cell>65.52</cell><cell>69.90</cell><cell>89.16</cell><cell>63.37</cell><cell>71.98</cell></row><row><cell>CAADG (pre-trained on source data)</cell><cell>66.15</cell><cell>70.29</cell><cell>89.68</cell><cell>63.74</cell><cell>72.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Ablation study to show the effect of using two discriminator and pre-trained source model on PACS dataset in the context of domain generalization.</figDesc><table><row><cell>Methods</cell><cell>W, D, C → A</cell><cell>A, W, D → C</cell><cell>A, C → D, W</cell><cell>D, W → A, C</cell><cell>Avg.</cell></row><row><cell>CAADG (2 Discriminator)</cell><cell>93.81</cell><cell>85.98</cell><cell>86.09</cell><cell>81.36</cell><cell>86.81</cell></row><row><cell>CAADG (without pre-trained)</cell><cell>95.74</cell><cell>88.91</cell><cell>86.79</cell><cell>82.11</cell><cell>88.39</cell></row><row><cell>CAADG (pre-trained on source data)</cell><cell>96.09</cell><cell>89.12</cell><cell>87.35</cell><cell>82.67</cell><cell>88.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Ablation study to show the effect of using two discriminator and pre-trained source model on Office-Caltech dataset in the context of domain generalization .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://imageclef.org/2014/adaptation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Background priorbased salient object detection via deep reconstruction residual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Co-saliency detection via a self-paced multipleinstance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="865" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep domain generalization with structured low-rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Generative adversarial nets</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Central moment discrepancy (CMD) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hemanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shayok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sethuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Domain adaptation by mixture of alignments of second-or higher-order scatter tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Minimal-entropy correlation alignment for unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Pietro Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Cavazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<editor>D. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luxburg</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>I. Guyon, R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Duplex generative adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalizing from several related classification tasks to a new unlabeled sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multicomponent image translation for deep domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems (NIPS)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>ImageNet: A Large-Scale Hierarchical Image Database</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 IEEE Signal Processing Society Workshop</title>
		<meeting>the 1999 IEEE Signal Processing Society Workshop</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
