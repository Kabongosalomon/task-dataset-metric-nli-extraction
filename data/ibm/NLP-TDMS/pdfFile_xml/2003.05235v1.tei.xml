<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Channel Interaction Networks for Fine-Grained Image Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
							<email>xinhan@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
							<email>xunwang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
							<email>whuang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
							<email>mscott@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Channel Interaction Networks for Fine-Grained Image Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained image categorization is challenging due to the subtle inter-class differences. We posit that exploiting the rich relationships between channels can help capture such differences since different channels correspond to different semantics. In this paper, we propose a channel interaction network (CIN), which models the channel-wise interplay both within an image and across images. For a single image, a self-channel interaction (SCI) module is proposed to explore channel-wise correlation within the image. This allows the model to learn the complementary features from the correlated channels, yielding stronger fine-grained features. Furthermore, given an image pair, we introduce a contrastive channel interaction (CCI) module to model the cross-sample channel interaction with a metric learning framework, allowing the CIN to distinguish the subtle visual differences between images. Our model can be trained efficiently in an end-to-end fashion without the need of multi-stage training and testing. Finally, comprehensive experiments are conducted on three publicly available benchmarks, where the proposed method consistently outperforms the state-of-theart approaches, such as DFL-CNN(Wang, Morariu, and Davis 2018) and NTS(Yang et al. 2018).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Fine-grained image categorization has become an important topic in computer vision community with broad application prospects such as new retail <ref type="bibr" target="#b7">(Karlinsky et al. 2017)</ref>, automatic driving <ref type="bibr" target="#b11">(Sochor, Herout, and Havel 2016)</ref>, etc. Going beyond classical image classification that recognizes basiclevel categories, fine-grained categories are much more challenging to be identified due to the subtle inter-class differences, many of which can only be effectively distinguished by concentrating on discriminative local parts. For instance, to distinguish three bird species in <ref type="figure" target="#fig_0">Figure 1</ref>, a neural network usually focuses on their wings and heads.</p><p>Previous work tends to learn discriminative features by locating distinct parts <ref type="bibr" target="#b6">(Jaderberg et al. 2015;</ref> or modeling higher order information <ref type="bibr" target="#b10">(Lin, RoyChowdhury, and Maji 2015;</ref> Weilin Huang is the corresponding author. Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 2016; Kong and Fowlkes 2017; <ref type="bibr" target="#b19">Yu et al. 2018)</ref>, which have been proven to be effective for fine-grained image classification. In this paper, we rethink the way of learning discriminative features with convolutional networks, and propose a new channel interaction network (CIN). First, different channels often correspond to different visual patterns <ref type="bibr" target="#b18">(Yosinski et al. 2015)</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, most of the channels are semantically complementary to each other. Motivated by this observation, we aim to discover the complementary channel information for each individual channel, and then aggregate the complementary channels with the original ones. Such complementary information can cooperatively contribute to the referred channel, making the channel more discriminative. Consequently, we propose a self-channel interaction (SCI) network that explicitly models the relationships between various channels to discover such channel-wise complementary clues. Existing methods usually apply the channel/part interplay for direct classification <ref type="bibr" target="#b10">(Lin, RoyChowdhury, and Maji 2015;</ref><ref type="bibr" target="#b19">Yu et al. 2018)</ref> or attempt to mine the closely related cues <ref type="bibr" target="#b20">Yue et al. 2018)</ref>, where the channelwise complementary clues are not fully explored.</p><p>Second, people tend to distinguish two images by focus-ing on their specific distinctions. For instance, when we compare two images, A and B, in <ref type="figure" target="#fig_0">Figure 1</ref>, it is easy to identify the difference of wings between two images: the bird in A has black wings while there is a red dot on the wings of B. However, when images of A and C are compared, more attention should be paid to the regions of heads, i.e., enhancing the importance of channel 2 for image A and image C. To this end, we propose a novel contrastive channel interaction (CCI) mechanism between samples, with the goal of capturing the subtle differences between images. Metric learning is incorporated with our framework to model the crosssample channel interactions, which is neglected by most of the existing methods <ref type="bibr">(Wang, Morariu, and Davis 2018;</ref>). Finally, we jointly optimize the SCI module and the CCI module, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The network can be trained end-to-end in one stage, and thus is more lightweight than the two-stage methods like HS-Net <ref type="bibr" target="#b9">(Lam, Mahasseni, and Todorovic 2017)</ref>, DFL-CNN <ref type="bibr">(Wang, Morariu, and Davis 2018)</ref>, <ref type="bibr">NTS (Yang et al. 2018)</ref>, etc.. Our major contributions are summarized as: 1) We propose a self-channel interaction (SCI) module able to model the interplay between different channels within an image. This enables it to capture the channelwise complementary information for each channel, which enhances the discriminative features learned by each channel. This results in a lightweight model that can be trained more effectively in one stage. The new model is flexible, and can be seamlessly integrated into existing networks to boost the performance.</p><p>2) We propose a novel contrastive channel interaction (CCI) module to learn channel-wise relationships between images. CCI is able to dynamically identify the distinct regions from two compared images, allowing the model to focus on such distinctive regions for better categorization.</p><p>3) Finally, we evaluate our approach on three publicly available datasets: <ref type="bibr">CUB-200-2011</ref><ref type="bibr">(Wah et al. 2011</ref>), Stanford Cars ) and FGVC Aircraft <ref type="bibr" target="#b11">(Maji et al. 2013)</ref>, where our method achieves better performance over current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Fine-grained feature representations. Building powerful feature representations has been broadly studied for finegrained image categorization. Unlike using the first-order features directly for classification <ref type="bibr" target="#b21">(Zhang et al. 2014;</ref><ref type="bibr" target="#b17">Wei et al. 2018)</ref>, (Lin, RoyChowdhury, and Maji 2015) employ bilinear pooling with two independent CNNs, which take pairwise feature interactions into consideration by computing the second-order information, leading to performance improvements. To reduce the computation complexity, <ref type="bibr" target="#b1">(Gao et al. 2016;</ref><ref type="bibr" target="#b7">Kong and Fowlkes 2017)</ref> tend to use less feature dimensions, while (Cui et al. 2017) attempt to model higher-order information to improve the accuracy. <ref type="bibr" target="#b16">(Wang, Li, and Zhang 2017;</ref>) apply a matrix power normalization for computing bilinear features. <ref type="bibr" target="#b19">(Yu et al. 2018)</ref> further explore cross-layer bilinear pooling to compute multi-layer knowledge. Unlike these methods using second or higher order information for direct classification, we compute second-order statistics between different channels, which are used jointly with the original features to capture the channel-wise complementary information, resulting in stronger deep representations.</p><p>Visual attention. Visual attention, which has been introduced in various computer vision applications, can be employed to capture the subtle inter-class differences in fine-grained image categorization. For example, hard attention based methods, such as <ref type="bibr" target="#b6">(Jaderberg et al. 2015;</ref>, usually detect local regions and then crop them out from the original image. But the main limitation is that each cropped region requires an extra feedforward operation. Instead, soft attention methods ) can be regarded as imposing a soft mask on the feature maps, by only using a single feedforward stage. Self-attention was proposed and applied in machine translate in <ref type="bibr" target="#b14">(Vaswani et al. 2017)</ref>. It can be categorized into the soft attention. The non-local block, introduced in , is highly related to the self-attention module, but captures long-range dependencies in spacetime dimension in images and videos. <ref type="bibr" target="#b20">(Yue et al. 2018;</ref><ref type="bibr" target="#b23">Zheng et al. 2019</ref>) further explore the non-local like ideas in fine-grained classification.</p><p>In contrast to these self-attention based methods, we exploit the interactions between channels to discover the channel-wise complementary information rather than mining the closely related channels. Moreover, we further propose a contrastive channel interaction module to model cross-sample channel interactions.</p><p>Metric learning. Deep metric learning aims to learn a feature embedding for better measuring the similarities between image pairs, i.e., the distance of positive pairs are encouraged to be closer and the negative pairs are pushed away from each other. It has been widely used in various domains such as face verification <ref type="bibr" target="#b5">(Hu, Lu, and Tan 2014;</ref><ref type="bibr" target="#b11">Schroff, Kalenichenko, and Philbin 2015)</ref>, image retrieval <ref type="bibr" target="#b15">(Wang et al. 2014)</ref>, person re-id <ref type="bibr" target="#b14">Varior, Haloi, and Wang 2016)</ref>, etc. Compared with softmax loss used in conventional classification networks, metric learning can embed the samples into a low-dimensional space capturing high intra-class variance, which is more suitable for fine-grained image categorization <ref type="bibr" target="#b0">(Cui et al. 2016)</ref>. Recent work of MAMC ) adopts metric learning to compute the rich correlations between object parts, which inspired the current work. But our major differences lie in two aspects: 1) MAMC utilizes two attention branches to compute the features of two different part, while we model the interplay between different channels explicitly to extract the discriminative features; 2) a novel contrastive channel interaction module is proposed in our networks to emphasize the differences between contrastive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we present our proposed channel interaction network (CIN) for fine-grained image categorization, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Given an image pair, the two images are first processed by a shared backbone, e.g., ResNet-50 <ref type="bibr" target="#b4">(He et al. 2016)</ref>, generating a pair of convolutional fea-  </p><formula xml:id="formula_0">W AB = |W A − ηW B |, W BA = |W B − γW A |,</formula><p>η and γ are learned by an fc layer controlling the encoded information computed from the contrastive image for highlighting differences. CCI module will be removed and softmax loss will be replaced by a softmax layer during inference.</p><p>ture maps. To compute channel-wise complementary information for each channel on the feature maps, a self-channel interaction (SCI) module is designed to model the correlations between different channels. Then we aggregate the discriminate features from the original feature maps and the complementary information jointly. Finally, a contrastive channel interaction (CCI) module is designed with a contrastive loss to model the channel-wise relationships between two images. Compared with existing methods, our proposed CIN can be trained end-to-end in one stage, and also is readily applicable to other convolution neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Channel Interaction</head><p>Being aware of the rich knowledge encoded in the feature channels, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we would like to explore the interaction between various channels. Recent work <ref type="bibr" target="#b6">(Hu, Shen, and Sun 2017;</ref>) tends to highlight the most distinct feature channels. However, only focusing on the most discriminate channels might not fully explore the rich information from all channels. Indeed, most of the channels are complementary to each other. We attempt to compute the channel-wise relationships to extract such complementary clues, and then encode them into the original features for fine-grained classification. Thus we propose a simple yet effective self-channel interaction (SCI) module to achieve such ability, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Given an image I, let X ∈ R w×h×c denote the input feature maps processed by the backbone, where w, h and c indicate the height, width and the number of channels. We first reshape the input feature maps X to X ∈ R c×l , l = w × h. Then the output of SCI is computed as:</p><formula xml:id="formula_1">Y = W X ∈ R c×l ,<label>(1)</label></formula><p>where W ∈ R c×c denotes the SCI weight matrix, which can be computed as follows. Firstly, we perform a bilinear operation between X and X , obtaining a bilinear matrix, XX . Then we add a minus sign to it and exploit a softmax function to get the weight matrix:</p><formula xml:id="formula_2">W ij = exp(−XX ij ) c k=1 exp(−XX ik ) ,<label>(2)</label></formula><p>where c k=1 W ik = 1. It is worth noting that Y i (the i th channel of the resulting features Y ) is the computed interaction between X i and all the channels of X, i.e.,</p><formula xml:id="formula_3">Y i = W i1 X 1 + · · · + W ic X c .</formula><p>According to the definition of W , the channels with larger weights tend to be semantically complementary with X i , as illustrated in <ref type="figure">Figure 3</ref>. The referred channel X i focuses on the head part, thus the channels highlighting the complementary parts, like wings and feet, have larger weights, while the channel with head part emphasized has a smaller weight. As the resulting features Y may discard some information from the original features, we aggregate the discriminate features (Z) from both the generated features and the original ones:</p><formula xml:id="formula_4">Z = φ(Y ) + X,<label>(3)</label></formula><p>where φ denotes a 3 × 3 convolutional layer.</p><p>Discussions. It is worth noting that our SCI module can be formalized as the non-local like operation described in :</p><formula xml:id="formula_5">Y = f (X, X)g(X),<label>(4)</label></formula><p>where f (X, X) = sof tmax(−XX ) ∈ R c×c , and g(X) = X ∈ R c×l . Unlike the original non-local block considering the interactions in spatial dimension, our module ... referred channel high weight low weight all channels <ref type="figure">Figure 3</ref>: An example of the relationship between the referred channel with all the channels in SCI module.</p><p>focuses on channel dimension. More importantly, the nonlocal operation tends to exploit the positive correlations between spatial positions, while our SCI module focuses on the negative correlations, which enables our model to discover the semantically complementary channel information.</p><p>The non-local operation is similar to the SE-Inception module <ref type="bibr" target="#b6">(Hu, Shen, and Sun 2017)</ref>. They highlight the discriminative features but do not make full use of the complementary clues, which are better explored by our SCI module to enhance the channel-wise features, as shown in <ref type="figure">Figure 5</ref>. Our method is related to CGNL <ref type="bibr" target="#b20">(Yue et al. 2018</ref>) and TSAN <ref type="bibr" target="#b23">(Zheng et al. 2019</ref>) which also compute the channel correlations, but has clear distinctions on measuring the correlations: 1) CGNL and TSAN explore positive channel interaction while our CIN focuses on negative channel interaction; 2) CGNL takes spatial correlation into account, and further posits a low-rank Hadamard product; 3) In TSAN, the authors further proposed an adaptive image sampling mechanism to enhance the detailed information and applied knowledge distilling to extract the learned details. 4) Beside computing the channel-wise relationship within an image, a key distinction of our method is to further apply metric learning to model the channel interplay between samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Channel Interaction</head><p>SCI module is able to compute meaningful discriminate features. A straightforward approach is to directly feed the features for classification, e.g., by using a softmax classifier as the most popular choice. However, a vanilla classifier usually fails to capture the subtle differences present for finegrained classification <ref type="bibr" target="#b0">(Cui et al. 2016)</ref>. To mitigate this problem, MAMC  was recently proposed to enforce the correlations between different object parts. It introduces multi-attention multi-class constraints by using a metric learning technology, which inspired the current work. We employ deep metric learning to compute rich cross-sample channel-wise correlations by introducing contrastive constraints to the features enhanced by SCI.</p><p>To model this interaction between two images I A and I B , a natural idea is to impose the contrastive constraints on the features Z A and Z B enhanced by SCI, and then measure their similarity. However, traditional deep metric learning approaches project an image into a fixed point in the learned embedding space. As a result, such a general representation often fails to capture the subtle differences between two images. In contrast, we attempt to learn the interactions between two images in a dynamic manner where the channels are emphasized by comparing to the feature channels computed from the contrastive image.</p><p>Consequently, we propose a contrastive channel interaction (CCI) module to compute such relationships between two images. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, we argue that a simple subtraction operation between the SCI weight matrices of image I A and I B might extract such mutual information, and generate CCI weight matrices W AB and W BA :</p><formula xml:id="formula_6">W AB = |W A − ηW B |, W BA = |W B − γW A |,<label>(5)</label></formula><p>where η and γ are the weights learned by</p><formula xml:id="formula_7">[Y A , Y B ] and [Y B , Y A ] through a FC layer ψ, i.e., η = ψ([Y A , Y B ]), γ = ψ([Y B , Y A ])</formula><p>, and || denotes the absolute value. The two weights indicate the amount of correlated information considered dynamically by the image to better distinguish itself from the compared one. We use a subtraction operation to compute the interaction. We also tried other operations like addition, multiplication, or concatenation, with slightly lower performance obtained. By subtraction, the CCI weight matrices suppress the commonality and highlight the distinct channel relationships between the two images.</p><p>Then similar to SCI module, the CCI weight matrices W AB and W BA are applied to the features X A and X B as:</p><formula xml:id="formula_8">Z A = φ(Y A ) + X A , Z B = φ(Y B ) + X A ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">Y A = W AB X A and Y B = W BA X B .</formula><p>Finally, a contrastive loss <ref type="bibr" target="#b3">(Hadsell, Chopra, and LeCun 2006)</ref> is applied to the features computed by the CCI module which aims to push the samples of different classes away while pulling the positive image pairs close. Suppose each batch contains N image pairs, i.e., 2N images. The contrastive loss is defined as follows:</p><formula xml:id="formula_10">L cont = 1 N A,B (Z A , Z B ).<label>(7)</label></formula><p>Beyond the contrastive loss, a triplet loss (Schroff, Kalenichenko, and Philbin 2015) and other losses of metric learning can be used in our framework as well. The reason we choose the contrastive loss is that it is simple, and perform well in metric learning and face verification <ref type="bibr" target="#b12">(Taigman et al. 2014;</ref><ref type="bibr" target="#b3">Hadsell, Chopra, and LeCun 2006)</ref>. We also tried to use a triplet loss in CCI, but did not improve the performance. Specifically, is defined as follows:</p><formula xml:id="formula_11">= ||h(Z A ) − h(Z B )|| 2 , if y AB = 1 max(0, β − ||h(Z A ) − h(Z B )||) 2 , if y AB = 0</formula><p>(8) where β is a predefined margin and || · || denotes the Euclidean distance, h is a fully-connected layer projecting features into an r-dimension space, i.e. H(Z) ∈ R r . r is set to 512 in our experiments. Here, y AB indicates whether the label of an image pair is the same or not, i.e., y AB = 1 denotes image I A and image I B come from the same class, while y AB = 0 means a negative pair.</p><p>Moreover, we use a softmax loss for classification based on the predictions that are generated by the features Z using SCI. We denote the softmax loss as L sof t . The total loss L total of our framework is defined as follows:</p><formula xml:id="formula_12">L total = L sof t + α · L cont ,<label>(9)</label></formula><p>where α is a hyper-parameter. We use the stochastic gradient method to optimize L total . Note that only SCI module is used in inference, with only a single image required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We report the experimental results, and compare our method with the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Baselines</head><p>Datasets. We employ three publicly available datasets in our experiments: <ref type="formula" target="#formula_1">(1)</ref>  Notice that we do not compare our method with the approaches which require additional information, such as SJS <ref type="bibr" target="#b2">(Ge and Yu 2017)</ref>, HS-Net <ref type="bibr" target="#b9">(Lam, Mahasseni, and Todorovic 2017)</ref>, and HSE (Chen and others 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>In all our experiments, we use ResNet-50 and ResNet-101 as our base networks. We remove the last pooling layer and fully-connected layer, and then fine-tune the networks pretrained on ImageNet <ref type="figure">(Russakovsky et al. )</ref>. The input image size is 448 × 448 as most state-of-the-art fine-grained categorization approaches. By following that of NTS (Yang et al. 2018), we implement data augmentation including random cropping and horizontal flipping during training. Only center cropping is involved in inference.</p><p>The model is trained for 100 epochs with SGD for all datasets, and the base learning rate is set to 0.001, which annealed by 0.5 every 20 epochs. we use a batch size of 20 and ensure that each batch contains 4 categories with 5 images in each category. And then, we randomly split these 20 images into 10 image pairs. We have tried to use all the O(n 2 ) pairs or apply hard negative mining, which hurt the performance and consume more memory. The weight decay is set to 2 × 10 −4 . β in Equation 8 is set to 0.5 empirically. and α in Equation 9 is set to 2.0. Top-1 accuracy is used as the evaluation metric. We use PyTorch to implement our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Analysis</head><p>We conduct ablation studies in order to better understand the impact of each component to our approach. The performance and efficiency are compared in in <ref type="table">Table 1</ref>. We use ResNet-50 and ResNet-101 <ref type="bibr" target="#b4">(He et al. 2016</ref>) as our backbone. SCI Module. SCI mines complementary channels through exploring channel interactions, contributing to learning more discriminative features. As illustrated in Table 1, compared with ResNet-50 alone (84.9%), by merely adding the SCI module, ResNet-50 + SCI obtains a performance improvement of 2.2%. Moreover, switching the interaction module from SCI to SE module leads to a significant performance drop (87.1% vs. 85.7%). SE module only focuses on the most discriminative features and ignores others, while our SCI module utilizes the complementary channel knowledge to enhance all the features. Compared to the Non-local block and ResNet-50+Pos-SCI (SCI weight matrix W without the negative sign) which model the positive space and channel-wise information respectively, our SCI module obtains better performance. Notice that our SCI also outperforms CGNL <ref type="bibr" target="#b20">(Yue et al. 2018</ref>) (87.0%) which models the correlations between the positions of all channels. Indeed, the channel information explored in our SCI module is involved in CGNL as well. The major difference about the channel information lies in that our SCI exploits the negative interplay to find the channel-wise complementary information, while CGNL does not fully explore such information and computes the positive interaction to capture the closely related clues. These results demonstrate that: 1) for fine-grained image classification, the information contained Method 1-Stage Acc(CUB) Acc(FGVC) Acc(Stanford Cars) MAMC   in the channel dimension is as powerful as complicated modeling across all dimensions; 2) finding the complementary channel clues can take full advantage of the channel interaction comparing with discovering the closely related channel information. CCI Module. We further investigate the effectiveness of the proposed CCI module. <ref type="table">Table 1</ref> shows that the CCI module (ResNet-50+SCI+CCI) provides 0.4% performance improvement compared to the method without a contrastive loss (ResNet-50+SCI). To further demonstrate the characteristics of the contrastive channel attention module, we consider the approach (ResNet-50+SCI+Cont) which explicitly applies a contrastive loss to the features computed by SCI module, i.e., η = 0 and γ = 0 in Equation 5. As presented in <ref type="table">Table 1</ref>, ResNet-50+SCI+Cont obtains a limited improvement with ResNet-50+SCI (87.2% vs. 87.1%). The reason might be that the common contrastive loss uses the same features of an image compared to any other image, which might reduce it is ability to focus on the distinct differences between two images, while our CCI module is capable of highlighting of the different regions. The results confirm that our CCI module has strong capability for modeling the relationship between two images.</p><p>Time cost. We report our inference time on a Nvidia TI-TAN XP GPU with PyTorch implementation. As shown in <ref type="table">Table 1</ref>, CIN introduces an overhead that is much smaller than that of two-stage methods (ResNet-50+NTS), and is comparable to the other one-stage approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-art</head><p>In this section, we compare our proposed network (CIN) with the state-of-the-art methods on the three publicly available datasets.</p><p>CUB-200-2011.   <ref type="table" target="#tab_3">Table 2</ref> reports the performance on FGVC Aircraft dataset. DFL-CNN achieves the highest accuracy of 92.0%, outperforming NTS with 91.4%. Our method has a clearly higher accuracy than existing methods even with the backbone ResNet-50. The excellent results further confirm the superiority of our method. It is worth noting that the accuracy on FGVC Aircraft is generally higher than that of CUB-200-2011, because images of CUB-200-2011 contain much more label noises (4.4% as reported in <ref type="bibr">(Van Horn et al. )</ref>) and class-irrelevant background, while images in FGVC Aircraft have relatively clean background, and airplanes often occupy a large portion of the image.</p><p>Stanford Cars. To verify the generalization ability of the proposed method. We further evaluate it on another realworld dataset, the Stanford Cars. <ref type="table" target="#tab_3">Table 2</ref> presents the performance of our method with the state-of-arts. Generally, the results are consistent with those of the previous two datasets. Again, the proposed CIN can achieve the highest accuracy compared with the state-of-arts. Combined with NTS. Furthermore, our module is general and flexible, and it can be readily integrated into other framework to improve the performance. In this experiment, we combine our module with the latest state-of-theart method NTS , which is a two-stage framework by leveraging a region proposal networks to localize discriminative parts with weakly-supervised learning. We integrate the SCI module at the end of the feature extractor networks. As NTS will discover multiple regions out of sequence, thus we only apply the CCI to the whole feature stream. <ref type="table" target="#tab_5">Table 3</ref> shows the performance of our method combined with NTS (NTS+CIN). As can be found, NTS+CIN achieves consistent performance improvements on all the three publicly available datasets compared with either NTS or CIN alone. The results further demonstrate the strong capability of our module. We expect that our CIN network can improve the performance on various computer vision tasks when simply plugged into existing framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Visualization</head><p>To better understand the intra-and inter-image channel interactions modeled by CIN, we visualize the channel correlations and neural activations in our SCI and CCI module. <ref type="figure" target="#fig_3">Figure 4</ref> shows the visualization of SCI module for images from three different datasets. Column 1 presents the activations of a randomly selected channel (assuming it is the i th channel) before SCI. Column 2 to Column 4 are the three most complementary channels to it. In other words, these  <ref type="figure">Figure 5</ref>: Visualization on the results of CCI module on CUB. (a) the original images; (b) the feature maps by CCI. It can be seen that different regions are highlighted conditioned on different image pairs. three channels correspond to the ones that have the largest values in the i th row of SCI matrix W defined in Equation 7. The last column represents Y i , which is the i th channel after SCI. We find that, for a referred channel, the top-3 complementary channels tend to capture different semantic. For instance, in the first example of <ref type="figure" target="#fig_3">Figure 4</ref>, the referred channel has a strong activation around wings, and its complementary channels focus more on head and tail regions. As a result, the attention feature channels are enhanced by this complementary information and activates also on other discriminative parts. Note that after our SCI module, the activations span most of the object parts, which indicates that SCI effectively models the interactions among different channels, and combine their complementary but discriminative parts to produce more informative features. <ref type="figure">Figure 5</ref> visualizes the results of our CCI module on CUB-200-2011 dataset. Line 2 shows the contrastive attention activations by averaging all feature maps after CCI across channels. "Salty Black Gull" and "Ivory Cull" have similar heads, and their features after CCI have weaker responses to the head. While comparing with "Fish Crow", the activations near the head becomes stronger. For the other two bird species, their appearance differences are huge and the CCI module provides strong responses to the whole body part. This result suggests that our proposal CCI module can focus on the key distinctions by modeling the interactions of channels between image pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have presented a new channel interaction network (CIN) for fine-grained image categorization. Our network first learns complementary channel information by a self-channel interaction (SCI) module taking the relationships between channels into account. It encourages to pull positive pairs closer while pushing negative pairs away via a contrastive channel interaction (CCI) module, which exploits channel correlations between samples. The proposed network can be trained end-to-end in one stage requiring no bounding box/part annotations. Extensive experiments demonstrate that CIN can achieve superior performance compared to the state-of-the-art approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Channel activations computed by our method (from the conv5 3 layer of ResNet50 trained on CUB-200-2011 dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>CUB-200-2011 (Wah et al. 2011 images from 200 wild bird species, (2) Stanford Cars including 16,185 images over 196 classes, and (3) FGVC Aircraft<ref type="bibr" target="#b11">(Maji et al. 2013)</ref> containing 196 classes about 10,000 images. Baselines. In the experiments, we compare our CIN with 10 methods described as follows. The first four methods can be trained in one stage. (1) MAMC: applying multi-attention multi-class constraints to enforce the correlations among different parts of objects.(2) CGNL (Yue et al. 2018): capturing the dependencies between positions across channels by non-local operation to classify. (3) HBP (Yu et al. 2018): hierarchical bilinear pooling framework integrating multiple cross-layer bilinear features. (4) iSQRT-COV (Yu et al. 2018): using an iterative matrix square root normalization to do covariance pooling. (5) RA-CNN (Fu, Zheng, and Mei 2017): recursively learning discriminative region attention and region-based feature representation at multiple scales. (6) Boost-CNN (Moghimi et al. 2016): a new boosting strategy to assemble weak classifiers for better performance. (7) DT-RAM (Li et al. 2017): a dynamic computational time model with reinforcement learning for recurrent visual attention. (8) MA-CNN (Zheng et al. 2017): multi-attention convolutional network including convolution, channel grouping and part classification sub-networks. (9) DFL-CNN (Wang, Morariu, and Davis 2018): capturing class-specific discriminative patches by learning a bank of convolutional filters. The performance might be unstable due to the complex layer initialization using k-means. (10) NTS (Yang et al. 2018): effectively localizing informative regions with self-supervision mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of channel activations before and after SCI moudle on CUB, Cars and Aircraft.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison results on CUB-200-2011, FGVC Aircraft and Stanford Cars.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 2presents the classification results of CIN and the state-of-the-art methods. First, the accuracy of our proposed CIN is higher than all existing methods.</figDesc><table><row><cell>Dataset</cell><cell>CIN</cell><cell>NTS</cell><cell>NTS+CIN</cell></row><row><cell cols="3">CUB-200-2011 87.5% 87.5%</cell><cell>88.3%</cell></row><row><cell cols="3">FGVC Aircraft 92.6% 91.4%</cell><cell>93.3%</cell></row><row><cell>Stanford Cars</cell><cell cols="2">94.1% 93.9%</cell><cell>94.4%</cell></row><row><cell>Even with ResNet-50, our method achieves comparable re-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>sult with NTS (Yang et al. 2018). However, NTS requires</cell><cell></cell><cell></cell><cell></cell></row><row><cell>multiple stages for learning discriminative regions, result-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ing in more expensive cost on both time and space. Com-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pared with the best one-stage method iSQRT-COV (8k) (Li</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Combined our network with NTS.</figDesc><table><row><cell>et al. 2018), our method outperforms it by 0.2%. Note that</cell></row><row><cell>the feature dimension of our method (2k) is significantly</cell></row><row><cell>lower than iSQRT-COV (8k). Moreover, our method im-</cell></row><row><cell>proves HBP (Yu et al. 2018) by 1.0%. The reason might</cell></row><row><cell>be that HBP ignores the interaction between samples. It is</cell></row><row><cell>notable that the backbone of HBP is VGG (Simonyan and</cell></row><row><cell>Zisserman 2014), while the accuracy of CIN with the same</cell></row><row><cell>backbone is 85.6%. We have tried to implement HBP and</cell></row><row><cell>found it does not work well with ResNet. DFL-CNN (Wang,</cell></row><row><cell>Morariu, and Davis 2018) achieves the best results on CUB</cell></row><row><cell>(87.4%) with ResNet-50 backbone while ResNet-50+CIN</cell></row><row><cell>achieves a higher accuracy with only one stage. As shown</cell></row><row><cell>in (Wang, Morariu, and Davis 2018), ResNet does not al-</cell></row><row><cell>ways outperform VGG.</cell></row><row><cell>FGVC Aircraft.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-grained categorization and dataset bootstrapping using deep metric learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04505</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei ;</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chopra</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><forename type="middle">;</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Squeeze-and-excitation networks. arXiv preprint</note>
	<note>Jaderberg et al. 2015</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fine-grained recognition of thousands of object categories with single-example training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Karlinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fine-grained recognition as hsnet search for informative image parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahasseni</forename><surname>Todorovic ; Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<idno type="arXiv">arXiv:1703.10332</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<editor>CVPR. [Lin, RoyChowdhury, and Maji</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Bilinear cnn models for finegrained visual recognition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<idno>arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Boxcars: 3d boxes as CNN input for improved fine-grained vehicle recognition</title>
		<meeting><address><addrLine>Philbin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haloi</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><forename type="middle">;</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<ptr target="Thecaltech-ucsdbirds-200-2011dataset" />
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Wah, C</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>NIPS. Wah et al. 2011</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<idno type="arXiv">arXiv:1711.07971</idno>
	</analytic>
	<monogr>
		<title level="m">Non-local neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">G2denet: Global gaussian distribution embedding network and its application to visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Morariu, V. I.</editor>
		<editor>and Davis, L. S</editor>
		<meeting><address><addrLine>Wang, Morariu, and Davis; Wang, Y</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mask-cnn: Localizing parts and selecting descriptors for fine-grained bird species categorization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00287</idno>
		<idno>arXiv:1506.06579</idno>
	</analytic>
	<monogr>
		<title level="m">Understanding neural networks through deep visualization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
