<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RectiNet-v2: A stacked network architecture for document image dewarping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hmrishav</forename><surname>Bandyopadhyay</surname></persName>
							<email>hmrishavbandyopadhyay@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Dasgupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nibaran</forename><surname>Das</surname></persName>
							<email>†nibaran.das@jadavpuruniversity.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mita</forename><surname>Nasipuri</surname></persName>
							<email>mitanasipuri@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electronics and Telecomm. Engg</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<addrLine>West Bengal</addrLine>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science and Engg</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<addrLine>West Bengal</addrLine>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RectiNet-v2: A stacked network architecture for document image dewarping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Document image dewarping</term>
					<term>warped document image rectification</term>
					<term>dense grid prediction</term>
					<term>stacked u-net</term>
					<term>gated networks</term>
					<term>residual networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Photographing a document with the help of a camera is the most popular method of storing it. With the large scale popularization of mobile devices with inbuilt camera and storage functionalities, capturing document images has been the norm of storing data. These captures, however, are done casually more than often, resulting in distorted and warped images that can be interpreted by humans only, but not by document recognition systems due to large differences in illumination, placement and condition of the documents. For machines to understand data contained in captured document images, dewarping of such images is a necessity.</p><p>A large number of classical image processing and optimization based methods have been proposed for dewarping document images. These however, fail when curves and folds occur simultaneously in document images, which require a more indepth and varied analysis. To rectify these complex document images, deep learning methods have been introduced recently by <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b3">[4]</ref>. These deep learning methods treat the problem of document dewarping as the prediction of a dense grid that can aid in the dewarping process. The dense grid based approach for dewarping images is preferred to the sparse grid based method as it can effectively capture very fine distortion that a very limited set of dewarping points or a sparse grid cannot. As a result, deep learning methods for document dewarping have been able to dewarp images of a complex nature with significantly higher precision as compared to their image processing counterparts.</p><p>In our model, similar to <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b3">[4]</ref>, we use a DNN architecture to predict a dense-grid that can dewarp a document image fed to it. Additionally, we make use of a bifurcated and gated network architecture to predict dense grids from warped document images. More specifically, our contributions can be summed up as :</p><p>• Use of a bifurcated network that takes in images of dimension 256x256 and regresses a dense grid that can unwarp the document represented by the image. This unwarping grid can be interpolated later so that the images are dewarped at their original resolution. The bifurcated network allows us to prevent intermingling of dense-grid values. • Use of Residual blocks in the skip connections of the U-Nets used in the stacked module. The use of residual blocks as proposed by <ref type="bibr" target="#b4">[5]</ref> enables us to leverage different receptive fields in the skip connections and allows us to pass on information from various levels to the decoder layers. • Use of Gated Convolutional Layers in the model architecture, inspired by <ref type="bibr" target="#b5">[6]</ref>. The presence of gates in these layers helps to capture edge and line level data and pass it on in later layers as information which the model has to focus on. In other words, the GCN (Gated Convolutional Network) acts as an attention module to the Secondary U-Net. • Use of a Boundary Weighted mean squared loss function that focuses more on the boundary of the dense-grids predicted by a Secondary U-Net. This ensures that poor detection of boundaries by the network is penalized more, and unwarps obtained from the module contain minimal background data of the document image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PREVIOUS WORKS</head><p>In the past several years, we have seen significant progress in the domain of document image dewarping. The methods proposed in past can be summarized briefly into the following categories:</p><p>1) Image Processing based methods 2) Deep Learning based methods arXiv:2102.01120v1 [cs.CV] 1 Feb 2021</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Processing Methods</head><p>A plethora of image processing based methods have been proposed and studies have been done on both single and multi image tasks in the field of document dewarping. Efforts have been made to reconstruct 3D views of documents both with the help of additional hardware and with the help of Image Processing Algorithms.</p><p>Vision systems have been designed that make use of well calibrated stereo cameras and structured laser light sources to capture a 3 dimensional perspective of the document image. <ref type="bibr" target="#b6">[7]</ref> used a 3D scanning system to create a 3D mesh which is then mapped onto a 2D plane with the help of conformal mapping. <ref type="bibr" target="#b7">[8]</ref> made use of structured laser beams for acquiring shape features from warped document images which were later used to generate dewarps of these documents. Further methods utilizing multiple images included <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref>. While <ref type="bibr" target="#b8">[9]</ref> matched feature points to register identical areas from images capturing different viewpoints and used that to model the 3D structure, <ref type="bibr" target="#b9">[10]</ref> recovered 3D point clouds from multiple images and used a modified conformal mapping to perform dewarps. Although many of these systems are able to procure results of significant quality, their application is very severely bound due to the limitation posed by additional hardware. Multi view system which don't need structured laser beams can do without additional hardware, but still need more than one image, which is hardly available for casually captured documents.</p><p>A different approach to estimating and generating 3D views of documents by <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref> include extracting shape and structural information from illumination effects in document images. These methods of dewarping images, although free from additional hardware, generally perform very poorly as the estimation step requires primary attention on illumination effect or shading for 3D modeling, which is prone to errors. This makes it highly unsuitable for performing dewarps of natural images.</p><p>Apart from focusing on reconstructing 3D views of documents, many Image Processing based methods made use of line based features to recognize warps and dewarp images on the basis of that. A coarse to fine technique was proposed by <ref type="bibr" target="#b12">[13]</ref> that performed detection of word and text lines in coarse scales and used pose normalization in the finer stages to create an unwarp of the document. <ref type="bibr" target="#b13">[14]</ref> proposed a global optimization based dewarping technique that recognized and converted warped lines to make them parallel, thus restoring documents from non linearly warped images. <ref type="bibr" target="#b14">[15]</ref> simultaneously made use of text line structure and character strokes to estimate a distortion grid and then performed unwarps on document images using this grid.</p><p>More sophisticated methods of text line detection involved segmentation and iterative procedures. A segmentation based approach for the detection of warps from text line segments was proposed by <ref type="bibr" target="#b15">[16]</ref>. Further methods involving segmentation were proposed by <ref type="bibr" target="#b16">[17]</ref>. Iterative methods demonstrated in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref> were significantly better than other image processing methods as they could dewarp images repetitively.</p><formula xml:id="formula_0">(a) (b) (c) (d) Fig. 1.</formula><p>Simulated dataset proposed in <ref type="bibr" target="#b2">[3]</ref> These methods made iterative checks while aligning text lines and would perform small dewarps at each iteration, getting an overall better result. The drawback with these iterative techniques was that they were too slow and would take up too much time to dewarp a single document image, making them impractical in most scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning based methods</head><p>Application of Deep learning based methods have been relatively low in this domain for quite a long period of time due to the absence of sufficient data for training DNN models. One of the first CNN based method was proposed by <ref type="bibr" target="#b19">[20]</ref> where CNNs were used only to detect paper creases for further stages of processing. The first end-to-end CNN model for dewarping document images was proposed by <ref type="bibr" target="#b1">[2]</ref> in DocUNet. DocUNet got rid of the requirement of a large scale dataset by synthetically warping scanned images and using them to train the model. The end-to-end network proposed in DocUNet consisted of a stacked U-Net architecture as the backbone. The method of data generation proposed by <ref type="bibr" target="#b1">[2]</ref> was used by <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b0">[1]</ref> in their networks.</p><p>A similar approach of data generation was proposed by <ref type="bibr" target="#b2">[3]</ref> where focus was given primarily on illumination and shape effects to make the generated data more realistic and to prevent unexpected result when testing on natural images. <ref type="bibr" target="#b2">[3]</ref> also came up with an end-to-end stacked CNN architecture to dewarp document images that surpassed <ref type="bibr" target="#b1">[2]</ref> in dewarping performance by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET</head><p>We make use of the data generation proposed by <ref type="bibr" target="#b2">[3]</ref> as their data is significantly more realistic and offers better generalization with natural images as compared to <ref type="bibr" target="#b1">[2]</ref>. In the  <ref type="figure">Fig. 3</ref>. Primary U-Net dataset proposed by <ref type="bibr" target="#b2">[3]</ref> 3D shapes and textures of naturally deformed documents were captured and rendered on images with the help of path tracing, taking in many camera positions and a variety of illumination effects and conditions. This allowed the creation of a large scale image dataset with the data being highly realistic, as the illumination and shape effects have been taken from real document images. This not only helps the model to generalize better when used on natural images, but also makes available various forms of groundtruth data including albedo maps, normal maps, depth maps, UV maps, and checkerboard fits, which can be used to further analyze the document structure. A general representation of the dataset by some of its images is available in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Overview</head><p>The overall architecture of our method has been expressed in <ref type="figure" target="#fig_0">Fig 2.</ref> We have made significant changes in the stacked U-Net architecture originally proposed by <ref type="bibr" target="#b1">[2]</ref>. The major changes lie in the addition of a gated convolutional network for proper processing of line level information and a bifurcation in the secondary U-Net of the stack. Inspired by <ref type="bibr" target="#b4">[5]</ref>, we also add residual networks in the skip connections of our model to enhance the features being concatenated in the later stages of our network.</p><p>The network, as in <ref type="figure" target="#fig_0">Fig 2,</ref> takes in images in the form of batches D n ∈ R 256×256×3 and predicts dense-grids in the form of D n ∈ R 256×256×2 where the 2 channels represent the coordinate axes we use for mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Primary U-Net</head><p>The first U-Net of our architecture, aka the primary U-Net is the main block to which we feed the image data. It consists of a series of up-sampling and down-sampling modules with skip connections powered by res-pathways.</p><p>The Primary U-Net module as in <ref type="figure">Fig. 3</ref> is fed with images of deformed documents D n ∈ R 256×256×3 which </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Residual Path:</head><p>One of the most ingenious parts of the U-Net architecture is the use of skip-connections. Skip connections in a U-Net help it to retrieve information lost in pooling layers. Inspired by <ref type="bibr" target="#b4">[5]</ref>, we make attempts to enhance the information carried by skip connections in a U-Net to the decoder layers by passing it through a Residual Path (res path). The addition of these pathways as shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, enable the decoder to get more spatial information, which allows the network to deal with images at various scales. The three layers of the res path have convolutions that have a receptive field of 3 × 3, 5 × 5, and 7 × 7 respectively. The concatenation of data processed through various receptive fields enables us to pass on more spatial information to the decoder than that would have been possible otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Gated Convolutional Network:</head><p>The Gated Network as in <ref type="figure">Fig. 5</ref> works on data extracted in the layers before the 2nd, 4th and 5th poolings. It is built up from Gated Convolutional Layers (GCLs) as proposed in <ref type="bibr" target="#b5">[6]</ref>.The presence of gates in the network architecture helps us to segregate line level data from the image, which is later fed as attention information to the Secondary U-Net. This helps the secondary U-Net to focus more on textual regions and perform better dewarps of document images. We train the Primary U-Net along with the Gated Network on the edge loss which is calculated as the binary cross entropy between the output from the GCN and a canny edge filter of the input. The mathematical form of the edge loss can be expressed as :</p><formula xml:id="formula_1">L edge = − 1 N N i=0 y i .log(ŷ i ) + (1 − y i ).log(1 −ŷ i )<label>(1)</label></formula><p>Whereŷ i represents pixel wise value of the predicted output and y i gives the ground truth measure. Convolutional networks fail badly when dealing with coordinate data as shown by <ref type="bibr" target="#b20">[21]</ref>. To deal with this issue better, <ref type="bibr" target="#b2">[3]</ref> makes use of the coord-conv module as suggested in <ref type="bibr" target="#b20">[21]</ref>. However, we find that a more task specific network can be designed which can enhance the ability of CNNs while working with dense grid predictions of 2D document images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Secondary U-Net</head><p>The general CNN works by summing up computed data across all input channels for specific window sizes. Ultimately the number of channels in the output is the number of filters that the convolutional block contains. During the summation, the CNN merges data from multiple channels together into vectors and then uses the merged data in the later stages to predict Dense-grids from images. Dense-grids, which are expressed as a set of 2D arrays containing coordinate points from the X and the Y axes of the image, however, don't have much inter-relationship at the channel level and mixing up of the channels at each stage of the network, like a single decoder would have done, doesn't work well. In other words, using a single decoder in the final U-Net block would mean that although information is extracted in all blocks, it is merged together at each layer and only the last two convolutional filters would be responsible to decode or separate the grid values into their respective channels for the final output. To get over this issue, we have come up with the usage of multiple decoder blocks for the single secondary U-Net encoder, so that channels in the dense grid output are developed separately.</p><p>The output of the Primary U-Net U 1 ∈ R 64×256×256 is fed as input to the secondary U-Net as in <ref type="figure" target="#fig_2">Fig. 6</ref>. The Bottleneck B ∈ R 1024×8×8 is split into B 1 ∈ R 512×8×8 and B 2 ∈ R 512×8×8 . These blocks go through the decoders to give outputs O 1 ∈ R 256×256×1 and O 2 ∈ R 256×256×1 , which are concatenated and normalized by a Tanh activation function to get the final grid g ∈ R 256×256×2 .</p><p>The Secondary U-Net is trained on the grid loss, a weighted Mean Squared Loss between the dense-grid predicted by the network and the ground truth. The weights of the mean squared loss are built in such a way that a wrong prediction of the boundary and the immediate surroundings of the boundary is penalized more as compared to possible errors in the interior. This is done with an intent of making the network focus on the boundaries of documents in the images it works with. The grid loss is used to train the entire network while the edge loss acts as a supportive loss specifically for the Primary U-Net. Mathematically, we can express the grid loss as:</p><formula xml:id="formula_2">L grid = W · 1 N N i=0 (g i −ĝ i ) 2<label>(2)</label></formula><p>with W being the boundary-weight in the loss function.</p><p>The combined loss function can be expressed as:</p><formula xml:id="formula_3">L = L grid (S(P (z i )),ĝ i ) + λL edge (G(z i ),ŷ i )<label>(3)</label></formula><p>Where the Gated Convolutional Network is expressed as G and the Primary and the Secondary U-Nets are expressed as P and S respectively. We take the value of λ as 0.9 for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Post-Processing</head><p>We further post-process the dewarped document images as part of our dewarping module with the help of bilateral filtering which smoothens folds and crumples. As a variation to post-processing by <ref type="bibr" target="#b0">[1]</ref> which leaves textual regions blurred in some images, we maintain the level of filtering used by monitoring the blur before and after the filter by computing the variance of a Laplacian filter on the image. V. EXPERIMENTS For comparison of the dewarp quality of our methods, metrics like MS-SSIM (Multi Scale Structural Similarity Index), SSIM (Structural Similarity Index), and LD(Local Distortion) are used. Following <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref>, all our results have been presented on the DocUNet benchmark. For the calculation of MS-SSIM and LD, our images have been scaled to approximately 598,400 sq. pixel areas (880x680), while we use original images for the calculation of SSIM along the levels to gauge the quality of dewarp at various levels in <ref type="table" target="#tab_0">Table II</ref>. The MS-SSIM values for our methods at the original resolution and comparison with RectiNet by <ref type="bibr" target="#b0">[1]</ref> can be found in the supplementary. We tabulate the results of our experiments on the benchmark both before and after the post-processing step in <ref type="table" target="#tab_0">Table I</ref>. A subset of the outcomes of our method is displayed in <ref type="figure">Fig. 7</ref> along with the input and corresponding scanned ground truth.</p><p>From <ref type="table" target="#tab_0">Table II</ref> and <ref type="table" target="#tab_0">Table I</ref>, we infer that our method outperforms methods proposed by <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b3">[4]</ref> in MS-SSIM while outperforming the method proposed by <ref type="bibr" target="#b2">[3]</ref> in SSIM measured at original resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we have proposed a method to dewarp document images by recognizing their structure and predicting dense-grids for mappings. We further demonstrate the effectiveness of a bifurcation of the traditional U-Net and the addition of a gated module and residual pathways by comparing our method with state-of-the-art methods on the DocUNet benchmark. However, we find that the performance of our method is not satisfactory in certain respects. In-spite of carefully set thresholds in the post-processing setup proposed by us, we observe a few images get blurred after post-processing due to contrast issues. We further observe the architecture proposed by us fails to dewarp document images which are not cropped to exact fit, demonstrating the necessity of a localization procedure for the same.</p><p>We find that MS-SSIM as a metric does not provide as much attention to line level detail as it does to overall image structure, texture etc. The area dependency of MS-SSIM and LD also causes them to give highly varied results for the same distortion level in images of different areas. Thus, future work on an area independent standardized metric is highly necessary for proper evaluation of results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Complete Architecture Gated Network C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Residual Path Fig. 5. Gated Convolutional Network gets passed through a series of encoders and to form the bottleneck B R 1024×8×8 . The layers L 2 R 64×128×128 , L 4 R 256×32×32 , L 5 R 512×16×16 are extracted from these encoders and passed to the gated module G. Finally, the decoded outputs O R 16×256×256 , gated network outputs G o R 16×256×256 and the initial convolution outputs X R 32×256×256 are concatenated to produce the output of the Primary U-Net U 1 R 64×256×256 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Secondary U-Net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I MS</head><label>I</label><figDesc>-SSIM AND LD VALUES ON COMPARISON WITH OTHER METHODS</figDesc><table><row><cell>Fig. 7. Rows from top to bottom: Cropped Images; Results with our methods; Ground Truth</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>MS-SSIM ↑</cell><cell>LD↓</cell></row><row><cell>[15]</cell><cell>0.13</cell><cell>33.69</cell></row><row><cell>DocUNet, [2]</cell><cell>0.4103</cell><cell>14.08</cell></row><row><cell>AGUN, [4]</cell><cell>0.4491</cell><cell>12.06</cell></row><row><cell>Output from Network</cell><cell>0.4437</cell><cell>10.71</cell></row><row><cell>Post-Processed Output</cell><cell>0.4500</cell><cell>10.40</cell></row><row><cell>DewarpNet, [3]</cell><cell>0.4658</cell><cell>8.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II SSIM</head><label>II</label><figDesc>VALUES ON VARYING LEVELS</figDesc><table><row><cell>Level</cell><cell>Our Method</cell><cell>RectiNet</cell><cell>DewarpNet</cell></row><row><cell>Original</cell><cell>0.507434</cell><cell>0.548736</cell><cell>0.493146</cell></row><row><cell>2</cell><cell>0.455497</cell><cell>0.482410</cell><cell>0.445168</cell></row><row><cell>3</cell><cell>0.405827</cell><cell>0.409842</cell><cell>0.402247</cell></row><row><cell>4</cell><cell>0.391321</cell><cell>0.352511</cell><cell>0.402949</cell></row><row><cell>5</cell><cell>0.501857</cell><cell>0.347734</cell><cell>0.544776</cell></row><row><cell>6</cell><cell>0.600473</cell><cell>0.465316</cell><cell>0.644614</cell></row><row><cell>7</cell><cell>0.649300</cell><cell>0.556033</cell><cell>0.673603</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A gated and bifurcated stacked u-net module for document image dewarping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasipuri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09824</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Docunet: document image unwarping via a stacked u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4700" to="4709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dewarpnet: Single-image document unwarping with stacked 3d and 2d regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shilkrot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="131" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric rectification of document images using adversarial gated unwarping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">107576</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.05740" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Document restoration using 3d shape: a general deskewing algorithm for arbitrarily warped documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Seales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV 2001</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active flattening of curved document images via two structured beams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3890" to="3897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Composition of a dewarped and enhanced document image from two view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1551" to="1562" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiview rectification of folded documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="505" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified framework for document restoration using inpainting and shape-from-shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Yip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2961" to="2978" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Restoring warped document images through 3d shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="208" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Goaloriented rectification of camera-based document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Stamatopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Perantonis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="910" to="920" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dewarping of document image by global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Document Analysis and Recognition (ICDAR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="302" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rectification and 3d reconstruction of curved document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segmentation based recovery of arbitrarily warped document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ntirogiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="989" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Method, apparatus, and computer-readable recording medium for converting document image captured by using camera to dewarped document image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-I</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Seo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="page">211</biblScope>
		</imprint>
	</monogr>
	<note>uS Patent 9</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel word spotting method based on recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Frinken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Text-line detection in camera-captured document images using the state estimation of connected components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5358" to="5368" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The common fold: utilizing the four-fold to dewarp printed documents from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sudharshana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shilkrot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Symposium on Document Engineering</title>
		<meeting>the 2017 ACM Symposium on Document Engineering</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="125" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno>abs/1807.03247</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<ptr target="http://arxiv.org/abs/1807.03247" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
