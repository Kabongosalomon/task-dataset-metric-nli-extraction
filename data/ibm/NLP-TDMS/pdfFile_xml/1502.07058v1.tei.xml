<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
							<email>aharley@scs.ryerson.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ufkes</surname></persName>
							<email>aufkes@scs.ryerson.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Many document types have a distinct visual style. For example, "letter" documents are typically written in a standard format, which is recognizable even at scales where the text is unreadable. Motivated by this observation, this paper addresses the problem of document classification and retrieval, based on the visual structure and layout of document images.</p><p>Content-based analysis of document images has a number of applications. In digital libraries, documents are often stored as images before they are processed by an optical character recognition (OCR) system, which means basic image analysis is the only available tool for initial indexing and classification <ref type="bibr" target="#b25">[26]</ref>. As a pre-processing stage, document image analysis can facilitate and improve OCR by providing information about each document's visual layout <ref type="bibr" target="#b9">[10]</ref>. Furthermore, document information that is lost in OCR, such as typeface, graphics, and layout, can only be stored and indexed using images or image descriptors. Therefore, image analysis is complementary to OCR at several stages of document analysis.</p><p>The challenge of document image analysis arises from the fact that within each document type, there exists a wide range of visual variability. For example, of the correspondence documents shown in <ref type="figure">Figure 1</ref>, no two documents share the exact same spatial arrangement of header, date, address, body, and signature; some of the documents even omit these components entirely. This level of intra-class variability renders spatial layout analysis difficult, and rigid template matching impossible <ref type="bibr" target="#b7">[8]</ref>. Another issue is that documents of different categories often have substantial visual similarities. For instance, there exist advertisements that look like news articles, and questionnaires that look like forms, and so on. From <ref type="figure">Fig. 1</ref>. Examples of document images that share the visual style of "letter". Note that even when the text of these documents is illegible, their style type is clear. The documents have similar spatial configurations of various parts: addresses and dates typically appear near the top, and signatures typically appear near the bottom, but no two documents share the exact same layout. Identifying the style type of these documents is therefore difficult, but can potentially facilitate the extraction of further information. the perspective of "visual styles", some erroneous retrievals in such circumstances may be justifiable, but in general the task of document image analysis is to effectively classify and retrieve documents despite intra-class variability, and interclass similarity.</p><p>Similar challenges appear in other fields, such as object recognition and scene classification. In those domains the current state-of-the-art approach involves training a deep convolutional neural network (CNN) to learn features for the task <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>. Inspired by the success of CNNs in other domains, this paper presents an extensive evaluation of CNNs for document classification and retrieval. In the end, it is determined that features extracted from deep CNNs exceed the performance of all popular alternative features on both classification and retrieval, by a large margin. Experiments are also presented on transfer learning, which demonstrate that CNNs trained on object recognition learn features that are surprisingly effective at describing documents. Furthermore, it is found that the deep net strategy is not significantly improved by additional guidance toward region-focused features, suggesting that a CNN trained on whole images may already be capable of learning some amount of the information that region-based analysis would add.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>In the past twenty years of document image analysis, research has oscillated from region-based analysis to whole image analysis, and simultaneously, from handcrafted features to machine-learned ones.</p><p>The power of region-based analysis of document images arXiv:1502.07058v1 [cs.CV] <ref type="bibr" target="#b24">25</ref> Feb 2015 has been clearly demonstrated in the domain of rigidly structured documents, such as forms and business letters <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. In general, this approach assumes that many document types have a distinct and consistent configuration of visually-identifiable components. For example, formal business letters typically share a particular spatial configuration of letterhead, date and salutation. To some extent, the classification of perfectly rigid documents (e.g., forms) can be reduced to the problem of template matching <ref type="bibr" target="#b6">[7]</ref>, and less-rigid document types (e.g., letters) can similarly be classified by fitting the geometric configuration of the document's components to one of several template configurations, via geometric transformations <ref type="bibr" target="#b14">[15]</ref>. The drawback of this approach is that it requires the manual definition of a template for each document type to be categorized. Furthermore, the approach is limited to documents for which a template definition is possible. For documents with more flexible structures, as considered herein, template-based approaches are inapplicable.</p><p>An alternative strategy is to treat document images holistically, or at least in very large regions, and search for discriminative "landmark" features that may appear anywhere in the document <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref>. This strategy is sometimes called a "bag of visual words" approach, since it describes images with a histogram over an orderless vocabulary of features <ref type="bibr" target="#b11">[12]</ref>. For example, a landmark feature discriminating letters from most other document classes is the salutation: finding a salutation in a document (potentially through OCR) is a good cue that the document is a letter, regardless of that feature's exact spatial position <ref type="bibr" target="#b31">[32]</ref>. The advantage of holistic analysis is that the resulting representation of documents is invariant to the geometric configuration of the features. This approach has therefore been successful in retrieving and classifying a broader range of documents than the templatebased approaches, although the approach is less discriminating in the domain of rigid-template documents.</p><p>Recently, there have been attempts to bridge the gap between region-based and holistic analyses. By concatenating image features pooled at several stages, beginning with a whole-image pool and proceeding into smaller and smaller regions, it is possible to build a descriptor that contains both global and local layout characteristics <ref type="bibr" target="#b22">[23]</ref>. This technique, known as spatial pyramid matching, was initially developed for categorizing scenes, but it has been shown to apply well to documents also, especially if the pooling regions are designed with document categorization in mind <ref type="bibr" target="#b21">[22]</ref>. For document retrieval, this type of representation represents the current state-of-the-art.</p><p>At the same time, many researchers have replaced handcrafted features and representations with machine-learned variants <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref>. A popular area of research in this domain concerns the task of learning document structure. This typically involves training a decision tree to navigate the various possible geometric configurations of fixed features (i.e. "landmarks") within each document type, toward the goal of structure-based classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. Most recently, it was shown that the entire pipeline of supervised document image classification, from feature-building to decision making, can be learned by a convolutional neural network (CNN) <ref type="bibr" target="#b16">[17]</ref>. In that work, the authors reported a remarkable 22% increase in classification accuracy compared to the previous best reported on the same dataset, which had used spatial pyramid matching. However, the CNN approach has not yet been applied to document retrieval.</p><p>A shift toward machine-learned features has been taking place in other areas of computer vision as well. In the object recognition literature, CNNs currently exceed the performance of every other approach by a very large margin <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>. The CNN approach has even been shown to apply well to domains for which it was traditionally believed ill-suited, such as attribute detection, and fine-grained object recognition <ref type="bibr" target="#b28">[29]</ref>. The success of CNNs in fine-grained object recognition is especially relevant to document image analysis, since the two fields share some significant challenges, e.g., (i) the items being distinguished are very similar to each other, and (ii) there do not exist problem-specific datasets large enough to train a powerful CNN without causing it to overfit. It makes sense, therefore, to draw inspiration from fine-grained object recognition research on how to overcome these challenges.</p><p>Two major points on the training and usage of CNNs can be gleaned from fine-grained classification research. First, before training the CNN on the data of interest, it is recommended to pre-train the network on a much larger related problem, such as the ILSVRC 2012 challenge <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6]</ref>. This regularization technique addresses the issue of overfitting, and allows large CNNs to be effectively applied to small problems. Second, in problems where spatial information is important, it is potentially better to encode this information in multiple networks trained on specific regions of interest than in a single network trained on the entire image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">33]</ref>. More generally, this second point suggests that it is unnecessary to rely entirely on machine learning, especially when human knowledge can be easily implemented in the system. This paper seeks to investigate whether these insights are relevant to document image analysis.</p><p>Finally, CNNs in other domains have recently been extended to the task of image retrieval. After a CNN is trained on classification, the layers of the network can be interpreted as forming a hierarchical chain of abstraction, where the lowest layers contain simple features, and the highest layers contain concise and descriptive representations <ref type="bibr" target="#b23">[24]</ref>. Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2]</ref>. The present work is the first to apply this idea toward document retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contributions</head><p>In the light of previous work, this paper makes the following contributions. First, the paper thoroughly evaluates the power of deep CNN features for representing document images. Toward this end, the paper presents experiments in CNN design, training, feature processing, and compression. Results show that features extracted from CNNs are superior to all handcrafted competitors, and furthermore can be compressed to very short codes with negligible loss in performance. Second, this work demonstrates that CNNs trained on nondocument images transfer well to document-related tasks. Third, this paper explores a strategy of embedding human knowledge of document structure into CNN architectures, by guiding an ensemble of CNNs toward learning region-specific features. Interestingly, results show little to no improvement in classification and retrieval after this augmentation, suggesting that a basic holistic CNN may be learning region-specific features (or perhaps better features) automatically. Finally, this work makes available a new labelled subset of the IIT-CDIP collection of tobacco litigation documents <ref type="bibr" target="#b24">[25]</ref>, containing 400,000 document images across 16 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. TECHNICAL APPROACH</head><p>In structured documents, the layout of text and graphics elements often reflects important information about genre. Therefore, documents of a category often share region-specific features. This paper attempts to learn these informative features by training either a single holistic CNN or an ensemble of region-based CNNs. Additionally, the paper explores two different initialization strategies: the first initializes the weights of the CNNs randomly, and relies entirely on the training process to find the features; the second transfers weights from a network trained on another task, and relies on training only to fine-tune the features to the domain of document analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Holistic convolutional neural networks</head><p>In most modern implementations of neural networks for computer vision, the network takes as input a square matrix of pixels as input, processes this input through a stack of convolutional layers, then classifies the output of those convolutional layers using two or three fully-connected layers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19]</ref>. A typical network of this type has approximately 60 million trainable parameters; this vast representational capacity, along with the hierarchical organization of that representation, is assumed to be responsible for the network's power as a featurebuilder and classifier <ref type="bibr" target="#b23">[24]</ref>.</p><p>Convolutional neural network activations are not geometrically invariant. In applications such as object detection, this is sometimes an inconvenient property. Much work has been done to add spatial invariance to CNNs, e.g., by "jittering" the training data to add geometric variants of each image in the dataset <ref type="bibr" target="#b23">[24]</ref>, or by altering the architecture of the CNN to process the input at multiple scales and positions <ref type="bibr" target="#b13">[14]</ref>. For document analysis, however, spatial specificity in CNN activations may be beneficial. For example, it makes sense to treat the header region of a document differently than the footer region. By design, a holistic CNN trained on a dataset of well-aligned document images should be capable of learning region-specific features automatically.</p><p>Typically, CNNs are trained to perform a classification task, but a CNN trained on classification can be exploited to perform retrieval also. It has been found that the activation patterns near the top of a deep CNN make very descriptive feature vectors <ref type="bibr" target="#b28">[29]</ref>. These feature vectors are high-dimensional (e.g., 4096 dimensions), but their dimensionality can be reduced significantly via principal component analysis (e.g., to 128 dimensions) without significantly affecting their discriminative power <ref type="bibr" target="#b2">[3]</ref>. Retrieval involves computing the Euclidean distance between a query descriptor and every descriptor of the training set. The sorted distances are then used to rank the images of the training data, and return a sorted list of documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Region-based guidance</head><p>Accounting for the possibility that a holistic CNN may not take advantage of region-specific information in document images, guiding CNNs to learn region-based features may aid fine-grained discrimination by isolating subtle region-specific appearance differences between document categories. Consider the example of discriminating letters and memos, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. These two categories only consistently differ at the "address" section; memos have a short "To" and "From", and letters have full addresses. It is possible that a holistic CNN will learn this automatically, but training a CNN to classify documents using only this region increases the likelihood that this feature will be learned. The idea of this approach is to devote one CNN to each region of interest, and therefore force multiple CNNs to learn rich region dependent representations, from which features can be extracted and combined.</p><p>Any number of region-specific CNNs can be used in this approach. In this work, a total of five CNNs are used. Four of these are region-tuned, placed at the header, left body, right body, and footer of the document images. The fifth is a holistic CNN, trained on the entire images. The final region-based representation of document images is built by combining and compressing features extracted from each region-tuned CNN. The final descriptor is represented by the concatenation of region specific features: [φ 0 , φ 1 , . . . , φ n ], where φ 0 represents the PCA-compressed feature vector extracted from the holistic CNN, and φ 1 , . . . , φ n represent the analogous vectors extracted from regions 1 through n. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the full process of this vector's construction. For retrieval, this new vector is used directly. For classification, a new fully-connected network is trained to classify the concatenated vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Transfer learning</head><p>The goal of transfer learning is to take advantage of shared structure in related problems, to facilitate learning on problems with little training data <ref type="bibr" target="#b0">[1]</ref>. In the context of CNNs, transfer learning can be implemented at the weight initialization step. The typical initialization strategy for CNNs is to set all weights to small random numbers, and set all biases to either 1 or 0 <ref type="bibr" target="#b23">[24]</ref>. An alternative strategy is to pretrain the network network on a complementary task, which potentially has more training data than the target task. This puts the network near a good solution in the target problem, and prevents it from descending into local minima early in the training process <ref type="bibr" target="#b28">[29]</ref>. A popular choice for pre-training is the ILSVRC 2012 ImageNet challenge, as it contains over a million training examples of natural images, categorized into 1000 object categories <ref type="bibr" target="#b29">[30]</ref>. Features extracted from an  ImageNet-trained network have been shown to be effective general-purpose features in a variety of other vision challenges, even without fine-tuning on the target problem <ref type="bibr" target="#b28">[29]</ref>.</p><p>This paper studies three questions about transfer learning for document analysis. First, the paper investigates whether the ImageNet features are general enough to be applied to documents. That is, with no fine-tuning on documents, are generic object-recognition features applicable to document analysis? Second, the paper addresses the question of whether the initialization provided by pre-training on the ILSVRC challenge provides better results than random initialization for documentclassifying CNNs. Third, the paper seeks to investigate the usefulness of transfer learning between document categories; if a CNN is trained with a small number of document categories, are the features learned in that process useful for discriminating between unseen document categories? These questions will be answered in the retrieval tasks to follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EMPIRICAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The performance of the various proposed approaches was evaluated on two versions of the IIT CDIP Test Collection <ref type="bibr" target="#b24">[25]</ref>. This collection contains high resolution images of scanned documents, collected from public records of lawsuits against American tobacco companies. In total, the database has over seven million documents, hand-labelled with tags. Often, the first tag of a document image is indicative of the document's category, but many documents in the dataset have missing or erroneous tags.</p><p>The first version of the dataset, listed in the results as SmallTobacco, is a sample of 3482 images from the collection, selected and labelled in another work <ref type="bibr" target="#b19">[20]</ref>. This version of the dataset was used in a number of related papers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17]</ref>. Each image has one of ten labels. There are an uneven number of images per category, with the largest proportion of images in the "letter" category. The distribution of categories is representative of the distribution present in the full dataset.</p><p>The second version of the dataset, listed in the results as BigTobacco, is a new random sample of 25000 images from each of 16 categories in the IIT CDIP collection, for a total of 400000 labelled images. This sample was collected specifically for the present paper. The 16 categories are "letter", "memo", "email", "filefolder", "form", "handwritten", "invoice", "advertisement", "budget", "news article", "presentation", "scientific publication", "questionnaire", "resume", "scientific report", and "specification". The selection of categories was guided by earlier work on document categorization <ref type="bibr" target="#b26">[27]</ref>, and also by the range of categories present in the already-existing SmallTobacco sample from the same collection. Another factor was the knowledge that CNNs do well with large datasets (e.g., over a million images) <ref type="bibr" target="#b18">[19]</ref>, so selection was restricted to categories that were well represented in the dataset. A representative sample of the dataset is shown in <ref type="figure">Figure 4</ref>. The final categories are not perfectly distinct: many images were originally labelled with multiple tags, which potentially covered several of the categories eventually selected; in this version of the dataset each image is labelled with a single category.</p><p>Each dataset was split into three subsets for the purposes of experimentation. The SmallTobacco dataset was split as in the related work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17]</ref>: 800 images were used for training, 200 for validation, and the remainder for testing. Since this is a small dataset, 10 random splits in those proportions were created; results reflect the median performance from those splits. In the case of retrieval, the median was selected based on mean average precision at the 10th retrieval (mAP@10). The BigTobacco dataset was split in proportions similar to those of ImageNet <ref type="bibr" target="#b29">[30]</ref>: 320000 images were used for training, 40000 images for validation, and 40000 images for testing. The validation sets were used to find plateaus in the CNN training process. All results are reported on the test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>The CNNs were implemented in Caffe <ref type="bibr" target="#b15">[16]</ref>. All networks computed an N -way softmax at the top layer, where N is the number of categories being learned.</p><p>All but two of the CNNs used Caffe's reference ImageNet architecture, which is based on the work of Krizhevsky et al. <ref type="bibr" target="#b18">[19]</ref>. This network has five convolutional layers, and three fully-connected layers. The network takes images of size 227 × 227. The full architecture can be written as</p><formula xml:id="formula_0">227 × 227 − 11 × 11 × 96 − 5 × 5 × 256 − 3 × 3 × 384 − 3 × 3 × 384 − 3 × 3 × 256 − 4096 − 4096 − N .</formula><p>Features were extracted from these CNNs by taking the output of the first fully-connected layer, which has 4096 dimensions.</p><p>The first network with a different architecture is listed in the results as "Small holistic CNN", which uses hyperparameters established in another work on document image analysis <ref type="bibr" target="#b16">[17]</ref>. This network has two convolutional layers and three fully-connected layers, with pooling, ReLU, and dropout employed at several stages in between. The network takes as input images of size 150 × 150. The full architecture can be written as 150×150−36×36×20−8×50−1000−1000−N . <ref type="figure">Fig. 4</ref>. Representative examples from each category of the dataset. For each category, three images are shown in a column. In order, the document classes shown are "letter", "memo", "email", "filefolder", "form", "handwritten", "invoice", "advertisement", "budget", "news article", "presentation", "scientific publication", "questionnaire", "resume", "scientific report", and "specification". Notice that although each category has certain distinctive features, there is wide variation within each category, and images from certain pairs of categories could easily be confused (e.g., "memo" and "letter").</p><p>As with the ImageNet networks, features were extracted from this network by taking the output of rhe first fully-connected network, which in this case has 1000 dimensions.</p><p>The second network with a different architecture is the "Ensemble of CNNs" network, which uses vectors extracted from the region-based CNNs to perform classification. Since a vector of length 4096 · 5 is too large to classify, the individual region-based vectors were compressed using principle component analysis (PCA) to 640 dimensions before they were concatenated for classification. The network architecture can be written as 3200 × 4096 × N . For retrieval, features for this approach were created by individually compressing each region's feature vector to 128 dimensions, and then concatenating, resulting in a vector with 640 dimensions.</p><p>To test the effect of transfer learning between categories of documents, one holistic CNN was trained using only two categories of the BigTobacco dataset: letters and memos. This network was pre-trained on ImageNet. In the results, it is listed as "LetterMemo CNN".</p><p>To extract regions from the images, all images were first resized to 780 × 600. The header region was defined by the first 256 rows of pixels in each image. The footer region was similarly defined by the last 256 rows of pixels in each image. The left body region was delineated by the intersection of the 400 central rows and the 300 left columns; the right body region was symmetrically defined. Every extracted region was resized to 227 × 227 before being used as input.</p><p>Several state-of-the-art bag of words (BoW) approaches to document representation were also implemented. As in previous work <ref type="bibr" target="#b21">[22]</ref>, the words were k-means clustered SURF features <ref type="bibr" target="#b3">[4]</ref>. These features were pooled in a spatial pyramid <ref type="bibr" target="#b22">[23]</ref>, as well as in various combinations of horizontal and vertical partitions <ref type="bibr" target="#b21">[22]</ref>. In the results, we denote these horizontalvertical partitioning schemes with HaVb, where a is the number of times the image was recursively split horizontally, and b is the number of times the image was recursively split vertically. For example, H0V3 has 15 bags: 1 for the original image, 2 for the first vertical split, 4 for the second vertical split, and 8 for the third. For the holistic bag of words, the resulting feature vector has 300 dimensions; H2V0 has 2100 dimensions; H0V3 has 4500 dimensions; H2V3 and L3 both have 6300 dimensions. For classification of the BoW features, a random forest with 500 trees and √ D feature dimensions was trained, where D was the length of the feature vector of the complete (concatenated) bag of words.</p><p>Three additional features were added as baselines to the featured approaches: the GIST descriptor <ref type="bibr" target="#b27">[28]</ref>, average brightness, and ensemble-of-regions average brightness. The GIST descriptor has been shown to perform well on image retrieval tasks <ref type="bibr" target="#b10">[11]</ref>, but has not yet been applied to document analysis. Average brightness acts as a baseline for minimum performance; images in this representation are represented with a single value. Ensemble-of-regions average brightness represents document images a vector of five elements, corresponding to the average brightness in each of the regions created for the ensemble of CNNs approach. This is intended to demonstrate on a small scale the basic benefit afforded by region-based analysis.</p><p>Retrieval was performed by computing the Euclidean distance between the test set descriptors and every descriptor of the training set. The sorted distances were then used to rank the images of the training data, and return a sorted list of documents for each test query. For all approaches with feature vectors larger than 128 dimensions, the vectors were first compressed to 128 dimensions using PCA before they were used for retrieval. This is consistent with the related work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14]</ref>; it not only enables fast retrieval, but also to keeps the task within reasonable memory limits. As in the related work, the feature vectors were L2-normalized before and after PCA compression. <ref type="table" target="#tab_0">Table I</ref> shows the classification accuracies of the various BoW approaches, along the various CNNs-based appraoches, on both the SmallTobacco dataset and the BigTobacco dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification results</head><p>On SmallTobacco, the ensemble of region-based CNNs performed better than any other approach, achieving 79.9% classification accuracy. The previous best reported result on this dataset was 65.4% with a randomly initialized "Small" CNN, which was approximately replicated here. The holistic network performed only slightly worse than the ensemble of CNNs, suggesting that the holistic CNN may be learning some amount of the information that region-based analysis was expected to add. Interestingly, the "Small" CNN compares similarly to the large-sized holistic CNN when both are initialized with random weights. This appears to indicate that the additional parameters in the large network are not necessarily beneficial. Initializing the larger networks with ImageNettrained weights improves performance substantially. Without this initialization, the CNNs perform similarly to (or worse than) the BoW approaches. Between the BoW approaches, the spatial-pyramid-pooled BoW performs best.</p><p>On BigTobacco, the holistic CNN finetuned from Imagenet performed better than any other approach, including the ensemble of CNNs. This suggests that given sufficient training data, the advantage gained by region-tuned analysis is eliminated by the learning power of the holistic CNN. In these results, the CNN approaches perform far better than the BoW approaches, likely due to the benefit of additional training data. As observed in SmallTobacco, finetuning improves results, although by a smaller margin here than in the small dataset. Comparing the performance of BoW approaches between the two datasets, it is interesting to observe that performance drops by nearly 20%, suggesting that (i) the larger dataset presents a more difficult classification task (likely because it has more categories), and perhaps also (ii) the additional training data does not help these approaches. The confusion matrix for the holistic CNN is shown in <ref type="figure" target="#fig_3">Figure 5</ref>.</p><p>The CNN trained to classify only letters and memos achieved 95% accuracy on that task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Retrieval results</head><p>Retrieval was measured using mean average precision (mAP). Average precision computes the average value of precision as a function of recall on some interval. Formally, the discrete version of this metric is given by</p><formula xml:id="formula_1">AP = n k=1 (P (k) × rel(k)) number of relevant documents ,<label>(1)</label></formula><p>where k is the rank of the document being retrieved, and rel(k) equals 1 if the document is relevant and 0 otherwise. This metric is sensitive to ranking order, so the score is higher if relevant documents are retrieved before irrelevant documents. Mean average precision is simply the average precision summed over all queries, divided by the number of queries. Retrieved documents were determined to be "relevant" if they had the same class label as the query image. Mean average precision for the first 10 retrievals on both datasets are summarized in <ref type="figure" target="#fig_5">Figure 6</ref>.</p><p>On the SmallTobacco dataset, the ensemble of region-tuned CNNs performs best, followed by a holistic CNN fine-tuned from ImageNet. Interestingly, the generic ImageNet descriptor performs well also, exceeding the performance of most other descriptors. Between the BoW approaches, the spatialpyramid-pooled BoW performs best. The GIST descriptor performs approximately as well as the BoW approaches.</p><p>On the BigTobacco dataset, the holistic CNN performs best, exceeding the ensemble of region-tuned CNNs by a small margin, but exceeding most other approaches by a large margin. The confusion matrix for the finetuned holistic CNN, computed using the first 10 retrievals, is shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The BoW approaches are outperformed by every CNN vector, including the generic ImageNet vector. The "LetterMemo" CNN slightly improves upon the generic ImageNet descriptor, suggesting that some of the knowledge learned from letters and memos transfers to all 16 categories, but the gain is only marginal. Between the BoW approaches, the spatial-pyramidpooled BoW performs best, as in SmallTobacco. Interestingly, the GIST descriptor exceeds the performance of the BoW descriptors by a large margin on this dataset. <ref type="figure">Figure 8</ref> shows a representative sample of the retrieval output of the holistic CNN on the BigTobacco dataset. In that figure, it is interesting to notice that in the first row, in which the query image is a memo, the top seven retrievals are all different memos from the same author (with the same signature) as the memo in the query image. The final row is similarly impressive: every document in the top ten retrievals has the same letterhead as the query document, despite variations in the other content, and also despite differing typefaces of the letterhead. There may exist biases in the dataset that lead to such fortunate retrievals (e.g., only a few letterheads, and only a few memo authors), but the results are still remarkable.</p><p>An additional experiment was performed to measure the effect of PCA compression on mAP@10 performance on the BigTobacco dataset, the results of which are summarized in <ref type="figure">Figure 7</ref>. Remarkably, the CNN vectors show almost no loss in performance until they are reduced to 16 dimensions. At all levels of compression, the holistic CNN performs exceeds the performance of every other approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>This paper established a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). Generic features extracted from a CNN trained on ImageNet exceeded the performance of the state-of-the-art alternatives, and fine-tuning these features on document images pushed results even higher. Interestingly, experiments also showed that given sufficient training data, enforcing region-specific feature-learning is unnecessary; a single CNN trained on entire images performed approximately as well as an ensemble of CNNs trained on specific subregions of document images. In all, this work showed that the CNN approach to document image representation exceeds the power of hand-crafted alternatives.      <ref type="figure">Fig. 7</ref>. The effect of PCA reduction on mean average precision at the 10th retrieval (mAP@10). The holistic CNN achieves the highest mAP at all levels of PCA reduction, with remarkably little loss across the first several steps of reduction. to A.W.H.). The authors thank Palomino Systems for helpful discussions. The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of a Tesla K40 GPU used for this research. <ref type="figure">Fig. 8</ref>. Representative output of the retrieval process. This figure is best viewed on a computer monitor, in a zoomable PDF. Query images are shown in the first column, and the top ten retrievals are shown in the following columns in order. Retrievals from the same class are shown with a green border; retrievals from a different class are shown with a red border. Retrievals from other classes are considered incorrect, but they are often good retrievals nonetheless.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Some document types differ only at specific regions. The letter (left) and memo (right) only differ at the address section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Construction of the region-based representation, delineated in three steps. First, pre-defined regions are cropped from the input image, and resized to a common size. Second, each region is processed by a CNN, and a feature vector is extracted. Third, the feature vectors are compressed by PCA and concatenated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Confusion matrices for classification performance (left) and retrieval performance (right) of the holistic CNN. Darkness of the off-diagonal cells was adjusted for better visibility. Cells with values greater than 0.05 are annotated with their actual values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Mean average precision at retrievals 1 through 10 for a variety of approaches on the SmallTobacco dataset (left) and the BigTobacco dataset (right). In each legend, the approaches are sorted in descending order according to their mAP@5 in the corresponding graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>CLASSIFICATION ACCURACIES</figDesc><table><row><cell>Approach</cell><cell>SmallTobacco</cell><cell>BigTobaccco</cell></row><row><cell>Holistic BoW</cell><cell>.645</cell><cell>.446</cell></row><row><cell>H0V3 BoW</cell><cell>.679</cell><cell>.483</cell></row><row><cell>H2V0 BoW</cell><cell>.652</cell><cell>.461</cell></row><row><cell>H2V3 BoW</cell><cell>.681</cell><cell>.493</cell></row><row><cell>Pyramid BoW</cell><cell>.687</cell><cell>.491</cell></row><row><cell>Small holistic CNN (random init.)</cell><cell>.643</cell><cell>.851</cell></row><row><cell>Header CNN</cell><cell>.710</cell><cell>.849</cell></row><row><cell>Left body CNN</cell><cell>.667</cell><cell>.827</cell></row><row><cell>Right body CNN</cell><cell>.708</cell><cell>.795</cell></row><row><cell>Footer CNN</cell><cell>.622</cell><cell>.794</cell></row><row><cell>Holistic CNN</cell><cell>.756</cell><cell>.898</cell></row><row><cell>Holistic CNN (random init.)</cell><cell>.634</cell><cell>.878</cell></row><row><cell>Ensemble of CNNs</cell><cell>.799</cell><cell>.893</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by NSERC Discovery and Engage grants (held by K.G.D.), and an NSERC USRA (awarded</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uncovering shared structures in multiclass classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">From generic to specific deep representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno>1406.5774</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<title level="m">Neural codes for image retrieval. ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8689</biblScope>
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SURF: Speeded Up Robust Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<editor>A. Leonardis, H. Bischof, and A. Pinz</editor>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3951</biblScope>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>1407.0717</idno>
		<title level="m">Deep poselets for human detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Form classification using DP matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the</title>
		<meeting>of the</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of document image classification: Problem statement, classifier architecture and performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDAR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A clustering-based algorithm for automatic document separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nickolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clustering and classification of document structure-a machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dubiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="587" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation of GIST descriptors for web-scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sandhawalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Int. Conf. on Image and Video Retrieval, CIVR &apos;09</title>
		<meeting>the ACM Int. Conf. on Image and Video Retrieval, CIVR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page" from="392" to="407" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparison and classification of documents based on layout similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilfong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="227" to="243" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for document image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">User-defined template for identifying document type and extracting information from documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kochi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saitoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="127" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised classification of structurally similar document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1225" to="1229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning document structure for retrieval and classication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1558" to="1561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structural similarity for document image classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">119</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PIEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building a test collection for complex document information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="665" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Digital libraries and document image retrieval techniques: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marinai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Structure and Schemas from Documents</title>
		<editor>M. Biba and F. Xhafa</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="page" from="181" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Twenty years of document image analysis in PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nagy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="38" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Classification of document pages using structure-based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">IJDAR</biblScope>
			<biblScope unit="page" from="232" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Classification and functional decomposition of business documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lipshutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Nilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PANDA: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
